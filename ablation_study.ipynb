{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I. Setup",
   "id": "9c24b66f196a7efc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After implemented from scratch multiclass logistic regression and after tested it we can now do next steps to increase efficiency, effectiveness and upgrade this base model.",
   "id": "f8a9643052e719d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here it is abstract class of Logistic regression with empty fit function, we need to implement this fit function by yourself, because in next steps we will be adding there some upgrades and we need different class to compare it later",
   "id": "42ebd9296c49367d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T09:08:57.791546Z",
     "start_time": "2025-05-25T09:08:57.699815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "from scipy.odr import polynomial\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "id": "b18a0e472c31bce8",
   "outputs": [],
   "execution_count": 332
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:20:38.529065Z",
     "start_time": "2025-05-23T09:20:38.500208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading data\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df.head()\n",
    "\n",
    "numerical_features = [\n",
    "    \"Application order\",\"Age at enrollment\", \"Curricular units 1st sem (credited)\", \"Curricular units 1st sem (enrolled)\",\n",
    "    \"Curricular units 1st sem (evaluations)\",\"Curricular units 1st sem (approved)\",\"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 1st sem (without evaluations)\",\"Curricular units 2nd sem (credited)\",\"Curricular units 2nd sem (enrolled)\",\n",
    "    \"Curricular units 2nd sem (evaluations)\",\"Curricular units 2nd sem (approved)\",\n",
    "    \"Curricular units 2nd sem (without evaluations)\",\"Unemployment rate\",\"Inflation rate\",\"GDP\",\"Curricular units 2nd sem (grade)\"\n",
    "]\n",
    "categorical_features = [\n",
    "    \"Marital status\",\"Application mode\",\"Course\",\"Daytime/evening attendance\",\"Previous qualification\",\"Nationality\",\n",
    "    \"Mother's qualification\",\"Father's qualification\",\"Mother's occupation\",\"Father's occupation\",\"Displaced\",\n",
    "    \"Educational special needs\",\"Debtor\",\"Tuition fees up to date\",\"Gender\",\"Scholarship holder\",\"International\"\n",
    "]\n",
    "\n",
    "target = \"Target\"\n",
    "\n",
    "preprocessor_full_set = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore',sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "def transform_in_pipeline(preproc,X_ttrain,X_ttest,y_ttrain,y_ttest):\n",
    "    return  (preproc.fit_transform(X_ttrain), preproc.transform(X_ttest),\n",
    "             pd.get_dummies(y_ttrain,columns=[\"Target\"]).astype(int).to_numpy(), pd.get_dummies(y_ttest,columns=[\"Target\"]).astype(int).to_numpy())\n"
   ],
   "id": "2998b9f60438c5cd",
   "outputs": [],
   "execution_count": 310
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:20:45.100366Z",
     "start_time": "2025-05-23T09:20:45.086792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# HERE Y has to be in onehotencoder [0 0 ...  1]\n",
    "X = df.drop([\"Target\"], axis=1)\n",
    "y = df[\"Target\"]\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=42)\n"
   ],
   "id": "2c6d179542de2ed6",
   "outputs": [],
   "execution_count": 312
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:10:05.313240Z",
     "start_time": "2025-05-22T18:10:05.289503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class LogisticRegression(ABC):\n",
    "    def __init__(self, lr=0.001, n_iters=5000, batch_size=64):\n",
    "        self.lr = lr\n",
    "        self.batch_idx = 0\n",
    "        self.n_iters = n_iters\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors = []\n",
    "        self.n_classes = None\n",
    "        self.probabilities = None\n",
    "    def error_function(self, y, predicted, m_samples):\n",
    "        return -np.sum(y * np.log(predicted + 1e-15)) / m_samples\n",
    "\n",
    "    def softmax(self,z):\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=1,keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.softmax(z)\n",
    "\n",
    "    def predict_class(self, X_test):\n",
    "        probabilities = self.predict(X_test)\n",
    "        self.probabilities = probabilities\n",
    "        return np.argmax(probabilities, axis=1).reshape(-1, 1)\n",
    "\n",
    "    def score(self, predicted, y):\n",
    "        y_true = np.argmax(y, axis=1)\n",
    "        correct = (predicted.flatten() == y_true)\n",
    "        accuracy = np.mean(correct)\n",
    "        return accuracy\n",
    "\n",
    "    def raw_error_data(self):\n",
    "        return np.arange(len(self.errors)), self.errors\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X, y):\n",
    "       ..."
   ],
   "id": "15d0c9d4380ea00b",
   "outputs": [],
   "execution_count": 236
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First we start with base multiclass logistic regression without any specific upgrades",
   "id": "f2357cf1d80c59e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:10:07.671046Z",
     "start_time": "2025-05-22T18:10:07.647953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaseLogisticRegression(LogisticRegression):\n",
    "    def fit(self,X,y):\n",
    "        m_samples, n_features = X.shape\n",
    "        self.n_classes = y.shape[1]\n",
    "        self.weights = np.zeros((n_features, self.n_classes))\n",
    "        self.bias = np.zeros((1, self.n_classes))\n",
    "        self.errors.clear()\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            indices = np.random.permutation(m_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for start_idx in range(0, m_samples, self.batch_size):\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                predict = self.predict(X_batch)\n",
    "                dw = (1 / X_batch.shape[0]) * np.dot(X_batch.T, (predict - y_batch))\n",
    "                db = (1 / X_batch.shape[0]) * np.sum(predict - y_batch, axis=0, keepdims=True)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "            full_predict = self.predict(X)\n",
    "            loss = self.error_function(y, full_predict, m_samples)\n",
    "            self.errors.append(loss)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "\n"
   ],
   "id": "9ce758dbc9c6b97f",
   "outputs": [],
   "execution_count": 237
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can train and predict results on these subsets, which were made with random_state=$42$",
   "id": "c3ded0e5c97c9396"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:42:20.611178Z",
     "start_time": "2025-05-21T16:42:20.574023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_proc,X_test_proc,y_train_proc,y_test_proc = transform_in_pipeline(preprocessor_full_set,X_train, X_test, y_train, y_test)\n",
    "\n",
    "logistic_regression1 = BaseLogisticRegression(n_iters=5000, lr=0.001, batch_size=64)"
   ],
   "id": "4d6cc1f7fe791b65",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:42:56.412616Z",
     "start_time": "2025-05-21T16:42:20.759694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logistic_regression1.fit(X_train_proc, y_train_proc)\n",
    "predicted = logistic_regression1.predict_class(X_test_proc)\n",
    "\n",
    "logistic_regression1.score(predicted, y_test_proc)"
   ],
   "id": "a341ca1843e6e7ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0448\n",
      "Iteration 100, Loss: 0.6512\n",
      "Iteration 200, Loss: 0.6082\n",
      "Iteration 300, Loss: 0.5876\n",
      "Iteration 400, Loss: 0.5747\n",
      "Iteration 500, Loss: 0.5655\n",
      "Iteration 600, Loss: 0.5584\n",
      "Iteration 700, Loss: 0.5527\n",
      "Iteration 800, Loss: 0.5480\n",
      "Iteration 900, Loss: 0.5440\n",
      "Iteration 1000, Loss: 0.5406\n",
      "Iteration 1100, Loss: 0.5376\n",
      "Iteration 1200, Loss: 0.5349\n",
      "Iteration 1300, Loss: 0.5325\n",
      "Iteration 1400, Loss: 0.5303\n",
      "Iteration 1500, Loss: 0.5284\n",
      "Iteration 1600, Loss: 0.5266\n",
      "Iteration 1700, Loss: 0.5250\n",
      "Iteration 1800, Loss: 0.5235\n",
      "Iteration 1900, Loss: 0.5221\n",
      "Iteration 2000, Loss: 0.5209\n",
      "Iteration 2100, Loss: 0.5197\n",
      "Iteration 2200, Loss: 0.5186\n",
      "Iteration 2300, Loss: 0.5175\n",
      "Iteration 2400, Loss: 0.5166\n",
      "Iteration 2500, Loss: 0.5157\n",
      "Iteration 2600, Loss: 0.5148\n",
      "Iteration 2700, Loss: 0.5140\n",
      "Iteration 2800, Loss: 0.5132\n",
      "Iteration 2900, Loss: 0.5125\n",
      "Iteration 3000, Loss: 0.5118\n",
      "Iteration 3100, Loss: 0.5111\n",
      "Iteration 3200, Loss: 0.5105\n",
      "Iteration 3300, Loss: 0.5099\n",
      "Iteration 3400, Loss: 0.5093\n",
      "Iteration 3500, Loss: 0.5088\n",
      "Iteration 3600, Loss: 0.5083\n",
      "Iteration 3700, Loss: 0.5078\n",
      "Iteration 3800, Loss: 0.5073\n",
      "Iteration 3900, Loss: 0.5068\n",
      "Iteration 4000, Loss: 0.5064\n",
      "Iteration 4100, Loss: 0.5060\n",
      "Iteration 4200, Loss: 0.5055\n",
      "Iteration 4300, Loss: 0.5051\n",
      "Iteration 4400, Loss: 0.5047\n",
      "Iteration 4500, Loss: 0.5044\n",
      "Iteration 4600, Loss: 0.5040\n",
      "Iteration 4700, Loss: 0.5037\n",
      "Iteration 4800, Loss: 0.5033\n",
      "Iteration 4900, Loss: 0.5030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.766566265060241)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "But now we can change this random_state variable to show that is random to generate a subsets,and based on this random we can get bad or worse score",
   "id": "987be98625793d9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:42:56.499385Z",
     "start_time": "2025-05-21T16:42:56.443441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train2,X_test2,y_train2,y_test2 = train_test_split(X,y,train_size=0.7,random_state=11)\n",
    "X_train_proc2,X_test_proc2,y_train_proc2,y_test_proc2 = transform_in_pipeline(preprocessor_full_set,X_train2, X_test2, y_train2, y_test2)\n",
    "\n",
    "logistic_regression2 = BaseLogisticRegression(n_iters=5000, lr=0.001, batch_size=64)\n"
   ],
   "id": "b5aa6c501825c4b1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:43:46.152052Z",
     "start_time": "2025-05-21T16:42:56.598538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logistic_regression2.fit(X_train_proc2, y_train_proc2)\n",
    "predicted2 = logistic_regression2.predict_class(X_test_proc2)\n",
    "logistic_regression2.score(predicted2, y_test_proc2)"
   ],
   "id": "5bf000f7cddd6189",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0468\n",
      "Iteration 100, Loss: 0.6528\n",
      "Iteration 200, Loss: 0.6092\n",
      "Iteration 300, Loss: 0.5881\n",
      "Iteration 400, Loss: 0.5749\n",
      "Iteration 500, Loss: 0.5653\n",
      "Iteration 600, Loss: 0.5580\n",
      "Iteration 700, Loss: 0.5520\n",
      "Iteration 800, Loss: 0.5471\n",
      "Iteration 900, Loss: 0.5429\n",
      "Iteration 1000, Loss: 0.5393\n",
      "Iteration 1100, Loss: 0.5362\n",
      "Iteration 1200, Loss: 0.5334\n",
      "Iteration 1300, Loss: 0.5310\n",
      "Iteration 1400, Loss: 0.5287\n",
      "Iteration 1500, Loss: 0.5267\n",
      "Iteration 1600, Loss: 0.5249\n",
      "Iteration 1700, Loss: 0.5233\n",
      "Iteration 1800, Loss: 0.5218\n",
      "Iteration 1900, Loss: 0.5204\n",
      "Iteration 2000, Loss: 0.5191\n",
      "Iteration 2100, Loss: 0.5179\n",
      "Iteration 2200, Loss: 0.5168\n",
      "Iteration 2300, Loss: 0.5157\n",
      "Iteration 2400, Loss: 0.5147\n",
      "Iteration 2500, Loss: 0.5138\n",
      "Iteration 2600, Loss: 0.5130\n",
      "Iteration 2700, Loss: 0.5122\n",
      "Iteration 2800, Loss: 0.5114\n",
      "Iteration 2900, Loss: 0.5107\n",
      "Iteration 3000, Loss: 0.5100\n",
      "Iteration 3100, Loss: 0.5093\n",
      "Iteration 3200, Loss: 0.5087\n",
      "Iteration 3300, Loss: 0.5081\n",
      "Iteration 3400, Loss: 0.5075\n",
      "Iteration 3500, Loss: 0.5070\n",
      "Iteration 3600, Loss: 0.5065\n",
      "Iteration 3700, Loss: 0.5060\n",
      "Iteration 3800, Loss: 0.5055\n",
      "Iteration 3900, Loss: 0.5051\n",
      "Iteration 4000, Loss: 0.5046\n",
      "Iteration 4100, Loss: 0.5042\n",
      "Iteration 4200, Loss: 0.5038\n",
      "Iteration 4300, Loss: 0.5034\n",
      "Iteration 4400, Loss: 0.5030\n",
      "Iteration 4500, Loss: 0.5026\n",
      "Iteration 4600, Loss: 0.5023\n",
      "Iteration 4700, Loss: 0.5019\n",
      "Iteration 4800, Loss: 0.5016\n",
      "Iteration 4900, Loss: 0.5013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.7710843373493976)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\\begin{align*}\n",
    "\\text{accuracy}(\\text{random\\_state} = 42) &= 0.7665 \\\\\n",
    "\\text{accuracy}(\\text{random\\_state} = 11) &= 0.7710\n",
    "\\end{align*}\n",
    "\n",
    "And we can see that is real random, so we have to find way to do this more independent of random_state\n"
   ],
   "id": "3365c359ef789587"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "II. Cross validation (StratifiedKFold)",
   "id": "dc26628d7a654471"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:43:46.399927Z",
     "start_time": "2025-05-21T16:43:46.393254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def make_cross_validation_predict(model,preproc, X_set, y_set, k=3):\n",
    "    scores = []\n",
    "    skf = StratifiedKFold(k, shuffle=True)\n",
    "\n",
    "    y_encoded = LabelEncoder().fit_transform(y_set)\n",
    "\n",
    "    for train_index, test_index in skf.split(X_set, y_encoded):\n",
    "        X_train_raw = X_set.iloc[train_index]\n",
    "        X_test_raw = X_set.iloc[test_index]\n",
    "\n",
    "        y_train_raw = y_set.iloc[train_index]\n",
    "        y_test_raw = y_set.iloc[test_index]\n",
    "\n",
    "        X_train = preproc.fit_transform(X_train_raw)\n",
    "        X_test = preproc.transform(X_test_raw)\n",
    "\n",
    "        y_train = pd.get_dummies(y_train_raw).astype(int).to_numpy()\n",
    "        y_test = pd.get_dummies(y_test_raw).astype(int).to_numpy()\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted = model.predict_class(X_test)\n",
    "        score = model.score(predicted, y_test)\n",
    "        scores.append(score)\n",
    "\n",
    "\n",
    "    return scores, np.mean(scores)\n"
   ],
   "id": "717707594501e66f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:44:46.494065Z",
     "start_time": "2025-05-21T16:43:46.424813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logistic_regression3 = BaseLogisticRegression(n_iters=5000, lr=0.001, batch_size=64)\n",
    "\n",
    "make_cross_validation_predict(logistic_regression3, preprocessor_full_set, X_train, y_train)"
   ],
   "id": "b1ba76db41058af0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0605\n",
      "Iteration 100, Loss: 0.6762\n",
      "Iteration 200, Loss: 0.6259\n",
      "Iteration 300, Loss: 0.6009\n",
      "Iteration 400, Loss: 0.5853\n",
      "Iteration 500, Loss: 0.5742\n",
      "Iteration 600, Loss: 0.5658\n",
      "Iteration 700, Loss: 0.5591\n",
      "Iteration 800, Loss: 0.5536\n",
      "Iteration 900, Loss: 0.5489\n",
      "Iteration 1000, Loss: 0.5448\n",
      "Iteration 1100, Loss: 0.5412\n",
      "Iteration 1200, Loss: 0.5380\n",
      "Iteration 1300, Loss: 0.5351\n",
      "Iteration 1400, Loss: 0.5325\n",
      "Iteration 1500, Loss: 0.5302\n",
      "Iteration 1600, Loss: 0.5280\n",
      "Iteration 1700, Loss: 0.5260\n",
      "Iteration 1800, Loss: 0.5241\n",
      "Iteration 1900, Loss: 0.5224\n",
      "Iteration 2000, Loss: 0.5208\n",
      "Iteration 2100, Loss: 0.5193\n",
      "Iteration 2200, Loss: 0.5179\n",
      "Iteration 2300, Loss: 0.5165\n",
      "Iteration 2400, Loss: 0.5153\n",
      "Iteration 2500, Loss: 0.5141\n",
      "Iteration 2600, Loss: 0.5130\n",
      "Iteration 2700, Loss: 0.5119\n",
      "Iteration 2800, Loss: 0.5109\n",
      "Iteration 2900, Loss: 0.5100\n",
      "Iteration 3000, Loss: 0.5091\n",
      "Iteration 3100, Loss: 0.5082\n",
      "Iteration 3200, Loss: 0.5074\n",
      "Iteration 3300, Loss: 0.5066\n",
      "Iteration 3400, Loss: 0.5058\n",
      "Iteration 3500, Loss: 0.5051\n",
      "Iteration 3600, Loss: 0.5044\n",
      "Iteration 3700, Loss: 0.5038\n",
      "Iteration 3800, Loss: 0.5031\n",
      "Iteration 3900, Loss: 0.5025\n",
      "Iteration 4000, Loss: 0.5019\n",
      "Iteration 4100, Loss: 0.5013\n",
      "Iteration 4200, Loss: 0.5008\n",
      "Iteration 4300, Loss: 0.5002\n",
      "Iteration 4400, Loss: 0.4997\n",
      "Iteration 4500, Loss: 0.4992\n",
      "Iteration 4600, Loss: 0.4987\n",
      "Iteration 4700, Loss: 0.4983\n",
      "Iteration 4800, Loss: 0.4978\n",
      "Iteration 4900, Loss: 0.4974\n",
      "Iteration 0, Loss: 1.0619\n",
      "Iteration 100, Loss: 0.6826\n",
      "Iteration 200, Loss: 0.6319\n",
      "Iteration 300, Loss: 0.6065\n",
      "Iteration 400, Loss: 0.5905\n",
      "Iteration 500, Loss: 0.5791\n",
      "Iteration 600, Loss: 0.5704\n",
      "Iteration 700, Loss: 0.5635\n",
      "Iteration 800, Loss: 0.5577\n",
      "Iteration 900, Loss: 0.5528\n",
      "Iteration 1000, Loss: 0.5486\n",
      "Iteration 1100, Loss: 0.5449\n",
      "Iteration 1200, Loss: 0.5417\n",
      "Iteration 1300, Loss: 0.5387\n",
      "Iteration 1400, Loss: 0.5360\n",
      "Iteration 1500, Loss: 0.5336\n",
      "Iteration 1600, Loss: 0.5314\n",
      "Iteration 1700, Loss: 0.5293\n",
      "Iteration 1800, Loss: 0.5275\n",
      "Iteration 1900, Loss: 0.5257\n",
      "Iteration 2000, Loss: 0.5241\n",
      "Iteration 2100, Loss: 0.5225\n",
      "Iteration 2200, Loss: 0.5211\n",
      "Iteration 2300, Loss: 0.5198\n",
      "Iteration 2400, Loss: 0.5185\n",
      "Iteration 2500, Loss: 0.5173\n",
      "Iteration 2600, Loss: 0.5162\n",
      "Iteration 2700, Loss: 0.5151\n",
      "Iteration 2800, Loss: 0.5141\n",
      "Iteration 2900, Loss: 0.5131\n",
      "Iteration 3000, Loss: 0.5122\n",
      "Iteration 3100, Loss: 0.5113\n",
      "Iteration 3200, Loss: 0.5104\n",
      "Iteration 3300, Loss: 0.5096\n",
      "Iteration 3400, Loss: 0.5088\n",
      "Iteration 3500, Loss: 0.5081\n",
      "Iteration 3600, Loss: 0.5074\n",
      "Iteration 3700, Loss: 0.5067\n",
      "Iteration 3800, Loss: 0.5060\n",
      "Iteration 3900, Loss: 0.5054\n",
      "Iteration 4000, Loss: 0.5048\n",
      "Iteration 4100, Loss: 0.5042\n",
      "Iteration 4200, Loss: 0.5036\n",
      "Iteration 4300, Loss: 0.5030\n",
      "Iteration 4400, Loss: 0.5025\n",
      "Iteration 4500, Loss: 0.5020\n",
      "Iteration 4600, Loss: 0.5015\n",
      "Iteration 4700, Loss: 0.5010\n",
      "Iteration 4800, Loss: 0.5005\n",
      "Iteration 4900, Loss: 0.5000\n",
      "Iteration 0, Loss: 1.0600\n",
      "Iteration 100, Loss: 0.6736\n",
      "Iteration 200, Loss: 0.6255\n",
      "Iteration 300, Loss: 0.6017\n",
      "Iteration 400, Loss: 0.5866\n",
      "Iteration 500, Loss: 0.5757\n",
      "Iteration 600, Loss: 0.5673\n",
      "Iteration 700, Loss: 0.5604\n",
      "Iteration 800, Loss: 0.5546\n",
      "Iteration 900, Loss: 0.5496\n",
      "Iteration 1000, Loss: 0.5452\n",
      "Iteration 1100, Loss: 0.5414\n",
      "Iteration 1200, Loss: 0.5379\n",
      "Iteration 1300, Loss: 0.5348\n",
      "Iteration 1400, Loss: 0.5319\n",
      "Iteration 1500, Loss: 0.5293\n",
      "Iteration 1600, Loss: 0.5270\n",
      "Iteration 1700, Loss: 0.5247\n",
      "Iteration 1800, Loss: 0.5227\n",
      "Iteration 1900, Loss: 0.5208\n",
      "Iteration 2000, Loss: 0.5190\n",
      "Iteration 2100, Loss: 0.5174\n",
      "Iteration 2200, Loss: 0.5158\n",
      "Iteration 2300, Loss: 0.5143\n",
      "Iteration 2400, Loss: 0.5130\n",
      "Iteration 2500, Loss: 0.5117\n",
      "Iteration 2600, Loss: 0.5104\n",
      "Iteration 2700, Loss: 0.5093\n",
      "Iteration 2800, Loss: 0.5081\n",
      "Iteration 2900, Loss: 0.5071\n",
      "Iteration 3000, Loss: 0.5061\n",
      "Iteration 3100, Loss: 0.5051\n",
      "Iteration 3200, Loss: 0.5042\n",
      "Iteration 3300, Loss: 0.5033\n",
      "Iteration 3400, Loss: 0.5024\n",
      "Iteration 3500, Loss: 0.5016\n",
      "Iteration 3600, Loss: 0.5008\n",
      "Iteration 3700, Loss: 0.5001\n",
      "Iteration 3800, Loss: 0.4994\n",
      "Iteration 3900, Loss: 0.4987\n",
      "Iteration 4000, Loss: 0.4980\n",
      "Iteration 4100, Loss: 0.4973\n",
      "Iteration 4200, Loss: 0.4967\n",
      "Iteration 4300, Loss: 0.4961\n",
      "Iteration 4400, Loss: 0.4955\n",
      "Iteration 4500, Loss: 0.4949\n",
      "Iteration 4600, Loss: 0.4944\n",
      "Iteration 4700, Loss: 0.4938\n",
      "Iteration 4800, Loss: 0.4933\n",
      "Iteration 4900, Loss: 0.4928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([np.float64(0.7761627906976745),\n",
       "  np.float64(0.7722868217054264),\n",
       "  np.float64(0.7664728682170543)],\n",
       " np.float64(0.7716408268733851))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now we can see this what we saw before, that in every run, we get different prediction. So for this reason using cross-validation is is important therefore in next step of upgrades we will be using this method.\n",
   "id": "c708e36bcfc9da8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "II. Over/under-fitting and error plot",
   "id": "eb9c179fc98035a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First we have to search if we have either overfitting or underfitting using our base logistic regression model.",
   "id": "1015a11e19398fd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:45:52.862409Z",
     "start_time": "2025-05-21T16:44:46.688050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_CV_logistic_regression = BaseLogisticRegression(n_iters=5000, lr=0.001, batch_size=64)\n",
    "make_cross_validation_predict(base_CV_logistic_regression, preprocessor_full_set, X_train, y_train)"
   ],
   "id": "fe04ee83aae72656",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0600\n",
      "Iteration 100, Loss: 0.6695\n",
      "Iteration 200, Loss: 0.6183\n",
      "Iteration 300, Loss: 0.5930\n",
      "Iteration 400, Loss: 0.5771\n",
      "Iteration 500, Loss: 0.5659\n",
      "Iteration 600, Loss: 0.5574\n",
      "Iteration 700, Loss: 0.5505\n",
      "Iteration 800, Loss: 0.5447\n",
      "Iteration 900, Loss: 0.5399\n",
      "Iteration 1000, Loss: 0.5356\n",
      "Iteration 1100, Loss: 0.5319\n",
      "Iteration 1200, Loss: 0.5285\n",
      "Iteration 1300, Loss: 0.5255\n",
      "Iteration 1400, Loss: 0.5228\n",
      "Iteration 1500, Loss: 0.5203\n",
      "Iteration 1600, Loss: 0.5180\n",
      "Iteration 1700, Loss: 0.5158\n",
      "Iteration 1800, Loss: 0.5139\n",
      "Iteration 1900, Loss: 0.5121\n",
      "Iteration 2000, Loss: 0.5104\n",
      "Iteration 2100, Loss: 0.5088\n",
      "Iteration 2200, Loss: 0.5073\n",
      "Iteration 2300, Loss: 0.5059\n",
      "Iteration 2400, Loss: 0.5046\n",
      "Iteration 2500, Loss: 0.5033\n",
      "Iteration 2600, Loss: 0.5021\n",
      "Iteration 2700, Loss: 0.5010\n",
      "Iteration 2800, Loss: 0.4999\n",
      "Iteration 2900, Loss: 0.4989\n",
      "Iteration 3000, Loss: 0.4980\n",
      "Iteration 3100, Loss: 0.4970\n",
      "Iteration 3200, Loss: 0.4961\n",
      "Iteration 3300, Loss: 0.4953\n",
      "Iteration 3400, Loss: 0.4945\n",
      "Iteration 3500, Loss: 0.4937\n",
      "Iteration 3600, Loss: 0.4930\n",
      "Iteration 3700, Loss: 0.4922\n",
      "Iteration 3800, Loss: 0.4915\n",
      "Iteration 3900, Loss: 0.4909\n",
      "Iteration 4000, Loss: 0.4902\n",
      "Iteration 4100, Loss: 0.4896\n",
      "Iteration 4200, Loss: 0.4890\n",
      "Iteration 4300, Loss: 0.4884\n",
      "Iteration 4400, Loss: 0.4879\n",
      "Iteration 4500, Loss: 0.4873\n",
      "Iteration 4600, Loss: 0.4868\n",
      "Iteration 4700, Loss: 0.4863\n",
      "Iteration 4800, Loss: 0.4858\n",
      "Iteration 4900, Loss: 0.4853\n",
      "Iteration 0, Loss: 1.0618\n",
      "Iteration 100, Loss: 0.6862\n",
      "Iteration 200, Loss: 0.6375\n",
      "Iteration 300, Loss: 0.6131\n",
      "Iteration 400, Loss: 0.5977\n",
      "Iteration 500, Loss: 0.5867\n",
      "Iteration 600, Loss: 0.5782\n",
      "Iteration 700, Loss: 0.5714\n",
      "Iteration 800, Loss: 0.5658\n",
      "Iteration 900, Loss: 0.5610\n",
      "Iteration 1000, Loss: 0.5568\n",
      "Iteration 1100, Loss: 0.5531\n",
      "Iteration 1200, Loss: 0.5499\n",
      "Iteration 1300, Loss: 0.5469\n",
      "Iteration 1400, Loss: 0.5443\n",
      "Iteration 1500, Loss: 0.5418\n",
      "Iteration 1600, Loss: 0.5396\n",
      "Iteration 1700, Loss: 0.5375\n",
      "Iteration 1800, Loss: 0.5356\n",
      "Iteration 1900, Loss: 0.5339\n",
      "Iteration 2000, Loss: 0.5322\n",
      "Iteration 2100, Loss: 0.5307\n",
      "Iteration 2200, Loss: 0.5293\n",
      "Iteration 2300, Loss: 0.5279\n",
      "Iteration 2400, Loss: 0.5267\n",
      "Iteration 2500, Loss: 0.5255\n",
      "Iteration 2600, Loss: 0.5243\n",
      "Iteration 2700, Loss: 0.5233\n",
      "Iteration 2800, Loss: 0.5222\n",
      "Iteration 2900, Loss: 0.5213\n",
      "Iteration 3000, Loss: 0.5204\n",
      "Iteration 3100, Loss: 0.5195\n",
      "Iteration 3200, Loss: 0.5186\n",
      "Iteration 3300, Loss: 0.5178\n",
      "Iteration 3400, Loss: 0.5171\n",
      "Iteration 3500, Loss: 0.5164\n",
      "Iteration 3600, Loss: 0.5157\n",
      "Iteration 3700, Loss: 0.5150\n",
      "Iteration 3800, Loss: 0.5143\n",
      "Iteration 3900, Loss: 0.5137\n",
      "Iteration 4000, Loss: 0.5131\n",
      "Iteration 4100, Loss: 0.5125\n",
      "Iteration 4200, Loss: 0.5120\n",
      "Iteration 4300, Loss: 0.5114\n",
      "Iteration 4400, Loss: 0.5109\n",
      "Iteration 4500, Loss: 0.5104\n",
      "Iteration 4600, Loss: 0.5099\n",
      "Iteration 4700, Loss: 0.5095\n",
      "Iteration 4800, Loss: 0.5090\n",
      "Iteration 4900, Loss: 0.5086\n",
      "Iteration 0, Loss: 1.0607\n",
      "Iteration 100, Loss: 0.6773\n",
      "Iteration 200, Loss: 0.6286\n",
      "Iteration 300, Loss: 0.6041\n",
      "Iteration 400, Loss: 0.5885\n",
      "Iteration 500, Loss: 0.5774\n",
      "Iteration 600, Loss: 0.5689\n",
      "Iteration 700, Loss: 0.5621\n",
      "Iteration 800, Loss: 0.5564\n",
      "Iteration 900, Loss: 0.5515\n",
      "Iteration 1000, Loss: 0.5473\n",
      "Iteration 1100, Loss: 0.5436\n",
      "Iteration 1200, Loss: 0.5403\n",
      "Iteration 1300, Loss: 0.5373\n",
      "Iteration 1400, Loss: 0.5346\n",
      "Iteration 1500, Loss: 0.5322\n",
      "Iteration 1600, Loss: 0.5299\n",
      "Iteration 1700, Loss: 0.5278\n",
      "Iteration 1800, Loss: 0.5259\n",
      "Iteration 1900, Loss: 0.5241\n",
      "Iteration 2000, Loss: 0.5224\n",
      "Iteration 2100, Loss: 0.5209\n",
      "Iteration 2200, Loss: 0.5194\n",
      "Iteration 2300, Loss: 0.5180\n",
      "Iteration 2400, Loss: 0.5167\n",
      "Iteration 2500, Loss: 0.5155\n",
      "Iteration 2600, Loss: 0.5143\n",
      "Iteration 2700, Loss: 0.5132\n",
      "Iteration 2800, Loss: 0.5122\n",
      "Iteration 2900, Loss: 0.5112\n",
      "Iteration 3000, Loss: 0.5103\n",
      "Iteration 3100, Loss: 0.5093\n",
      "Iteration 3200, Loss: 0.5085\n",
      "Iteration 3300, Loss: 0.5077\n",
      "Iteration 3400, Loss: 0.5069\n",
      "Iteration 3500, Loss: 0.5061\n",
      "Iteration 3600, Loss: 0.5054\n",
      "Iteration 3700, Loss: 0.5047\n",
      "Iteration 3800, Loss: 0.5040\n",
      "Iteration 3900, Loss: 0.5033\n",
      "Iteration 4000, Loss: 0.5027\n",
      "Iteration 4100, Loss: 0.5021\n",
      "Iteration 4200, Loss: 0.5015\n",
      "Iteration 4300, Loss: 0.5009\n",
      "Iteration 4400, Loss: 0.5004\n",
      "Iteration 4500, Loss: 0.4999\n",
      "Iteration 4600, Loss: 0.4993\n",
      "Iteration 4700, Loss: 0.4988\n",
      "Iteration 4800, Loss: 0.4983\n",
      "Iteration 4900, Loss: 0.4979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([np.float64(0.7742248062015504),\n",
       "  np.float64(0.7771317829457365),\n",
       "  np.float64(0.7625968992248062)],\n",
       " np.float64(0.7713178294573644))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:45:52.931348Z",
     "start_time": "2025-05-21T16:45:52.926714Z"
    }
   },
   "cell_type": "code",
   "source": "base_logistic_regression = BaseLogisticRegression(n_iters=5000, lr=0.001, batch_size=64)",
   "id": "dd711648b667e1d8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:46:28.970622Z",
     "start_time": "2025-05-21T16:45:52.998052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#train base_logistic_regression\n",
    "base_logistic_regression.fit(X_train_proc,y_train_proc)"
   ],
   "id": "29a1d05c76da69e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0453\n",
      "Iteration 100, Loss: 0.6511\n",
      "Iteration 200, Loss: 0.6082\n",
      "Iteration 300, Loss: 0.5876\n",
      "Iteration 400, Loss: 0.5746\n",
      "Iteration 500, Loss: 0.5655\n",
      "Iteration 600, Loss: 0.5584\n",
      "Iteration 700, Loss: 0.5528\n",
      "Iteration 800, Loss: 0.5480\n",
      "Iteration 900, Loss: 0.5440\n",
      "Iteration 1000, Loss: 0.5406\n",
      "Iteration 1100, Loss: 0.5376\n",
      "Iteration 1200, Loss: 0.5349\n",
      "Iteration 1300, Loss: 0.5325\n",
      "Iteration 1400, Loss: 0.5303\n",
      "Iteration 1500, Loss: 0.5284\n",
      "Iteration 1600, Loss: 0.5266\n",
      "Iteration 1700, Loss: 0.5250\n",
      "Iteration 1800, Loss: 0.5235\n",
      "Iteration 1900, Loss: 0.5221\n",
      "Iteration 2000, Loss: 0.5209\n",
      "Iteration 2100, Loss: 0.5197\n",
      "Iteration 2200, Loss: 0.5186\n",
      "Iteration 2300, Loss: 0.5175\n",
      "Iteration 2400, Loss: 0.5166\n",
      "Iteration 2500, Loss: 0.5157\n",
      "Iteration 2600, Loss: 0.5148\n",
      "Iteration 2700, Loss: 0.5140\n",
      "Iteration 2800, Loss: 0.5132\n",
      "Iteration 2900, Loss: 0.5125\n",
      "Iteration 3000, Loss: 0.5118\n",
      "Iteration 3100, Loss: 0.5111\n",
      "Iteration 3200, Loss: 0.5105\n",
      "Iteration 3300, Loss: 0.5099\n",
      "Iteration 3400, Loss: 0.5094\n",
      "Iteration 3500, Loss: 0.5088\n",
      "Iteration 3600, Loss: 0.5083\n",
      "Iteration 3700, Loss: 0.5078\n",
      "Iteration 3800, Loss: 0.5073\n",
      "Iteration 3900, Loss: 0.5068\n",
      "Iteration 4000, Loss: 0.5064\n",
      "Iteration 4100, Loss: 0.5060\n",
      "Iteration 4200, Loss: 0.5055\n",
      "Iteration 4300, Loss: 0.5051\n",
      "Iteration 4400, Loss: 0.5047\n",
      "Iteration 4500, Loss: 0.5044\n",
      "Iteration 4600, Loss: 0.5040\n",
      "Iteration 4700, Loss: 0.5037\n",
      "Iteration 4800, Loss: 0.5033\n",
      "Iteration 4900, Loss: 0.5030\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:46:28.986058Z",
     "start_time": "2025-05-21T16:46:28.979664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predicted = base_logistic_regression.predict_class(X_train_proc)\n",
    "print(base_logistic_regression.score(predicted, y_train_proc))\n",
    "x1,y1 = base_logistic_regression.raw_error_data()\n"
   ],
   "id": "6d668fcedb357bc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8020025839793282\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:46:29.041478Z",
     "start_time": "2025-05-21T16:46:29.035236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predicted = base_logistic_regression.predict_class(X_test_proc)\n",
    "print(base_logistic_regression.score(predicted, y_test_proc))\n",
    "x2,y2 = base_logistic_regression.raw_error_data()"
   ],
   "id": "f2f1cdca053df48e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766566265060241\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:46:29.274023Z",
     "start_time": "2025-05-21T16:46:29.089136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(x1, y1, label=\"Train set\", color='blue', linestyle='-')\n",
    "plt.plot(x2, y2, label=\"Test set\", color='orange', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "ce668da263984e75",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Based on this plot we can say that our lines are close to each other, so we don't have problem with under or over-fitting\n",
    "\n",
    "And also we have scores:\n",
    "\\begin{align*}\n",
    "\\text{accuracy}(\\text{CV}) &= 0.7719 \\\\\n",
    "\\text{accuracy}(\\text{on train}) &= 0.8016 \\\\\n",
    "\\text{accuracy}(\\text{on test}) &= 0.7665\n",
    "\\end{align*}\n",
    "We can see that $Test~\\approx~CV~\\approx~Train$ so it means that we don't have problem with underfitting or overfitting"
   ],
   "id": "98386976865dc571"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Although we don't have problem with under/over - fitting still we can try to improve model with PolynomialFeatures\n",
   "id": "b067d0b51f296300"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:46:29.334888Z",
     "start_time": "2025-05-21T16:46:29.329634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numerical_pipeline_poly = Pipeline([\n",
    "    ('poly',PolynomialFeatures(degree=2,include_bias=False)),\n",
    "    ('scaler',StandardScaler())\n",
    "])\n",
    "\n",
    "polynomial_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline_poly, numerical_features),\n",
    "        ('cat',OneHotEncoder(handle_unknown='ignore',sparse_output=False),categorical_features)\n",
    "    ]\n",
    ")\n"
   ],
   "id": "c98ee8f8ba69b7e9",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:48:40.683250Z",
     "start_time": "2025-05-21T16:46:29.398433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "polynomial_CV_logistic_regression = BaseLogisticRegression(n_iters=5000, lr=0.001, batch_size=64)\n",
    "make_cross_validation_predict(polynomial_CV_logistic_regression, polynomial_preprocessor, X_train, y_train)"
   ],
   "id": "a39c4629e8d0a803",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.9904\n",
      "Iteration 100, Loss: 0.5995\n",
      "Iteration 200, Loss: 0.5679\n",
      "Iteration 300, Loss: 0.5498\n",
      "Iteration 400, Loss: 0.5371\n",
      "Iteration 500, Loss: 0.5273\n",
      "Iteration 600, Loss: 0.5195\n",
      "Iteration 700, Loss: 0.5130\n",
      "Iteration 800, Loss: 0.5074\n",
      "Iteration 900, Loss: 0.5025\n",
      "Iteration 1000, Loss: 0.4983\n",
      "Iteration 1100, Loss: 0.4944\n",
      "Iteration 1200, Loss: 0.4909\n",
      "Iteration 1300, Loss: 0.4878\n",
      "Iteration 1400, Loss: 0.4849\n",
      "Iteration 1500, Loss: 0.4822\n",
      "Iteration 1600, Loss: 0.4798\n",
      "Iteration 1700, Loss: 0.4774\n",
      "Iteration 1800, Loss: 0.4753\n",
      "Iteration 1900, Loss: 0.4732\n",
      "Iteration 2000, Loss: 0.4713\n",
      "Iteration 2100, Loss: 0.4695\n",
      "Iteration 2200, Loss: 0.4678\n",
      "Iteration 2300, Loss: 0.4662\n",
      "Iteration 2400, Loss: 0.4646\n",
      "Iteration 2500, Loss: 0.4632\n",
      "Iteration 2600, Loss: 0.4617\n",
      "Iteration 2700, Loss: 0.4604\n",
      "Iteration 2800, Loss: 0.4591\n",
      "Iteration 2900, Loss: 0.4579\n",
      "Iteration 3000, Loss: 0.4567\n",
      "Iteration 3100, Loss: 0.4556\n",
      "Iteration 3200, Loss: 0.4545\n",
      "Iteration 3300, Loss: 0.4534\n",
      "Iteration 3400, Loss: 0.4524\n",
      "Iteration 3500, Loss: 0.4514\n",
      "Iteration 3600, Loss: 0.4504\n",
      "Iteration 3700, Loss: 0.4495\n",
      "Iteration 3800, Loss: 0.4486\n",
      "Iteration 3900, Loss: 0.4477\n",
      "Iteration 4000, Loss: 0.4469\n",
      "Iteration 4100, Loss: 0.4461\n",
      "Iteration 4200, Loss: 0.4453\n",
      "Iteration 4300, Loss: 0.4445\n",
      "Iteration 4400, Loss: 0.4438\n",
      "Iteration 4500, Loss: 0.4431\n",
      "Iteration 4600, Loss: 0.4423\n",
      "Iteration 4700, Loss: 0.4417\n",
      "Iteration 4800, Loss: 0.4410\n",
      "Iteration 4900, Loss: 0.4403\n",
      "Iteration 0, Loss: 0.9842\n",
      "Iteration 100, Loss: 0.6063\n",
      "Iteration 200, Loss: 0.5785\n",
      "Iteration 300, Loss: 0.5629\n",
      "Iteration 400, Loss: 0.5520\n",
      "Iteration 500, Loss: 0.5438\n",
      "Iteration 600, Loss: 0.5371\n",
      "Iteration 700, Loss: 0.5316\n",
      "Iteration 800, Loss: 0.5267\n",
      "Iteration 900, Loss: 0.5225\n",
      "Iteration 1000, Loss: 0.5187\n",
      "Iteration 1100, Loss: 0.5153\n",
      "Iteration 1200, Loss: 0.5122\n",
      "Iteration 1300, Loss: 0.5094\n",
      "Iteration 1400, Loss: 0.5068\n",
      "Iteration 1500, Loss: 0.5043\n",
      "Iteration 1600, Loss: 0.5020\n",
      "Iteration 1700, Loss: 0.4999\n",
      "Iteration 1800, Loss: 0.4978\n",
      "Iteration 1900, Loss: 0.4959\n",
      "Iteration 2000, Loss: 0.4941\n",
      "Iteration 2100, Loss: 0.4924\n",
      "Iteration 2200, Loss: 0.4908\n",
      "Iteration 2300, Loss: 0.4893\n",
      "Iteration 2400, Loss: 0.4878\n",
      "Iteration 2500, Loss: 0.4864\n",
      "Iteration 2600, Loss: 0.4850\n",
      "Iteration 2700, Loss: 0.4837\n",
      "Iteration 2800, Loss: 0.4825\n",
      "Iteration 2900, Loss: 0.4813\n",
      "Iteration 3000, Loss: 0.4801\n",
      "Iteration 3100, Loss: 0.4790\n",
      "Iteration 3200, Loss: 0.4779\n",
      "Iteration 3300, Loss: 0.4769\n",
      "Iteration 3400, Loss: 0.4759\n",
      "Iteration 3500, Loss: 0.4749\n",
      "Iteration 3600, Loss: 0.4740\n",
      "Iteration 3700, Loss: 0.4730\n",
      "Iteration 3800, Loss: 0.4722\n",
      "Iteration 3900, Loss: 0.4713\n",
      "Iteration 4000, Loss: 0.4705\n",
      "Iteration 4100, Loss: 0.4697\n",
      "Iteration 4200, Loss: 0.4689\n",
      "Iteration 4300, Loss: 0.4681\n",
      "Iteration 4400, Loss: 0.4674\n",
      "Iteration 4500, Loss: 0.4666\n",
      "Iteration 4600, Loss: 0.4659\n",
      "Iteration 4700, Loss: 0.4652\n",
      "Iteration 4800, Loss: 0.4646\n",
      "Iteration 4900, Loss: 0.4639\n",
      "Iteration 0, Loss: 0.9874\n",
      "Iteration 100, Loss: 0.5986\n",
      "Iteration 200, Loss: 0.5691\n",
      "Iteration 300, Loss: 0.5525\n",
      "Iteration 400, Loss: 0.5410\n",
      "Iteration 500, Loss: 0.5323\n",
      "Iteration 600, Loss: 0.5254\n",
      "Iteration 700, Loss: 0.5196\n",
      "Iteration 800, Loss: 0.5146\n",
      "Iteration 900, Loss: 0.5103\n",
      "Iteration 1000, Loss: 0.5065\n",
      "Iteration 1100, Loss: 0.5030\n",
      "Iteration 1200, Loss: 0.4999\n",
      "Iteration 1300, Loss: 0.4971\n",
      "Iteration 1400, Loss: 0.4944\n",
      "Iteration 1500, Loss: 0.4920\n",
      "Iteration 1600, Loss: 0.4897\n",
      "Iteration 1700, Loss: 0.4876\n",
      "Iteration 1800, Loss: 0.4856\n",
      "Iteration 1900, Loss: 0.4837\n",
      "Iteration 2000, Loss: 0.4819\n",
      "Iteration 2100, Loss: 0.4802\n",
      "Iteration 2200, Loss: 0.4785\n",
      "Iteration 2300, Loss: 0.4770\n",
      "Iteration 2400, Loss: 0.4755\n",
      "Iteration 2500, Loss: 0.4741\n",
      "Iteration 2600, Loss: 0.4727\n",
      "Iteration 2700, Loss: 0.4714\n",
      "Iteration 2800, Loss: 0.4701\n",
      "Iteration 2900, Loss: 0.4689\n",
      "Iteration 3000, Loss: 0.4677\n",
      "Iteration 3100, Loss: 0.4666\n",
      "Iteration 3200, Loss: 0.4655\n",
      "Iteration 3300, Loss: 0.4644\n",
      "Iteration 3400, Loss: 0.4634\n",
      "Iteration 3500, Loss: 0.4624\n",
      "Iteration 3600, Loss: 0.4614\n",
      "Iteration 3700, Loss: 0.4604\n",
      "Iteration 3800, Loss: 0.4595\n",
      "Iteration 3900, Loss: 0.4586\n",
      "Iteration 4000, Loss: 0.4577\n",
      "Iteration 4100, Loss: 0.4569\n",
      "Iteration 4200, Loss: 0.4560\n",
      "Iteration 4300, Loss: 0.4552\n",
      "Iteration 4400, Loss: 0.4544\n",
      "Iteration 4500, Loss: 0.4536\n",
      "Iteration 4600, Loss: 0.4529\n",
      "Iteration 4700, Loss: 0.4521\n",
      "Iteration 4800, Loss: 0.4514\n",
      "Iteration 4900, Loss: 0.4507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([np.float64(0.7616279069767442),\n",
       "  np.float64(0.7790697674418605),\n",
       "  np.float64(0.7790697674418605)],\n",
       " np.float64(0.7732558139534884))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:48:40.848145Z",
     "start_time": "2025-05-21T16:48:40.793726Z"
    }
   },
   "cell_type": "code",
   "source": "X_train_proc_poly,X_test_proc_poly,y_train_proc_poly,y_test_proc_poly = transform_in_pipeline(polynomial_preprocessor,X_train, X_test, y_train, y_test)",
   "id": "e71b30ff90204ff6",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:49:52.014471Z",
     "start_time": "2025-05-21T16:48:40.860496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "polynomial_base_logistic_regression = BaseLogisticRegression(n_iters=5000, lr=0.001, batch_size=64)\n",
    "polynomial_base_logistic_regression.fit(X_train_proc_poly,y_train_proc_poly)"
   ],
   "id": "93a047095c5d2115",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.9519\n",
      "Iteration 100, Loss: 0.5905\n",
      "Iteration 200, Loss: 0.5638\n",
      "Iteration 300, Loss: 0.5489\n",
      "Iteration 400, Loss: 0.5388\n",
      "Iteration 500, Loss: 0.5311\n",
      "Iteration 600, Loss: 0.5250\n",
      "Iteration 700, Loss: 0.5199\n",
      "Iteration 800, Loss: 0.5156\n",
      "Iteration 900, Loss: 0.5118\n",
      "Iteration 1000, Loss: 0.5085\n",
      "Iteration 1100, Loss: 0.5055\n",
      "Iteration 1200, Loss: 0.5029\n",
      "Iteration 1300, Loss: 0.5004\n",
      "Iteration 1400, Loss: 0.4982\n",
      "Iteration 1500, Loss: 0.4961\n",
      "Iteration 1600, Loss: 0.4942\n",
      "Iteration 1700, Loss: 0.4924\n",
      "Iteration 1800, Loss: 0.4907\n",
      "Iteration 1900, Loss: 0.4891\n",
      "Iteration 2000, Loss: 0.4876\n",
      "Iteration 2100, Loss: 0.4862\n",
      "Iteration 2200, Loss: 0.4849\n",
      "Iteration 2300, Loss: 0.4836\n",
      "Iteration 2400, Loss: 0.4824\n",
      "Iteration 2500, Loss: 0.4812\n",
      "Iteration 2600, Loss: 0.4802\n",
      "Iteration 2700, Loss: 0.4791\n",
      "Iteration 2800, Loss: 0.4781\n",
      "Iteration 2900, Loss: 0.4771\n",
      "Iteration 3000, Loss: 0.4762\n",
      "Iteration 3100, Loss: 0.4753\n",
      "Iteration 3200, Loss: 0.4745\n",
      "Iteration 3300, Loss: 0.4737\n",
      "Iteration 3400, Loss: 0.4729\n",
      "Iteration 3500, Loss: 0.4721\n",
      "Iteration 3600, Loss: 0.4714\n",
      "Iteration 3700, Loss: 0.4707\n",
      "Iteration 3800, Loss: 0.4700\n",
      "Iteration 3900, Loss: 0.4693\n",
      "Iteration 4000, Loss: 0.4687\n",
      "Iteration 4100, Loss: 0.4680\n",
      "Iteration 4200, Loss: 0.4674\n",
      "Iteration 4300, Loss: 0.4668\n",
      "Iteration 4400, Loss: 0.4662\n",
      "Iteration 4500, Loss: 0.4657\n",
      "Iteration 4600, Loss: 0.4651\n",
      "Iteration 4700, Loss: 0.4646\n",
      "Iteration 4800, Loss: 0.4641\n",
      "Iteration 4900, Loss: 0.4636\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:49:52.140232Z",
     "start_time": "2025-05-21T16:49:52.133975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predicted = polynomial_base_logistic_regression.predict_class(X_train_proc_poly)\n",
    "print(polynomial_base_logistic_regression.score(predicted, y_train_proc_poly))"
   ],
   "id": "239db0dfe44d03fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8126614987080103\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:49:52.185813Z",
     "start_time": "2025-05-21T16:49:52.180366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predicted = polynomial_base_logistic_regression.predict_class(X_test_proc_poly)\n",
    "print(polynomial_base_logistic_regression.score(predicted, y_test_proc_poly))"
   ],
   "id": "81cf5c1eb2b43d53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7718373493975904\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And we can see that we get little better scores using this modified preprocessor.",
   "id": "81dfeb6626b3b264"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can ask question, do we need all of the columns in training our model, maybe we can delete some of them and our score won't be worse",
   "id": "d8306be74b724638"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:23:46.029441Z",
     "start_time": "2025-05-23T09:23:46.009223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numerical_features_modified = [\n",
    "    \"Application order\",\"Age at enrollment\", \"Curricular units 1st sem (credited)\", \"Curricular units 1st sem (enrolled)\",\n",
    "    \"Curricular units 1st sem (evaluations)\",\"Curricular units 1st sem (approved)\",\"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 1st sem (without evaluations)\",\"Curricular units 2nd sem (credited)\",\"Curricular units 2nd sem (enrolled)\",\n",
    "    \"Curricular units 2nd sem (evaluations)\",\"Curricular units 2nd sem (approved)\",\n",
    "    \"Curricular units 2nd sem (without evaluations)\",\"Curricular units 2nd sem (grade)\"\n",
    "]\n",
    "categorical_features_modified = [\n",
    "    \"Marital status\",\"Application mode\",\"Course\",\"Daytime/evening attendance\",\"Previous qualification\",\n",
    "    \"Mother's qualification\",\"Father's qualification\",\"Mother's occupation\",\"Father's occupation\",\"Displaced\",\n",
    "    \"Educational special needs\",\"Debtor\",\"Tuition fees up to date\",\"Gender\",\"Scholarship holder\"\n",
    "]\n",
    "\n",
    "\n",
    "df_modified = pd.read_csv(\"dataset.csv\")\n",
    "#pipeline"
   ],
   "id": "125393cdadf93117",
   "outputs": [],
   "execution_count": 314
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:23:46.311892Z",
     "start_time": "2025-05-23T09:23:46.304661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = df_modified[\"Target\"]\n",
    "X = df_modified[numerical_features_modified+categorical_features_modified]"
   ],
   "id": "5648b8ee5e02e3f1",
   "outputs": [],
   "execution_count": 315
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:23:46.527999Z",
     "start_time": "2025-05-23T09:23:46.517499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numerical_pipeline_modified_features = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "modified_features_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline_modified_features, numerical_features_modified),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features_modified)\n",
    "    ]\n",
    ")\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=42)"
   ],
   "id": "3fdeae8f87f70ee5",
   "outputs": [],
   "execution_count": 316
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:06:08.509084Z",
     "start_time": "2025-05-22T19:04:32.050544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "modified_features_CV_logistic_regression = BaseLogisticRegression(n_iters=5000, lr=0.001, batch_size=64)\n",
    "print(make_cross_validation_predict(modified_features_CV_logistic_regression, modified_features_preprocessor, X_train, y_train))\n",
    "print(modified_features_CV_logistic_regression.weights)"
   ],
   "id": "aca99bb7d1ed264c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0001\n",
      "Iteration 100, Loss: 0.6038\n",
      "Iteration 200, Loss: 0.5732\n",
      "Iteration 300, Loss: 0.5563\n",
      "Iteration 400, Loss: 0.5449\n",
      "Iteration 500, Loss: 0.5364\n",
      "Iteration 600, Loss: 0.5297\n",
      "Iteration 700, Loss: 0.5242\n",
      "Iteration 800, Loss: 0.5195\n",
      "Iteration 900, Loss: 0.5155\n",
      "Iteration 1000, Loss: 0.5119\n",
      "Iteration 1100, Loss: 0.5087\n",
      "Iteration 1200, Loss: 0.5059\n",
      "Iteration 1300, Loss: 0.5032\n",
      "Iteration 1400, Loss: 0.5008\n",
      "Iteration 1500, Loss: 0.4986\n",
      "Iteration 1600, Loss: 0.4966\n",
      "Iteration 1700, Loss: 0.4946\n",
      "Iteration 1800, Loss: 0.4928\n",
      "Iteration 1900, Loss: 0.4911\n",
      "Iteration 2000, Loss: 0.4894\n",
      "Iteration 2100, Loss: 0.4879\n",
      "Iteration 2200, Loss: 0.4865\n",
      "Iteration 2300, Loss: 0.4851\n",
      "Iteration 2400, Loss: 0.4838\n",
      "Iteration 2500, Loss: 0.4825\n",
      "Iteration 2600, Loss: 0.4813\n",
      "Iteration 2700, Loss: 0.4801\n",
      "Iteration 2800, Loss: 0.4790\n",
      "Iteration 2900, Loss: 0.4780\n",
      "Iteration 3000, Loss: 0.4769\n",
      "Iteration 3100, Loss: 0.4759\n",
      "Iteration 3200, Loss: 0.4750\n",
      "Iteration 3300, Loss: 0.4741\n",
      "Iteration 3400, Loss: 0.4732\n",
      "Iteration 3500, Loss: 0.4723\n",
      "Iteration 3600, Loss: 0.4715\n",
      "Iteration 3700, Loss: 0.4707\n",
      "Iteration 3800, Loss: 0.4699\n",
      "Iteration 3900, Loss: 0.4692\n",
      "Iteration 4000, Loss: 0.4684\n",
      "Iteration 4100, Loss: 0.4677\n",
      "Iteration 4200, Loss: 0.4670\n",
      "Iteration 4300, Loss: 0.4664\n",
      "Iteration 4400, Loss: 0.4657\n",
      "Iteration 4500, Loss: 0.4651\n",
      "Iteration 4600, Loss: 0.4645\n",
      "Iteration 4700, Loss: 0.4639\n",
      "Iteration 4800, Loss: 0.4633\n",
      "Iteration 4900, Loss: 0.4627\n",
      "Iteration 0, Loss: 0.9968\n",
      "Iteration 100, Loss: 0.6091\n",
      "Iteration 200, Loss: 0.5811\n",
      "Iteration 300, Loss: 0.5651\n",
      "Iteration 400, Loss: 0.5541\n",
      "Iteration 500, Loss: 0.5456\n",
      "Iteration 600, Loss: 0.5388\n",
      "Iteration 700, Loss: 0.5331\n",
      "Iteration 800, Loss: 0.5282\n",
      "Iteration 900, Loss: 0.5240\n",
      "Iteration 1000, Loss: 0.5202\n",
      "Iteration 1100, Loss: 0.5168\n",
      "Iteration 1200, Loss: 0.5137\n",
      "Iteration 1300, Loss: 0.5109\n",
      "Iteration 1400, Loss: 0.5083\n",
      "Iteration 1500, Loss: 0.5059\n",
      "Iteration 1600, Loss: 0.5037\n",
      "Iteration 1700, Loss: 0.5016\n",
      "Iteration 1800, Loss: 0.4996\n",
      "Iteration 1900, Loss: 0.4977\n",
      "Iteration 2000, Loss: 0.4960\n",
      "Iteration 2100, Loss: 0.4943\n",
      "Iteration 2200, Loss: 0.4928\n",
      "Iteration 2300, Loss: 0.4912\n",
      "Iteration 2400, Loss: 0.4898\n",
      "Iteration 2500, Loss: 0.4884\n",
      "Iteration 2600, Loss: 0.4871\n",
      "Iteration 2700, Loss: 0.4859\n",
      "Iteration 2800, Loss: 0.4847\n",
      "Iteration 2900, Loss: 0.4835\n",
      "Iteration 3000, Loss: 0.4824\n",
      "Iteration 3100, Loss: 0.4813\n",
      "Iteration 3200, Loss: 0.4803\n",
      "Iteration 3300, Loss: 0.4793\n",
      "Iteration 3400, Loss: 0.4783\n",
      "Iteration 3500, Loss: 0.4774\n",
      "Iteration 3600, Loss: 0.4765\n",
      "Iteration 3700, Loss: 0.4756\n",
      "Iteration 3800, Loss: 0.4747\n",
      "Iteration 3900, Loss: 0.4739\n",
      "Iteration 4000, Loss: 0.4731\n",
      "Iteration 4100, Loss: 0.4723\n",
      "Iteration 4200, Loss: 0.4716\n",
      "Iteration 4300, Loss: 0.4708\n",
      "Iteration 4400, Loss: 0.4701\n",
      "Iteration 4500, Loss: 0.4694\n",
      "Iteration 4600, Loss: 0.4688\n",
      "Iteration 4700, Loss: 0.4681\n",
      "Iteration 4800, Loss: 0.4674\n",
      "Iteration 4900, Loss: 0.4668\n",
      "Iteration 0, Loss: 1.0042\n",
      "Iteration 100, Loss: 0.6233\n",
      "Iteration 200, Loss: 0.5942\n",
      "Iteration 300, Loss: 0.5786\n",
      "Iteration 400, Loss: 0.5679\n",
      "Iteration 500, Loss: 0.5599\n",
      "Iteration 600, Loss: 0.5536\n",
      "Iteration 700, Loss: 0.5483\n",
      "Iteration 800, Loss: 0.5437\n",
      "Iteration 900, Loss: 0.5398\n",
      "Iteration 1000, Loss: 0.5363\n",
      "Iteration 1100, Loss: 0.5331\n",
      "Iteration 1200, Loss: 0.5302\n",
      "Iteration 1300, Loss: 0.5276\n",
      "Iteration 1400, Loss: 0.5252\n",
      "Iteration 1500, Loss: 0.5230\n",
      "Iteration 1600, Loss: 0.5209\n",
      "Iteration 1700, Loss: 0.5190\n",
      "Iteration 1800, Loss: 0.5172\n",
      "Iteration 1900, Loss: 0.5155\n",
      "Iteration 2000, Loss: 0.5139\n",
      "Iteration 2100, Loss: 0.5124\n",
      "Iteration 2200, Loss: 0.5110\n",
      "Iteration 2300, Loss: 0.5096\n",
      "Iteration 2400, Loss: 0.5083\n",
      "Iteration 2500, Loss: 0.5070\n",
      "Iteration 2600, Loss: 0.5058\n",
      "Iteration 2700, Loss: 0.5047\n",
      "Iteration 2800, Loss: 0.5036\n",
      "Iteration 2900, Loss: 0.5025\n",
      "Iteration 3000, Loss: 0.5016\n",
      "Iteration 3100, Loss: 0.5006\n",
      "Iteration 3200, Loss: 0.4996\n",
      "Iteration 3300, Loss: 0.4987\n",
      "Iteration 3400, Loss: 0.4979\n",
      "Iteration 3500, Loss: 0.4970\n",
      "Iteration 3600, Loss: 0.4962\n",
      "Iteration 3700, Loss: 0.4954\n",
      "Iteration 3800, Loss: 0.4947\n",
      "Iteration 3900, Loss: 0.4940\n",
      "Iteration 4000, Loss: 0.4932\n",
      "Iteration 4100, Loss: 0.4925\n",
      "Iteration 4200, Loss: 0.4919\n",
      "Iteration 4300, Loss: 0.4912\n",
      "Iteration 4400, Loss: 0.4906\n",
      "Iteration 4500, Loss: 0.4900\n",
      "Iteration 4600, Loss: 0.4894\n",
      "Iteration 4700, Loss: 0.4888\n",
      "Iteration 4800, Loss: 0.4882\n",
      "Iteration 4900, Loss: 0.4876\n",
      "([np.float64(0.7742248062015504), np.float64(0.75), np.float64(0.7916666666666666)], np.float64(0.7719638242894057))\n",
      "[[ 3.66716692e-02 -5.05845018e-02  1.39128326e-02]\n",
      " [ 2.05684313e-01 -1.20396684e-01 -8.52876290e-02]\n",
      " [ 7.76070203e-02  3.85956090e-02 -1.16202629e-01]\n",
      " [ 2.82527076e-01 -1.46588722e-02 -2.67868204e-01]\n",
      " [-1.76237953e-01  3.03508026e-01 -1.27270073e-01]\n",
      " [-2.11320130e-01 -6.02140094e-02  2.71534139e-01]\n",
      " [-9.21076344e-04  8.58742580e-02 -8.49531817e-02]\n",
      " [ 2.82398986e-03 -4.47471478e-02  4.19231579e-02]\n",
      " [ 1.19492376e-01 -6.66924810e-02 -5.27998955e-02]\n",
      " [ 3.54221166e-01  2.86238579e-02 -3.82845024e-01]\n",
      " [-7.67971599e-02  3.47034744e-01 -2.70237584e-01]\n",
      " [-4.18469192e-01 -7.23682474e-02  4.90837440e-01]\n",
      " [ 9.19951762e-02 -1.07125308e-02 -8.12826454e-02]\n",
      " [-2.11331580e-01  1.60377760e-01  5.09538200e-02]\n",
      " [-3.73402853e-02  5.20368044e-03  3.21366049e-02]\n",
      " [-1.90742911e-02  1.35821128e-02  5.49217826e-03]\n",
      " [-2.01568047e-01 -1.62375740e-01  3.63943787e-01]\n",
      " [ 2.04035463e-01 -1.76523853e-01 -2.75116103e-02]\n",
      " [-1.14357155e-01  1.08094769e-02  1.03547678e-01]\n",
      " [ 5.65815478e-02 -1.38674060e-01  8.20925126e-02]\n",
      " [ 1.16938245e-01  6.27075373e-02 -1.79645782e-01]\n",
      " [ 8.86401221e-02 -5.12859830e-02 -3.73541391e-02]\n",
      " [-1.47255469e-01 -1.37417781e-01  2.84673250e-01]\n",
      " [ 2.41948337e-01 -1.15415676e-01 -1.26532661e-01]\n",
      " [-2.00307756e-01 -1.47865646e-02  2.15094321e-01]\n",
      " [-1.55940210e-01  2.08088034e-01 -5.21478232e-02]\n",
      " [-6.76852062e-02  2.52640262e-02  4.24211800e-02]\n",
      " [-1.11505805e-01  1.80041202e-01 -6.85353971e-02]\n",
      " [ 1.04594822e-01 -2.10086662e-01  1.05491840e-01]\n",
      " [ 2.22816292e-01  6.27899989e-02 -2.85606291e-01]\n",
      " [ 1.87157073e-01  4.49536138e-02 -2.32110687e-01]\n",
      " [ 3.37044023e-02  1.52471120e-01 -1.86175522e-01]\n",
      " [-3.10461184e-01  9.18539431e-02  2.18607241e-01]\n",
      " [-2.87179012e-03 -4.94180205e-02  5.22898106e-02]\n",
      " [-3.34861979e-02 -1.08165802e-01  1.41652000e-01]\n",
      " [ 2.31918068e-02  1.05518279e-01 -1.28710086e-01]\n",
      " [ 1.66135656e-01  6.37845006e-02 -2.29920157e-01]\n",
      " [ 4.80754948e-02  8.12975828e-02 -1.29373078e-01]\n",
      " [-5.07010437e-01  1.96155999e-01  3.10854438e-01]\n",
      " [-9.98315959e-02 -1.69853781e-01  2.69685376e-01]\n",
      " [-1.84370443e-01  4.20892868e-02  1.42281157e-01]\n",
      " [ 1.32645691e-01 -1.21861216e-01 -1.07844752e-02]\n",
      " [-3.70806689e-02 -2.30763539e-02  6.01570228e-02]\n",
      " [ 4.04024660e-03 -1.50275819e-01  1.46235573e-01]\n",
      " [ 3.60051102e-02  6.76930740e-02 -1.03698184e-01]\n",
      " [-2.29722344e-02  1.25976686e-01 -1.03004451e-01]\n",
      " [ 7.45987022e-02 -1.78402035e-01  1.03803333e-01]\n",
      " [ 7.54775913e-02 -1.03477669e-01  2.80000774e-02]\n",
      " [-4.07448674e-02  4.26794573e-02 -1.93458986e-03]\n",
      " [-1.14149600e-01  1.41139993e-01 -2.69903922e-02]\n",
      " [ 1.11510468e-01  4.63436235e-02 -1.57854091e-01]\n",
      " [ 6.91649352e-02 -1.22186588e-01  5.30216527e-02]\n",
      " [ 3.76042578e-02  5.52631071e-02 -9.28673649e-02]\n",
      " [ 2.61759936e-03 -4.10530958e-02  3.84354964e-02]\n",
      " [-1.10252988e-01  1.84167896e-02  9.18361982e-02]\n",
      " [-1.48263785e-01 -4.79926415e-03  1.53063049e-01]\n",
      " [ 1.95902721e-02  3.71423064e-02 -5.67325785e-02]\n",
      " [ 3.04410679e-02 -7.96219391e-02  4.91808712e-02]\n",
      " [-8.06481439e-03 -5.51224998e-02  6.31873142e-02]\n",
      " [ 8.96607117e-02 -1.37963878e-02 -7.58643239e-02]\n",
      " [-9.76295022e-02  1.22012272e-01 -2.43827698e-02]\n",
      " [-1.04725163e-01 -1.57850784e-02  1.20510241e-01]\n",
      " [ 5.30815540e-02  5.73367370e-03 -5.88152277e-02]\n",
      " [-3.58006978e-02 -2.76558019e-02  6.34564997e-02]\n",
      " [ 4.68128336e-02 -2.28105537e-01  1.81292704e-01]\n",
      " [-1.33311258e-01 -5.14094105e-02  1.84720668e-01]\n",
      " [ 6.87827463e-02  3.05698038e-03 -7.18397267e-02]\n",
      " [ 9.30604536e-03 -9.02107241e-02  8.09046787e-02]\n",
      " [-1.07807198e-02 -1.24573633e-01  1.35354353e-01]\n",
      " [-5.55716817e-02  9.88056842e-02 -4.32340025e-02]\n",
      " [ 1.34834595e-01 -1.03011018e-01 -3.18235772e-02]\n",
      " [-1.42793279e-01  5.37593819e-03  1.37417341e-01]\n",
      " [ 1.42143811e-01 -1.14459025e-01 -2.76847864e-02]\n",
      " [ 1.16239123e-01 -1.20256183e-01  4.01706045e-03]\n",
      " [-1.48089125e-01  5.09388369e-03  1.42995242e-01]\n",
      " [-1.20360177e-02 -2.10167768e-01  2.22203785e-01]\n",
      " [ 1.08164570e-01 -8.81852927e-03 -9.93460408e-02]\n",
      " [ 5.74883954e-02  4.17674520e-02 -9.92558474e-02]\n",
      " [-1.20518447e-01  1.72241827e-02  1.03294264e-01]\n",
      " [-2.10372451e-01  3.16595177e-02  1.78712934e-01]\n",
      " [-8.41737627e-03 -1.06480229e-01  1.14897605e-01]\n",
      " [-5.14575146e-02 -6.12945787e-03  5.75869724e-02]\n",
      " [-1.10253030e-01 -1.73184511e-01  2.83437541e-01]\n",
      " [ 2.04548457e-01 -1.67230236e-01 -3.73182206e-02]\n",
      " [ 3.19454928e-02  2.13157945e-01 -2.45103438e-01]\n",
      " [ 2.30845838e-02  1.80810100e-02 -4.11655938e-02]\n",
      " [ 1.98068435e-02  7.61611943e-02 -9.59680378e-02]\n",
      " [-2.44323533e-02  1.72624480e-01 -1.48192126e-01]\n",
      " [-1.39531014e-01 -2.23906997e-01  3.63438011e-01]\n",
      " [-6.96903884e-02  1.04481736e-01 -3.47913474e-02]\n",
      " [ 4.80378000e-02 -7.76720287e-02  2.96342287e-02]\n",
      " [-8.78103750e-02  5.06967740e-02  3.71136010e-02]\n",
      " [ 5.86457641e-02 -1.50222348e-01  9.15765842e-02]\n",
      " [ 1.10937706e-02 -5.85250346e-02  4.74312640e-02]\n",
      " [-1.05635447e-02 -7.75032871e-02  8.80668318e-02]\n",
      " [-5.96392162e-03 -6.14326837e-02  6.73966053e-02]\n",
      " [ 8.23802397e-02  2.05026981e-02 -1.02882938e-01]\n",
      " [-1.33242357e-01  1.14485016e-01  1.87573410e-02]\n",
      " [ 1.96069007e-02 -5.97389513e-02  4.01320506e-02]\n",
      " [ 5.30332829e-02 -4.68788761e-03 -4.83453953e-02]\n",
      " [-1.47020045e-02  1.07893361e-01 -9.31913566e-02]\n",
      " [ 1.78257620e-01  3.31749137e-02 -2.11432534e-01]\n",
      " [ 7.57526513e-02 -7.16261361e-02 -4.12651525e-03]\n",
      " [ 5.58896594e-02 -6.70597459e-02  1.11700864e-02]\n",
      " [ 2.77936733e-01 -2.05446015e-02 -2.57392132e-01]\n",
      " [ 4.64282782e-02  1.50658319e-01 -1.97086598e-01]\n",
      " [-2.80482681e-02  1.53698705e-02  1.26783977e-02]\n",
      " [ 3.66856999e-02  5.36932230e-02 -9.03789228e-02]\n",
      " [-6.48029255e-02  3.94456830e-02  2.53572425e-02]\n",
      " [ 9.78507790e-02 -2.42578153e-01  1.44727374e-01]\n",
      " [-2.10936807e-02  1.06550614e-01 -8.54569333e-02]\n",
      " [-6.84197655e-02 -1.35413525e-01  2.03833291e-01]\n",
      " [ 1.78542142e-01 -1.05816731e-01 -7.27254103e-02]\n",
      " [ 7.58275486e-02 -1.84754706e-01  1.08927157e-01]\n",
      " [-1.81940580e-02  4.42951502e-02 -2.61010922e-02]\n",
      " [-2.27110338e-01 -2.58061770e-01  4.85172108e-01]\n",
      " [ 6.61706788e-02 -8.19793120e-02  1.58086332e-02]\n",
      " [-3.00009190e-03  1.63589766e-02 -1.33588847e-02]\n",
      " [ 1.25436428e-02 -1.06824713e-02 -1.86117153e-03]\n",
      " [ 1.19223487e-01  3.31567006e-02 -1.52380188e-01]\n",
      " [ 4.91853787e-02 -6.85665019e-02  1.93811232e-02]\n",
      " [-2.66130386e-02  4.95905188e-02 -2.29774802e-02]\n",
      " [ 6.63688270e-03 -5.50211891e-02  4.83843064e-02]\n",
      " [-9.82847021e-03 -5.74874810e-02  6.73159512e-02]\n",
      " [ 8.39352536e-04 -5.26643287e-04 -3.12709249e-04]\n",
      " [-6.61567867e-02 -6.41443022e-02  1.30301089e-01]\n",
      " [ 1.09272327e-02 -1.17002311e-02  7.72998363e-04]\n",
      " [ 3.37780006e-03  4.26485690e-02 -4.60263690e-02]\n",
      " [ 2.66122879e-01 -1.95003851e-01 -7.11190286e-02]\n",
      " [ 3.41567458e-02 -6.27207950e-02  2.85640493e-02]\n",
      " [-2.00463029e-01 -1.04331973e-01  3.04795002e-01]\n",
      " [-7.86082842e-02  9.89804419e-02 -2.03721578e-02]\n",
      " [ 2.25155195e-01 -1.26044910e-01 -9.91102850e-02]\n",
      " [ 2.08446239e-01 -3.68251666e-02 -1.71621072e-01]\n",
      " [ 1.35029431e-02 -2.47305022e-03 -1.10298929e-02]\n",
      " [ 2.83935074e-01  3.21062909e-04 -2.84256137e-01]\n",
      " [-5.03182959e-02  4.87040867e-02  1.61420923e-03]\n",
      " [-1.38272162e-01  4.38303998e-02  9.44417624e-02]\n",
      " [-1.91987063e-01  1.62499478e-02  1.75737115e-01]\n",
      " [ 1.96642276e-02  1.32912559e-01 -1.52576787e-01]\n",
      " [-2.00039124e-01  1.20742616e-01  7.92965077e-02]\n",
      " [-9.53628335e-03  4.45198528e-02 -3.49835695e-02]\n",
      " [-1.95066271e-01  1.66288347e-01  2.87779244e-02]\n",
      " [-1.71185527e-01 -1.81334572e-01  3.52520099e-01]\n",
      " [ 2.98065501e-03 -2.27547574e-02  1.97741024e-02]\n",
      " [-1.19588140e-01  1.55061750e-01 -3.54736106e-02]\n",
      " [-2.56820239e-01  1.43715770e-04  2.56676523e-01]\n",
      " [-1.15737210e-02  6.13675736e-01 -6.02102015e-01]\n",
      " [ 2.97857142e-01 -4.21006972e-01  1.23149830e-01]\n",
      " [-6.65518105e-02  9.82790730e-02 -3.17272625e-02]\n",
      " [-3.76676652e-01 -3.11733967e-01  6.88410619e-01]\n",
      " [ 7.21201713e-02 -3.53801149e-01  2.81680977e-01]\n",
      " [ 8.00723810e-02  7.21370691e-02 -1.52209450e-01]\n",
      " [ 1.33837789e-02 -5.25814919e-02  3.91977130e-02]\n",
      " [ 1.24160971e-01 -1.51137624e-01  2.69766526e-02]\n",
      " [ 1.04922613e-01 -5.88171795e-02 -4.61054334e-02]\n",
      " [ 4.20842638e-01  2.25167187e-01 -6.46009825e-01]\n",
      " [ 2.30101886e-01  7.90403848e-02 -3.09142270e-01]\n",
      " [ 5.89163586e-02 -1.02294188e-01  4.33778290e-02]\n",
      " [ 8.05272336e-02  3.43959173e-03 -8.39668253e-02]\n",
      " [-2.14538497e-01  1.68601721e-01  4.59367754e-02]\n",
      " [ 4.70648896e-02  1.89870085e-02 -6.60518981e-02]\n",
      " [ 2.76295015e-01 -1.76655546e-01 -9.96394699e-02]\n",
      " [ 9.42947588e-02 -6.89994409e-02 -2.52953179e-02]\n",
      " [ 2.84221373e-04 -1.95390747e-04 -8.88306261e-05]\n",
      " [ 8.09667067e-02 -7.54606010e-02 -5.50610568e-03]\n",
      " [ 1.80466336e-02 -1.71905936e-02 -8.56040027e-04]\n",
      " [-7.39294418e-04 -1.51925993e-03  2.25855434e-03]\n",
      " [-9.86509994e-02  6.17024006e-02  3.69485987e-02]\n",
      " [ 2.76338956e-03 -2.61465995e-03 -1.48729609e-04]\n",
      " [ 4.27776806e-04 -2.56106645e-04 -1.71670162e-04]\n",
      " [ 1.60632762e-01 -1.47331680e-01 -1.33010814e-02]\n",
      " [-2.23897287e-02  5.96479259e-02 -3.72581972e-02]\n",
      " [-1.26413643e-01  7.90524068e-02  4.73612362e-02]\n",
      " [ 1.03678409e-01 -7.14540729e-02 -3.22243357e-02]\n",
      " [-1.78044937e-01  7.88370481e-02  9.92078893e-02]\n",
      " [-4.23387016e-03 -4.00575580e-03  8.23962596e-03]\n",
      " [-4.34254779e-02  6.10927834e-02 -1.76673055e-02]\n",
      " [-1.20670094e-01  8.87209325e-02  3.19491612e-02]\n",
      " [ 1.45919694e-01  7.64264816e-02 -2.22346176e-01]\n",
      " [-1.76787548e-01  1.75594433e-01  1.19311525e-03]\n",
      " [-1.18043482e-02  6.19956986e-02 -5.01913504e-02]\n",
      " [ 4.06734274e-03 -3.29899214e-03 -7.68350602e-04]\n",
      " [-1.59161329e-03 -1.75871577e-03  3.35032906e-03]\n",
      " [ 5.35689401e-02 -1.17306686e-02 -4.18382714e-02]\n",
      " [ 5.10234181e-02 -3.97561289e-02 -1.12672892e-02]\n",
      " [ 9.14100605e-02 -4.39439395e-02 -4.74661210e-02]\n",
      " [ 1.94895502e-03 -1.57075275e-03 -3.78202270e-04]\n",
      " [-5.41626549e-02  7.96042272e-02 -2.54415723e-02]\n",
      " [-1.36043838e-03 -2.58798794e-03  3.94842632e-03]\n",
      " [-2.23701192e-02 -1.97995669e-02  4.21696861e-02]\n",
      " [ 2.64217512e-02 -1.53325570e-02 -1.10891942e-02]\n",
      " [ 1.66117330e-01 -2.95312830e-01  1.29195500e-01]\n",
      " [ 2.86510771e-02 -2.65853939e-02 -2.06568320e-03]\n",
      " [ 1.99880042e-02 -1.59361025e-02 -4.05190169e-03]\n",
      " [-2.38306268e-02 -5.70057912e-02  8.08364181e-02]\n",
      " [-4.77657457e-02 -1.60527818e-02  6.38185275e-02]\n",
      " [ 1.99026884e-02  5.34468852e-03 -2.52473769e-02]\n",
      " [ 4.22127364e-02 -3.33492700e-02 -8.86346643e-03]\n",
      " [ 4.43785872e-03 -1.59269347e-02  1.14890760e-02]\n",
      " [-1.20825835e-02 -2.75859054e-02  3.96684890e-02]\n",
      " [-3.75014773e-04 -2.00995219e-02  2.04745367e-02]\n",
      " [ 1.61658554e-02  5.02648449e-02 -6.64307003e-02]\n",
      " [ 6.16396351e-02  1.97239519e-01 -2.58879154e-01]\n",
      " [-6.78438185e-02  1.85031832e-01 -1.17188014e-01]\n",
      " [ 3.12077234e-02 -1.67615935e-01  1.36408212e-01]\n",
      " [-2.69255623e-02 -2.94480452e-02  5.63736075e-02]\n",
      " [ 5.63235190e-03 -1.26414247e-02  7.00907283e-03]\n",
      " [ 5.91986739e-02 -1.60217858e-02 -4.31768881e-02]\n",
      " [ 1.83133264e-02 -5.02962246e-02  3.19828981e-02]\n",
      " [ 5.23740572e-02 -1.35508553e-01  8.31344959e-02]\n",
      " [ 1.86316685e-02 -1.48962300e-02 -3.73543846e-03]\n",
      " [-6.74758177e-02  5.02612205e-02  1.72145972e-02]\n",
      " [ 1.56392803e-03 -1.37843070e-03 -1.85497325e-04]\n",
      " [ 5.28399250e-02 -3.60428840e-02 -1.67970409e-02]\n",
      " [ 4.06734274e-03 -3.29899214e-03 -7.68350602e-04]\n",
      " [ 1.96081296e-03 -1.88599301e-03 -7.48199465e-05]\n",
      " [-7.25066184e-03  4.47545111e-02 -3.75038492e-02]\n",
      " [ 1.04089948e-01 -2.23543647e-01  1.19453700e-01]\n",
      " [ 2.86510771e-02 -2.65853939e-02 -2.06568320e-03]\n",
      " [ 3.24642108e-02 -4.74416123e-02  1.49774015e-02]\n",
      " [-4.45209342e-02  2.30125155e-02  2.15084187e-02]\n",
      " [-4.29127519e-02 -2.80190727e-02  7.09318246e-02]\n",
      " [-8.73561280e-02  1.60725615e-01 -7.33694867e-02]\n",
      " [ 8.07596894e-02 -1.69549209e-02 -6.38047685e-02]\n",
      " [-3.99072573e-02  4.03892461e-02 -4.81988790e-04]\n",
      " [-1.92644631e-02 -1.36044803e-02  3.28689434e-02]\n",
      " [-3.10688374e-02 -2.15776009e-02  5.26464382e-02]\n",
      " [ 4.40959877e-03 -3.77267327e-03 -6.36925491e-04]\n",
      " [ 3.53544383e-01 -4.11685521e-01  5.81411384e-02]\n",
      " [ 1.24946336e-01 -1.96490738e-01  7.15444027e-02]\n",
      " [-6.20598664e-02  1.36473200e-01 -7.44133334e-02]\n",
      " [ 1.67262951e-01  1.17464702e-01 -2.84727653e-01]\n",
      " [-3.25898353e-02 -3.17588706e-04  3.29074240e-02]\n",
      " [ 2.08134794e-01 -1.82671659e-01 -2.54631345e-02]\n",
      " [-1.24270215e-03 -2.09720873e-01  2.10963575e-01]\n",
      " [-5.71805746e-02  2.22293096e-02  3.49512651e-02]\n",
      " [ 1.19960256e-01  3.12393124e-02 -1.51199568e-01]\n",
      " [-8.85070025e-02  7.20514621e-02  1.64555404e-02]\n",
      " [-4.79228288e-02  3.00010328e-02  1.79217960e-02]\n",
      " [ 2.34044467e-01 -2.50628988e-01  1.65845216e-02]\n",
      " [-8.84135325e-02  7.73770863e-02  1.10364462e-02]\n",
      " [-2.09128220e-02  3.15700020e-02 -1.06571800e-02]\n",
      " [-9.42526158e-03 -2.58018808e-02  3.52271424e-02]\n",
      " [-1.51137561e-02  4.86667350e-02 -3.35529789e-02]\n",
      " [-1.70454669e-02  1.10945118e-01 -9.38996514e-02]\n",
      " [-6.25425660e-02  8.74007637e-02 -2.48581977e-02]\n",
      " [-5.18704216e-02  5.51267562e-02 -3.25633463e-03]\n",
      " [-6.82365790e-02  6.91453893e-02 -9.08810267e-04]\n",
      " [-9.68234153e-03 -2.08412490e-02  3.05235906e-02]\n",
      " [ 8.93834814e-03 -1.91246920e-02  1.01863439e-02]\n",
      " [-2.24154347e-01  1.70572449e-01  5.35818982e-02]\n",
      " [-8.51790276e-02  1.45004505e-01 -5.98254777e-02]\n",
      " [-5.58561446e-04 -3.50413657e-02  3.55999271e-02]\n",
      " [-1.34750448e-01  4.82021362e-02  8.65483119e-02]\n",
      " [ 1.96323664e-01 -3.45600872e-01  1.49277208e-01]\n",
      " [-3.69434063e-02 -9.79201053e-02  1.34863512e-01]\n",
      " [-1.01533559e-01  1.99786540e-02  8.15549050e-02]\n",
      " [-6.52697431e-02 -2.16606246e-01  2.81875989e-01]\n",
      " [ 2.38803710e-01  4.93225404e-02 -2.88126251e-01]\n",
      " [-3.21659524e-01  1.50740901e-01  1.70918623e-01]\n",
      " [-2.28594568e-01  2.44510150e-01 -1.59155814e-02]\n",
      " [ 1.61465707e-01  9.98566320e-03 -1.71451370e-01]\n",
      " [ 3.37486997e-02 -9.32430108e-02  5.94943111e-02]\n",
      " [ 3.72962555e-01 -2.26397423e-01 -1.46565132e-01]\n",
      " [ 3.29549542e-01 -3.58663848e-02 -2.93683157e-01]\n",
      " [ 2.64002916e-01 -2.06711834e-01 -5.72910823e-02]\n",
      " [-9.27783579e-02  7.39893760e-02  1.87889819e-02]\n",
      " [-1.05107708e-02 -2.04938517e-02  3.10046225e-02]\n",
      " [-2.04094185e-03 -3.02712175e-03  5.06806360e-03]\n",
      " [-5.26162934e-03  6.18079755e-02 -5.65463462e-02]\n",
      " [-7.29807011e-02  6.14663528e-02  1.15143483e-02]\n",
      " [-7.04688385e-03 -3.56991302e-02  4.27460140e-02]\n",
      " [-8.66771291e-03  5.48682918e-02 -4.62005789e-02]\n",
      " [-1.35920709e-02  3.76370003e-02 -2.40449293e-02]\n",
      " [-1.12744067e-02 -2.75400707e-02  3.88144774e-02]\n",
      " [-1.47523763e-01  1.54288765e-01 -6.76500222e-03]\n",
      " [-7.19517264e-02  8.49947246e-02 -1.30429982e-02]\n",
      " [-2.73029714e-02  2.40590952e-02  3.24387622e-03]\n",
      " [-1.92317163e-02  3.47771418e-02 -1.55454255e-02]\n",
      " [-1.71046017e-02  4.93686660e-02 -3.22640643e-02]\n",
      " [-3.19474272e-03 -7.73548503e-03  1.09302278e-02]\n",
      " [-4.84395074e-02  1.97094401e-02  2.87300673e-02]\n",
      " [-1.29517743e-01  1.23261009e-01  6.25673424e-03]\n",
      " [-1.49921532e-02 -3.67788071e-02  5.17709604e-02]\n",
      " [-4.94716053e-02  5.02854208e-02 -8.13815518e-04]\n",
      " [ 1.88915197e-01 -1.49140017e-01 -3.97751808e-02]\n",
      " [ 1.20768628e-01 -9.16624985e-02 -2.91061297e-02]\n",
      " [ 1.86749640e-02 -7.19209735e-03 -1.14828667e-02]\n",
      " [-1.58914174e-01 -1.52233289e-01  3.11147462e-01]\n",
      " [ 2.98357766e-01  5.33786930e-02 -3.51736459e-01]\n",
      " [ 6.99088191e-01 -1.21312438e-01 -5.77775753e-01]\n",
      " [-5.59644599e-01  2.24578420e-02  5.37186757e-01]\n",
      " [-2.41662641e-02 -2.59017938e-02  5.00680578e-02]\n",
      " [ 1.63609856e-01 -7.29528020e-02 -9.06570542e-02]\n",
      " [ 1.73552978e-01  8.37961952e-02 -2.57349173e-01]\n",
      " [-3.41093857e-02 -1.82650791e-01  2.16760177e-01]]\n"
     ]
    }
   ],
   "execution_count": 257
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "hAfter few tries we got new sets of features, which is smaller that entry features set, and now score after CV is slightly better.",
   "id": "b130519f3688db3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Moving to the next steps, we are going with:\n",
    "\n",
    "- smaller dataset\n",
    "- `modified_features_preprocessor` preprocessor with `PolynomialFeatures`\n",
    "- `X_train`, `X_test`, `y_train`, `y_test`\n"
   ],
   "id": "397e45fc42d2136"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "III. L1, L2 regularization\n",
    "importance of the input\n",
    "\n",
    "Regularization method are applied to regression model because we want to have more control on weights in our model, because weights are responsible for features importance in\n",
    "training model. So we can use regularization techniques to work on these  weights. We to penalize model for to high weights.\n",
    "\n",
    "- `L1 (lasso)`\n",
    "$\\lambda \\sum_{i,j} |W_{ij}|$\n",
    "\n",
    "This method are going to even make some features zero by zero weight to eliminate unnecessary feautres\n",
    "- `L2 (ridge)`\n",
    "$\\lambda \\sum_{i,j} W_{ij}^2$\n",
    "\n",
    "This method are going to penalize model more significant because of square"
   ],
   "id": "16cf4e852321f817"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:52:01.349431Z",
     "start_time": "2025-05-21T16:52:01.344018Z"
    }
   },
   "cell_type": "code",
   "source": "#X_train_proc,X_test_proc,y_train_proc,y_test_proc = transform_in_pipeline(polynomial_preprocessor,X_train, X_test, y_train, y_test)",
   "id": "70aa65765ac642de",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:10:16.126728Z",
     "start_time": "2025-05-22T18:10:16.109607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaseLogisticRegressionRegL1(LogisticRegression):\n",
    "    def __init__(self,n_iters=5000, lr=0.001, batch_size=64,l=0.001):\n",
    "        super().__init__(lr, n_iters, batch_size)\n",
    "        self.l = l\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        m_samples, n_features = X.shape\n",
    "        self.n_classes = y.shape[1]\n",
    "        self.weights = np.zeros((n_features, self.n_classes))\n",
    "        self.bias = np.zeros((1, self.n_classes))\n",
    "        self.errors.clear()\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            indices = np.random.permutation(m_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for start_idx in range(0, m_samples, self.batch_size):\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                predict = self.predict(X_batch)\n",
    "                dw = (1 / X_batch.shape[0]) * np.dot(X_batch.T, (predict - y_batch))\n",
    "\n",
    "                dw+=self.l * np.sign(self.weights)\n",
    "\n",
    "                db = (1 / X_batch.shape[0]) * np.sum(predict - y_batch, axis=0, keepdims=True)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "            full_predict = self.predict(X)\n",
    "            loss = self.error_function(y, full_predict, m_samples)\n",
    "            self.errors.append(loss)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "\n"
   ],
   "id": "7e0d234db830e04e",
   "outputs": [],
   "execution_count": 238
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:10:17.525961Z",
     "start_time": "2025-05-22T18:10:17.506282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaseLogisticRegressionRegL2(LogisticRegression):\n",
    "    def __init__(self,n_iters=5000, lr=0.001, batch_size=64,l=0.001):\n",
    "        super().__init__(lr, n_iters, batch_size)\n",
    "        self.l = l\n",
    "    def fit(self,X,y):\n",
    "        m_samples, n_features = X.shape\n",
    "        self.n_classes = y.shape[1]\n",
    "        self.weights = np.zeros((n_features, self.n_classes))\n",
    "        self.bias = np.zeros((1, self.n_classes))\n",
    "        self.errors.clear()\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            indices = np.random.permutation(m_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for start_idx in range(0, m_samples, self.batch_size):\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                predict = self.predict(X_batch)\n",
    "                dw = (1 / X_batch.shape[0]) * np.dot(X_batch.T, (predict - y_batch))\n",
    "\n",
    "                dw += self.l * 2 * self.weights\n",
    "\n",
    "                db = (1 / X_batch.shape[0]) * np.sum(predict - y_batch, axis=0, keepdims=True)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "            full_predict = self.predict(X)\n",
    "            loss = self.error_function(y, full_predict, m_samples)\n",
    "            self.errors.append(loss)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "\n"
   ],
   "id": "1dda45975b476039",
   "outputs": [],
   "execution_count": 239
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:13:00.999562Z",
     "start_time": "2025-05-22T18:10:19.596193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logistic_regression_L1_reg = BaseLogisticRegressionRegL1(n_iters=5000, lr=0.001, batch_size=64,l=0.001)\n",
    "print(make_cross_validation_predict(logistic_regression_L1_reg, modified_features_preprocessor, X_train, y_train))\n",
    "print(logistic_regression_L1_reg.weights)"
   ],
   "id": "2f5e426eb3651a87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0025\n",
      "Iteration 100, Loss: 0.6145\n",
      "Iteration 200, Loss: 0.5860\n",
      "Iteration 300, Loss: 0.5710\n",
      "Iteration 400, Loss: 0.5612\n",
      "Iteration 500, Loss: 0.5541\n",
      "Iteration 600, Loss: 0.5486\n",
      "Iteration 700, Loss: 0.5442\n",
      "Iteration 800, Loss: 0.5405\n",
      "Iteration 900, Loss: 0.5374\n",
      "Iteration 1000, Loss: 0.5346\n",
      "Iteration 1100, Loss: 0.5322\n",
      "Iteration 1200, Loss: 0.5301\n",
      "Iteration 1300, Loss: 0.5282\n",
      "Iteration 1400, Loss: 0.5264\n",
      "Iteration 1500, Loss: 0.5248\n",
      "Iteration 1600, Loss: 0.5233\n",
      "Iteration 1700, Loss: 0.5219\n",
      "Iteration 1800, Loss: 0.5207\n",
      "Iteration 1900, Loss: 0.5194\n",
      "Iteration 2000, Loss: 0.5183\n",
      "Iteration 2100, Loss: 0.5172\n",
      "Iteration 2200, Loss: 0.5162\n",
      "Iteration 2300, Loss: 0.5153\n",
      "Iteration 2400, Loss: 0.5144\n",
      "Iteration 2500, Loss: 0.5136\n",
      "Iteration 2600, Loss: 0.5128\n",
      "Iteration 2700, Loss: 0.5120\n",
      "Iteration 2800, Loss: 0.5112\n",
      "Iteration 2900, Loss: 0.5106\n",
      "Iteration 3000, Loss: 0.5099\n",
      "Iteration 3100, Loss: 0.5092\n",
      "Iteration 3200, Loss: 0.5086\n",
      "Iteration 3300, Loss: 0.5080\n",
      "Iteration 3400, Loss: 0.5075\n",
      "Iteration 3500, Loss: 0.5069\n",
      "Iteration 3600, Loss: 0.5064\n",
      "Iteration 3700, Loss: 0.5059\n",
      "Iteration 3800, Loss: 0.5054\n",
      "Iteration 3900, Loss: 0.5049\n",
      "Iteration 4000, Loss: 0.5045\n",
      "Iteration 4100, Loss: 0.5040\n",
      "Iteration 4200, Loss: 0.5036\n",
      "Iteration 4300, Loss: 0.5032\n",
      "Iteration 4400, Loss: 0.5028\n",
      "Iteration 4500, Loss: 0.5025\n",
      "Iteration 4600, Loss: 0.5021\n",
      "Iteration 4700, Loss: 0.5017\n",
      "Iteration 4800, Loss: 0.5014\n",
      "Iteration 4900, Loss: 0.5010\n",
      "Iteration 0, Loss: 1.0057\n",
      "Iteration 100, Loss: 0.6184\n",
      "Iteration 200, Loss: 0.5888\n",
      "Iteration 300, Loss: 0.5728\n",
      "Iteration 400, Loss: 0.5624\n",
      "Iteration 500, Loss: 0.5547\n",
      "Iteration 600, Loss: 0.5488\n",
      "Iteration 700, Loss: 0.5440\n",
      "Iteration 800, Loss: 0.5400\n",
      "Iteration 900, Loss: 0.5366\n",
      "Iteration 1000, Loss: 0.5337\n",
      "Iteration 1100, Loss: 0.5311\n",
      "Iteration 1200, Loss: 0.5288\n",
      "Iteration 1300, Loss: 0.5268\n",
      "Iteration 1400, Loss: 0.5249\n",
      "Iteration 1500, Loss: 0.5232\n",
      "Iteration 1600, Loss: 0.5216\n",
      "Iteration 1700, Loss: 0.5202\n",
      "Iteration 1800, Loss: 0.5188\n",
      "Iteration 1900, Loss: 0.5176\n",
      "Iteration 2000, Loss: 0.5164\n",
      "Iteration 2100, Loss: 0.5153\n",
      "Iteration 2200, Loss: 0.5143\n",
      "Iteration 2300, Loss: 0.5133\n",
      "Iteration 2400, Loss: 0.5124\n",
      "Iteration 2500, Loss: 0.5115\n",
      "Iteration 2600, Loss: 0.5107\n",
      "Iteration 2700, Loss: 0.5099\n",
      "Iteration 2800, Loss: 0.5092\n",
      "Iteration 2900, Loss: 0.5085\n",
      "Iteration 3000, Loss: 0.5078\n",
      "Iteration 3100, Loss: 0.5071\n",
      "Iteration 3200, Loss: 0.5065\n",
      "Iteration 3300, Loss: 0.5059\n",
      "Iteration 3400, Loss: 0.5054\n",
      "Iteration 3500, Loss: 0.5048\n",
      "Iteration 3600, Loss: 0.5043\n",
      "Iteration 3700, Loss: 0.5038\n",
      "Iteration 3800, Loss: 0.5033\n",
      "Iteration 3900, Loss: 0.5029\n",
      "Iteration 4000, Loss: 0.5024\n",
      "Iteration 4100, Loss: 0.5020\n",
      "Iteration 4200, Loss: 0.5016\n",
      "Iteration 4300, Loss: 0.5012\n",
      "Iteration 4400, Loss: 0.5008\n",
      "Iteration 4500, Loss: 0.5004\n",
      "Iteration 4600, Loss: 0.5001\n",
      "Iteration 4700, Loss: 0.4997\n",
      "Iteration 4800, Loss: 0.4994\n",
      "Iteration 4900, Loss: 0.4990\n",
      "Iteration 0, Loss: 0.9963\n",
      "Iteration 100, Loss: 0.6131\n",
      "Iteration 200, Loss: 0.5877\n",
      "Iteration 300, Loss: 0.5739\n",
      "Iteration 400, Loss: 0.5647\n",
      "Iteration 500, Loss: 0.5577\n",
      "Iteration 600, Loss: 0.5521\n",
      "Iteration 700, Loss: 0.5475\n",
      "Iteration 800, Loss: 0.5436\n",
      "Iteration 900, Loss: 0.5403\n",
      "Iteration 1000, Loss: 0.5373\n",
      "Iteration 1100, Loss: 0.5347\n",
      "Iteration 1200, Loss: 0.5322\n",
      "Iteration 1300, Loss: 0.5300\n",
      "Iteration 1400, Loss: 0.5280\n",
      "Iteration 1500, Loss: 0.5262\n",
      "Iteration 1600, Loss: 0.5244\n",
      "Iteration 1700, Loss: 0.5228\n",
      "Iteration 1800, Loss: 0.5214\n",
      "Iteration 1900, Loss: 0.5200\n",
      "Iteration 2000, Loss: 0.5187\n",
      "Iteration 2100, Loss: 0.5175\n",
      "Iteration 2200, Loss: 0.5163\n",
      "Iteration 2300, Loss: 0.5152\n",
      "Iteration 2400, Loss: 0.5142\n",
      "Iteration 2500, Loss: 0.5132\n",
      "Iteration 2600, Loss: 0.5123\n",
      "Iteration 2700, Loss: 0.5115\n",
      "Iteration 2800, Loss: 0.5106\n",
      "Iteration 2900, Loss: 0.5098\n",
      "Iteration 3000, Loss: 0.5091\n",
      "Iteration 3100, Loss: 0.5083\n",
      "Iteration 3200, Loss: 0.5076\n",
      "Iteration 3300, Loss: 0.5070\n",
      "Iteration 3400, Loss: 0.5063\n",
      "Iteration 3500, Loss: 0.5057\n",
      "Iteration 3600, Loss: 0.5051\n",
      "Iteration 3700, Loss: 0.5046\n",
      "Iteration 3800, Loss: 0.5040\n",
      "Iteration 3900, Loss: 0.5035\n",
      "Iteration 4000, Loss: 0.5030\n",
      "Iteration 4100, Loss: 0.5025\n",
      "Iteration 4200, Loss: 0.5020\n",
      "Iteration 4300, Loss: 0.5015\n",
      "Iteration 4400, Loss: 0.5011\n",
      "Iteration 4500, Loss: 0.5007\n",
      "Iteration 4600, Loss: 0.5003\n",
      "Iteration 4700, Loss: 0.4998\n",
      "Iteration 4800, Loss: 0.4994\n",
      "Iteration 4900, Loss: 0.4990\n",
      "([np.float64(0.7800387596899225), np.float64(0.7751937984496124), np.float64(0.7577519379844961)], np.float64(0.7709948320413437))\n",
      "[[-2.04165376e-04 -1.29303834e-02  1.31037549e-01]\n",
      " [ 3.01514444e-01 -9.11310761e-02 -4.54003678e-02]\n",
      " [-2.95560761e-05  3.95443787e-05 -3.45609883e-02]\n",
      " [ 1.72634835e-01  8.32776055e-05 -3.15003113e-01]\n",
      " [-1.08773235e-04  2.47829485e-01 -1.57971712e-01]\n",
      " [-1.71745589e-01  1.22390563e-05  2.77920350e-01]\n",
      " [-6.33252724e-05  5.26251806e-02 -4.65148554e-02]\n",
      " [ 6.34516110e-05 -1.85839525e-02  2.35500900e-04]\n",
      " [ 3.22893825e-02  8.75793143e-06 -6.03011404e-02]\n",
      " [ 4.11778780e-01  6.70355382e-05 -4.98676815e-01]\n",
      " [-3.38536451e-05  3.20118653e-01 -2.96405799e-01]\n",
      " [-4.43313096e-01  9.80586738e-05  4.35986038e-01]\n",
      " [ 6.29615181e-05 -4.75548565e-05  5.59333837e-06]\n",
      " [-1.71595729e-01  1.18529079e-02 -4.17902792e-06]\n",
      " [-9.29567693e-05  1.70285318e-05  1.22928238e-04]\n",
      " [-1.12598255e-04  1.10414963e-04  2.11832921e-05]\n",
      " [-9.36273191e-02 -9.78262523e-02  3.53224571e-01]\n",
      " [ 8.65245069e-02 -3.45123492e-02 -1.51577659e-05]\n",
      " [-3.76717921e-02  6.68827860e-05  7.50059093e-02]\n",
      " [-7.39518641e-07 -3.86270599e-03  8.44550580e-06]\n",
      " [-5.09505335e-05  1.91752728e-04 -4.66238022e-02]\n",
      " [ 1.01421840e-04  9.78528527e-02 -1.40675275e-01]\n",
      " [-2.27776746e-02 -3.84938433e-02  2.19990518e-01]\n",
      " [ 2.51275722e-01 -2.04171161e-02 -6.66476064e-02]\n",
      " [-1.24120117e-04 -3.72276280e-04  6.79633964e-02]\n",
      " [-8.19957087e-05  3.28473060e-02 -9.82310260e-04]\n",
      " [-3.00222784e-01  4.67280061e-02  9.00237784e-02]\n",
      " [-1.89634960e-02  1.37615307e-01 -1.30811001e-04]\n",
      " [ 3.39522924e-02 -5.34843036e-02 -6.98879643e-06]\n",
      " [ 2.24020463e-04  2.13578968e-02 -1.01592917e-01]\n",
      " [ 7.46022165e-03  7.02380189e-05 -1.35091460e-01]\n",
      " [ 5.58648829e-02  4.51596524e-05 -1.10885043e-01]\n",
      " [-3.39092032e-01  6.03971380e-02  1.27325894e-01]\n",
      " [ 4.52787879e-05  1.36365699e-04 -1.24644487e-04]\n",
      " [ 7.21385192e-02 -5.49337501e-02 -9.76906286e-06]\n",
      " [ 5.08297413e-05  6.17400203e-03 -1.14681832e-01]\n",
      " [ 1.14123786e-01 -1.79715295e-04 -7.07570704e-02]\n",
      " [ 1.44650676e-03  4.35547610e-05 -1.01891062e-01]\n",
      " [-6.35993878e-01  1.27448111e-01  3.51086767e-01]\n",
      " [ 1.75980301e-01 -5.62501847e-02 -2.11160530e-05]\n",
      " [-7.77645540e-02 -3.40167416e-05  1.53575708e-02]\n",
      " [ 1.88818808e-01 -6.59663326e-05 -7.53378414e-02]\n",
      " [-6.17533536e-05 -2.03457006e-06  5.67879237e-05]\n",
      " [-3.69054591e-05 -2.74596906e-02  1.67559601e-03]\n",
      " [-4.89529490e-05  7.54767939e-05 -2.28395238e-02]\n",
      " [ 4.53021355e-02  6.36522746e-05 -1.38882788e-01]\n",
      " [ 9.65966279e-03 -1.82851360e-01  1.47946973e-02]\n",
      " [ 1.20341569e-01 -1.61090383e-02  5.24688677e-05]\n",
      " [-9.63837427e-05 -6.65321439e-07  9.40490642e-05]\n",
      " [-8.82754034e-03  9.80738901e-05  4.44664509e-05]\n",
      " [-4.74723251e-05  3.41505686e-05 -1.65336782e-02]\n",
      " [ 2.33948541e-03 -7.71989790e-07 -1.77134172e-05]\n",
      " [ 2.24611671e-02  1.55746764e-05 -6.26917418e-02]\n",
      " [-8.16535528e-05  1.30169604e-05  6.56365924e-05]\n",
      " [-4.27488687e-02  5.80006972e-05  2.31867997e-04]\n",
      " [-1.16617360e-04  9.07842495e-06  2.98205389e-02]\n",
      " [-2.38266985e-05  1.55591224e-04 -9.82047645e-02]\n",
      " [ 2.42837291e-05 -1.33488408e-02  3.75555705e-03]\n",
      " [-6.30776219e-05 -3.04477854e-05  9.05254073e-05]\n",
      " [-5.12720109e-05  1.00698664e-05  3.42021445e-05]\n",
      " [-1.00307693e-01  1.06242985e-01  1.57082349e-05]\n",
      " [-1.64445299e-03  2.35096012e-05  4.25539434e-02]\n",
      " [ 1.15061551e-05  1.00309254e-05 -1.05370805e-05]\n",
      " [-2.75556165e-05  2.11087817e-05 -2.45531653e-05]\n",
      " [-3.34419924e-05 -8.38791789e-02  1.04695621e-01]\n",
      " [-2.30389081e-02  6.19497807e-06  1.33027713e-01]\n",
      " [ 1.07846851e-05  3.34272389e-03 -1.33084509e-01]\n",
      " [ 1.49311860e-05 -3.31426498e-02  2.16807186e-02]\n",
      " [-5.25915060e-05 -1.71814941e-05  8.67730001e-05]\n",
      " [-3.76539408e-03  9.26302022e-05  2.17638760e-05]\n",
      " [ 1.85418966e-05 -6.53842981e-02  8.82075616e-03]\n",
      " [-1.40677945e-01  8.21156948e-05  1.23844829e-01]\n",
      " [ 3.66034717e-05 -1.71581729e-02  1.85694421e-05]\n",
      " [ 3.08398458e-02 -2.96054732e-02 -5.13725809e-05]\n",
      " [-7.50744646e-05 -2.44925256e-05  9.47665670e-02]\n",
      " [-7.89996001e-06 -1.95313453e-01  2.33584353e-01]\n",
      " [ 1.77200016e-05 -9.86043525e-06 -8.59566325e-07]\n",
      " [-5.35815839e-05  1.60650597e-05  2.65165242e-05]\n",
      " [-9.00009456e-05  1.79904812e-06  2.36412019e-02]\n",
      " [-1.22930537e-01  4.86769516e-05  1.29578860e-01]\n",
      " [ 4.60407192e-06 -8.36416118e-02  1.11342008e-01]\n",
      " [-5.46787948e-02  7.23877752e-02 -3.98035965e-06]\n",
      " [-1.25601885e-01 -9.01840668e-02  3.80784951e-01]\n",
      " [ 1.76677083e-02  8.45133353e-05 -1.21221678e-04]\n",
      " [ 1.62939714e-02 -9.40652961e-06 -3.25356485e-03]\n",
      " [ 1.28091903e-01  1.19772020e-05 -1.55562881e-01]\n",
      " [ 3.16435875e-02  1.53608508e-04 -1.36520196e-01]\n",
      " [-4.26011773e-06  6.60965766e-02 -1.77489317e-01]\n",
      " [-1.10431398e-01 -2.61461586e-02  3.01576556e-01]\n",
      " [-3.44809833e-02  4.73575679e-03  1.62265132e-05]\n",
      " [-8.89411910e-02  1.96497975e-04  4.60369304e-03]\n",
      " [-2.43565690e-02  5.45746324e-06  3.60281116e-02]\n",
      " [ 1.29939313e-05 -1.57290042e-01  7.35240484e-02]\n",
      " [ 2.65240598e-05 -5.81211956e-05  1.08805971e-02]\n",
      " [ 2.56096836e-05 -4.24332755e-03  1.80307179e-02]\n",
      " [ 6.84213766e-06 -1.89589815e-05  1.69511684e-03]\n",
      " [-8.77913574e-03 -2.03682359e-06  1.29821726e-02]\n",
      " [-1.14284835e-01  3.25414055e-02  1.92429327e-04]\n",
      " [ 1.37388483e-02 -1.75051789e-02  1.21330563e-04]\n",
      " [-7.72825880e-05 -1.29771268e-05  7.12597148e-05]\n",
      " [-8.70175222e-05  2.16376503e-03  8.25249354e-06]\n",
      " [-4.29106754e-05  2.83147683e-05 -5.16984041e-02]\n",
      " [ 3.71779744e-02 -8.10141521e-07 -1.61642090e-05]\n",
      " [ 1.16644742e-01 -3.63789237e-05 -3.14053633e-02]\n",
      " [ 1.58704234e-01 -2.08343264e-06 -1.29981151e-01]\n",
      " [-8.05616601e-05  5.57150586e-02 -6.74969713e-05]\n",
      " [-3.33817306e-06  3.46545206e-05 -2.23163476e-05]\n",
      " [ 2.09969470e-05  1.03690185e-05 -2.03659655e-05]\n",
      " [ 5.64980700e-05  2.11468033e-06 -5.16127503e-05]\n",
      " [-2.90893298e-05 -1.02292099e-01  1.07754188e-01]\n",
      " [-4.39086257e-05  9.26513933e-05 -5.37427675e-05]\n",
      " [ 2.11759183e-02 -2.70885455e-02  5.62713481e-06]\n",
      " [ 1.11773181e-01 -4.90484696e-02 -8.97118746e-05]\n",
      " [ 3.99645802e-02 -1.39059095e-01  5.79055144e-02]\n",
      " [-1.43573840e-05  1.08308859e-06  1.42742954e-05]\n",
      " [-1.46187184e-01 -2.15794487e-01  5.26980671e-01]\n",
      " [ 1.06446780e-01 -1.55186525e-02 -7.12719486e-06]\n",
      " [ 2.20183668e-06 -2.91408454e-04  3.94206617e-04]\n",
      " [-4.63280096e-06 -7.16165935e-02  1.81178226e-01]\n",
      " [ 2.28782290e-02  7.28554465e-03 -1.77902774e-01]\n",
      " [ 1.06677655e-01 -8.99223662e-02 -6.28866674e-06]\n",
      " [ 4.25908184e-07  9.61330894e-07  6.12760922e-07]\n",
      " [-1.39574769e-05  1.79399621e-05 -9.82485166e-07]\n",
      " [-6.94581012e-06 -8.10629108e-07  4.92387564e-02]\n",
      " [ 7.84352405e-07  7.00731949e-07  5.14915646e-07]\n",
      " [-8.32836916e-02  7.30919302e-05  3.76255996e-02]\n",
      " [-6.87295629e-07 -6.77047408e-07  3.64343037e-07]\n",
      " [-2.53767184e-07  1.27918748e-05  4.61892426e-07]\n",
      " [ 5.84971152e-03 -1.10817795e-01  7.08310126e-06]\n",
      " [ 2.72387267e-07 -2.42556059e-06  1.53173321e-07]\n",
      " [-6.11995519e-02 -1.47700256e-05  2.14740322e-01]\n",
      " [ 6.40665168e-06  7.25936792e-06 -9.66601960e-06]\n",
      " [ 2.18979866e-01 -3.71978387e-05 -8.49116681e-02]\n",
      " [ 1.36455540e-05 -2.48633914e-05  6.21783743e-06]\n",
      " [ 3.63941983e-07 -1.90808714e-07  8.26866730e-07]\n",
      " [ 4.36528224e-01 -1.36369159e-05 -2.78023587e-01]\n",
      " [ 1.29149111e-01 -4.59029896e-03 -4.81223116e-06]\n",
      " [-7.36790702e-02  2.64513280e-01 -2.96452097e-02]\n",
      " [-4.19069362e-02 -1.33815193e-05  1.14323318e-01]\n",
      " [-1.67089615e-05  1.16875589e-05  2.14026344e-08]\n",
      " [-7.14505102e-02  4.30958569e-05  4.14298101e-07]\n",
      " [ 9.70550142e-07  3.40373502e-07 -3.10923643e-07]\n",
      " [-3.81997666e-02  8.73300258e-03  3.17640078e-05]\n",
      " [-8.20405926e-06 -1.29917328e-05  8.54521958e-02]\n",
      " [-1.57261175e-02  2.50257057e-07  1.48672834e-05]\n",
      " [-2.73391443e-03  4.92967520e-02 -3.83760428e-06]\n",
      " [-2.33276618e-01  3.51203117e-05  2.79968498e-01]\n",
      " [ 1.02854499e-05  2.58613950e-01 -3.55189236e-01]\n",
      " [ 1.92521109e-01 -4.64124583e-01  1.06614474e-01]\n",
      " [-1.31418673e-01  9.63280292e-02 -8.35621275e-06]\n",
      " [-3.38601130e-01 -1.39370154e-01  6.42968284e-01]\n",
      " [ 7.04885036e-02 -2.03173343e-01  1.98388801e-05]\n",
      " [ 1.15971752e-05  9.83204346e-03 -9.25386406e-02]\n",
      " [-6.18764619e-07  2.30241481e-02 -2.31045293e-02]\n",
      " [ 1.04657744e-01 -1.02424409e-01  2.56647941e-05]\n",
      " [ 1.43349974e-01 -9.31387037e-02 -4.02698251e-05]\n",
      " [ 3.26685139e-01  4.62497922e-02 -5.37489931e-01]\n",
      " [ 7.18691101e-03  6.97737248e-02 -2.38995636e-01]\n",
      " [-1.42930446e-05  3.57330912e-05 -4.31244005e-03]\n",
      " [ 3.44839115e-05 -5.42817633e-02  2.02793776e-05]\n",
      " [-8.05708379e-05  2.96572620e-02 -7.16911614e-05]\n",
      " [-7.45540279e-07  5.01013116e-06  7.35409122e-07]\n",
      " [ 7.41463265e-02 -1.82286313e-03 -8.46338004e-06]\n",
      " [ 1.78705927e-07  6.23964853e-07 -8.02670781e-07]\n",
      " [ 7.86586969e-07 -5.09417044e-07 -2.77169925e-07]\n",
      " [-1.34663230e-07  6.07177649e-07 -4.72514418e-07]\n",
      " [ 3.93025978e-07  1.53566678e-07 -5.46592655e-07]\n",
      " [-8.60594916e-07 -2.04907353e-07 -9.34497731e-07]\n",
      " [-9.11694870e-06  1.66219210e-05  4.95027737e-07]\n",
      " [-8.99064947e-07  8.78708328e-07  2.03566193e-08]\n",
      " [ 5.99851989e-02 -4.22809204e-02 -9.27851415e-06]\n",
      " [-7.61560211e-07  7.81784341e-06 -5.05628319e-06]\n",
      " [-8.34126104e-06  7.85998386e-07  6.55526266e-06]\n",
      " [-9.08909418e-08  1.70513052e-06 -2.61423958e-06]\n",
      " [-4.22575142e-02  2.50479965e-05 -5.33752146e-07]\n",
      " [-6.57221598e-07  9.53170737e-07 -2.95949139e-07]\n",
      " [-1.00563149e-01  1.11325565e-01 -3.14163151e-05]\n",
      " [-1.68142333e-02  1.61409114e-02 -6.78101694e-07]\n",
      " [ 5.22355623e-02  3.72012446e-05 -1.08113764e-01]\n",
      " [-1.26487317e-01  7.40431357e-02  1.31809143e-05]\n",
      " [-2.66955986e-07  1.36435448e-06 -9.73984936e-08]\n",
      " [-9.42989242e-07 -8.75272406e-07 -1.81738352e-07]\n",
      " [ 1.85125404e-08 -5.28765734e-07  5.10253194e-07]\n",
      " [ 4.18732048e-07 -2.34009556e-07  8.15277508e-07]\n",
      " [ 5.57215157e-07 -3.56718812e-08 -5.21543276e-07]\n",
      " [ 3.46085641e-02 -1.01054606e-01  5.04221738e-06]\n",
      " [-6.43756560e-07  6.09195617e-07  3.45609430e-08]\n",
      " [-9.40803986e-02  4.82160917e-02  7.30683602e-06]\n",
      " [-6.32153668e-07 -6.92251135e-07 -6.75595197e-07]\n",
      " [ 4.58469034e-07 -9.11186877e-07  4.52717842e-07]\n",
      " [-4.63440664e-07 -3.58445747e-07  8.21886412e-07]\n",
      " [ 2.37125414e-07 -5.23754668e-09  7.68112132e-07]\n",
      " [ 1.27613642e-01 -1.41555160e-01  5.17263367e-07]\n",
      " [ 1.22022399e-07  7.75895194e-07 -8.97917593e-07]\n",
      " [ 8.74383926e-07  4.35664822e-07  6.89951252e-07]\n",
      " [ 2.52185498e-04 -5.14331084e-02  3.59229129e-05]\n",
      " [ 2.39174177e-02 -3.27231623e-05 -3.96945400e-05]\n",
      " [ 1.28189086e-05 -1.16408903e-06 -1.06548196e-05]\n",
      " [ 6.97708121e-06  5.51071316e-07 -4.52815253e-06]\n",
      " [-4.17078243e-07 -9.69407530e-07  3.86485773e-07]\n",
      " [-6.48074026e-07 -8.57370647e-07 -4.94555328e-07]\n",
      " [-5.51856227e-07 -4.42327481e-07  9.94183708e-07]\n",
      " [ 7.67306287e-02  9.28458021e-05 -7.63064745e-02]\n",
      " [-5.17428160e-06  2.23631812e-01 -2.55853637e-01]\n",
      " [ 3.60837368e-07 -8.48140452e-05  1.99864532e-02]\n",
      " [ 7.64392099e-06 -5.68579535e-06  4.18743587e-08]\n",
      " [ 6.10598045e-06 -9.64846951e-06  1.54248906e-06]\n",
      " [-3.37409828e-07 -9.28997484e-08  1.43030958e-06]\n",
      " [-9.23171875e-06 -1.23470317e-05  2.05787504e-05]\n",
      " [-2.33406111e-06 -1.11905735e-02  1.39075858e-05]\n",
      " [-1.73662875e-07 -3.75362894e-07 -4.50974231e-07]\n",
      " [-5.41677795e-07 -4.77162290e-07 -9.81159914e-07]\n",
      " [-1.76360884e-01  8.56564987e-02  7.38568777e-06]\n",
      " [ 3.66292807e-07 -5.30299869e-07  1.64007061e-07]\n",
      " [ 3.06044817e-07  3.96474144e-08  6.54307768e-07]\n",
      " [ 9.47954706e-07  2.85195298e-07 -2.33150005e-07]\n",
      " [ 1.33466540e-06 -3.92092034e-07  5.74266325e-08]\n",
      " [ 4.46454775e-07  3.97757810e-07 -8.44212584e-07]\n",
      " [ 8.89400124e-07 -3.35771542e-08 -8.55822970e-07]\n",
      " [ 1.25881428e-01 -1.19860706e-01  2.77742873e-07]\n",
      " [ 1.22022399e-07  7.75895194e-07 -8.97917593e-07]\n",
      " [-5.44363941e-07  1.57972761e-07 -6.13608820e-07]\n",
      " [-6.09307567e-03  7.69931060e-02  1.49697094e-05]\n",
      " [-7.49204114e-06 -1.16239151e-02  1.06992407e-01]\n",
      " [-4.95390933e-07  5.78378767e-02 -3.81357184e-07]\n",
      " [ 8.21546643e-06  5.30607463e-07 -5.74607389e-06]\n",
      " [ 7.46955793e-08 -3.02547631e-07 -7.72147948e-07]\n",
      " [ 8.04546446e-07 -8.05594352e-07  1.04790661e-09]\n",
      " [ 8.00215641e-07  3.43481774e-07 -1.43697415e-07]\n",
      " [ 1.37495459e-01 -2.52817891e-01  2.54318364e-05]\n",
      " [ 5.38454316e-05 -1.19937993e-01 -8.52586957e-07]\n",
      " [-7.89895210e-02  7.78642322e-02 -3.17112775e-05]\n",
      " [ 8.86754525e-02  2.94916835e-02 -2.82376136e-01]\n",
      " [ 4.24346459e-06 -1.09996563e-01  3.31911461e-06]\n",
      " [ 8.95232243e-05 -1.81522388e-04 -1.00083605e-06]\n",
      " [-8.82229361e-06 -1.21462925e-01  2.04216747e-01]\n",
      " [ 3.19176414e-05  5.32960407e-03 -1.12796522e-01]\n",
      " [ 7.30114448e-07  1.47645675e-05 -8.92584947e-02]\n",
      " [-3.05396494e-02 -2.12920528e-05  9.26099414e-02]\n",
      " [-6.61358837e-08  3.91506139e-06 -1.84892551e-06]\n",
      " [ 8.17491979e-02 -1.37946055e-01  1.85706640e-06]\n",
      " [-5.51039459e-07 -7.27400650e-07 -7.21559892e-07]\n",
      " [ 8.38418325e-06 -5.33479903e-07 -5.85070335e-06]\n",
      " [ 3.00994638e-07  8.08718871e-07  8.90286491e-07]\n",
      " [-5.75649152e-07  9.21354656e-07  6.54294496e-07]\n",
      " [-5.51431090e-07  7.52350088e-06 -6.97206979e-06]\n",
      " [ 7.12350759e-07  1.65758786e-06 -3.69938619e-07]\n",
      " [-6.25519858e-07  3.48536676e-06  1.40153096e-07]\n",
      " [-6.67988406e-06  6.62232061e-06  5.75634463e-08]\n",
      " [-6.64921669e-07 -2.05018528e-07 -1.30059803e-07]\n",
      " [ 4.35715313e-07  2.24548924e-07 -6.60264237e-07]\n",
      " [ 7.26174835e-07  1.80957755e-07 -9.07132591e-07]\n",
      " [ 8.30224009e-07  7.08001451e-08 -9.01024154e-07]\n",
      " [ 7.13317664e-08 -8.56288888e-07  7.84957122e-07]\n",
      " [-7.20096990e-07  2.14701308e-07  5.05395682e-07]\n",
      " [-1.24311829e-01  7.16054061e-02 -5.77556112e-07]\n",
      " [-3.80422414e-07 -7.75877412e-07  1.15629983e-06]\n",
      " [-6.33161444e-07 -1.54723402e-07  7.87884846e-07]\n",
      " [-3.70283533e-07 -3.85197828e-06  7.22226182e-06]\n",
      " [ 1.16227253e-01 -2.08354119e-01  2.38658545e-05]\n",
      " [ 4.71260468e-04 -1.59352338e-06 -2.26669450e-05]\n",
      " [-2.42732193e-05  1.82717537e-05  6.00146552e-06]\n",
      " [ 3.60987327e-03 -1.72959876e-01  7.27900302e-03]\n",
      " [ 1.78358179e-01  5.33847071e-02 -3.95807886e-01]\n",
      " [-1.76523522e-01  7.04723418e-06  1.26789474e-01]\n",
      " [-1.94697710e-01  1.08892243e-01 -9.53317515e-06]\n",
      " [ 2.10843680e-01 -6.32213683e-06 -8.98003574e-02]\n",
      " [-1.19598190e-01  5.20388130e-05  1.91509252e-05]\n",
      " [ 2.13646299e-01 -1.68158659e-05 -1.12254483e-01]\n",
      " [ 3.00196945e-05 -9.59205028e-02  2.48094831e-02]\n",
      " [ 7.08644224e-03 -9.63130531e-02  3.61086022e-06]\n",
      " [ 5.76422784e-07  8.56583055e-07  5.66994161e-07]\n",
      " [ 8.28398751e-06 -8.59773981e-07 -5.42421353e-06]\n",
      " [-2.45250874e-07  9.21799673e-07 -6.76548800e-07]\n",
      " [-6.83274677e-07  6.22434529e-07 -9.39159852e-07]\n",
      " [ 9.86331826e-07  6.59345542e-06 -7.57978725e-06]\n",
      " [ 3.55723211e-07  5.62528400e-07  8.17483897e-08]\n",
      " [ 3.00994638e-07  8.08718871e-07  8.90286491e-07]\n",
      " [ 4.68482826e-07 -5.57939694e-06  6.11091412e-06]\n",
      " [-4.57425232e-07  1.72838801e-06  7.29037226e-07]\n",
      " [-6.97647770e-07 -2.59210990e-07  9.56858760e-07]\n",
      " [ 7.05253538e-07  4.70487403e-07 -1.75740940e-07]\n",
      " [ 9.36226205e-07  2.00321614e-07  8.63452181e-07]\n",
      " [ 4.02254468e-07  6.91094803e-08 -4.71363949e-07]\n",
      " [ 7.76071989e-08  1.34306956e-07 -2.11914155e-07]\n",
      " [-6.79674497e-06  1.28114190e-02 -6.22219311e-07]\n",
      " [-9.90564588e-08  5.48165963e-07  5.50890495e-07]\n",
      " [-2.16224679e-07  5.51628305e-07 -3.35403625e-07]\n",
      " [-5.76205638e-07 -1.63599315e-07 -2.60195047e-07]\n",
      " [-4.24075006e-07  4.97013835e-07  9.27061171e-07]\n",
      " [-8.46261874e-07  8.77186438e-07  9.69075436e-07]\n",
      " [ 9.77383031e-07  1.16118660e-07  9.06498309e-07]\n",
      " [ 3.03978748e-07 -9.45706884e-07 -3.58271863e-07]\n",
      " [ 4.72984490e-07  8.17199393e-07 -2.90183882e-07]\n",
      " [-1.68226508e-02  1.09496376e-01 -7.25431219e-07]\n",
      " [ 7.25040695e-07  5.81554523e-07 -3.06595218e-07]\n",
      " [-1.18968573e-04  1.97993390e-04 -3.40248169e-05]\n",
      " [ 7.88411594e-02 -7.41802359e-03  5.86414791e-06]\n",
      " [ 6.17908210e-03 -4.28941093e-02  1.40272098e-05]\n",
      " [ 1.10876765e-06  8.07911118e-06 -4.18787882e-06]\n",
      " [-1.26514382e-05 -1.23443249e-01  1.63588900e-01]\n",
      " [ 2.49988423e-02  1.58272188e-02 -1.99767061e-01]\n",
      " [ 7.74018749e-01 -1.16050847e-01 -4.92968901e-01]\n",
      " [-6.24166558e-01  1.09328172e-02  4.56790741e-01]\n",
      " [-1.69081297e-02  1.29907803e-04  4.25352219e-02]\n",
      " [ 1.66760321e-01 -1.47938001e-04 -7.99633826e-02]\n",
      " [ 1.46391278e-02  1.65541745e-02 -1.91102302e-01]\n",
      " [ 1.50630574e-05 -1.22858205e-01  1.52644142e-01]]\n"
     ]
    }
   ],
   "execution_count": 240
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:32:36.204558Z",
     "start_time": "2025-05-22T18:31:09.418100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logistic_regression_L2_reg = BaseLogisticRegressionRegL2(n_iters=5000, lr=0.001, batch_size=64,l=0.001)\n",
    "print(make_cross_validation_predict(logistic_regression_L2_reg, modified_features_preprocessor, X_train, y_train))\n",
    "print(logistic_regression_L2_reg.weights)\n"
   ],
   "id": "dcaaf6aafdf76b3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0017\n",
      "Iteration 100, Loss: 0.6003\n",
      "Iteration 200, Loss: 0.5704\n",
      "Iteration 300, Loss: 0.5541\n",
      "Iteration 400, Loss: 0.5430\n",
      "Iteration 500, Loss: 0.5348\n",
      "Iteration 600, Loss: 0.5282\n",
      "Iteration 700, Loss: 0.5228\n",
      "Iteration 800, Loss: 0.5182\n",
      "Iteration 900, Loss: 0.5142\n",
      "Iteration 1000, Loss: 0.5107\n",
      "Iteration 1100, Loss: 0.5076\n",
      "Iteration 1200, Loss: 0.5047\n",
      "Iteration 1300, Loss: 0.5022\n",
      "Iteration 1400, Loss: 0.4998\n",
      "Iteration 1500, Loss: 0.4976\n",
      "Iteration 1600, Loss: 0.4956\n",
      "Iteration 1700, Loss: 0.4937\n",
      "Iteration 1800, Loss: 0.4920\n",
      "Iteration 1900, Loss: 0.4903\n",
      "Iteration 2000, Loss: 0.4888\n",
      "Iteration 2100, Loss: 0.4873\n",
      "Iteration 2200, Loss: 0.4859\n",
      "Iteration 2300, Loss: 0.4846\n",
      "Iteration 2400, Loss: 0.4834\n",
      "Iteration 2500, Loss: 0.4822\n",
      "Iteration 2600, Loss: 0.4810\n",
      "Iteration 2700, Loss: 0.4800\n",
      "Iteration 2800, Loss: 0.4789\n",
      "Iteration 2900, Loss: 0.4780\n",
      "Iteration 3000, Loss: 0.4770\n",
      "Iteration 3100, Loss: 0.4761\n",
      "Iteration 3200, Loss: 0.4752\n",
      "Iteration 3300, Loss: 0.4744\n",
      "Iteration 3400, Loss: 0.4736\n",
      "Iteration 3500, Loss: 0.4728\n",
      "Iteration 3600, Loss: 0.4721\n",
      "Iteration 3700, Loss: 0.4713\n",
      "Iteration 3800, Loss: 0.4706\n",
      "Iteration 3900, Loss: 0.4700\n",
      "Iteration 4000, Loss: 0.4693\n",
      "Iteration 4100, Loss: 0.4687\n",
      "Iteration 4200, Loss: 0.4681\n",
      "Iteration 4300, Loss: 0.4675\n",
      "Iteration 4400, Loss: 0.4669\n",
      "Iteration 4500, Loss: 0.4664\n",
      "Iteration 4600, Loss: 0.4658\n",
      "Iteration 4700, Loss: 0.4653\n",
      "Iteration 4800, Loss: 0.4648\n",
      "Iteration 4900, Loss: 0.4643\n",
      "Iteration 0, Loss: 1.0000\n",
      "Iteration 100, Loss: 0.6139\n",
      "Iteration 200, Loss: 0.5859\n",
      "Iteration 300, Loss: 0.5705\n",
      "Iteration 400, Loss: 0.5601\n",
      "Iteration 500, Loss: 0.5522\n",
      "Iteration 600, Loss: 0.5460\n",
      "Iteration 700, Loss: 0.5408\n",
      "Iteration 800, Loss: 0.5364\n",
      "Iteration 900, Loss: 0.5326\n",
      "Iteration 1000, Loss: 0.5292\n",
      "Iteration 1100, Loss: 0.5262\n",
      "Iteration 1200, Loss: 0.5235\n",
      "Iteration 1300, Loss: 0.5210\n",
      "Iteration 1400, Loss: 0.5187\n",
      "Iteration 1500, Loss: 0.5166\n",
      "Iteration 1600, Loss: 0.5146\n",
      "Iteration 1700, Loss: 0.5128\n",
      "Iteration 1800, Loss: 0.5111\n",
      "Iteration 1900, Loss: 0.5095\n",
      "Iteration 2000, Loss: 0.5080\n",
      "Iteration 2100, Loss: 0.5066\n",
      "Iteration 2200, Loss: 0.5052\n",
      "Iteration 2300, Loss: 0.5039\n",
      "Iteration 2400, Loss: 0.5027\n",
      "Iteration 2500, Loss: 0.5016\n",
      "Iteration 2600, Loss: 0.5005\n",
      "Iteration 2700, Loss: 0.4994\n",
      "Iteration 2800, Loss: 0.4984\n",
      "Iteration 2900, Loss: 0.4975\n",
      "Iteration 3000, Loss: 0.4966\n",
      "Iteration 3100, Loss: 0.4957\n",
      "Iteration 3200, Loss: 0.4948\n",
      "Iteration 3300, Loss: 0.4940\n",
      "Iteration 3400, Loss: 0.4932\n",
      "Iteration 3500, Loss: 0.4925\n",
      "Iteration 3600, Loss: 0.4918\n",
      "Iteration 3700, Loss: 0.4911\n",
      "Iteration 3800, Loss: 0.4904\n",
      "Iteration 3900, Loss: 0.4897\n",
      "Iteration 4000, Loss: 0.4891\n",
      "Iteration 4100, Loss: 0.4885\n",
      "Iteration 4200, Loss: 0.4879\n",
      "Iteration 4300, Loss: 0.4873\n",
      "Iteration 4400, Loss: 0.4868\n",
      "Iteration 4500, Loss: 0.4862\n",
      "Iteration 4600, Loss: 0.4857\n",
      "Iteration 4700, Loss: 0.4852\n",
      "Iteration 4800, Loss: 0.4847\n",
      "Iteration 4900, Loss: 0.4843\n",
      "Iteration 0, Loss: 1.0026\n",
      "Iteration 100, Loss: 0.6249\n",
      "Iteration 200, Loss: 0.5959\n",
      "Iteration 300, Loss: 0.5797\n",
      "Iteration 400, Loss: 0.5687\n",
      "Iteration 500, Loss: 0.5605\n",
      "Iteration 600, Loss: 0.5539\n",
      "Iteration 700, Loss: 0.5485\n",
      "Iteration 800, Loss: 0.5439\n",
      "Iteration 900, Loss: 0.5400\n",
      "Iteration 1000, Loss: 0.5365\n",
      "Iteration 1100, Loss: 0.5334\n",
      "Iteration 1200, Loss: 0.5306\n",
      "Iteration 1300, Loss: 0.5281\n",
      "Iteration 1400, Loss: 0.5258\n",
      "Iteration 1500, Loss: 0.5237\n",
      "Iteration 1600, Loss: 0.5217\n",
      "Iteration 1700, Loss: 0.5199\n",
      "Iteration 1800, Loss: 0.5182\n",
      "Iteration 1900, Loss: 0.5166\n",
      "Iteration 2000, Loss: 0.5151\n",
      "Iteration 2100, Loss: 0.5137\n",
      "Iteration 2200, Loss: 0.5124\n",
      "Iteration 2300, Loss: 0.5112\n",
      "Iteration 2400, Loss: 0.5100\n",
      "Iteration 2500, Loss: 0.5088\n",
      "Iteration 2600, Loss: 0.5077\n",
      "Iteration 2700, Loss: 0.5067\n",
      "Iteration 2800, Loss: 0.5057\n",
      "Iteration 2900, Loss: 0.5048\n",
      "Iteration 3000, Loss: 0.5039\n",
      "Iteration 3100, Loss: 0.5031\n",
      "Iteration 3200, Loss: 0.5022\n",
      "Iteration 3300, Loss: 0.5014\n",
      "Iteration 3400, Loss: 0.5007\n",
      "Iteration 3500, Loss: 0.4999\n",
      "Iteration 3600, Loss: 0.4992\n",
      "Iteration 3700, Loss: 0.4985\n",
      "Iteration 3800, Loss: 0.4979\n",
      "Iteration 3900, Loss: 0.4972\n",
      "Iteration 4000, Loss: 0.4966\n",
      "Iteration 4100, Loss: 0.4960\n",
      "Iteration 4200, Loss: 0.4954\n",
      "Iteration 4300, Loss: 0.4949\n",
      "Iteration 4400, Loss: 0.4943\n",
      "Iteration 4500, Loss: 0.4938\n",
      "Iteration 4600, Loss: 0.4933\n",
      "Iteration 4700, Loss: 0.4928\n",
      "Iteration 4800, Loss: 0.4923\n",
      "Iteration 4900, Loss: 0.4919\n",
      "([np.float64(0.7722868217054264), np.float64(0.7868217054263565), np.float64(0.7839147286821705)], np.float64(0.7810077519379844))\n",
      "[[ 6.80675346e-02 -1.69309896e-01  1.01242361e-01]\n",
      " [ 1.89768116e-01 -9.86311071e-02 -9.11370091e-02]\n",
      " [-3.61905115e-02  6.02187619e-02 -2.40282505e-02]\n",
      " [ 1.56276966e-01  7.83625307e-02 -2.34639497e-01]\n",
      " [-1.26349846e-01  2.59835740e-01 -1.33485894e-01]\n",
      " [-2.20869893e-01 -5.43807252e-02  2.75250618e-01]\n",
      " [-9.84697682e-02  1.32773552e-01 -3.43037838e-02]\n",
      " [ 6.89165231e-02 -4.81196611e-03 -6.41045570e-02]\n",
      " [-8.35785297e-03  2.71117155e-02 -1.87538625e-02]\n",
      " [ 3.25311703e-01  3.07651210e-02 -3.56076824e-01]\n",
      " [-8.31288811e-02  3.64268352e-01 -2.81139471e-01]\n",
      " [-4.68336889e-01  4.62694060e-03  4.63709948e-01]\n",
      " [ 8.46579143e-02 -7.16421740e-03 -7.74936969e-02]\n",
      " [-2.14738084e-01  1.68591877e-01  4.61462073e-02]\n",
      " [-5.08661623e-02  7.11647777e-02 -2.02986154e-02]\n",
      " [ 2.94901135e-03 -2.32008040e-03 -6.28930956e-04]\n",
      " [-3.48755512e-02 -2.09989009e-01  2.44864560e-01]\n",
      " [ 1.72482926e-02 -2.89831795e-03 -1.43499747e-02]\n",
      " [-3.50788221e-02  2.05039890e-02  1.45748331e-02]\n",
      " [ 3.61641922e-02 -1.73483029e-01  1.37318837e-01]\n",
      " [ 3.65614377e-02  5.56722514e-03 -4.21286629e-02]\n",
      " [-6.52881676e-03  5.35250592e-02 -4.69962425e-02]\n",
      " [-1.47238131e-01 -1.05714272e-01  2.52952402e-01]\n",
      " [ 1.44100055e-01  3.13289267e-02 -1.75428982e-01]\n",
      " [ 3.79923107e-02  1.89550313e-02 -5.69473421e-02]\n",
      " [-1.89476628e-02  2.27777750e-02 -3.83011220e-03]\n",
      " [-1.65843166e-01  1.13580660e-01  5.22625062e-02]\n",
      " [-1.33860775e-01  1.20686416e-01  1.31743593e-02]\n",
      " [ 3.83624716e-02 -2.28040718e-02 -1.55583998e-02]\n",
      " [ 1.28376421e-01  1.03010261e-01 -2.31386682e-01]\n",
      " [ 1.54712724e-01  2.24366308e-02 -1.77149355e-01]\n",
      " [ 9.64899359e-02 -1.38365196e-02 -8.26534163e-02]\n",
      " [-2.55997147e-01  9.17305006e-02  1.64266646e-01]\n",
      " [-1.02257287e-01  4.46481584e-02  5.76091284e-02]\n",
      " [ 1.05948325e-01 -1.31760461e-01  2.58121360e-02]\n",
      " [ 5.74321603e-02  1.36214016e-01 -1.93646176e-01]\n",
      " [ 1.99817913e-01 -6.40040920e-02 -1.35813821e-01]\n",
      " [-9.23608490e-02 -1.27580870e-02  1.05118936e-01]\n",
      " [-4.55578123e-01  1.35770143e-01  3.19807980e-01]\n",
      " [-2.32186799e-01 -1.45094742e-02  2.46696274e-01]\n",
      " [-9.73469235e-02  2.10752846e-02  7.62716389e-02]\n",
      " [ 1.35181643e-01 -6.69335911e-02 -6.82480523e-02]\n",
      " [-1.94739375e-02  9.15659258e-03  1.03173449e-02]\n",
      " [ 1.02443115e-03 -1.64387176e-01  1.63362744e-01]\n",
      " [ 4.39254340e-02  7.85793460e-02 -1.22504780e-01]\n",
      " [ 9.49658489e-02  9.39657303e-02 -1.88931579e-01]\n",
      " [ 4.93430355e-02 -1.72254210e-01  1.22911174e-01]\n",
      " [ 8.30622633e-02 -9.04365815e-02  7.37431821e-03]\n",
      " [-1.01096795e-01  3.83809390e-02  6.27158561e-02]\n",
      " [-1.29671161e-01  2.86048239e-02  1.01066337e-01]\n",
      " [ 3.91392966e-02  4.44849836e-02 -8.36242801e-02]\n",
      " [-3.97186950e-02  1.82009874e-02  2.15177075e-02]\n",
      " [ 7.63622376e-02  3.80786848e-02 -1.14440922e-01]\n",
      " [-5.94151736e-03  3.32812634e-02 -2.73397460e-02]\n",
      " [-7.97752357e-02  3.61349769e-02  4.36402589e-02]\n",
      " [-7.74191627e-02 -1.29708186e-03  7.87162446e-02]\n",
      " [ 1.03488521e-01 -4.85790011e-03 -9.86306207e-02]\n",
      " [-9.70616495e-03 -1.19821045e-02  2.16882695e-02]\n",
      " [ 1.64863766e-02 -3.96875773e-02  2.32012007e-02]\n",
      " [ 5.51128259e-02  1.25395677e-02 -6.76523936e-02]\n",
      " [-1.30659505e-01  1.40731741e-01 -1.00722361e-02]\n",
      " [-1.10086042e-01 -2.59855655e-02  1.36071607e-01]\n",
      " [-5.91106525e-03  9.23545176e-02 -8.64434523e-02]\n",
      " [-1.79030926e-02  2.10895525e-02 -3.18645998e-03]\n",
      " [ 8.55877148e-02 -2.10775939e-01  1.25188224e-01]\n",
      " [-7.96006313e-02  4.85922953e-03  7.47414017e-02]\n",
      " [ 8.76903924e-02  7.25063560e-02 -1.60196748e-01]\n",
      " [-3.92258131e-02 -1.07239656e-01  1.46465469e-01]\n",
      " [-2.51581608e-03 -1.54093418e-01  1.56609234e-01]\n",
      " [-9.81095659e-03  4.03571724e-02 -3.05462158e-02]\n",
      " [ 1.25967119e-01 -1.88227215e-01  6.22600954e-02]\n",
      " [-1.87353250e-01  7.55899866e-02  1.11763264e-01]\n",
      " [ 1.26933183e-01 -1.42392566e-01  1.54593830e-02]\n",
      " [ 9.50143569e-03  1.73286501e-02 -2.68300858e-02]\n",
      " [-1.01607449e-01  2.73480604e-02  7.42593891e-02]\n",
      " [ 9.57950092e-02 -2.69588531e-01  1.73793521e-01]\n",
      " [ 9.75749986e-02 -3.47979293e-02 -6.27770693e-02]\n",
      " [ 3.40109715e-02  2.06193261e-02 -5.46302976e-02]\n",
      " [-7.16429621e-02 -2.35732873e-02  9.52162493e-02]\n",
      " [-1.98588537e-01  7.44002017e-02  1.24188335e-01]\n",
      " [ 1.77609749e-02 -1.45367517e-01  1.27606542e-01]\n",
      " [-3.43911808e-02  8.20314898e-03  2.61880318e-02]\n",
      " [-6.28148721e-02 -1.43164328e-01  2.05979200e-01]\n",
      " [ 1.28508328e-01 -1.30314350e-01  1.80602261e-03]\n",
      " [ 1.67092909e-02  4.02374869e-02 -5.69467778e-02]\n",
      " [ 6.57554653e-02  7.18767851e-02 -1.37632250e-01]\n",
      " [ 1.10605072e-01 -2.01049843e-02 -9.05000879e-02]\n",
      " [ 5.05800549e-02  9.93384740e-02 -1.49918529e-01]\n",
      " [-1.10618658e-01 -2.17635551e-01  3.28254209e-01]\n",
      " [-1.20819902e-02 -1.26101207e-03  1.33430023e-02]\n",
      " [-3.22136721e-03 -5.81274309e-02  6.13487981e-02]\n",
      " [-6.67847419e-02  5.25179246e-02  1.42668173e-02]\n",
      " [-1.75338585e-02 -1.63946869e-01  1.81480728e-01]\n",
      " [-3.06886621e-02  9.48145098e-03  2.12072111e-02]\n",
      " [-6.88025825e-02 -3.22329324e-03  7.20258758e-02]\n",
      " [-3.11780258e-02  6.63299664e-03  2.45450291e-02]\n",
      " [ 5.42804096e-02 -4.17771099e-02 -1.25032998e-02]\n",
      " [-1.42052089e-01  1.77790191e-01 -3.57381022e-02]\n",
      " [ 1.05299248e-01 -6.87428399e-02 -3.65564076e-02]\n",
      " [ 3.71682271e-02  2.36922174e-03 -3.95374489e-02]\n",
      " [ 2.71906292e-04  1.65592963e-02 -1.68312026e-02]\n",
      " [ 1.10799248e-01  2.44620193e-02 -1.35261267e-01]\n",
      " [-2.61428010e-02  4.27995855e-02 -1.66567845e-02]\n",
      " [ 3.76715846e-02  1.93873227e-04 -3.78654578e-02]\n",
      " [ 2.44576387e-01 -2.75062382e-02 -2.17070149e-01]\n",
      " [ 6.78112935e-02  6.91573793e-02 -1.36968673e-01]\n",
      " [-3.03092936e-02 -2.89485221e-02  5.92578158e-02]\n",
      " [-3.80266125e-02  1.60675638e-01 -1.22649025e-01]\n",
      " [-2.89650775e-02  4.63586499e-02 -1.73935724e-02]\n",
      " [ 2.15422169e-01 -2.72167213e-01  5.67450438e-02]\n",
      " [-1.93335721e-01  9.56919505e-02  9.76437709e-02]\n",
      " [ 1.49707917e-02 -8.40162815e-02  6.90454898e-02]\n",
      " [ 4.55998697e-02  5.34223173e-02 -9.90221870e-02]\n",
      " [ 6.11844463e-02 -1.85342634e-01  1.24158188e-01]\n",
      " [-1.99072927e-03  9.66474336e-02 -9.46567043e-02]\n",
      " [-1.92523899e-01 -2.11611301e-01  4.04135201e-01]\n",
      " [ 3.01424230e-02 -3.89280322e-02  8.78560926e-03]\n",
      " [ 1.87060532e-01 -1.85697703e-01 -1.36282841e-03]\n",
      " [ 1.07534635e-01 -9.92476741e-02 -8.28696064e-03]\n",
      " [ 1.10592995e-01 -1.64782316e-02 -9.41147631e-02]\n",
      " [ 5.77343174e-02 -3.27495078e-02 -2.49848096e-02]\n",
      " [ 1.55231152e-02 -2.54588832e-02  9.93576804e-03]\n",
      " [-2.80333525e-02  2.90234514e-02 -9.90098907e-04]\n",
      " [-3.58524453e-02 -5.69511015e-02  9.28035468e-02]\n",
      " [ 1.12907820e-03 -7.78057380e-04 -3.51020823e-04]\n",
      " [-1.14326624e-01 -9.16135406e-02  2.05940164e-01]\n",
      " [ 1.26201566e-02 -1.38853335e-02  1.26517687e-03]\n",
      " [-1.91735426e-02  1.05091486e-01 -8.59179432e-02]\n",
      " [ 2.85811914e-01 -1.95175767e-01 -9.06361468e-02]\n",
      " [ 2.32053109e-02 -5.30383394e-02  2.98330284e-02]\n",
      " [-1.34304267e-01 -1.82709813e-02  1.52575248e-01]\n",
      " [-4.32359008e-02  1.24413515e-02  3.07945493e-02]\n",
      " [ 2.11472603e-01 -1.50706854e-01 -6.07657494e-02]\n",
      " [ 1.80761574e-01 -5.31201579e-02 -1.27641416e-01]\n",
      " [ 3.36661656e-02 -5.75564689e-03 -2.79105187e-02]\n",
      " [ 2.29461122e-01 -4.95795294e-02 -1.79881593e-01]\n",
      " [-1.87374773e-02 -9.68949696e-02  1.15632447e-01]\n",
      " [-6.32067484e-02  2.43648621e-01 -1.80441873e-01]\n",
      " [-1.11804383e-01  4.67004453e-02  6.51039381e-02]\n",
      " [-1.42596042e-01  1.22321931e-01  2.02741112e-02]\n",
      " [-1.99520392e-01  1.14067756e-01  8.54526366e-02]\n",
      " [-8.99976180e-03 -1.96228007e-02  2.86225625e-02]\n",
      " [-1.41750308e-02  3.75151761e-02 -2.33401453e-02]\n",
      " [-1.43792995e-01  2.03782866e-01 -5.99898703e-02]\n",
      " [-1.82430400e-01 -9.30640547e-02  2.75494455e-01]\n",
      " [-1.11387263e-01 -4.49095015e-02  1.56296764e-01]\n",
      " [-1.62510752e-01  1.56607801e-01  5.90295164e-03]\n",
      " [-2.15519127e-01  7.45245065e-02  1.40994621e-01]\n",
      " [ 2.26128367e-02  4.79582001e-01 -5.02194838e-01]\n",
      " [ 2.61285452e-01 -2.92105233e-01  3.08197814e-02]\n",
      " [-7.89934754e-02 -1.01371937e-02  8.91306691e-02]\n",
      " [-3.92041408e-01 -2.36640793e-01  6.28682200e-01]\n",
      " [ 5.39589217e-02 -2.74853906e-01  2.20894984e-01]\n",
      " [ 7.19326793e-02  7.96511896e-02 -1.51583869e-01]\n",
      " [ 1.12727158e-01  1.11658977e-01 -2.24386135e-01]\n",
      " [-5.38116262e-02 -8.72143529e-02  1.41025979e-01]\n",
      " [ 2.58297968e-01 -1.89744787e-01 -6.85531809e-02]\n",
      " [ 4.83000785e-01 -6.15018048e-02 -4.21498980e-01]\n",
      " [ 2.11939985e-01  4.34567801e-02 -2.55396765e-01]\n",
      " [ 2.95095852e-02 -4.96072746e-02  2.00976894e-02]\n",
      " [ 9.15841224e-02 -5.37850555e-02 -3.77990669e-02]\n",
      " [-1.69947928e-01  2.70611722e-01 -1.00663794e-01]\n",
      " [ 4.68871972e-02 -1.42865181e-02 -3.26006792e-02]\n",
      " [ 2.17135860e-01 -1.54758758e-01 -6.23771026e-02]\n",
      " [ 1.26478113e-01 -6.48323949e-02 -6.16457182e-02]\n",
      " [ 7.27563664e-02 -7.65452892e-02  3.78892282e-03]\n",
      " [ 2.40121211e-02 -2.12418324e-02 -2.77028869e-03]\n",
      " [-2.74738756e-03 -2.29092713e-03  5.03831468e-03]\n",
      " [-1.04890124e-01  1.64450084e-01 -5.95599594e-02]\n",
      " [ 2.51019564e-03 -2.40899656e-03 -1.01199085e-04]\n",
      " [ 1.11406069e-03 -8.24029950e-04 -2.90030737e-04]\n",
      " [ 1.92762019e-02 -1.50308257e-01  1.31032055e-01]\n",
      " [ 4.88237473e-04 -9.22369363e-03  8.73545616e-03]\n",
      " [-4.88313256e-02 -3.15780687e-02  8.04093944e-02]\n",
      " [ 1.23630447e-01 -7.78397567e-02 -4.57906908e-02]\n",
      " [-1.81303371e-01  7.78922174e-02  1.03411154e-01]\n",
      " [-5.47495665e-03 -1.02078315e-02  1.56827881e-02]\n",
      " [-1.97510833e-01  1.78015672e-01  1.94951615e-02]\n",
      " [ 9.63378276e-02  4.15454505e-03 -1.00492373e-01]\n",
      " [ 5.04782077e-02 -2.35971855e-02 -2.68810222e-02]\n",
      " [-1.53136336e-01  1.76220016e-01 -2.30836807e-02]\n",
      " [ 3.06067123e-03  5.07341525e-02 -5.37948238e-02]\n",
      " [ 7.14854047e-03 -6.09148472e-03 -1.05705576e-03]\n",
      " [ 6.90653157e-03 -5.08709080e-03 -1.81944077e-03]\n",
      " [ 5.17829200e-02 -1.27168678e-02 -3.90660522e-02]\n",
      " [ 3.65250253e-02 -2.23712986e-02 -1.41537267e-02]\n",
      " [ 8.36576052e-02 -1.41722068e-01  5.80644629e-02]\n",
      " [-2.84167628e-03 -1.06244527e-02  1.34661289e-02]\n",
      " [-1.52943747e-01  1.99879985e-01 -4.69362387e-02]\n",
      " [-1.35653916e-03 -3.02367491e-03  4.38021407e-03]\n",
      " [ 8.01955423e-03 -6.26316357e-03 -1.75639066e-03]\n",
      " [-1.62669850e-02 -1.68796931e-02  3.31466781e-02]\n",
      " [-4.12998342e-03 -7.67100071e-03  1.18009841e-02]\n",
      " [ 1.73256737e-01 -2.43094592e-01  6.98378551e-02]\n",
      " [ 2.20122463e-02 -1.93885340e-02 -2.62371221e-03]\n",
      " [ 2.73138875e-02 -2.08800574e-02 -6.43383005e-03]\n",
      " [ 4.00181949e-02 -1.06191159e-01  6.61729645e-02]\n",
      " [ 4.83680823e-02 -5.41390628e-02  5.77098058e-03]\n",
      " [-5.96117227e-02  7.52399046e-02 -1.56281819e-02]\n",
      " [ 6.07747285e-02 -2.76016734e-02 -3.31730551e-02]\n",
      " [ 4.29258190e-03 -1.42461628e-02  9.95358086e-03]\n",
      " [-1.14032822e-02 -3.02040136e-02  4.16072958e-02]\n",
      " [ 3.41470231e-04 -1.58433699e-02  1.55018996e-02]\n",
      " [ 1.92399530e-02  1.54520267e-02 -3.46919797e-02]\n",
      " [ 4.62847540e-03  1.91261395e-01 -1.95889870e-01]\n",
      " [ 2.11309470e-02  1.29640357e-01 -1.50771304e-01]\n",
      " [-4.70368919e-02 -2.91460216e-02  7.61829135e-02]\n",
      " [-1.10655969e-02 -2.49074616e-02  3.59730585e-02]\n",
      " [ 1.65612219e-03 -1.42703923e-03 -2.29082958e-04]\n",
      " [-1.04722591e-02 -3.17998486e-02  4.22721077e-02]\n",
      " [ 4.93812744e-02 -9.72871733e-03 -3.96525571e-02]\n",
      " [ 3.37017231e-02 -3.91087660e-02  5.40704289e-03]\n",
      " [ 1.67391890e-02 -1.07774985e-01  9.10357960e-02]\n",
      " [ 3.52736109e-03 -3.32073207e-03 -2.06629024e-04]\n",
      " [-2.10420400e-03 -4.56123999e-02  4.77166039e-02]\n",
      " [ 4.02683942e-02 -7.26247551e-02  3.23563609e-02]\n",
      " [ 1.19721894e-02 -1.02629787e-02 -1.70921066e-03]\n",
      " [ 7.15887365e-04 -6.39681738e-04 -7.62056267e-05]\n",
      " [ 2.73057243e-03 -2.08514599e-03 -6.45426437e-04]\n",
      " [ 7.61936409e-03 -6.26920123e-03 -1.35016287e-03]\n",
      " [-1.36488130e-02  4.03501796e-02 -2.67013667e-02]\n",
      " [ 4.97204118e-02 -1.84398929e-02 -3.12805189e-02]\n",
      " [ 1.31033838e-01 -1.81296860e-01  5.02630224e-02]\n",
      " [ 2.20122463e-02 -1.93885340e-02 -2.62371221e-03]\n",
      " [ 2.03783914e-02 -6.12950524e-02  4.09166609e-02]\n",
      " [-1.59766148e-02  4.36069844e-04  1.55405450e-02]\n",
      " [-1.97982525e-01  5.10944832e-02  1.46888042e-01]\n",
      " [-7.49471877e-02  1.61464194e-01 -8.65170065e-02]\n",
      " [ 7.04808238e-02 -1.52616079e-02 -5.52192159e-02]\n",
      " [ 6.92637770e-04 -6.10783231e-04 -8.18545391e-05]\n",
      " [-1.33020010e-02 -1.20905714e-02  2.53925723e-02]\n",
      " [ 3.47931869e-01 -3.08033091e-01 -3.98987784e-02]\n",
      " [ 1.46677715e-01 -2.18607186e-01  7.19294709e-02]\n",
      " [-1.69360508e-01  2.35589626e-01 -6.62291178e-02]\n",
      " [ 1.24083544e-01  1.32316351e-01 -2.56399895e-01]\n",
      " [ 8.42106070e-02 -7.37350659e-02 -1.04755411e-02]\n",
      " [-2.98729800e-02 -1.01707295e-01  1.31580275e-01]\n",
      " [ 1.31314989e-02 -1.35672416e-01  1.22540917e-01]\n",
      " [ 5.35173019e-02 -8.85463870e-02  3.50290851e-02]\n",
      " [ 1.20658974e-01  1.36761380e-01 -2.57420355e-01]\n",
      " [-1.01014486e-01  6.13054197e-02  3.97090668e-02]\n",
      " [-2.01438758e-02 -1.26458649e-02  3.27897407e-02]\n",
      " [ 1.13240977e-01 -2.28056659e-01  1.14815682e-01]\n",
      " [-1.33530010e-03  5.17994709e-03 -3.84464700e-03]\n",
      " [-1.87055778e-02  2.65312270e-02 -7.82564929e-03]\n",
      " [ 3.07900691e-02 -3.45216074e-02  3.73153829e-03]\n",
      " [-1.26215881e-02 -2.90137895e-02  4.16353776e-02]\n",
      " [-1.27651808e-02  3.96274468e-02 -2.68622660e-02]\n",
      " [-2.09731051e-02  9.14954606e-02 -7.05223555e-02]\n",
      " [-5.84610604e-02  6.08106856e-02 -2.34962521e-03]\n",
      " [-1.29095074e-03 -6.89785932e-03  8.18881006e-03]\n",
      " [-1.55174906e-02 -2.59365608e-02  4.14540515e-02]\n",
      " [-7.90010217e-03 -1.44165258e-02  2.23166279e-02]\n",
      " [-2.70010369e-02  2.87471035e-02 -1.74606659e-03]\n",
      " [-1.59448296e-02  4.05591104e-02 -2.46142808e-02]\n",
      " [-2.32838783e-01  1.66270329e-01  6.65684540e-02]\n",
      " [-6.51257411e-02  7.24717667e-02 -7.34602561e-03]\n",
      " [-7.38957214e-03 -2.35992498e-02  3.09888220e-02]\n",
      " [-9.48866797e-02  1.00331372e-01 -5.44469249e-03]\n",
      " [ 1.38306129e-01 -2.93704133e-01  1.55398004e-01]\n",
      " [ 2.54243146e-01 -9.01476257e-02 -1.64095520e-01]\n",
      " [-1.03265482e-01  3.67651333e-02  6.65003489e-02]\n",
      " [ 7.40820051e-02 -1.62128400e-01  8.80463952e-02]\n",
      " [ 2.45065686e-01  1.18022607e-02 -2.56867946e-01]\n",
      " [-2.41789862e-01  7.67746766e-02  1.65015185e-01]\n",
      " [-1.32687205e-01 -3.08041719e-02  1.63491377e-01]\n",
      " [ 1.92848235e-01  8.11040259e-02 -2.73952261e-01]\n",
      " [-1.16520948e-01  1.54722347e-01 -3.82013983e-02]\n",
      " [ 1.37150480e-01 -5.00675913e-02 -8.70828888e-02]\n",
      " [-3.41140128e-02 -7.25444631e-02  1.06658476e-01]\n",
      " [ 2.04233620e-01 -1.88671723e-01 -1.55618970e-02]\n",
      " [-4.59641773e-03  7.97765247e-03 -3.38123474e-03]\n",
      " [-5.01343400e-03 -1.29874848e-02  1.80009188e-02]\n",
      " [ 3.67710039e-02 -2.51587478e-02 -1.16122561e-02]\n",
      " [-1.57276660e-03 -2.73393581e-03  4.30670241e-03]\n",
      " [-1.28197762e-02  4.83682024e-02 -3.55484262e-02]\n",
      " [-5.63113474e-03  4.54449051e-02 -3.98137704e-02]\n",
      " [-6.46774762e-03 -1.02708658e-02  1.67386134e-02]\n",
      " [-1.26215881e-02 -2.90137895e-02  4.16353776e-02]\n",
      " [-9.67967833e-03  5.08929825e-02 -4.12133042e-02]\n",
      " [-5.17547923e-03 -3.93442079e-03  9.10990002e-03]\n",
      " [-1.31599058e-02 -1.64076452e-02  2.95675510e-02]\n",
      " [-3.49874895e-02  5.55412767e-02 -2.05537872e-02]\n",
      " [-8.95978126e-03 -1.81396032e-02  2.70993844e-02]\n",
      " [-7.60117066e-02  1.11356416e-01 -3.53447095e-02]\n",
      " [-6.19318341e-02  7.46809908e-02 -1.27491567e-02]\n",
      " [-1.29095074e-03 -6.89785932e-03  8.18881006e-03]\n",
      " [-6.15211877e-03 -2.21926679e-02  2.83447866e-02]\n",
      " [-3.90861030e-02  1.05988751e-02  2.84872279e-02]\n",
      " [-1.64430949e-02  2.72741978e-02 -1.08311030e-02]\n",
      " [-3.12505341e-03 -7.80909206e-03  1.09341455e-02]\n",
      " [-2.39393033e-02 -1.30371120e-02  3.69764153e-02]\n",
      " [-1.67202016e-01  1.84823267e-01 -1.76212511e-02]\n",
      " [-1.73617073e-02 -2.48682065e-02  4.22299138e-02]\n",
      " [-2.51730405e-02  1.09903104e-02  1.41827301e-02]\n",
      " [ 1.46266748e-01 -1.14382640e-01 -3.18841076e-02]\n",
      " [ 1.14866231e-01 -8.78120801e-02 -2.70541508e-02]\n",
      " [ 6.22747664e-03 -1.55802499e-02  9.35277329e-03]\n",
      " [-2.03523391e-01 -1.37699077e-01  3.41222468e-01]\n",
      " [ 3.24617099e-01  3.43067472e-02 -3.58923846e-01]\n",
      " [ 6.50424499e-01 -1.87914273e-01 -4.62510226e-01]\n",
      " [-5.29330791e-01  8.45219428e-02  4.44808849e-01]\n",
      " [ 1.14650468e-02 -3.02729585e-03 -8.43775100e-03]\n",
      " [ 1.09628661e-01 -1.00365034e-01 -9.26362656e-03]\n",
      " [ 2.15896888e-01  4.66982063e-02 -2.62595095e-01]\n",
      " [-9.48031808e-02 -1.50090536e-01  2.44893717e-01]]\n"
     ]
    }
   ],
   "execution_count": 249
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12e09ed5a64648c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:25:27.313041Z",
     "start_time": "2025-05-22T18:25:27.299209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaseLogisticRegressionRegCombined(LogisticRegression):\n",
    "    def __init__(self,n_iters=5000, lr=0.001, batch_size=64,l=0.001,alpha=0.7):\n",
    "        super().__init__(lr, n_iters, batch_size)\n",
    "        self.l = l\n",
    "        self.alpha = alpha\n",
    "    def fit(self,X,y):\n",
    "        m_samples, n_features = X.shape\n",
    "        self.n_classes = y.shape[1]\n",
    "        self.weights = np.zeros((n_features, self.n_classes))\n",
    "        self.bias = np.zeros((1, self.n_classes))\n",
    "        self.errors.clear()\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            indices = np.random.permutation(m_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for start_idx in range(0, m_samples, self.batch_size):\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                predict = self.predict(X_batch)\n",
    "                l2_gradient = self.l * (1 - self.alpha) * 2 * self.weights\n",
    "                l1_gradient = self.l * self.alpha * np.sign(self.weights)\n",
    "                dw = (1 / X_batch.shape[0]) * np.dot(X_batch.T, (predict - y_batch))\n",
    "                dw += l1_gradient+l2_gradient\n",
    "                db = (1 / X_batch.shape[0]) * np.sum(predict - y_batch, axis=0, keepdims=True)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "            full_predict = self.predict(X)\n",
    "            loss = self.error_function(y, full_predict, m_samples)\n",
    "            self.errors.append(loss)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "\n"
   ],
   "id": "88db976382c63027",
   "outputs": [],
   "execution_count": 246
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T18:34:41.489023Z",
     "start_time": "2025-05-22T18:32:36.359363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logistic_regression_L1L2 = BaseLogisticRegressionRegCombined(n_iters=5000, lr=0.001, batch_size=128,l=0.001,alpha=0.50)\n",
    "print(make_cross_validation_predict(logistic_regression_L1L2, modified_features_preprocessor, X_train, y_train))"
   ],
   "id": "5acd04aec94657b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0420\n",
      "Iteration 100, Loss: 0.6503\n",
      "Iteration 200, Loss: 0.6198\n",
      "Iteration 300, Loss: 0.6038\n",
      "Iteration 400, Loss: 0.5931\n",
      "Iteration 500, Loss: 0.5850\n",
      "Iteration 600, Loss: 0.5786\n",
      "Iteration 700, Loss: 0.5733\n",
      "Iteration 800, Loss: 0.5688\n",
      "Iteration 900, Loss: 0.5649\n",
      "Iteration 1000, Loss: 0.5615\n",
      "Iteration 1100, Loss: 0.5585\n",
      "Iteration 1200, Loss: 0.5558\n",
      "Iteration 1300, Loss: 0.5533\n",
      "Iteration 1400, Loss: 0.5511\n",
      "Iteration 1500, Loss: 0.5490\n",
      "Iteration 1600, Loss: 0.5470\n",
      "Iteration 1700, Loss: 0.5452\n",
      "Iteration 1800, Loss: 0.5436\n",
      "Iteration 1900, Loss: 0.5420\n",
      "Iteration 2000, Loss: 0.5405\n",
      "Iteration 2100, Loss: 0.5391\n",
      "Iteration 2200, Loss: 0.5378\n",
      "Iteration 2300, Loss: 0.5365\n",
      "Iteration 2400, Loss: 0.5354\n",
      "Iteration 2500, Loss: 0.5342\n",
      "Iteration 2600, Loss: 0.5331\n",
      "Iteration 2700, Loss: 0.5321\n",
      "Iteration 2800, Loss: 0.5311\n",
      "Iteration 2900, Loss: 0.5302\n",
      "Iteration 3000, Loss: 0.5293\n",
      "Iteration 3100, Loss: 0.5284\n",
      "Iteration 3200, Loss: 0.5276\n",
      "Iteration 3300, Loss: 0.5267\n",
      "Iteration 3400, Loss: 0.5260\n",
      "Iteration 3500, Loss: 0.5252\n",
      "Iteration 3600, Loss: 0.5245\n",
      "Iteration 3700, Loss: 0.5238\n",
      "Iteration 3800, Loss: 0.5231\n",
      "Iteration 3900, Loss: 0.5225\n",
      "Iteration 4000, Loss: 0.5218\n",
      "Iteration 4100, Loss: 0.5212\n",
      "Iteration 4200, Loss: 0.5206\n",
      "Iteration 4300, Loss: 0.5200\n",
      "Iteration 4400, Loss: 0.5194\n",
      "Iteration 4500, Loss: 0.5189\n",
      "Iteration 4600, Loss: 0.5183\n",
      "Iteration 4700, Loss: 0.5178\n",
      "Iteration 4800, Loss: 0.5173\n",
      "Iteration 4900, Loss: 0.5168\n",
      "Iteration 0, Loss: 1.0406\n",
      "Iteration 100, Loss: 0.6475\n",
      "Iteration 200, Loss: 0.6157\n",
      "Iteration 300, Loss: 0.5987\n",
      "Iteration 400, Loss: 0.5872\n",
      "Iteration 500, Loss: 0.5785\n",
      "Iteration 600, Loss: 0.5715\n",
      "Iteration 700, Loss: 0.5658\n",
      "Iteration 800, Loss: 0.5611\n",
      "Iteration 900, Loss: 0.5569\n",
      "Iteration 1000, Loss: 0.5533\n",
      "Iteration 1100, Loss: 0.5501\n",
      "Iteration 1200, Loss: 0.5472\n",
      "Iteration 1300, Loss: 0.5446\n",
      "Iteration 1400, Loss: 0.5422\n",
      "Iteration 1500, Loss: 0.5400\n",
      "Iteration 1600, Loss: 0.5379\n",
      "Iteration 1700, Loss: 0.5360\n",
      "Iteration 1800, Loss: 0.5343\n",
      "Iteration 1900, Loss: 0.5327\n",
      "Iteration 2000, Loss: 0.5312\n",
      "Iteration 2100, Loss: 0.5297\n",
      "Iteration 2200, Loss: 0.5283\n",
      "Iteration 2300, Loss: 0.5270\n",
      "Iteration 2400, Loss: 0.5258\n",
      "Iteration 2500, Loss: 0.5246\n",
      "Iteration 2600, Loss: 0.5234\n",
      "Iteration 2700, Loss: 0.5223\n",
      "Iteration 2800, Loss: 0.5213\n",
      "Iteration 2900, Loss: 0.5203\n",
      "Iteration 3000, Loss: 0.5194\n",
      "Iteration 3100, Loss: 0.5185\n",
      "Iteration 3200, Loss: 0.5176\n",
      "Iteration 3300, Loss: 0.5167\n",
      "Iteration 3400, Loss: 0.5159\n",
      "Iteration 3500, Loss: 0.5151\n",
      "Iteration 3600, Loss: 0.5143\n",
      "Iteration 3700, Loss: 0.5136\n",
      "Iteration 3800, Loss: 0.5129\n",
      "Iteration 3900, Loss: 0.5122\n",
      "Iteration 4000, Loss: 0.5115\n",
      "Iteration 4100, Loss: 0.5108\n",
      "Iteration 4200, Loss: 0.5102\n",
      "Iteration 4300, Loss: 0.5096\n",
      "Iteration 4400, Loss: 0.5090\n",
      "Iteration 4500, Loss: 0.5084\n",
      "Iteration 4600, Loss: 0.5078\n",
      "Iteration 4700, Loss: 0.5073\n",
      "Iteration 4800, Loss: 0.5067\n",
      "Iteration 4900, Loss: 0.5061\n",
      "Iteration 0, Loss: 1.0410\n",
      "Iteration 100, Loss: 0.6366\n",
      "Iteration 200, Loss: 0.6034\n",
      "Iteration 300, Loss: 0.5859\n",
      "Iteration 400, Loss: 0.5741\n",
      "Iteration 500, Loss: 0.5654\n",
      "Iteration 600, Loss: 0.5585\n",
      "Iteration 700, Loss: 0.5529\n",
      "Iteration 800, Loss: 0.5482\n",
      "Iteration 900, Loss: 0.5440\n",
      "Iteration 1000, Loss: 0.5404\n",
      "Iteration 1100, Loss: 0.5373\n",
      "Iteration 1200, Loss: 0.5345\n",
      "Iteration 1300, Loss: 0.5319\n",
      "Iteration 1400, Loss: 0.5296\n",
      "Iteration 1500, Loss: 0.5275\n",
      "Iteration 1600, Loss: 0.5255\n",
      "Iteration 1700, Loss: 0.5237\n",
      "Iteration 1800, Loss: 0.5221\n",
      "Iteration 1900, Loss: 0.5205\n",
      "Iteration 2000, Loss: 0.5190\n",
      "Iteration 2100, Loss: 0.5176\n",
      "Iteration 2200, Loss: 0.5163\n",
      "Iteration 2300, Loss: 0.5151\n",
      "Iteration 2400, Loss: 0.5139\n",
      "Iteration 2500, Loss: 0.5128\n",
      "Iteration 2600, Loss: 0.5117\n",
      "Iteration 2700, Loss: 0.5107\n",
      "Iteration 2800, Loss: 0.5097\n",
      "Iteration 2900, Loss: 0.5088\n",
      "Iteration 3000, Loss: 0.5079\n",
      "Iteration 3100, Loss: 0.5071\n",
      "Iteration 3200, Loss: 0.5063\n",
      "Iteration 3300, Loss: 0.5055\n",
      "Iteration 3400, Loss: 0.5047\n",
      "Iteration 3500, Loss: 0.5040\n",
      "Iteration 3600, Loss: 0.5033\n",
      "Iteration 3700, Loss: 0.5026\n",
      "Iteration 3800, Loss: 0.5019\n",
      "Iteration 3900, Loss: 0.5013\n",
      "Iteration 4000, Loss: 0.5007\n",
      "Iteration 4100, Loss: 0.5001\n",
      "Iteration 4200, Loss: 0.4995\n",
      "Iteration 4300, Loss: 0.4989\n",
      "Iteration 4400, Loss: 0.4984\n",
      "Iteration 4500, Loss: 0.4979\n",
      "Iteration 4600, Loss: 0.4974\n",
      "Iteration 4700, Loss: 0.4968\n",
      "Iteration 4800, Loss: 0.4964\n",
      "Iteration 4900, Loss: 0.4959\n",
      "([np.float64(0.7936046511627907), np.float64(0.7839147286821705), np.float64(0.7558139534883721)], np.float64(0.7777777777777777))\n"
     ]
    }
   ],
   "execution_count": 250
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:59:46.119099Z",
     "start_time": "2025-05-21T16:59:45.899640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#weights before L1\n",
    "plt.hist((modified_features_CV_logistic_regression.weights).flatten(), bins=100)\n",
    "plt.xlabel(\"Absolute weight value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of absolute weights\")\n",
    "plt.show()"
   ],
   "id": "bacb67f1f979ad6e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:59:46.308992Z",
     "start_time": "2025-05-21T16:59:46.129490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#weights after L1\n",
    "plt.hist(logistic_regression_L1_reg.weights.flatten(), bins=100)\n",
    "plt.xlabel(\"Absolute weight value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of absolute weights\")\n",
    "plt.show()"
   ],
   "id": "bbc235edff39a000",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "We can also see that mean accuracy before and after implemented L1 and L2 are very similar, thus can confirm that our model is not overfitted."
   ],
   "id": "7847b5ed3b123a26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "IV. Dataset balancing\n",
    "\n",
    "Dataset balancing is crucial in model training because when we are training our data on some part of full dataset it can be possible that occurrence one class can be much bigger than others, so after training model on unbalanced dataset model can simply always guess this class which occurrence were the biggest in training set. It is oversampling, we have similar situation with undersampling when it's the other way around."
   ],
   "id": "bc49ab18fba2ad40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:59:47.064265Z",
     "start_time": "2025-05-21T16:59:46.432661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "d_set = numerical_features_modified+categorical_features_modified\n",
    "\n",
    "def sampling_method_pipeline(X_ttrain, y_ttrain, method):\n",
    "    X_ttrain = X_ttrain.copy()\n",
    "    y_ttrain = y_ttrain.copy()\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_ttrain = le.fit_transform(y_ttrain.values.ravel() if isinstance(y_ttrain, pd.DataFrame) else y_ttrain)\n",
    "    resampler = None\n",
    "    # Resampling\n",
    "    match method:\n",
    "        case \"SMOTE\":\n",
    "            resampler = SMOTENC(\n",
    "            categorical_features=categorical_features_modified,\n",
    "            random_state=42\n",
    "            )\n",
    "        case \"RandomUnderSampler\":\n",
    "            resampler = RandomUnderSampler(random_state=42)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X_ttrain, y_ttrain)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "    ..."
   ],
   "id": "43a58635fbe8c058",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T16:59:47.200728Z",
     "start_time": "2025-05-21T16:59:47.194117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_cross_validation_predict_sampling(model, preproc, X_set, y_set, sampling_method=None, k=3):\n",
    "    scores = []\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    y_encoded = LabelEncoder().fit_transform(y_set)\n",
    "\n",
    "    for train_index, test_index in skf.split(X_set, y_encoded):\n",
    "        X_train_raw = X_set.iloc[train_index]\n",
    "        X_test_raw = X_set.iloc[test_index]\n",
    "\n",
    "        y_train_raw = y_set.iloc[train_index]\n",
    "        y_test_raw = y_set.iloc[test_index]\n",
    "\n",
    "        if sampling_method is not None:\n",
    "            X_train_resampled, y_train_resampled = sampling_method_pipeline(pd.DataFrame(X_train_raw), y_train_raw, sampling_method)\n",
    "            X_train = X_train_resampled\n",
    "            y_train = y_train_resampled\n",
    "        else:\n",
    "            X_train = X_train_raw\n",
    "            y_train = y_train_raw.values.ravel()\n",
    "\n",
    "        X_train = preproc.fit_transform(X_train)\n",
    "        X_test = preproc.transform(X_test_raw)\n",
    "\n",
    "        y_test_encoded = pd.get_dummies(y_test_raw).astype(int).to_numpy()\n",
    "        y_train_encoded = pd.get_dummies(y_train).astype(int).to_numpy()\n",
    "\n",
    "        model.fit(X_train, y_train_encoded)\n",
    "\n",
    "        predicted = model.predict_class(X_test)\n",
    "        score = model.score(predicted, y_test_encoded)\n",
    "        scores.append(score)\n",
    "        y_encoded_t =np.argmax(y_test_encoded,axis=1)\n",
    "        print(classification_report(y_encoded_t, predicted, digits=4))\n",
    "\n",
    "    return scores, np.mean(scores)\n",
    "\n"
   ],
   "id": "63c915de7fe5a80b",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:49:31.064360Z",
     "start_time": "2025-05-22T19:48:39.890841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logistic_regression_L2 = BaseLogisticRegressionRegL2(n_iters=5000, lr=0.001, batch_size=64,l=0.001)\n",
    "print(make_cross_validation_predict_sampling(logistic_regression_L2, modified_features_preprocessor, X_train, y_train,sampling_method=\"RandomUnderSampler\"))"
   ],
   "id": "68501e9d2cee1deb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0566\n",
      "Iteration 100, Loss: 0.7356\n",
      "Iteration 200, Loss: 0.7009\n",
      "Iteration 300, Loss: 0.6816\n",
      "Iteration 400, Loss: 0.6679\n",
      "Iteration 500, Loss: 0.6572\n",
      "Iteration 600, Loss: 0.6483\n",
      "Iteration 700, Loss: 0.6407\n",
      "Iteration 800, Loss: 0.6342\n",
      "Iteration 900, Loss: 0.6285\n",
      "Iteration 1000, Loss: 0.6234\n",
      "Iteration 1100, Loss: 0.6188\n",
      "Iteration 1200, Loss: 0.6146\n",
      "Iteration 1300, Loss: 0.6107\n",
      "Iteration 1400, Loss: 0.6072\n",
      "Iteration 1500, Loss: 0.6040\n",
      "Iteration 1600, Loss: 0.6009\n",
      "Iteration 1700, Loss: 0.5981\n",
      "Iteration 1800, Loss: 0.5954\n",
      "Iteration 1900, Loss: 0.5929\n",
      "Iteration 2000, Loss: 0.5905\n",
      "Iteration 2100, Loss: 0.5883\n",
      "Iteration 2200, Loss: 0.5861\n",
      "Iteration 2300, Loss: 0.5841\n",
      "Iteration 2400, Loss: 0.5822\n",
      "Iteration 2500, Loss: 0.5803\n",
      "Iteration 2600, Loss: 0.5785\n",
      "Iteration 2700, Loss: 0.5769\n",
      "Iteration 2800, Loss: 0.5752\n",
      "Iteration 2900, Loss: 0.5737\n",
      "Iteration 3000, Loss: 0.5722\n",
      "Iteration 3100, Loss: 0.5707\n",
      "Iteration 3200, Loss: 0.5693\n",
      "Iteration 3300, Loss: 0.5680\n",
      "Iteration 3400, Loss: 0.5667\n",
      "Iteration 3500, Loss: 0.5654\n",
      "Iteration 3600, Loss: 0.5642\n",
      "Iteration 3700, Loss: 0.5631\n",
      "Iteration 3800, Loss: 0.5619\n",
      "Iteration 3900, Loss: 0.5609\n",
      "Iteration 4000, Loss: 0.5598\n",
      "Iteration 4100, Loss: 0.5587\n",
      "Iteration 4200, Loss: 0.5578\n",
      "Iteration 4300, Loss: 0.5568\n",
      "Iteration 4400, Loss: 0.5559\n",
      "Iteration 4500, Loss: 0.5550\n",
      "Iteration 4600, Loss: 0.5540\n",
      "Iteration 4700, Loss: 0.5532\n",
      "Iteration 4800, Loss: 0.5523\n",
      "Iteration 4900, Loss: 0.5515\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8885    0.7309    0.8020       327\n",
      "           1     0.4419    0.6448    0.5244       183\n",
      "           2     0.8468    0.8046    0.8251       522\n",
      "\n",
      "    accuracy                         0.7529      1032\n",
      "   macro avg     0.7257    0.7268    0.7172      1032\n",
      "weighted avg     0.7882    0.7529    0.7645      1032\n",
      "\n",
      "Iteration 0, Loss: 1.0532\n",
      "Iteration 100, Loss: 0.7366\n",
      "Iteration 200, Loss: 0.7052\n",
      "Iteration 300, Loss: 0.6875\n",
      "Iteration 400, Loss: 0.6748\n",
      "Iteration 500, Loss: 0.6648\n",
      "Iteration 600, Loss: 0.6566\n",
      "Iteration 700, Loss: 0.6497\n",
      "Iteration 800, Loss: 0.6437\n",
      "Iteration 900, Loss: 0.6385\n",
      "Iteration 1000, Loss: 0.6339\n",
      "Iteration 1100, Loss: 0.6297\n",
      "Iteration 1200, Loss: 0.6258\n",
      "Iteration 1300, Loss: 0.6224\n",
      "Iteration 1400, Loss: 0.6191\n",
      "Iteration 1500, Loss: 0.6161\n",
      "Iteration 1600, Loss: 0.6133\n",
      "Iteration 1700, Loss: 0.6107\n",
      "Iteration 1800, Loss: 0.6082\n",
      "Iteration 1900, Loss: 0.6059\n",
      "Iteration 2000, Loss: 0.6037\n",
      "Iteration 2100, Loss: 0.6017\n",
      "Iteration 2200, Loss: 0.5997\n",
      "Iteration 2300, Loss: 0.5978\n",
      "Iteration 2400, Loss: 0.5960\n",
      "Iteration 2500, Loss: 0.5943\n",
      "Iteration 2600, Loss: 0.5926\n",
      "Iteration 2700, Loss: 0.5910\n",
      "Iteration 2800, Loss: 0.5895\n",
      "Iteration 2900, Loss: 0.5881\n",
      "Iteration 3000, Loss: 0.5867\n",
      "Iteration 3100, Loss: 0.5853\n",
      "Iteration 3200, Loss: 0.5840\n",
      "Iteration 3300, Loss: 0.5828\n",
      "Iteration 3400, Loss: 0.5815\n",
      "Iteration 3500, Loss: 0.5804\n",
      "Iteration 3600, Loss: 0.5792\n",
      "Iteration 3700, Loss: 0.5781\n",
      "Iteration 3800, Loss: 0.5770\n",
      "Iteration 3900, Loss: 0.5760\n",
      "Iteration 4000, Loss: 0.5750\n",
      "Iteration 4100, Loss: 0.5740\n",
      "Iteration 4200, Loss: 0.5730\n",
      "Iteration 4300, Loss: 0.5721\n",
      "Iteration 4400, Loss: 0.5712\n",
      "Iteration 4500, Loss: 0.5703\n",
      "Iteration 4600, Loss: 0.5695\n",
      "Iteration 4700, Loss: 0.5686\n",
      "Iteration 4800, Loss: 0.5678\n",
      "Iteration 4900, Loss: 0.5670\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8396    0.7523    0.7935       327\n",
      "           1     0.4618    0.6284    0.5324       183\n",
      "           2     0.8694    0.8161    0.8419       522\n",
      "\n",
      "    accuracy                         0.7626      1032\n",
      "   macro avg     0.7236    0.7323    0.7226      1032\n",
      "weighted avg     0.7877    0.7626    0.7717      1032\n",
      "\n",
      "Iteration 0, Loss: 1.0595\n",
      "Iteration 100, Loss: 0.7496\n",
      "Iteration 200, Loss: 0.7114\n",
      "Iteration 300, Loss: 0.6897\n",
      "Iteration 400, Loss: 0.6741\n",
      "Iteration 500, Loss: 0.6623\n",
      "Iteration 600, Loss: 0.6526\n",
      "Iteration 700, Loss: 0.6445\n",
      "Iteration 800, Loss: 0.6376\n",
      "Iteration 900, Loss: 0.6316\n",
      "Iteration 1000, Loss: 0.6263\n",
      "Iteration 1100, Loss: 0.6214\n",
      "Iteration 1200, Loss: 0.6171\n",
      "Iteration 1300, Loss: 0.6131\n",
      "Iteration 1400, Loss: 0.6094\n",
      "Iteration 1500, Loss: 0.6060\n",
      "Iteration 1600, Loss: 0.6028\n",
      "Iteration 1700, Loss: 0.5998\n",
      "Iteration 1800, Loss: 0.5970\n",
      "Iteration 1900, Loss: 0.5944\n",
      "Iteration 2000, Loss: 0.5919\n",
      "Iteration 2100, Loss: 0.5895\n",
      "Iteration 2200, Loss: 0.5872\n",
      "Iteration 2300, Loss: 0.5851\n",
      "Iteration 2400, Loss: 0.5830\n",
      "Iteration 2500, Loss: 0.5810\n",
      "Iteration 2600, Loss: 0.5791\n",
      "Iteration 2700, Loss: 0.5773\n",
      "Iteration 2800, Loss: 0.5755\n",
      "Iteration 2900, Loss: 0.5739\n",
      "Iteration 3000, Loss: 0.5723\n",
      "Iteration 3100, Loss: 0.5707\n",
      "Iteration 3200, Loss: 0.5692\n",
      "Iteration 3300, Loss: 0.5677\n",
      "Iteration 3400, Loss: 0.5663\n",
      "Iteration 3500, Loss: 0.5649\n",
      "Iteration 3600, Loss: 0.5636\n",
      "Iteration 3700, Loss: 0.5623\n",
      "Iteration 3800, Loss: 0.5610\n",
      "Iteration 3900, Loss: 0.5598\n",
      "Iteration 4000, Loss: 0.5587\n",
      "Iteration 4100, Loss: 0.5575\n",
      "Iteration 4200, Loss: 0.5564\n",
      "Iteration 4300, Loss: 0.5553\n",
      "Iteration 4400, Loss: 0.5542\n",
      "Iteration 4500, Loss: 0.5532\n",
      "Iteration 4600, Loss: 0.5522\n",
      "Iteration 4700, Loss: 0.5512\n",
      "Iteration 4800, Loss: 0.5503\n",
      "Iteration 4900, Loss: 0.5494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7993    0.6718    0.7300       326\n",
      "           1     0.4060    0.6612    0.5031       183\n",
      "           2     0.8848    0.7782    0.8281       523\n",
      "\n",
      "    accuracy                         0.7238      1032\n",
      "   macro avg     0.6967    0.7037    0.6871      1032\n",
      "weighted avg     0.7729    0.7238    0.7395      1032\n",
      "\n",
      "([np.float64(0.752906976744186), np.float64(0.7625968992248062), np.float64(0.7238372093023255)], np.float64(0.7464470284237725))\n"
     ]
    }
   ],
   "execution_count": 258
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:00:28.191224Z",
     "start_time": "2025-05-21T17:00:28.187196Z"
    }
   },
   "cell_type": "code",
   "source": "from sklearn.metrics import classification_report",
   "id": "13c16d525e4f682b",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:50:40.564462Z",
     "start_time": "2025-05-22T19:49:31.287849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_proc, X_test_proc, y_train_proc, y_test_proc = transform_in_pipeline(modified_features_preprocessor, X_train, X_test,y_train,y_test)\n",
    "log_reg = BaseLogisticRegressionRegL2(n_iters=5000, lr=0.001, batch_size=64)\n",
    "log_reg.fit(X_train_proc, y_train_proc)\n",
    "predicted = log_reg.predict_class(X_test_proc)\n",
    "\n"
   ],
   "id": "d91fb96837bbebc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.9678\n",
      "Iteration 100, Loss: 0.6002\n",
      "Iteration 200, Loss: 0.5740\n",
      "Iteration 300, Loss: 0.5601\n",
      "Iteration 400, Loss: 0.5508\n",
      "Iteration 500, Loss: 0.5439\n",
      "Iteration 600, Loss: 0.5385\n",
      "Iteration 700, Loss: 0.5341\n",
      "Iteration 800, Loss: 0.5303\n",
      "Iteration 900, Loss: 0.5271\n",
      "Iteration 1000, Loss: 0.5243\n",
      "Iteration 1100, Loss: 0.5217\n",
      "Iteration 1200, Loss: 0.5195\n",
      "Iteration 1300, Loss: 0.5174\n",
      "Iteration 1400, Loss: 0.5156\n",
      "Iteration 1500, Loss: 0.5139\n",
      "Iteration 1600, Loss: 0.5123\n",
      "Iteration 1700, Loss: 0.5108\n",
      "Iteration 1800, Loss: 0.5095\n",
      "Iteration 1900, Loss: 0.5082\n",
      "Iteration 2000, Loss: 0.5071\n",
      "Iteration 2100, Loss: 0.5060\n",
      "Iteration 2200, Loss: 0.5049\n",
      "Iteration 2300, Loss: 0.5040\n",
      "Iteration 2400, Loss: 0.5030\n",
      "Iteration 2500, Loss: 0.5022\n",
      "Iteration 2600, Loss: 0.5013\n",
      "Iteration 2700, Loss: 0.5006\n",
      "Iteration 2800, Loss: 0.4998\n",
      "Iteration 2900, Loss: 0.4991\n",
      "Iteration 3000, Loss: 0.4984\n",
      "Iteration 3100, Loss: 0.4978\n",
      "Iteration 3200, Loss: 0.4972\n",
      "Iteration 3300, Loss: 0.4966\n",
      "Iteration 3400, Loss: 0.4960\n",
      "Iteration 3500, Loss: 0.4955\n",
      "Iteration 3600, Loss: 0.4950\n",
      "Iteration 3700, Loss: 0.4945\n",
      "Iteration 3800, Loss: 0.4940\n",
      "Iteration 3900, Loss: 0.4936\n",
      "Iteration 4000, Loss: 0.4931\n",
      "Iteration 4100, Loss: 0.4927\n",
      "Iteration 4200, Loss: 0.4923\n",
      "Iteration 4300, Loss: 0.4919\n",
      "Iteration 4400, Loss: 0.4915\n",
      "Iteration 4500, Loss: 0.4911\n",
      "Iteration 4600, Loss: 0.4908\n",
      "Iteration 4700, Loss: 0.4904\n",
      "Iteration 4800, Loss: 0.4901\n",
      "Iteration 4900, Loss: 0.4898\n"
     ]
    }
   ],
   "execution_count": 259
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:50:40.842654Z",
     "start_time": "2025-05-22T19:50:40.810690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_test_proc = np.argmax(y_test_proc,axis=1) if y_test_proc.ndim>1 else y_test_proc\n",
    "print(classification_report(y_test_proc, predicted, digits=4))\n"
   ],
   "id": "26933b7a2004d841",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8113    0.7800    0.7954       441\n",
      "           1     0.5500    0.3592    0.4346       245\n",
      "           2     0.7917    0.9174    0.8499       642\n",
      "\n",
      "    accuracy                         0.7688      1328\n",
      "   macro avg     0.7177    0.6856    0.6933      1328\n",
      "weighted avg     0.7536    0.7688    0.7552      1328\n",
      "\n"
     ]
    }
   ],
   "execution_count": 260
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:52:27.401615Z",
     "start_time": "2025-05-22T19:50:41.021412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_res_train,y_res_train = sampling_method_pipeline(X_train,y_train,method=\"SMOTE\")\n",
    "X_train_proc, X_test_proc, y_train_proc, y_test_proc = transform_in_pipeline(modified_features_preprocessor, X_res_train, X_test,y_res_train,y_test)\n",
    "log_reg = BaseLogisticRegressionRegL2(n_iters=5000, lr=0.001, batch_size=64)\n",
    "log_reg.fit(X_train_proc, y_train_proc)\n",
    "predicted = log_reg.predict_class(X_test_proc)\n"
   ],
   "id": "3d0f94765e4c1f7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.9712\n",
      "Iteration 100, Loss: 0.6439\n",
      "Iteration 200, Loss: 0.6085\n",
      "Iteration 300, Loss: 0.5897\n",
      "Iteration 400, Loss: 0.5774\n",
      "Iteration 500, Loss: 0.5686\n",
      "Iteration 600, Loss: 0.5618\n",
      "Iteration 700, Loss: 0.5563\n",
      "Iteration 800, Loss: 0.5517\n",
      "Iteration 900, Loss: 0.5479\n",
      "Iteration 1000, Loss: 0.5445\n",
      "Iteration 1100, Loss: 0.5416\n",
      "Iteration 1200, Loss: 0.5390\n",
      "Iteration 1300, Loss: 0.5368\n",
      "Iteration 1400, Loss: 0.5347\n",
      "Iteration 1500, Loss: 0.5328\n",
      "Iteration 1600, Loss: 0.5312\n",
      "Iteration 1700, Loss: 0.5296\n",
      "Iteration 1800, Loss: 0.5282\n",
      "Iteration 1900, Loss: 0.5269\n",
      "Iteration 2000, Loss: 0.5257\n",
      "Iteration 2100, Loss: 0.5246\n",
      "Iteration 2200, Loss: 0.5235\n",
      "Iteration 2300, Loss: 0.5226\n",
      "Iteration 2400, Loss: 0.5217\n",
      "Iteration 2500, Loss: 0.5208\n",
      "Iteration 2600, Loss: 0.5200\n",
      "Iteration 2700, Loss: 0.5193\n",
      "Iteration 2800, Loss: 0.5186\n",
      "Iteration 2900, Loss: 0.5179\n",
      "Iteration 3000, Loss: 0.5173\n",
      "Iteration 3100, Loss: 0.5167\n",
      "Iteration 3200, Loss: 0.5161\n",
      "Iteration 3300, Loss: 0.5156\n",
      "Iteration 3400, Loss: 0.5151\n",
      "Iteration 3500, Loss: 0.5146\n",
      "Iteration 3600, Loss: 0.5142\n",
      "Iteration 3700, Loss: 0.5137\n",
      "Iteration 3800, Loss: 0.5133\n",
      "Iteration 3900, Loss: 0.5129\n",
      "Iteration 4000, Loss: 0.5125\n",
      "Iteration 4100, Loss: 0.5121\n",
      "Iteration 4200, Loss: 0.5118\n",
      "Iteration 4300, Loss: 0.5115\n",
      "Iteration 4400, Loss: 0.5111\n",
      "Iteration 4500, Loss: 0.5108\n",
      "Iteration 4600, Loss: 0.5105\n",
      "Iteration 4700, Loss: 0.5102\n",
      "Iteration 4800, Loss: 0.5100\n",
      "Iteration 4900, Loss: 0.5097\n"
     ]
    }
   ],
   "execution_count": 261
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:52:27.675196Z",
     "start_time": "2025-05-22T19:52:27.656043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_test_proc = np.argmax(y_test_proc,axis=1) if y_test_proc.ndim>1 else y_test_proc\n",
    "print(classification_report(y_test_proc, predicted, digits=4))\n"
   ],
   "id": "86532d5685941318",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8406    0.7415    0.7880       441\n",
      "           1     0.4533    0.5551    0.4991       245\n",
      "           2     0.8372    0.8333    0.8353       642\n",
      "\n",
      "    accuracy                         0.7515      1328\n",
      "   macro avg     0.7104    0.7100    0.7074      1328\n",
      "weighted avg     0.7675    0.7515    0.7575      1328\n",
      "\n"
     ]
    }
   ],
   "execution_count": 262
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:52:27.961369Z",
     "start_time": "2025-05-22T19:52:27.936179Z"
    }
   },
   "cell_type": "code",
   "source": "X_res_train,y_res_train = sampling_method_pipeline(X_train,y_train,method=\"RandomUnderSampler\")",
   "id": "48444a9119cdbaf1",
   "outputs": [],
   "execution_count": 263
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:52:54.576808Z",
     "start_time": "2025-05-22T19:52:28.204524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_proc, X_test_proc, y_train_proc, y_test_proc = transform_in_pipeline(modified_features_preprocessor, X_res_train, X_test,y_res_train,y_test)\n",
    "log_reg = BaseLogisticRegressionRegL2(n_iters=5000, lr=0.001, batch_size=64)\n",
    "log_reg.fit(X_train_proc, y_train_proc)\n",
    "predicted = log_reg.predict_class(X_test_proc)\n",
    "\n"
   ],
   "id": "688b679ec117b920",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0440\n",
      "Iteration 100, Loss: 0.7177\n",
      "Iteration 200, Loss: 0.6848\n",
      "Iteration 300, Loss: 0.6662\n",
      "Iteration 400, Loss: 0.6532\n",
      "Iteration 500, Loss: 0.6433\n",
      "Iteration 600, Loss: 0.6353\n",
      "Iteration 700, Loss: 0.6288\n",
      "Iteration 800, Loss: 0.6232\n",
      "Iteration 900, Loss: 0.6184\n",
      "Iteration 1000, Loss: 0.6141\n",
      "Iteration 1100, Loss: 0.6103\n",
      "Iteration 1200, Loss: 0.6069\n",
      "Iteration 1300, Loss: 0.6038\n",
      "Iteration 1400, Loss: 0.6009\n",
      "Iteration 1500, Loss: 0.5983\n",
      "Iteration 1600, Loss: 0.5958\n",
      "Iteration 1700, Loss: 0.5936\n",
      "Iteration 1800, Loss: 0.5915\n",
      "Iteration 1900, Loss: 0.5895\n",
      "Iteration 2000, Loss: 0.5876\n",
      "Iteration 2100, Loss: 0.5858\n",
      "Iteration 2200, Loss: 0.5842\n",
      "Iteration 2300, Loss: 0.5826\n",
      "Iteration 2400, Loss: 0.5811\n",
      "Iteration 2500, Loss: 0.5797\n",
      "Iteration 2600, Loss: 0.5784\n",
      "Iteration 2700, Loss: 0.5771\n",
      "Iteration 2800, Loss: 0.5758\n",
      "Iteration 2900, Loss: 0.5747\n",
      "Iteration 3000, Loss: 0.5735\n",
      "Iteration 3100, Loss: 0.5724\n",
      "Iteration 3200, Loss: 0.5714\n",
      "Iteration 3300, Loss: 0.5704\n",
      "Iteration 3400, Loss: 0.5694\n",
      "Iteration 3500, Loss: 0.5685\n",
      "Iteration 3600, Loss: 0.5676\n",
      "Iteration 3700, Loss: 0.5668\n",
      "Iteration 3800, Loss: 0.5659\n",
      "Iteration 3900, Loss: 0.5651\n",
      "Iteration 4000, Loss: 0.5643\n",
      "Iteration 4100, Loss: 0.5636\n",
      "Iteration 4200, Loss: 0.5628\n",
      "Iteration 4300, Loss: 0.5621\n",
      "Iteration 4400, Loss: 0.5614\n",
      "Iteration 4500, Loss: 0.5608\n",
      "Iteration 4600, Loss: 0.5601\n",
      "Iteration 4700, Loss: 0.5595\n",
      "Iteration 4800, Loss: 0.5589\n",
      "Iteration 4900, Loss: 0.5583\n"
     ]
    }
   ],
   "execution_count": 264
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T19:52:54.737164Z",
     "start_time": "2025-05-22T19:52:54.719746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_test_proc = np.argmax(y_test_proc,axis=1) if y_test_proc.ndim>1 else y_test_proc\n",
    "print(classification_report(y_test_proc, predicted, digits=4))\n"
   ],
   "id": "a441156c8f5c0e14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8464    0.7120    0.7734       441\n",
      "           1     0.4448    0.6245    0.5195       245\n",
      "           2     0.8450    0.8069    0.8255       642\n",
      "\n",
      "    accuracy                         0.7417      1328\n",
      "   macro avg     0.7121    0.7145    0.7061      1328\n",
      "weighted avg     0.7716    0.7417    0.7517      1328\n",
      "\n"
     ]
    }
   ],
   "execution_count": 265
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T20:09:36.550959Z",
     "start_time": "2025-05-22T20:09:36.546144Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4f9d77d197aa6889",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So here me made three runs of regression model to show differences between dataset without sampling and with sampling.\n",
    "- I scores\n",
    "We can see that this model ignores 1 class and favors 2 class, so we can assume that this is unbalanced dataset, because of better model correctness of dominated class, than minor class\n",
    "- II scores (after balanced)\n",
    "We can see that here was used some balanced techniques because of better balance between recall factor for all three classes\n",
    "- III scores (after balanced)\n",
    "This scores looks the best from this three because the recall value for all of them is high and similar to each other, so we don't have situation that model favours one of classes"
   ],
   "id": "452111189440d95e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "V. Optimizing hiperparmeters",
   "id": "8481a4d875989d2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Out models have a variables that is constant during training our model so we have to be sure that our set values are good enough. To be sure we can optimizing it in `GridSearchCV`\n",
    "We are working of logistic regression variances so we will optimize these parameters:\n",
    "- `BaseLogisticRegresion` - `n_iter`  |  `lr`  |  `batch_size`\n",
    "- `BaseLogisticRegressionRegCombined` - `n_iter`  |  `lr`  |  `batch_size` | `l` |`alpha`"
   ],
   "id": "21e0640dd4799e82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T13:51:55.508937Z",
     "start_time": "2025-05-21T12:39:01.718321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import product\n",
    "# BASE LOGISTIC REGRESSION\n",
    "param_grid = {\n",
    "    \"n_iters\":[3000,4000,5000,6000,7000,8000,9000,10000],\n",
    "    \"lr\":[0.001,0.0001,0.00001],\n",
    "    \"batch_size\":[32,64,128]\n",
    "}\n",
    "best = -1\n",
    "best_params = None\n",
    "keys = list(param_grid.keys())\n",
    "combinations = list(product(*param_grid.values()))\n",
    "for combo in combinations:\n",
    "    params = dict(zip(keys,combo))\n",
    "    model = BaseLogisticRegression(**params)\n",
    "    mean = make_cross_validation_predict(model,\n",
    "                                           modified_features_preprocessor,\n",
    "                                           X_train, y_train,\n",
    "                                           k=2)[1]\n",
    "    if mean > best:\n",
    "        best = mean\n",
    "        best_params = params\n",
    "print(best_params,best)"
   ],
   "id": "2d57fb700511158d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.9720\n",
      "Iteration 100, Loss: 0.6001\n",
      "Iteration 200, Loss: 0.5682\n",
      "Iteration 300, Loss: 0.5505\n",
      "Iteration 400, Loss: 0.5387\n",
      "Iteration 500, Loss: 0.5298\n",
      "Iteration 600, Loss: 0.5228\n",
      "Iteration 700, Loss: 0.5171\n",
      "Iteration 800, Loss: 0.5121\n",
      "Iteration 900, Loss: 0.5078\n",
      "Iteration 1000, Loss: 0.5040\n",
      "Iteration 1100, Loss: 0.5006\n",
      "Iteration 1200, Loss: 0.4975\n",
      "Iteration 1300, Loss: 0.4946\n",
      "Iteration 1400, Loss: 0.4920\n",
      "Iteration 1500, Loss: 0.4896\n",
      "Iteration 1600, Loss: 0.4873\n",
      "Iteration 1700, Loss: 0.4851\n",
      "Iteration 1800, Loss: 0.4831\n",
      "Iteration 1900, Loss: 0.4812\n",
      "Iteration 2000, Loss: 0.4795\n",
      "Iteration 2100, Loss: 0.4778\n",
      "Iteration 2200, Loss: 0.4762\n",
      "Iteration 2300, Loss: 0.4746\n",
      "Iteration 2400, Loss: 0.4732\n",
      "Iteration 2500, Loss: 0.4718\n",
      "Iteration 2600, Loss: 0.4705\n",
      "Iteration 2700, Loss: 0.4692\n",
      "Iteration 2800, Loss: 0.4680\n",
      "Iteration 2900, Loss: 0.4668\n",
      "Iteration 0, Loss: 0.9634\n",
      "Iteration 100, Loss: 0.5825\n",
      "Iteration 200, Loss: 0.5542\n",
      "Iteration 300, Loss: 0.5377\n",
      "Iteration 400, Loss: 0.5260\n",
      "Iteration 500, Loss: 0.5170\n",
      "Iteration 600, Loss: 0.5097\n",
      "Iteration 700, Loss: 0.5035\n",
      "Iteration 800, Loss: 0.4983\n",
      "Iteration 900, Loss: 0.4937\n",
      "Iteration 1000, Loss: 0.4896\n",
      "Iteration 1100, Loss: 0.4859\n",
      "Iteration 1200, Loss: 0.4826\n",
      "Iteration 1300, Loss: 0.4795\n",
      "Iteration 1400, Loss: 0.4767\n",
      "Iteration 1500, Loss: 0.4741\n",
      "Iteration 1600, Loss: 0.4717\n",
      "Iteration 1700, Loss: 0.4695\n",
      "Iteration 1800, Loss: 0.4674\n",
      "Iteration 1900, Loss: 0.4654\n",
      "Iteration 2000, Loss: 0.4635\n",
      "Iteration 2100, Loss: 0.4617\n",
      "Iteration 2200, Loss: 0.4600\n",
      "Iteration 2300, Loss: 0.4584\n",
      "Iteration 2400, Loss: 0.4569\n",
      "Iteration 2500, Loss: 0.4554\n",
      "Iteration 2600, Loss: 0.4540\n",
      "Iteration 2700, Loss: 0.4527\n",
      "Iteration 2800, Loss: 0.4514\n",
      "Iteration 2900, Loss: 0.4501\n",
      "Iteration 0, Loss: 1.0228\n",
      "Iteration 100, Loss: 0.6240\n",
      "Iteration 200, Loss: 0.5911\n",
      "Iteration 300, Loss: 0.5736\n",
      "Iteration 400, Loss: 0.5618\n",
      "Iteration 500, Loss: 0.5530\n",
      "Iteration 600, Loss: 0.5461\n",
      "Iteration 700, Loss: 0.5403\n",
      "Iteration 800, Loss: 0.5353\n",
      "Iteration 900, Loss: 0.5310\n",
      "Iteration 1000, Loss: 0.5272\n",
      "Iteration 1100, Loss: 0.5238\n",
      "Iteration 1200, Loss: 0.5207\n",
      "Iteration 1300, Loss: 0.5178\n",
      "Iteration 1400, Loss: 0.5152\n",
      "Iteration 1500, Loss: 0.5128\n",
      "Iteration 1600, Loss: 0.5105\n",
      "Iteration 1700, Loss: 0.5084\n",
      "Iteration 1800, Loss: 0.5064\n",
      "Iteration 1900, Loss: 0.5045\n",
      "Iteration 2000, Loss: 0.5027\n",
      "Iteration 2100, Loss: 0.5010\n",
      "Iteration 2200, Loss: 0.4993\n",
      "Iteration 2300, Loss: 0.4978\n",
      "Iteration 2400, Loss: 0.4963\n",
      "Iteration 2500, Loss: 0.4949\n",
      "Iteration 2600, Loss: 0.4935\n",
      "Iteration 2700, Loss: 0.4922\n",
      "Iteration 2800, Loss: 0.4909\n",
      "Iteration 2900, Loss: 0.4897\n",
      "Iteration 0, Loss: 1.0174\n",
      "Iteration 100, Loss: 0.6165\n",
      "Iteration 200, Loss: 0.5862\n",
      "Iteration 300, Loss: 0.5686\n",
      "Iteration 400, Loss: 0.5560\n",
      "Iteration 500, Loss: 0.5462\n",
      "Iteration 600, Loss: 0.5384\n",
      "Iteration 700, Loss: 0.5318\n",
      "Iteration 800, Loss: 0.5262\n",
      "Iteration 900, Loss: 0.5213\n",
      "Iteration 1000, Loss: 0.5170\n",
      "Iteration 1100, Loss: 0.5131\n",
      "Iteration 1200, Loss: 0.5096\n",
      "Iteration 1300, Loss: 0.5064\n",
      "Iteration 1400, Loss: 0.5035\n",
      "Iteration 1500, Loss: 0.5008\n",
      "Iteration 1600, Loss: 0.4983\n",
      "Iteration 1700, Loss: 0.4959\n",
      "Iteration 1800, Loss: 0.4937\n",
      "Iteration 1900, Loss: 0.4916\n",
      "Iteration 2000, Loss: 0.4897\n",
      "Iteration 2100, Loss: 0.4878\n",
      "Iteration 2200, Loss: 0.4861\n",
      "Iteration 2300, Loss: 0.4844\n",
      "Iteration 2400, Loss: 0.4828\n",
      "Iteration 2500, Loss: 0.4813\n",
      "Iteration 2600, Loss: 0.4798\n",
      "Iteration 2700, Loss: 0.4785\n",
      "Iteration 2800, Loss: 0.4771\n",
      "Iteration 2900, Loss: 0.4758\n",
      "Iteration 0, Loss: 1.0529\n",
      "Iteration 100, Loss: 0.6399\n",
      "Iteration 200, Loss: 0.6034\n",
      "Iteration 300, Loss: 0.5851\n",
      "Iteration 400, Loss: 0.5726\n",
      "Iteration 500, Loss: 0.5632\n",
      "Iteration 600, Loss: 0.5555\n",
      "Iteration 700, Loss: 0.5492\n",
      "Iteration 800, Loss: 0.5437\n",
      "Iteration 900, Loss: 0.5389\n",
      "Iteration 1000, Loss: 0.5346\n",
      "Iteration 1100, Loss: 0.5308\n",
      "Iteration 1200, Loss: 0.5275\n",
      "Iteration 1300, Loss: 0.5242\n",
      "Iteration 1400, Loss: 0.5213\n",
      "Iteration 1500, Loss: 0.5186\n",
      "Iteration 1600, Loss: 0.5162\n",
      "Iteration 1700, Loss: 0.5138\n",
      "Iteration 1800, Loss: 0.5117\n",
      "Iteration 1900, Loss: 0.5096\n",
      "Iteration 2000, Loss: 0.5077\n",
      "Iteration 2100, Loss: 0.5059\n",
      "Iteration 2200, Loss: 0.5041\n",
      "Iteration 2300, Loss: 0.5024\n",
      "Iteration 2400, Loss: 0.5008\n",
      "Iteration 2500, Loss: 0.4993\n",
      "Iteration 2600, Loss: 0.4978\n",
      "Iteration 2700, Loss: 0.4964\n",
      "Iteration 2800, Loss: 0.4951\n",
      "Iteration 2900, Loss: 0.4938\n",
      "Iteration 0, Loss: 1.0549\n",
      "Iteration 100, Loss: 0.6703\n",
      "Iteration 200, Loss: 0.6353\n",
      "Iteration 300, Loss: 0.6163\n",
      "Iteration 400, Loss: 0.6033\n",
      "Iteration 500, Loss: 0.5934\n",
      "Iteration 600, Loss: 0.5855\n",
      "Iteration 700, Loss: 0.5787\n",
      "Iteration 800, Loss: 0.5730\n",
      "Iteration 900, Loss: 0.5680\n",
      "Iteration 1000, Loss: 0.5635\n",
      "Iteration 1100, Loss: 0.5596\n",
      "Iteration 1200, Loss: 0.5560\n",
      "Iteration 1300, Loss: 0.5527\n",
      "Iteration 1400, Loss: 0.5497\n",
      "Iteration 1500, Loss: 0.5469\n",
      "Iteration 1600, Loss: 0.5444\n",
      "Iteration 1700, Loss: 0.5419\n",
      "Iteration 1800, Loss: 0.5397\n",
      "Iteration 1900, Loss: 0.5376\n",
      "Iteration 2000, Loss: 0.5356\n",
      "Iteration 2100, Loss: 0.5337\n",
      "Iteration 2200, Loss: 0.5319\n",
      "Iteration 2300, Loss: 0.5302\n",
      "Iteration 2400, Loss: 0.5286\n",
      "Iteration 2500, Loss: 0.5270\n",
      "Iteration 2600, Loss: 0.5256\n",
      "Iteration 2700, Loss: 0.5241\n",
      "Iteration 2800, Loss: 0.5228\n",
      "Iteration 2900, Loss: 0.5215\n",
      "Iteration 0, Loss: 1.0817\n",
      "Iteration 100, Loss: 0.7274\n",
      "Iteration 200, Loss: 0.6774\n",
      "Iteration 300, Loss: 0.6535\n",
      "Iteration 400, Loss: 0.6383\n",
      "Iteration 500, Loss: 0.6274\n",
      "Iteration 600, Loss: 0.6189\n",
      "Iteration 700, Loss: 0.6119\n",
      "Iteration 800, Loss: 0.6060\n",
      "Iteration 900, Loss: 0.6008\n",
      "Iteration 1000, Loss: 0.5963\n",
      "Iteration 1100, Loss: 0.5921\n",
      "Iteration 1200, Loss: 0.5884\n",
      "Iteration 1300, Loss: 0.5849\n",
      "Iteration 1400, Loss: 0.5818\n",
      "Iteration 1500, Loss: 0.5788\n",
      "Iteration 1600, Loss: 0.5761\n",
      "Iteration 1700, Loss: 0.5735\n",
      "Iteration 1800, Loss: 0.5711\n",
      "Iteration 1900, Loss: 0.5688\n",
      "Iteration 2000, Loss: 0.5666\n",
      "Iteration 2100, Loss: 0.5646\n",
      "Iteration 2200, Loss: 0.5626\n",
      "Iteration 2300, Loss: 0.5607\n",
      "Iteration 2400, Loss: 0.5590\n",
      "Iteration 2500, Loss: 0.5573\n",
      "Iteration 2600, Loss: 0.5556\n",
      "Iteration 2700, Loss: 0.5541\n",
      "Iteration 2800, Loss: 0.5526\n",
      "Iteration 2900, Loss: 0.5511\n",
      "Iteration 0, Loss: 1.0793\n",
      "Iteration 100, Loss: 0.7153\n",
      "Iteration 200, Loss: 0.6674\n",
      "Iteration 300, Loss: 0.6443\n",
      "Iteration 400, Loss: 0.6295\n",
      "Iteration 500, Loss: 0.6188\n",
      "Iteration 600, Loss: 0.6103\n",
      "Iteration 700, Loss: 0.6034\n",
      "Iteration 800, Loss: 0.5976\n",
      "Iteration 900, Loss: 0.5925\n",
      "Iteration 1000, Loss: 0.5880\n",
      "Iteration 1100, Loss: 0.5840\n",
      "Iteration 1200, Loss: 0.5804\n",
      "Iteration 1300, Loss: 0.5771\n",
      "Iteration 1400, Loss: 0.5740\n",
      "Iteration 1500, Loss: 0.5712\n",
      "Iteration 1600, Loss: 0.5686\n",
      "Iteration 1700, Loss: 0.5662\n",
      "Iteration 1800, Loss: 0.5639\n",
      "Iteration 1900, Loss: 0.5618\n",
      "Iteration 2000, Loss: 0.5598\n",
      "Iteration 2100, Loss: 0.5579\n",
      "Iteration 2200, Loss: 0.5561\n",
      "Iteration 2300, Loss: 0.5544\n",
      "Iteration 2400, Loss: 0.5528\n",
      "Iteration 2500, Loss: 0.5512\n",
      "Iteration 2600, Loss: 0.5497\n",
      "Iteration 2700, Loss: 0.5483\n",
      "Iteration 2800, Loss: 0.5470\n",
      "Iteration 2900, Loss: 0.5457\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7876\n",
      "Iteration 200, Loss: 0.7217\n",
      "Iteration 300, Loss: 0.6918\n",
      "Iteration 400, Loss: 0.6738\n",
      "Iteration 500, Loss: 0.6610\n",
      "Iteration 600, Loss: 0.6512\n",
      "Iteration 700, Loss: 0.6433\n",
      "Iteration 800, Loss: 0.6368\n",
      "Iteration 900, Loss: 0.6313\n",
      "Iteration 1000, Loss: 0.6264\n",
      "Iteration 1100, Loss: 0.6221\n",
      "Iteration 1200, Loss: 0.6182\n",
      "Iteration 1300, Loss: 0.6146\n",
      "Iteration 1400, Loss: 0.6114\n",
      "Iteration 1500, Loss: 0.6084\n",
      "Iteration 1600, Loss: 0.6056\n",
      "Iteration 1700, Loss: 0.6030\n",
      "Iteration 1800, Loss: 0.6006\n",
      "Iteration 1900, Loss: 0.5983\n",
      "Iteration 2000, Loss: 0.5961\n",
      "Iteration 2100, Loss: 0.5940\n",
      "Iteration 2200, Loss: 0.5921\n",
      "Iteration 2300, Loss: 0.5902\n",
      "Iteration 2400, Loss: 0.5884\n",
      "Iteration 2500, Loss: 0.5867\n",
      "Iteration 2600, Loss: 0.5850\n",
      "Iteration 2700, Loss: 0.5834\n",
      "Iteration 2800, Loss: 0.5819\n",
      "Iteration 2900, Loss: 0.5804\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7850\n",
      "Iteration 200, Loss: 0.7207\n",
      "Iteration 300, Loss: 0.6906\n",
      "Iteration 400, Loss: 0.6717\n",
      "Iteration 500, Loss: 0.6581\n",
      "Iteration 600, Loss: 0.6476\n",
      "Iteration 700, Loss: 0.6392\n",
      "Iteration 800, Loss: 0.6322\n",
      "Iteration 900, Loss: 0.6262\n",
      "Iteration 1000, Loss: 0.6210\n",
      "Iteration 1100, Loss: 0.6164\n",
      "Iteration 1200, Loss: 0.6123\n",
      "Iteration 1300, Loss: 0.6086\n",
      "Iteration 1400, Loss: 0.6051\n",
      "Iteration 1500, Loss: 0.6019\n",
      "Iteration 1600, Loss: 0.5990\n",
      "Iteration 1700, Loss: 0.5963\n",
      "Iteration 1800, Loss: 0.5937\n",
      "Iteration 1900, Loss: 0.5913\n",
      "Iteration 2000, Loss: 0.5890\n",
      "Iteration 2100, Loss: 0.5868\n",
      "Iteration 2200, Loss: 0.5847\n",
      "Iteration 2300, Loss: 0.5828\n",
      "Iteration 2400, Loss: 0.5809\n",
      "Iteration 2500, Loss: 0.5791\n",
      "Iteration 2600, Loss: 0.5774\n",
      "Iteration 2700, Loss: 0.5758\n",
      "Iteration 2800, Loss: 0.5742\n",
      "Iteration 2900, Loss: 0.5727\n",
      "Iteration 0, Loss: 1.0940\n",
      "Iteration 100, Loss: 0.8677\n",
      "Iteration 200, Loss: 0.7896\n",
      "Iteration 300, Loss: 0.7505\n",
      "Iteration 400, Loss: 0.7267\n",
      "Iteration 500, Loss: 0.7104\n",
      "Iteration 600, Loss: 0.6979\n",
      "Iteration 700, Loss: 0.6881\n",
      "Iteration 800, Loss: 0.6799\n",
      "Iteration 900, Loss: 0.6728\n",
      "Iteration 1000, Loss: 0.6667\n",
      "Iteration 1100, Loss: 0.6613\n",
      "Iteration 1200, Loss: 0.6565\n",
      "Iteration 1300, Loss: 0.6522\n",
      "Iteration 1400, Loss: 0.6483\n",
      "Iteration 1500, Loss: 0.6447\n",
      "Iteration 1600, Loss: 0.6414\n",
      "Iteration 1700, Loss: 0.6383\n",
      "Iteration 1800, Loss: 0.6354\n",
      "Iteration 1900, Loss: 0.6327\n",
      "Iteration 2000, Loss: 0.6302\n",
      "Iteration 2100, Loss: 0.6278\n",
      "Iteration 2200, Loss: 0.6255\n",
      "Iteration 2300, Loss: 0.6234\n",
      "Iteration 2400, Loss: 0.6214\n",
      "Iteration 2500, Loss: 0.6194\n",
      "Iteration 2600, Loss: 0.6176\n",
      "Iteration 2700, Loss: 0.6158\n",
      "Iteration 2800, Loss: 0.6141\n",
      "Iteration 2900, Loss: 0.6125\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8524\n",
      "Iteration 200, Loss: 0.7739\n",
      "Iteration 300, Loss: 0.7341\n",
      "Iteration 400, Loss: 0.7094\n",
      "Iteration 500, Loss: 0.6922\n",
      "Iteration 600, Loss: 0.6794\n",
      "Iteration 700, Loss: 0.6692\n",
      "Iteration 800, Loss: 0.6608\n",
      "Iteration 900, Loss: 0.6537\n",
      "Iteration 1000, Loss: 0.6476\n",
      "Iteration 1100, Loss: 0.6422\n",
      "Iteration 1200, Loss: 0.6375\n",
      "Iteration 1300, Loss: 0.6332\n",
      "Iteration 1400, Loss: 0.6293\n",
      "Iteration 1500, Loss: 0.6258\n",
      "Iteration 1600, Loss: 0.6225\n",
      "Iteration 1700, Loss: 0.6196\n",
      "Iteration 1800, Loss: 0.6168\n",
      "Iteration 1900, Loss: 0.6142\n",
      "Iteration 2000, Loss: 0.6118\n",
      "Iteration 2100, Loss: 0.6095\n",
      "Iteration 2200, Loss: 0.6074\n",
      "Iteration 2300, Loss: 0.6053\n",
      "Iteration 2400, Loss: 0.6034\n",
      "Iteration 2500, Loss: 0.6015\n",
      "Iteration 2600, Loss: 0.5998\n",
      "Iteration 2700, Loss: 0.5981\n",
      "Iteration 2800, Loss: 0.5965\n",
      "Iteration 2900, Loss: 0.5949\n",
      "Iteration 0, Loss: 1.0966\n",
      "Iteration 100, Loss: 0.9612\n",
      "Iteration 200, Loss: 0.8889\n",
      "Iteration 300, Loss: 0.8424\n",
      "Iteration 400, Loss: 0.8097\n",
      "Iteration 500, Loss: 0.7852\n",
      "Iteration 600, Loss: 0.7660\n",
      "Iteration 700, Loss: 0.7507\n",
      "Iteration 800, Loss: 0.7380\n",
      "Iteration 900, Loss: 0.7273\n",
      "Iteration 1000, Loss: 0.7182\n",
      "Iteration 1100, Loss: 0.7103\n",
      "Iteration 1200, Loss: 0.7033\n",
      "Iteration 1300, Loss: 0.6972\n",
      "Iteration 1400, Loss: 0.6916\n",
      "Iteration 1500, Loss: 0.6866\n",
      "Iteration 1600, Loss: 0.6821\n",
      "Iteration 1700, Loss: 0.6779\n",
      "Iteration 1800, Loss: 0.6740\n",
      "Iteration 1900, Loss: 0.6704\n",
      "Iteration 2000, Loss: 0.6671\n",
      "Iteration 2100, Loss: 0.6640\n",
      "Iteration 2200, Loss: 0.6611\n",
      "Iteration 2300, Loss: 0.6583\n",
      "Iteration 2400, Loss: 0.6557\n",
      "Iteration 2500, Loss: 0.6532\n",
      "Iteration 2600, Loss: 0.6509\n",
      "Iteration 2700, Loss: 0.6487\n",
      "Iteration 2800, Loss: 0.6466\n",
      "Iteration 2900, Loss: 0.6446\n",
      "Iteration 0, Loss: 1.0969\n",
      "Iteration 100, Loss: 0.9719\n",
      "Iteration 200, Loss: 0.8997\n",
      "Iteration 300, Loss: 0.8519\n",
      "Iteration 400, Loss: 0.8180\n",
      "Iteration 500, Loss: 0.7930\n",
      "Iteration 600, Loss: 0.7738\n",
      "Iteration 700, Loss: 0.7586\n",
      "Iteration 800, Loss: 0.7463\n",
      "Iteration 900, Loss: 0.7362\n",
      "Iteration 1000, Loss: 0.7276\n",
      "Iteration 1100, Loss: 0.7202\n",
      "Iteration 1200, Loss: 0.7138\n",
      "Iteration 1300, Loss: 0.7082\n",
      "Iteration 1400, Loss: 0.7031\n",
      "Iteration 1500, Loss: 0.6985\n",
      "Iteration 1600, Loss: 0.6944\n",
      "Iteration 1700, Loss: 0.6906\n",
      "Iteration 1800, Loss: 0.6871\n",
      "Iteration 1900, Loss: 0.6838\n",
      "Iteration 2000, Loss: 0.6808\n",
      "Iteration 2100, Loss: 0.6780\n",
      "Iteration 2200, Loss: 0.6753\n",
      "Iteration 2300, Loss: 0.6728\n",
      "Iteration 2400, Loss: 0.6705\n",
      "Iteration 2500, Loss: 0.6682\n",
      "Iteration 2600, Loss: 0.6661\n",
      "Iteration 2700, Loss: 0.6641\n",
      "Iteration 2800, Loss: 0.6622\n",
      "Iteration 2900, Loss: 0.6603\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0209\n",
      "Iteration 200, Loss: 0.9677\n",
      "Iteration 300, Loss: 0.9280\n",
      "Iteration 400, Loss: 0.8967\n",
      "Iteration 500, Loss: 0.8716\n",
      "Iteration 600, Loss: 0.8509\n",
      "Iteration 700, Loss: 0.8335\n",
      "Iteration 800, Loss: 0.8187\n",
      "Iteration 900, Loss: 0.8059\n",
      "Iteration 1000, Loss: 0.7948\n",
      "Iteration 1100, Loss: 0.7850\n",
      "Iteration 1200, Loss: 0.7763\n",
      "Iteration 1300, Loss: 0.7686\n",
      "Iteration 1400, Loss: 0.7616\n",
      "Iteration 1500, Loss: 0.7553\n",
      "Iteration 1600, Loss: 0.7495\n",
      "Iteration 1700, Loss: 0.7443\n",
      "Iteration 1800, Loss: 0.7394\n",
      "Iteration 1900, Loss: 0.7350\n",
      "Iteration 2000, Loss: 0.7309\n",
      "Iteration 2100, Loss: 0.7270\n",
      "Iteration 2200, Loss: 0.7234\n",
      "Iteration 2300, Loss: 0.7201\n",
      "Iteration 2400, Loss: 0.7169\n",
      "Iteration 2500, Loss: 0.7140\n",
      "Iteration 2600, Loss: 0.7112\n",
      "Iteration 2700, Loss: 0.7085\n",
      "Iteration 2800, Loss: 0.7060\n",
      "Iteration 2900, Loss: 0.7036\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0178\n",
      "Iteration 200, Loss: 0.9618\n",
      "Iteration 300, Loss: 0.9196\n",
      "Iteration 400, Loss: 0.8865\n",
      "Iteration 500, Loss: 0.8596\n",
      "Iteration 600, Loss: 0.8375\n",
      "Iteration 700, Loss: 0.8189\n",
      "Iteration 800, Loss: 0.8030\n",
      "Iteration 900, Loss: 0.7894\n",
      "Iteration 1000, Loss: 0.7776\n",
      "Iteration 1100, Loss: 0.7672\n",
      "Iteration 1200, Loss: 0.7581\n",
      "Iteration 1300, Loss: 0.7499\n",
      "Iteration 1400, Loss: 0.7426\n",
      "Iteration 1500, Loss: 0.7361\n",
      "Iteration 1600, Loss: 0.7301\n",
      "Iteration 1700, Loss: 0.7247\n",
      "Iteration 1800, Loss: 0.7197\n",
      "Iteration 1900, Loss: 0.7151\n",
      "Iteration 2000, Loss: 0.7108\n",
      "Iteration 2100, Loss: 0.7069\n",
      "Iteration 2200, Loss: 0.7032\n",
      "Iteration 2300, Loss: 0.6998\n",
      "Iteration 2400, Loss: 0.6965\n",
      "Iteration 2500, Loss: 0.6935\n",
      "Iteration 2600, Loss: 0.6906\n",
      "Iteration 2700, Loss: 0.6879\n",
      "Iteration 2800, Loss: 0.6853\n",
      "Iteration 2900, Loss: 0.6828\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0549\n",
      "Iteration 200, Loss: 1.0203\n",
      "Iteration 300, Loss: 0.9913\n",
      "Iteration 400, Loss: 0.9668\n",
      "Iteration 500, Loss: 0.9457\n",
      "Iteration 600, Loss: 0.9271\n",
      "Iteration 700, Loss: 0.9107\n",
      "Iteration 800, Loss: 0.8961\n",
      "Iteration 900, Loss: 0.8829\n",
      "Iteration 1000, Loss: 0.8710\n",
      "Iteration 1100, Loss: 0.8602\n",
      "Iteration 1200, Loss: 0.8504\n",
      "Iteration 1300, Loss: 0.8413\n",
      "Iteration 1400, Loss: 0.8331\n",
      "Iteration 1500, Loss: 0.8253\n",
      "Iteration 1600, Loss: 0.8182\n",
      "Iteration 1700, Loss: 0.8115\n",
      "Iteration 1800, Loss: 0.8054\n",
      "Iteration 1900, Loss: 0.7996\n",
      "Iteration 2000, Loss: 0.7942\n",
      "Iteration 2100, Loss: 0.7891\n",
      "Iteration 2200, Loss: 0.7844\n",
      "Iteration 2300, Loss: 0.7799\n",
      "Iteration 2400, Loss: 0.7757\n",
      "Iteration 2500, Loss: 0.7717\n",
      "Iteration 2600, Loss: 0.7679\n",
      "Iteration 2700, Loss: 0.7643\n",
      "Iteration 2800, Loss: 0.7609\n",
      "Iteration 2900, Loss: 0.7577\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0522\n",
      "Iteration 200, Loss: 1.0151\n",
      "Iteration 300, Loss: 0.9839\n",
      "Iteration 400, Loss: 0.9574\n",
      "Iteration 500, Loss: 0.9346\n",
      "Iteration 600, Loss: 0.9144\n",
      "Iteration 700, Loss: 0.8967\n",
      "Iteration 800, Loss: 0.8810\n",
      "Iteration 900, Loss: 0.8668\n",
      "Iteration 1000, Loss: 0.8541\n",
      "Iteration 1100, Loss: 0.8425\n",
      "Iteration 1200, Loss: 0.8321\n",
      "Iteration 1300, Loss: 0.8224\n",
      "Iteration 1400, Loss: 0.8135\n",
      "Iteration 1500, Loss: 0.8054\n",
      "Iteration 1600, Loss: 0.7979\n",
      "Iteration 1700, Loss: 0.7909\n",
      "Iteration 1800, Loss: 0.7844\n",
      "Iteration 1900, Loss: 0.7784\n",
      "Iteration 2000, Loss: 0.7728\n",
      "Iteration 2100, Loss: 0.7675\n",
      "Iteration 2200, Loss: 0.7625\n",
      "Iteration 2300, Loss: 0.7578\n",
      "Iteration 2400, Loss: 0.7535\n",
      "Iteration 2500, Loss: 0.7493\n",
      "Iteration 2600, Loss: 0.7454\n",
      "Iteration 2700, Loss: 0.7416\n",
      "Iteration 2800, Loss: 0.7381\n",
      "Iteration 2900, Loss: 0.7348\n",
      "Iteration 0, Loss: 0.9671\n",
      "Iteration 100, Loss: 0.5993\n",
      "Iteration 200, Loss: 0.5702\n",
      "Iteration 300, Loss: 0.5533\n",
      "Iteration 400, Loss: 0.5414\n",
      "Iteration 500, Loss: 0.5323\n",
      "Iteration 600, Loss: 0.5248\n",
      "Iteration 700, Loss: 0.5184\n",
      "Iteration 800, Loss: 0.5130\n",
      "Iteration 900, Loss: 0.5081\n",
      "Iteration 1000, Loss: 0.5038\n",
      "Iteration 1100, Loss: 0.4999\n",
      "Iteration 1200, Loss: 0.4964\n",
      "Iteration 1300, Loss: 0.4931\n",
      "Iteration 1400, Loss: 0.4901\n",
      "Iteration 1500, Loss: 0.4874\n",
      "Iteration 1600, Loss: 0.4848\n",
      "Iteration 1700, Loss: 0.4824\n",
      "Iteration 1800, Loss: 0.4801\n",
      "Iteration 1900, Loss: 0.4780\n",
      "Iteration 2000, Loss: 0.4760\n",
      "Iteration 2100, Loss: 0.4741\n",
      "Iteration 2200, Loss: 0.4723\n",
      "Iteration 2300, Loss: 0.4706\n",
      "Iteration 2400, Loss: 0.4690\n",
      "Iteration 2500, Loss: 0.4674\n",
      "Iteration 2600, Loss: 0.4659\n",
      "Iteration 2700, Loss: 0.4645\n",
      "Iteration 2800, Loss: 0.4632\n",
      "Iteration 2900, Loss: 0.4619\n",
      "Iteration 3000, Loss: 0.4606\n",
      "Iteration 3100, Loss: 0.4594\n",
      "Iteration 3200, Loss: 0.4583\n",
      "Iteration 3300, Loss: 0.4572\n",
      "Iteration 3400, Loss: 0.4561\n",
      "Iteration 3500, Loss: 0.4551\n",
      "Iteration 3600, Loss: 0.4541\n",
      "Iteration 3700, Loss: 0.4531\n",
      "Iteration 3800, Loss: 0.4522\n",
      "Iteration 3900, Loss: 0.4512\n",
      "Iteration 0, Loss: 0.9663\n",
      "Iteration 100, Loss: 0.5834\n",
      "Iteration 200, Loss: 0.5534\n",
      "Iteration 300, Loss: 0.5369\n",
      "Iteration 400, Loss: 0.5257\n",
      "Iteration 500, Loss: 0.5173\n",
      "Iteration 600, Loss: 0.5107\n",
      "Iteration 700, Loss: 0.5051\n",
      "Iteration 800, Loss: 0.5004\n",
      "Iteration 900, Loss: 0.4963\n",
      "Iteration 1000, Loss: 0.4927\n",
      "Iteration 1100, Loss: 0.4894\n",
      "Iteration 1200, Loss: 0.4865\n",
      "Iteration 1300, Loss: 0.4838\n",
      "Iteration 1400, Loss: 0.4813\n",
      "Iteration 1500, Loss: 0.4790\n",
      "Iteration 1600, Loss: 0.4769\n",
      "Iteration 1700, Loss: 0.4749\n",
      "Iteration 1800, Loss: 0.4730\n",
      "Iteration 1900, Loss: 0.4712\n",
      "Iteration 2000, Loss: 0.4695\n",
      "Iteration 2100, Loss: 0.4679\n",
      "Iteration 2200, Loss: 0.4664\n",
      "Iteration 2300, Loss: 0.4650\n",
      "Iteration 2400, Loss: 0.4636\n",
      "Iteration 2500, Loss: 0.4623\n",
      "Iteration 2600, Loss: 0.4610\n",
      "Iteration 2700, Loss: 0.4598\n",
      "Iteration 2800, Loss: 0.4586\n",
      "Iteration 2900, Loss: 0.4575\n",
      "Iteration 3000, Loss: 0.4564\n",
      "Iteration 3100, Loss: 0.4553\n",
      "Iteration 3200, Loss: 0.4543\n",
      "Iteration 3300, Loss: 0.4533\n",
      "Iteration 3400, Loss: 0.4523\n",
      "Iteration 3500, Loss: 0.4514\n",
      "Iteration 3600, Loss: 0.4505\n",
      "Iteration 3700, Loss: 0.4496\n",
      "Iteration 3800, Loss: 0.4488\n",
      "Iteration 3900, Loss: 0.4479\n",
      "Iteration 0, Loss: 1.0251\n",
      "Iteration 100, Loss: 0.6218\n",
      "Iteration 200, Loss: 0.5892\n",
      "Iteration 300, Loss: 0.5716\n",
      "Iteration 400, Loss: 0.5596\n",
      "Iteration 500, Loss: 0.5507\n",
      "Iteration 600, Loss: 0.5435\n",
      "Iteration 700, Loss: 0.5376\n",
      "Iteration 800, Loss: 0.5326\n",
      "Iteration 900, Loss: 0.5282\n",
      "Iteration 1000, Loss: 0.5244\n",
      "Iteration 1100, Loss: 0.5209\n",
      "Iteration 1200, Loss: 0.5178\n",
      "Iteration 1300, Loss: 0.5149\n",
      "Iteration 1400, Loss: 0.5123\n",
      "Iteration 1500, Loss: 0.5099\n",
      "Iteration 1600, Loss: 0.5076\n",
      "Iteration 1700, Loss: 0.5054\n",
      "Iteration 1800, Loss: 0.5034\n",
      "Iteration 1900, Loss: 0.5015\n",
      "Iteration 2000, Loss: 0.4997\n",
      "Iteration 2100, Loss: 0.4980\n",
      "Iteration 2200, Loss: 0.4963\n",
      "Iteration 2300, Loss: 0.4947\n",
      "Iteration 2400, Loss: 0.4932\n",
      "Iteration 2500, Loss: 0.4918\n",
      "Iteration 2600, Loss: 0.4903\n",
      "Iteration 2700, Loss: 0.4890\n",
      "Iteration 2800, Loss: 0.4877\n",
      "Iteration 2900, Loss: 0.4865\n",
      "Iteration 3000, Loss: 0.4852\n",
      "Iteration 3100, Loss: 0.4841\n",
      "Iteration 3200, Loss: 0.4829\n",
      "Iteration 3300, Loss: 0.4818\n",
      "Iteration 3400, Loss: 0.4808\n",
      "Iteration 3500, Loss: 0.4797\n",
      "Iteration 3600, Loss: 0.4787\n",
      "Iteration 3700, Loss: 0.4778\n",
      "Iteration 3800, Loss: 0.4768\n",
      "Iteration 3900, Loss: 0.4759\n",
      "Iteration 0, Loss: 1.0166\n",
      "Iteration 100, Loss: 0.6215\n",
      "Iteration 200, Loss: 0.5915\n",
      "Iteration 300, Loss: 0.5743\n",
      "Iteration 400, Loss: 0.5623\n",
      "Iteration 500, Loss: 0.5531\n",
      "Iteration 600, Loss: 0.5456\n",
      "Iteration 700, Loss: 0.5394\n",
      "Iteration 800, Loss: 0.5340\n",
      "Iteration 900, Loss: 0.5292\n",
      "Iteration 1000, Loss: 0.5250\n",
      "Iteration 1100, Loss: 0.5212\n",
      "Iteration 1200, Loss: 0.5178\n",
      "Iteration 1300, Loss: 0.5146\n",
      "Iteration 1400, Loss: 0.5117\n",
      "Iteration 1500, Loss: 0.5090\n",
      "Iteration 1600, Loss: 0.5065\n",
      "Iteration 1700, Loss: 0.5041\n",
      "Iteration 1800, Loss: 0.5020\n",
      "Iteration 1900, Loss: 0.4998\n",
      "Iteration 2000, Loss: 0.4979\n",
      "Iteration 2100, Loss: 0.4960\n",
      "Iteration 2200, Loss: 0.4942\n",
      "Iteration 2300, Loss: 0.4925\n",
      "Iteration 2400, Loss: 0.4909\n",
      "Iteration 2500, Loss: 0.4894\n",
      "Iteration 2600, Loss: 0.4879\n",
      "Iteration 2700, Loss: 0.4865\n",
      "Iteration 2800, Loss: 0.4851\n",
      "Iteration 2900, Loss: 0.4839\n",
      "Iteration 3000, Loss: 0.4826\n",
      "Iteration 3100, Loss: 0.4814\n",
      "Iteration 3200, Loss: 0.4802\n",
      "Iteration 3300, Loss: 0.4791\n",
      "Iteration 3400, Loss: 0.4780\n",
      "Iteration 3500, Loss: 0.4770\n",
      "Iteration 3600, Loss: 0.4760\n",
      "Iteration 3700, Loss: 0.4750\n",
      "Iteration 3800, Loss: 0.4741\n",
      "Iteration 3900, Loss: 0.4731\n",
      "Iteration 0, Loss: 1.0535\n",
      "Iteration 100, Loss: 0.6498\n",
      "Iteration 200, Loss: 0.6139\n",
      "Iteration 300, Loss: 0.5954\n",
      "Iteration 400, Loss: 0.5830\n",
      "Iteration 500, Loss: 0.5737\n",
      "Iteration 600, Loss: 0.5664\n",
      "Iteration 700, Loss: 0.5604\n",
      "Iteration 800, Loss: 0.5551\n",
      "Iteration 900, Loss: 0.5506\n",
      "Iteration 1000, Loss: 0.5466\n",
      "Iteration 1100, Loss: 0.5430\n",
      "Iteration 1200, Loss: 0.5398\n",
      "Iteration 1300, Loss: 0.5368\n",
      "Iteration 1400, Loss: 0.5341\n",
      "Iteration 1500, Loss: 0.5315\n",
      "Iteration 1600, Loss: 0.5291\n",
      "Iteration 1700, Loss: 0.5269\n",
      "Iteration 1800, Loss: 0.5248\n",
      "Iteration 1900, Loss: 0.5228\n",
      "Iteration 2000, Loss: 0.5209\n",
      "Iteration 2100, Loss: 0.5191\n",
      "Iteration 2200, Loss: 0.5174\n",
      "Iteration 2300, Loss: 0.5158\n",
      "Iteration 2400, Loss: 0.5142\n",
      "Iteration 2500, Loss: 0.5127\n",
      "Iteration 2600, Loss: 0.5112\n",
      "Iteration 2700, Loss: 0.5098\n",
      "Iteration 2800, Loss: 0.5085\n",
      "Iteration 2900, Loss: 0.5072\n",
      "Iteration 3000, Loss: 0.5060\n",
      "Iteration 3100, Loss: 0.5047\n",
      "Iteration 3200, Loss: 0.5035\n",
      "Iteration 3300, Loss: 0.5024\n",
      "Iteration 3400, Loss: 0.5013\n",
      "Iteration 3500, Loss: 0.5002\n",
      "Iteration 3600, Loss: 0.4992\n",
      "Iteration 3700, Loss: 0.4981\n",
      "Iteration 3800, Loss: 0.4972\n",
      "Iteration 3900, Loss: 0.4962\n",
      "Iteration 0, Loss: 1.0545\n",
      "Iteration 100, Loss: 0.6620\n",
      "Iteration 200, Loss: 0.6273\n",
      "Iteration 300, Loss: 0.6088\n",
      "Iteration 400, Loss: 0.5958\n",
      "Iteration 500, Loss: 0.5858\n",
      "Iteration 600, Loss: 0.5777\n",
      "Iteration 700, Loss: 0.5708\n",
      "Iteration 800, Loss: 0.5649\n",
      "Iteration 900, Loss: 0.5596\n",
      "Iteration 1000, Loss: 0.5550\n",
      "Iteration 1100, Loss: 0.5508\n",
      "Iteration 1200, Loss: 0.5470\n",
      "Iteration 1300, Loss: 0.5435\n",
      "Iteration 1400, Loss: 0.5403\n",
      "Iteration 1500, Loss: 0.5374\n",
      "Iteration 1600, Loss: 0.5346\n",
      "Iteration 1700, Loss: 0.5321\n",
      "Iteration 1800, Loss: 0.5297\n",
      "Iteration 1900, Loss: 0.5275\n",
      "Iteration 2000, Loss: 0.5254\n",
      "Iteration 2100, Loss: 0.5234\n",
      "Iteration 2200, Loss: 0.5215\n",
      "Iteration 2300, Loss: 0.5197\n",
      "Iteration 2400, Loss: 0.5180\n",
      "Iteration 2500, Loss: 0.5163\n",
      "Iteration 2600, Loss: 0.5148\n",
      "Iteration 2700, Loss: 0.5133\n",
      "Iteration 2800, Loss: 0.5119\n",
      "Iteration 2900, Loss: 0.5105\n",
      "Iteration 3000, Loss: 0.5092\n",
      "Iteration 3100, Loss: 0.5079\n",
      "Iteration 3200, Loss: 0.5067\n",
      "Iteration 3300, Loss: 0.5055\n",
      "Iteration 3400, Loss: 0.5043\n",
      "Iteration 3500, Loss: 0.5032\n",
      "Iteration 3600, Loss: 0.5022\n",
      "Iteration 3700, Loss: 0.5011\n",
      "Iteration 3800, Loss: 0.5001\n",
      "Iteration 3900, Loss: 0.4992\n",
      "Iteration 0, Loss: 1.0789\n",
      "Iteration 100, Loss: 0.7086\n",
      "Iteration 200, Loss: 0.6570\n",
      "Iteration 300, Loss: 0.6319\n",
      "Iteration 400, Loss: 0.6158\n",
      "Iteration 500, Loss: 0.6043\n",
      "Iteration 600, Loss: 0.5952\n",
      "Iteration 700, Loss: 0.5879\n",
      "Iteration 800, Loss: 0.5816\n",
      "Iteration 900, Loss: 0.5762\n",
      "Iteration 1000, Loss: 0.5715\n",
      "Iteration 1100, Loss: 0.5672\n",
      "Iteration 1200, Loss: 0.5634\n",
      "Iteration 1300, Loss: 0.5598\n",
      "Iteration 1400, Loss: 0.5566\n",
      "Iteration 1500, Loss: 0.5536\n",
      "Iteration 1600, Loss: 0.5508\n",
      "Iteration 1700, Loss: 0.5482\n",
      "Iteration 1800, Loss: 0.5458\n",
      "Iteration 1900, Loss: 0.5435\n",
      "Iteration 2000, Loss: 0.5413\n",
      "Iteration 2100, Loss: 0.5392\n",
      "Iteration 2200, Loss: 0.5373\n",
      "Iteration 2300, Loss: 0.5354\n",
      "Iteration 2400, Loss: 0.5336\n",
      "Iteration 2500, Loss: 0.5319\n",
      "Iteration 2600, Loss: 0.5303\n",
      "Iteration 2700, Loss: 0.5287\n",
      "Iteration 2800, Loss: 0.5272\n",
      "Iteration 2900, Loss: 0.5258\n",
      "Iteration 3000, Loss: 0.5244\n",
      "Iteration 3100, Loss: 0.5231\n",
      "Iteration 3200, Loss: 0.5218\n",
      "Iteration 3300, Loss: 0.5205\n",
      "Iteration 3400, Loss: 0.5193\n",
      "Iteration 3500, Loss: 0.5181\n",
      "Iteration 3600, Loss: 0.5170\n",
      "Iteration 3700, Loss: 0.5159\n",
      "Iteration 3800, Loss: 0.5149\n",
      "Iteration 3900, Loss: 0.5138\n",
      "Iteration 0, Loss: 1.0819\n",
      "Iteration 100, Loss: 0.7348\n",
      "Iteration 200, Loss: 0.6887\n",
      "Iteration 300, Loss: 0.6667\n",
      "Iteration 400, Loss: 0.6525\n",
      "Iteration 500, Loss: 0.6420\n",
      "Iteration 600, Loss: 0.6336\n",
      "Iteration 700, Loss: 0.6267\n",
      "Iteration 800, Loss: 0.6207\n",
      "Iteration 900, Loss: 0.6155\n",
      "Iteration 1000, Loss: 0.6107\n",
      "Iteration 1100, Loss: 0.6065\n",
      "Iteration 1200, Loss: 0.6026\n",
      "Iteration 1300, Loss: 0.5991\n",
      "Iteration 1400, Loss: 0.5958\n",
      "Iteration 1500, Loss: 0.5927\n",
      "Iteration 1600, Loss: 0.5898\n",
      "Iteration 1700, Loss: 0.5872\n",
      "Iteration 1800, Loss: 0.5846\n",
      "Iteration 1900, Loss: 0.5823\n",
      "Iteration 2000, Loss: 0.5800\n",
      "Iteration 2100, Loss: 0.5779\n",
      "Iteration 2200, Loss: 0.5759\n",
      "Iteration 2300, Loss: 0.5740\n",
      "Iteration 2400, Loss: 0.5721\n",
      "Iteration 2500, Loss: 0.5703\n",
      "Iteration 2600, Loss: 0.5687\n",
      "Iteration 2700, Loss: 0.5670\n",
      "Iteration 2800, Loss: 0.5655\n",
      "Iteration 2900, Loss: 0.5640\n",
      "Iteration 3000, Loss: 0.5625\n",
      "Iteration 3100, Loss: 0.5611\n",
      "Iteration 3200, Loss: 0.5598\n",
      "Iteration 3300, Loss: 0.5585\n",
      "Iteration 3400, Loss: 0.5572\n",
      "Iteration 3500, Loss: 0.5560\n",
      "Iteration 3600, Loss: 0.5548\n",
      "Iteration 3700, Loss: 0.5536\n",
      "Iteration 3800, Loss: 0.5525\n",
      "Iteration 3900, Loss: 0.5514\n",
      "Iteration 0, Loss: 1.0894\n",
      "Iteration 100, Loss: 0.7888\n",
      "Iteration 200, Loss: 0.7243\n",
      "Iteration 300, Loss: 0.6951\n",
      "Iteration 400, Loss: 0.6772\n",
      "Iteration 500, Loss: 0.6645\n",
      "Iteration 600, Loss: 0.6547\n",
      "Iteration 700, Loss: 0.6469\n",
      "Iteration 800, Loss: 0.6403\n",
      "Iteration 900, Loss: 0.6346\n",
      "Iteration 1000, Loss: 0.6297\n",
      "Iteration 1100, Loss: 0.6253\n",
      "Iteration 1200, Loss: 0.6213\n",
      "Iteration 1300, Loss: 0.6177\n",
      "Iteration 1400, Loss: 0.6143\n",
      "Iteration 1500, Loss: 0.6112\n",
      "Iteration 1600, Loss: 0.6083\n",
      "Iteration 1700, Loss: 0.6056\n",
      "Iteration 1800, Loss: 0.6031\n",
      "Iteration 1900, Loss: 0.6007\n",
      "Iteration 2000, Loss: 0.5984\n",
      "Iteration 2100, Loss: 0.5962\n",
      "Iteration 2200, Loss: 0.5942\n",
      "Iteration 2300, Loss: 0.5922\n",
      "Iteration 2400, Loss: 0.5903\n",
      "Iteration 2500, Loss: 0.5885\n",
      "Iteration 2600, Loss: 0.5868\n",
      "Iteration 2700, Loss: 0.5852\n",
      "Iteration 2800, Loss: 0.5836\n",
      "Iteration 2900, Loss: 0.5821\n",
      "Iteration 3000, Loss: 0.5806\n",
      "Iteration 3100, Loss: 0.5792\n",
      "Iteration 3200, Loss: 0.5778\n",
      "Iteration 3300, Loss: 0.5765\n",
      "Iteration 3400, Loss: 0.5753\n",
      "Iteration 3500, Loss: 0.5740\n",
      "Iteration 3600, Loss: 0.5728\n",
      "Iteration 3700, Loss: 0.5717\n",
      "Iteration 3800, Loss: 0.5706\n",
      "Iteration 3900, Loss: 0.5695\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7822\n",
      "Iteration 200, Loss: 0.7153\n",
      "Iteration 300, Loss: 0.6845\n",
      "Iteration 400, Loss: 0.6655\n",
      "Iteration 500, Loss: 0.6521\n",
      "Iteration 600, Loss: 0.6418\n",
      "Iteration 700, Loss: 0.6335\n",
      "Iteration 800, Loss: 0.6266\n",
      "Iteration 900, Loss: 0.6208\n",
      "Iteration 1000, Loss: 0.6157\n",
      "Iteration 1100, Loss: 0.6112\n",
      "Iteration 1200, Loss: 0.6072\n",
      "Iteration 1300, Loss: 0.6036\n",
      "Iteration 1400, Loss: 0.6003\n",
      "Iteration 1500, Loss: 0.5972\n",
      "Iteration 1600, Loss: 0.5944\n",
      "Iteration 1700, Loss: 0.5917\n",
      "Iteration 1800, Loss: 0.5893\n",
      "Iteration 1900, Loss: 0.5870\n",
      "Iteration 2000, Loss: 0.5848\n",
      "Iteration 2100, Loss: 0.5827\n",
      "Iteration 2200, Loss: 0.5808\n",
      "Iteration 2300, Loss: 0.5789\n",
      "Iteration 2400, Loss: 0.5771\n",
      "Iteration 2500, Loss: 0.5754\n",
      "Iteration 2600, Loss: 0.5738\n",
      "Iteration 2700, Loss: 0.5722\n",
      "Iteration 2800, Loss: 0.5707\n",
      "Iteration 2900, Loss: 0.5693\n",
      "Iteration 3000, Loss: 0.5679\n",
      "Iteration 3100, Loss: 0.5665\n",
      "Iteration 3200, Loss: 0.5652\n",
      "Iteration 3300, Loss: 0.5640\n",
      "Iteration 3400, Loss: 0.5627\n",
      "Iteration 3500, Loss: 0.5616\n",
      "Iteration 3600, Loss: 0.5604\n",
      "Iteration 3700, Loss: 0.5593\n",
      "Iteration 3800, Loss: 0.5582\n",
      "Iteration 3900, Loss: 0.5572\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8609\n",
      "Iteration 200, Loss: 0.7801\n",
      "Iteration 300, Loss: 0.7388\n",
      "Iteration 400, Loss: 0.7135\n",
      "Iteration 500, Loss: 0.6962\n",
      "Iteration 600, Loss: 0.6832\n",
      "Iteration 700, Loss: 0.6728\n",
      "Iteration 800, Loss: 0.6644\n",
      "Iteration 900, Loss: 0.6572\n",
      "Iteration 1000, Loss: 0.6510\n",
      "Iteration 1100, Loss: 0.6456\n",
      "Iteration 1200, Loss: 0.6407\n",
      "Iteration 1300, Loss: 0.6365\n",
      "Iteration 1400, Loss: 0.6326\n",
      "Iteration 1500, Loss: 0.6290\n",
      "Iteration 1600, Loss: 0.6258\n",
      "Iteration 1700, Loss: 0.6227\n",
      "Iteration 1800, Loss: 0.6199\n",
      "Iteration 1900, Loss: 0.6173\n",
      "Iteration 2000, Loss: 0.6148\n",
      "Iteration 2100, Loss: 0.6125\n",
      "Iteration 2200, Loss: 0.6103\n",
      "Iteration 2300, Loss: 0.6082\n",
      "Iteration 2400, Loss: 0.6062\n",
      "Iteration 2500, Loss: 0.6044\n",
      "Iteration 2600, Loss: 0.6026\n",
      "Iteration 2700, Loss: 0.6009\n",
      "Iteration 2800, Loss: 0.5992\n",
      "Iteration 2900, Loss: 0.5977\n",
      "Iteration 3000, Loss: 0.5961\n",
      "Iteration 3100, Loss: 0.5947\n",
      "Iteration 3200, Loss: 0.5932\n",
      "Iteration 3300, Loss: 0.5919\n",
      "Iteration 3400, Loss: 0.5905\n",
      "Iteration 3500, Loss: 0.5892\n",
      "Iteration 3600, Loss: 0.5880\n",
      "Iteration 3700, Loss: 0.5868\n",
      "Iteration 3800, Loss: 0.5856\n",
      "Iteration 3900, Loss: 0.5844\n",
      "Iteration 0, Loss: 1.0931\n",
      "Iteration 100, Loss: 0.8604\n",
      "Iteration 200, Loss: 0.7826\n",
      "Iteration 300, Loss: 0.7430\n",
      "Iteration 400, Loss: 0.7186\n",
      "Iteration 500, Loss: 0.7015\n",
      "Iteration 600, Loss: 0.6888\n",
      "Iteration 700, Loss: 0.6786\n",
      "Iteration 800, Loss: 0.6702\n",
      "Iteration 900, Loss: 0.6630\n",
      "Iteration 1000, Loss: 0.6568\n",
      "Iteration 1100, Loss: 0.6514\n",
      "Iteration 1200, Loss: 0.6466\n",
      "Iteration 1300, Loss: 0.6422\n",
      "Iteration 1400, Loss: 0.6382\n",
      "Iteration 1500, Loss: 0.6346\n",
      "Iteration 1600, Loss: 0.6312\n",
      "Iteration 1700, Loss: 0.6281\n",
      "Iteration 1800, Loss: 0.6252\n",
      "Iteration 1900, Loss: 0.6224\n",
      "Iteration 2000, Loss: 0.6199\n",
      "Iteration 2100, Loss: 0.6174\n",
      "Iteration 2200, Loss: 0.6151\n",
      "Iteration 2300, Loss: 0.6130\n",
      "Iteration 2400, Loss: 0.6109\n",
      "Iteration 2500, Loss: 0.6090\n",
      "Iteration 2600, Loss: 0.6071\n",
      "Iteration 2700, Loss: 0.6053\n",
      "Iteration 2800, Loss: 0.6035\n",
      "Iteration 2900, Loss: 0.6019\n",
      "Iteration 3000, Loss: 0.6003\n",
      "Iteration 3100, Loss: 0.5987\n",
      "Iteration 3200, Loss: 0.5972\n",
      "Iteration 3300, Loss: 0.5958\n",
      "Iteration 3400, Loss: 0.5944\n",
      "Iteration 3500, Loss: 0.5930\n",
      "Iteration 3600, Loss: 0.5917\n",
      "Iteration 3700, Loss: 0.5905\n",
      "Iteration 3800, Loss: 0.5892\n",
      "Iteration 3900, Loss: 0.5880\n",
      "Iteration 0, Loss: 1.0969\n",
      "Iteration 100, Loss: 0.9748\n",
      "Iteration 200, Loss: 0.9046\n",
      "Iteration 300, Loss: 0.8581\n",
      "Iteration 400, Loss: 0.8252\n",
      "Iteration 500, Loss: 0.8006\n",
      "Iteration 600, Loss: 0.7817\n",
      "Iteration 700, Loss: 0.7665\n",
      "Iteration 800, Loss: 0.7542\n",
      "Iteration 900, Loss: 0.7440\n",
      "Iteration 1000, Loss: 0.7352\n",
      "Iteration 1100, Loss: 0.7277\n",
      "Iteration 1200, Loss: 0.7211\n",
      "Iteration 1300, Loss: 0.7153\n",
      "Iteration 1400, Loss: 0.7101\n",
      "Iteration 1500, Loss: 0.7053\n",
      "Iteration 1600, Loss: 0.7011\n",
      "Iteration 1700, Loss: 0.6971\n",
      "Iteration 1800, Loss: 0.6935\n",
      "Iteration 1900, Loss: 0.6901\n",
      "Iteration 2000, Loss: 0.6870\n",
      "Iteration 2100, Loss: 0.6841\n",
      "Iteration 2200, Loss: 0.6813\n",
      "Iteration 2300, Loss: 0.6788\n",
      "Iteration 2400, Loss: 0.6763\n",
      "Iteration 2500, Loss: 0.6740\n",
      "Iteration 2600, Loss: 0.6719\n",
      "Iteration 2700, Loss: 0.6698\n",
      "Iteration 2800, Loss: 0.6678\n",
      "Iteration 2900, Loss: 0.6660\n",
      "Iteration 3000, Loss: 0.6642\n",
      "Iteration 3100, Loss: 0.6624\n",
      "Iteration 3200, Loss: 0.6608\n",
      "Iteration 3300, Loss: 0.6592\n",
      "Iteration 3400, Loss: 0.6577\n",
      "Iteration 3500, Loss: 0.6563\n",
      "Iteration 3600, Loss: 0.6548\n",
      "Iteration 3700, Loss: 0.6535\n",
      "Iteration 3800, Loss: 0.6522\n",
      "Iteration 3900, Loss: 0.6509\n",
      "Iteration 0, Loss: 1.0965\n",
      "Iteration 100, Loss: 0.9572\n",
      "Iteration 200, Loss: 0.8823\n",
      "Iteration 300, Loss: 0.8341\n",
      "Iteration 400, Loss: 0.8003\n",
      "Iteration 500, Loss: 0.7750\n",
      "Iteration 600, Loss: 0.7555\n",
      "Iteration 700, Loss: 0.7398\n",
      "Iteration 800, Loss: 0.7270\n",
      "Iteration 900, Loss: 0.7163\n",
      "Iteration 1000, Loss: 0.7072\n",
      "Iteration 1100, Loss: 0.6994\n",
      "Iteration 1200, Loss: 0.6925\n",
      "Iteration 1300, Loss: 0.6865\n",
      "Iteration 1400, Loss: 0.6811\n",
      "Iteration 1500, Loss: 0.6762\n",
      "Iteration 1600, Loss: 0.6718\n",
      "Iteration 1700, Loss: 0.6677\n",
      "Iteration 1800, Loss: 0.6640\n",
      "Iteration 1900, Loss: 0.6605\n",
      "Iteration 2000, Loss: 0.6573\n",
      "Iteration 2100, Loss: 0.6543\n",
      "Iteration 2200, Loss: 0.6515\n",
      "Iteration 2300, Loss: 0.6488\n",
      "Iteration 2400, Loss: 0.6463\n",
      "Iteration 2500, Loss: 0.6439\n",
      "Iteration 2600, Loss: 0.6417\n",
      "Iteration 2700, Loss: 0.6395\n",
      "Iteration 2800, Loss: 0.6375\n",
      "Iteration 2900, Loss: 0.6355\n",
      "Iteration 3000, Loss: 0.6337\n",
      "Iteration 3100, Loss: 0.6319\n",
      "Iteration 3200, Loss: 0.6302\n",
      "Iteration 3300, Loss: 0.6285\n",
      "Iteration 3400, Loss: 0.6269\n",
      "Iteration 3500, Loss: 0.6254\n",
      "Iteration 3600, Loss: 0.6239\n",
      "Iteration 3700, Loss: 0.6224\n",
      "Iteration 3800, Loss: 0.6211\n",
      "Iteration 3900, Loss: 0.6197\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0256\n",
      "Iteration 200, Loss: 0.9745\n",
      "Iteration 300, Loss: 0.9356\n",
      "Iteration 400, Loss: 0.9046\n",
      "Iteration 500, Loss: 0.8793\n",
      "Iteration 600, Loss: 0.8580\n",
      "Iteration 700, Loss: 0.8400\n",
      "Iteration 800, Loss: 0.8245\n",
      "Iteration 900, Loss: 0.8110\n",
      "Iteration 1000, Loss: 0.7993\n",
      "Iteration 1100, Loss: 0.7889\n",
      "Iteration 1200, Loss: 0.7797\n",
      "Iteration 1300, Loss: 0.7714\n",
      "Iteration 1400, Loss: 0.7639\n",
      "Iteration 1500, Loss: 0.7572\n",
      "Iteration 1600, Loss: 0.7510\n",
      "Iteration 1700, Loss: 0.7454\n",
      "Iteration 1800, Loss: 0.7402\n",
      "Iteration 1900, Loss: 0.7354\n",
      "Iteration 2000, Loss: 0.7310\n",
      "Iteration 2100, Loss: 0.7269\n",
      "Iteration 2200, Loss: 0.7231\n",
      "Iteration 2300, Loss: 0.7195\n",
      "Iteration 2400, Loss: 0.7161\n",
      "Iteration 2500, Loss: 0.7129\n",
      "Iteration 2600, Loss: 0.7099\n",
      "Iteration 2700, Loss: 0.7071\n",
      "Iteration 2800, Loss: 0.7044\n",
      "Iteration 2900, Loss: 0.7018\n",
      "Iteration 3000, Loss: 0.6994\n",
      "Iteration 3100, Loss: 0.6970\n",
      "Iteration 3200, Loss: 0.6948\n",
      "Iteration 3300, Loss: 0.6927\n",
      "Iteration 3400, Loss: 0.6906\n",
      "Iteration 3500, Loss: 0.6887\n",
      "Iteration 3600, Loss: 0.6868\n",
      "Iteration 3700, Loss: 0.6850\n",
      "Iteration 3800, Loss: 0.6832\n",
      "Iteration 3900, Loss: 0.6815\n",
      "Iteration 0, Loss: 1.0975\n",
      "Iteration 100, Loss: 1.0124\n",
      "Iteration 200, Loss: 0.9542\n",
      "Iteration 300, Loss: 0.9112\n",
      "Iteration 400, Loss: 0.8780\n",
      "Iteration 500, Loss: 0.8516\n",
      "Iteration 600, Loss: 0.8301\n",
      "Iteration 700, Loss: 0.8122\n",
      "Iteration 800, Loss: 0.7970\n",
      "Iteration 900, Loss: 0.7840\n",
      "Iteration 1000, Loss: 0.7728\n",
      "Iteration 1100, Loss: 0.7629\n",
      "Iteration 1200, Loss: 0.7542\n",
      "Iteration 1300, Loss: 0.7465\n",
      "Iteration 1400, Loss: 0.7395\n",
      "Iteration 1500, Loss: 0.7333\n",
      "Iteration 1600, Loss: 0.7276\n",
      "Iteration 1700, Loss: 0.7225\n",
      "Iteration 1800, Loss: 0.7178\n",
      "Iteration 1900, Loss: 0.7134\n",
      "Iteration 2000, Loss: 0.7094\n",
      "Iteration 2100, Loss: 0.7057\n",
      "Iteration 2200, Loss: 0.7022\n",
      "Iteration 2300, Loss: 0.6989\n",
      "Iteration 2400, Loss: 0.6959\n",
      "Iteration 2500, Loss: 0.6930\n",
      "Iteration 2600, Loss: 0.6903\n",
      "Iteration 2700, Loss: 0.6877\n",
      "Iteration 2800, Loss: 0.6853\n",
      "Iteration 2900, Loss: 0.6830\n",
      "Iteration 3000, Loss: 0.6808\n",
      "Iteration 3100, Loss: 0.6786\n",
      "Iteration 3200, Loss: 0.6766\n",
      "Iteration 3300, Loss: 0.6747\n",
      "Iteration 3400, Loss: 0.6728\n",
      "Iteration 3500, Loss: 0.6711\n",
      "Iteration 3600, Loss: 0.6693\n",
      "Iteration 3700, Loss: 0.6677\n",
      "Iteration 3800, Loss: 0.6661\n",
      "Iteration 3900, Loss: 0.6646\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0532\n",
      "Iteration 200, Loss: 1.0168\n",
      "Iteration 300, Loss: 0.9866\n",
      "Iteration 400, Loss: 0.9612\n",
      "Iteration 500, Loss: 0.9393\n",
      "Iteration 600, Loss: 0.9201\n",
      "Iteration 700, Loss: 0.9032\n",
      "Iteration 800, Loss: 0.8882\n",
      "Iteration 900, Loss: 0.8748\n",
      "Iteration 1000, Loss: 0.8628\n",
      "Iteration 1100, Loss: 0.8518\n",
      "Iteration 1200, Loss: 0.8418\n",
      "Iteration 1300, Loss: 0.8328\n",
      "Iteration 1400, Loss: 0.8244\n",
      "Iteration 1500, Loss: 0.8167\n",
      "Iteration 1600, Loss: 0.8096\n",
      "Iteration 1700, Loss: 0.8030\n",
      "Iteration 1800, Loss: 0.7969\n",
      "Iteration 1900, Loss: 0.7912\n",
      "Iteration 2000, Loss: 0.7859\n",
      "Iteration 2100, Loss: 0.7809\n",
      "Iteration 2200, Loss: 0.7762\n",
      "Iteration 2300, Loss: 0.7718\n",
      "Iteration 2400, Loss: 0.7677\n",
      "Iteration 2500, Loss: 0.7637\n",
      "Iteration 2600, Loss: 0.7601\n",
      "Iteration 2700, Loss: 0.7565\n",
      "Iteration 2800, Loss: 0.7532\n",
      "Iteration 2900, Loss: 0.7501\n",
      "Iteration 3000, Loss: 0.7471\n",
      "Iteration 3100, Loss: 0.7442\n",
      "Iteration 3200, Loss: 0.7414\n",
      "Iteration 3300, Loss: 0.7388\n",
      "Iteration 3400, Loss: 0.7363\n",
      "Iteration 3500, Loss: 0.7339\n",
      "Iteration 3600, Loss: 0.7316\n",
      "Iteration 3700, Loss: 0.7294\n",
      "Iteration 3800, Loss: 0.7273\n",
      "Iteration 3900, Loss: 0.7252\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0536\n",
      "Iteration 200, Loss: 1.0178\n",
      "Iteration 300, Loss: 0.9877\n",
      "Iteration 400, Loss: 0.9621\n",
      "Iteration 500, Loss: 0.9398\n",
      "Iteration 600, Loss: 0.9204\n",
      "Iteration 700, Loss: 0.9031\n",
      "Iteration 800, Loss: 0.8877\n",
      "Iteration 900, Loss: 0.8738\n",
      "Iteration 1000, Loss: 0.8611\n",
      "Iteration 1100, Loss: 0.8497\n",
      "Iteration 1200, Loss: 0.8392\n",
      "Iteration 1300, Loss: 0.8297\n",
      "Iteration 1400, Loss: 0.8208\n",
      "Iteration 1500, Loss: 0.8126\n",
      "Iteration 1600, Loss: 0.8050\n",
      "Iteration 1700, Loss: 0.7981\n",
      "Iteration 1800, Loss: 0.7915\n",
      "Iteration 1900, Loss: 0.7854\n",
      "Iteration 2000, Loss: 0.7797\n",
      "Iteration 2100, Loss: 0.7743\n",
      "Iteration 2200, Loss: 0.7693\n",
      "Iteration 2300, Loss: 0.7646\n",
      "Iteration 2400, Loss: 0.7601\n",
      "Iteration 2500, Loss: 0.7559\n",
      "Iteration 2600, Loss: 0.7519\n",
      "Iteration 2700, Loss: 0.7482\n",
      "Iteration 2800, Loss: 0.7446\n",
      "Iteration 2900, Loss: 0.7412\n",
      "Iteration 3000, Loss: 0.7380\n",
      "Iteration 3100, Loss: 0.7349\n",
      "Iteration 3200, Loss: 0.7320\n",
      "Iteration 3300, Loss: 0.7292\n",
      "Iteration 3400, Loss: 0.7265\n",
      "Iteration 3500, Loss: 0.7239\n",
      "Iteration 3600, Loss: 0.7214\n",
      "Iteration 3700, Loss: 0.7191\n",
      "Iteration 3800, Loss: 0.7168\n",
      "Iteration 3900, Loss: 0.7146\n",
      "Iteration 0, Loss: 0.9722\n",
      "Iteration 100, Loss: 0.5967\n",
      "Iteration 200, Loss: 0.5645\n",
      "Iteration 300, Loss: 0.5464\n",
      "Iteration 400, Loss: 0.5340\n",
      "Iteration 500, Loss: 0.5248\n",
      "Iteration 600, Loss: 0.5175\n",
      "Iteration 700, Loss: 0.5115\n",
      "Iteration 800, Loss: 0.5065\n",
      "Iteration 900, Loss: 0.5022\n",
      "Iteration 1000, Loss: 0.4984\n",
      "Iteration 1100, Loss: 0.4950\n",
      "Iteration 1200, Loss: 0.4920\n",
      "Iteration 1300, Loss: 0.4892\n",
      "Iteration 1400, Loss: 0.4867\n",
      "Iteration 1500, Loss: 0.4844\n",
      "Iteration 1600, Loss: 0.4823\n",
      "Iteration 1700, Loss: 0.4803\n",
      "Iteration 1800, Loss: 0.4784\n",
      "Iteration 1900, Loss: 0.4767\n",
      "Iteration 2000, Loss: 0.4750\n",
      "Iteration 2100, Loss: 0.4735\n",
      "Iteration 2200, Loss: 0.4720\n",
      "Iteration 2300, Loss: 0.4706\n",
      "Iteration 2400, Loss: 0.4693\n",
      "Iteration 2500, Loss: 0.4680\n",
      "Iteration 2600, Loss: 0.4668\n",
      "Iteration 2700, Loss: 0.4656\n",
      "Iteration 2800, Loss: 0.4645\n",
      "Iteration 2900, Loss: 0.4634\n",
      "Iteration 3000, Loss: 0.4623\n",
      "Iteration 3100, Loss: 0.4613\n",
      "Iteration 3200, Loss: 0.4604\n",
      "Iteration 3300, Loss: 0.4594\n",
      "Iteration 3400, Loss: 0.4585\n",
      "Iteration 3500, Loss: 0.4576\n",
      "Iteration 3600, Loss: 0.4568\n",
      "Iteration 3700, Loss: 0.4560\n",
      "Iteration 3800, Loss: 0.4552\n",
      "Iteration 3900, Loss: 0.4544\n",
      "Iteration 4000, Loss: 0.4536\n",
      "Iteration 4100, Loss: 0.4529\n",
      "Iteration 4200, Loss: 0.4522\n",
      "Iteration 4300, Loss: 0.4515\n",
      "Iteration 4400, Loss: 0.4508\n",
      "Iteration 4500, Loss: 0.4501\n",
      "Iteration 4600, Loss: 0.4494\n",
      "Iteration 4700, Loss: 0.4488\n",
      "Iteration 4800, Loss: 0.4482\n",
      "Iteration 4900, Loss: 0.4476\n",
      "Iteration 0, Loss: 0.9607\n",
      "Iteration 100, Loss: 0.5837\n",
      "Iteration 200, Loss: 0.5558\n",
      "Iteration 300, Loss: 0.5397\n",
      "Iteration 400, Loss: 0.5281\n",
      "Iteration 500, Loss: 0.5192\n",
      "Iteration 600, Loss: 0.5118\n",
      "Iteration 700, Loss: 0.5055\n",
      "Iteration 800, Loss: 0.5001\n",
      "Iteration 900, Loss: 0.4953\n",
      "Iteration 1000, Loss: 0.4911\n",
      "Iteration 1100, Loss: 0.4872\n",
      "Iteration 1200, Loss: 0.4836\n",
      "Iteration 1300, Loss: 0.4804\n",
      "Iteration 1400, Loss: 0.4774\n",
      "Iteration 1500, Loss: 0.4746\n",
      "Iteration 1600, Loss: 0.4720\n",
      "Iteration 1700, Loss: 0.4695\n",
      "Iteration 1800, Loss: 0.4672\n",
      "Iteration 1900, Loss: 0.4651\n",
      "Iteration 2000, Loss: 0.4630\n",
      "Iteration 2100, Loss: 0.4611\n",
      "Iteration 2200, Loss: 0.4592\n",
      "Iteration 2300, Loss: 0.4574\n",
      "Iteration 2400, Loss: 0.4558\n",
      "Iteration 2500, Loss: 0.4542\n",
      "Iteration 2600, Loss: 0.4526\n",
      "Iteration 2700, Loss: 0.4511\n",
      "Iteration 2800, Loss: 0.4497\n",
      "Iteration 2900, Loss: 0.4484\n",
      "Iteration 3000, Loss: 0.4471\n",
      "Iteration 3100, Loss: 0.4458\n",
      "Iteration 3200, Loss: 0.4446\n",
      "Iteration 3300, Loss: 0.4434\n",
      "Iteration 3400, Loss: 0.4423\n",
      "Iteration 3500, Loss: 0.4412\n",
      "Iteration 3600, Loss: 0.4401\n",
      "Iteration 3700, Loss: 0.4391\n",
      "Iteration 3800, Loss: 0.4381\n",
      "Iteration 3900, Loss: 0.4371\n",
      "Iteration 4000, Loss: 0.4362\n",
      "Iteration 4100, Loss: 0.4352\n",
      "Iteration 4200, Loss: 0.4343\n",
      "Iteration 4300, Loss: 0.4335\n",
      "Iteration 4400, Loss: 0.4326\n",
      "Iteration 4500, Loss: 0.4318\n",
      "Iteration 4600, Loss: 0.4310\n",
      "Iteration 4700, Loss: 0.4302\n",
      "Iteration 4800, Loss: 0.4295\n",
      "Iteration 4900, Loss: 0.4287\n",
      "Iteration 0, Loss: 1.0147\n",
      "Iteration 100, Loss: 0.6115\n",
      "Iteration 200, Loss: 0.5791\n",
      "Iteration 300, Loss: 0.5615\n",
      "Iteration 400, Loss: 0.5498\n",
      "Iteration 500, Loss: 0.5410\n",
      "Iteration 600, Loss: 0.5339\n",
      "Iteration 700, Loss: 0.5281\n",
      "Iteration 800, Loss: 0.5231\n",
      "Iteration 900, Loss: 0.5188\n",
      "Iteration 1000, Loss: 0.5150\n",
      "Iteration 1100, Loss: 0.5116\n",
      "Iteration 1200, Loss: 0.5084\n",
      "Iteration 1300, Loss: 0.5056\n",
      "Iteration 1400, Loss: 0.5030\n",
      "Iteration 1500, Loss: 0.5005\n",
      "Iteration 1600, Loss: 0.4983\n",
      "Iteration 1700, Loss: 0.4961\n",
      "Iteration 1800, Loss: 0.4941\n",
      "Iteration 1900, Loss: 0.4922\n",
      "Iteration 2000, Loss: 0.4904\n",
      "Iteration 2100, Loss: 0.4887\n",
      "Iteration 2200, Loss: 0.4871\n",
      "Iteration 2300, Loss: 0.4856\n",
      "Iteration 2400, Loss: 0.4841\n",
      "Iteration 2500, Loss: 0.4826\n",
      "Iteration 2600, Loss: 0.4813\n",
      "Iteration 2700, Loss: 0.4800\n",
      "Iteration 2800, Loss: 0.4787\n",
      "Iteration 2900, Loss: 0.4775\n",
      "Iteration 3000, Loss: 0.4763\n",
      "Iteration 3100, Loss: 0.4752\n",
      "Iteration 3200, Loss: 0.4741\n",
      "Iteration 3300, Loss: 0.4730\n",
      "Iteration 3400, Loss: 0.4720\n",
      "Iteration 3500, Loss: 0.4710\n",
      "Iteration 3600, Loss: 0.4701\n",
      "Iteration 3700, Loss: 0.4691\n",
      "Iteration 3800, Loss: 0.4682\n",
      "Iteration 3900, Loss: 0.4673\n",
      "Iteration 4000, Loss: 0.4665\n",
      "Iteration 4100, Loss: 0.4656\n",
      "Iteration 4200, Loss: 0.4648\n",
      "Iteration 4300, Loss: 0.4640\n",
      "Iteration 4400, Loss: 0.4632\n",
      "Iteration 4500, Loss: 0.4625\n",
      "Iteration 4600, Loss: 0.4617\n",
      "Iteration 4700, Loss: 0.4610\n",
      "Iteration 4800, Loss: 0.4603\n",
      "Iteration 4900, Loss: 0.4596\n",
      "Iteration 0, Loss: 1.0246\n",
      "Iteration 100, Loss: 0.6355\n",
      "Iteration 200, Loss: 0.6053\n",
      "Iteration 300, Loss: 0.5881\n",
      "Iteration 400, Loss: 0.5762\n",
      "Iteration 500, Loss: 0.5671\n",
      "Iteration 600, Loss: 0.5598\n",
      "Iteration 700, Loss: 0.5537\n",
      "Iteration 800, Loss: 0.5484\n",
      "Iteration 900, Loss: 0.5438\n",
      "Iteration 1000, Loss: 0.5397\n",
      "Iteration 1100, Loss: 0.5360\n",
      "Iteration 1200, Loss: 0.5326\n",
      "Iteration 1300, Loss: 0.5296\n",
      "Iteration 1400, Loss: 0.5267\n",
      "Iteration 1500, Loss: 0.5241\n",
      "Iteration 1600, Loss: 0.5216\n",
      "Iteration 1700, Loss: 0.5193\n",
      "Iteration 1800, Loss: 0.5172\n",
      "Iteration 1900, Loss: 0.5151\n",
      "Iteration 2000, Loss: 0.5131\n",
      "Iteration 2100, Loss: 0.5113\n",
      "Iteration 2200, Loss: 0.5095\n",
      "Iteration 2300, Loss: 0.5079\n",
      "Iteration 2400, Loss: 0.5063\n",
      "Iteration 2500, Loss: 0.5048\n",
      "Iteration 2600, Loss: 0.5033\n",
      "Iteration 2700, Loss: 0.5019\n",
      "Iteration 2800, Loss: 0.5005\n",
      "Iteration 2900, Loss: 0.4993\n",
      "Iteration 3000, Loss: 0.4980\n",
      "Iteration 3100, Loss: 0.4968\n",
      "Iteration 3200, Loss: 0.4956\n",
      "Iteration 3300, Loss: 0.4945\n",
      "Iteration 3400, Loss: 0.4934\n",
      "Iteration 3500, Loss: 0.4923\n",
      "Iteration 3600, Loss: 0.4913\n",
      "Iteration 3700, Loss: 0.4903\n",
      "Iteration 3800, Loss: 0.4893\n",
      "Iteration 3900, Loss: 0.4884\n",
      "Iteration 4000, Loss: 0.4875\n",
      "Iteration 4100, Loss: 0.4866\n",
      "Iteration 4200, Loss: 0.4857\n",
      "Iteration 4300, Loss: 0.4849\n",
      "Iteration 4400, Loss: 0.4840\n",
      "Iteration 4500, Loss: 0.4832\n",
      "Iteration 4600, Loss: 0.4824\n",
      "Iteration 4700, Loss: 0.4817\n",
      "Iteration 4800, Loss: 0.4809\n",
      "Iteration 4900, Loss: 0.4802\n",
      "Iteration 0, Loss: 1.0496\n",
      "Iteration 100, Loss: 0.6381\n",
      "Iteration 200, Loss: 0.6014\n",
      "Iteration 300, Loss: 0.5818\n",
      "Iteration 400, Loss: 0.5681\n",
      "Iteration 500, Loss: 0.5577\n",
      "Iteration 600, Loss: 0.5494\n",
      "Iteration 700, Loss: 0.5425\n",
      "Iteration 800, Loss: 0.5367\n",
      "Iteration 900, Loss: 0.5316\n",
      "Iteration 1000, Loss: 0.5271\n",
      "Iteration 1100, Loss: 0.5231\n",
      "Iteration 1200, Loss: 0.5195\n",
      "Iteration 1300, Loss: 0.5163\n",
      "Iteration 1400, Loss: 0.5133\n",
      "Iteration 1500, Loss: 0.5107\n",
      "Iteration 1600, Loss: 0.5081\n",
      "Iteration 1700, Loss: 0.5057\n",
      "Iteration 1800, Loss: 0.5035\n",
      "Iteration 1900, Loss: 0.5015\n",
      "Iteration 2000, Loss: 0.4996\n",
      "Iteration 2100, Loss: 0.4977\n",
      "Iteration 2200, Loss: 0.4960\n",
      "Iteration 2300, Loss: 0.4943\n",
      "Iteration 2400, Loss: 0.4928\n",
      "Iteration 2500, Loss: 0.4913\n",
      "Iteration 2600, Loss: 0.4899\n",
      "Iteration 2700, Loss: 0.4885\n",
      "Iteration 2800, Loss: 0.4872\n",
      "Iteration 2900, Loss: 0.4859\n",
      "Iteration 3000, Loss: 0.4847\n",
      "Iteration 3100, Loss: 0.4835\n",
      "Iteration 3200, Loss: 0.4824\n",
      "Iteration 3300, Loss: 0.4813\n",
      "Iteration 3400, Loss: 0.4803\n",
      "Iteration 3500, Loss: 0.4793\n",
      "Iteration 3600, Loss: 0.4783\n",
      "Iteration 3700, Loss: 0.4773\n",
      "Iteration 3800, Loss: 0.4764\n",
      "Iteration 3900, Loss: 0.4755\n",
      "Iteration 4000, Loss: 0.4746\n",
      "Iteration 4100, Loss: 0.4737\n",
      "Iteration 4200, Loss: 0.4729\n",
      "Iteration 4300, Loss: 0.4721\n",
      "Iteration 4400, Loss: 0.4713\n",
      "Iteration 4500, Loss: 0.4705\n",
      "Iteration 4600, Loss: 0.4698\n",
      "Iteration 4700, Loss: 0.4690\n",
      "Iteration 4800, Loss: 0.4683\n",
      "Iteration 4900, Loss: 0.4676\n",
      "Iteration 0, Loss: 1.0570\n",
      "Iteration 100, Loss: 0.6737\n",
      "Iteration 200, Loss: 0.6399\n",
      "Iteration 300, Loss: 0.6225\n",
      "Iteration 400, Loss: 0.6107\n",
      "Iteration 500, Loss: 0.6016\n",
      "Iteration 600, Loss: 0.5943\n",
      "Iteration 700, Loss: 0.5882\n",
      "Iteration 800, Loss: 0.5828\n",
      "Iteration 900, Loss: 0.5781\n",
      "Iteration 1000, Loss: 0.5740\n",
      "Iteration 1100, Loss: 0.5702\n",
      "Iteration 1200, Loss: 0.5667\n",
      "Iteration 1300, Loss: 0.5636\n",
      "Iteration 1400, Loss: 0.5607\n",
      "Iteration 1500, Loss: 0.5580\n",
      "Iteration 1600, Loss: 0.5554\n",
      "Iteration 1700, Loss: 0.5531\n",
      "Iteration 1800, Loss: 0.5508\n",
      "Iteration 1900, Loss: 0.5488\n",
      "Iteration 2000, Loss: 0.5468\n",
      "Iteration 2100, Loss: 0.5449\n",
      "Iteration 2200, Loss: 0.5431\n",
      "Iteration 2300, Loss: 0.5414\n",
      "Iteration 2400, Loss: 0.5397\n",
      "Iteration 2500, Loss: 0.5381\n",
      "Iteration 2600, Loss: 0.5366\n",
      "Iteration 2700, Loss: 0.5351\n",
      "Iteration 2800, Loss: 0.5337\n",
      "Iteration 2900, Loss: 0.5324\n",
      "Iteration 3000, Loss: 0.5311\n",
      "Iteration 3100, Loss: 0.5298\n",
      "Iteration 3200, Loss: 0.5286\n",
      "Iteration 3300, Loss: 0.5274\n",
      "Iteration 3400, Loss: 0.5263\n",
      "Iteration 3500, Loss: 0.5252\n",
      "Iteration 3600, Loss: 0.5241\n",
      "Iteration 3700, Loss: 0.5231\n",
      "Iteration 3800, Loss: 0.5221\n",
      "Iteration 3900, Loss: 0.5212\n",
      "Iteration 4000, Loss: 0.5202\n",
      "Iteration 4100, Loss: 0.5193\n",
      "Iteration 4200, Loss: 0.5184\n",
      "Iteration 4300, Loss: 0.5175\n",
      "Iteration 4400, Loss: 0.5167\n",
      "Iteration 4500, Loss: 0.5159\n",
      "Iteration 4600, Loss: 0.5151\n",
      "Iteration 4700, Loss: 0.5143\n",
      "Iteration 4800, Loss: 0.5135\n",
      "Iteration 4900, Loss: 0.5128\n",
      "Iteration 0, Loss: 1.0799\n",
      "Iteration 100, Loss: 0.7187\n",
      "Iteration 200, Loss: 0.6714\n",
      "Iteration 300, Loss: 0.6488\n",
      "Iteration 400, Loss: 0.6341\n",
      "Iteration 500, Loss: 0.6233\n",
      "Iteration 600, Loss: 0.6148\n",
      "Iteration 700, Loss: 0.6078\n",
      "Iteration 800, Loss: 0.6018\n",
      "Iteration 900, Loss: 0.5966\n",
      "Iteration 1000, Loss: 0.5921\n",
      "Iteration 1100, Loss: 0.5880\n",
      "Iteration 1200, Loss: 0.5843\n",
      "Iteration 1300, Loss: 0.5809\n",
      "Iteration 1400, Loss: 0.5778\n",
      "Iteration 1500, Loss: 0.5750\n",
      "Iteration 1600, Loss: 0.5723\n",
      "Iteration 1700, Loss: 0.5699\n",
      "Iteration 1800, Loss: 0.5676\n",
      "Iteration 1900, Loss: 0.5654\n",
      "Iteration 2000, Loss: 0.5634\n",
      "Iteration 2100, Loss: 0.5615\n",
      "Iteration 2200, Loss: 0.5597\n",
      "Iteration 2300, Loss: 0.5579\n",
      "Iteration 2400, Loss: 0.5563\n",
      "Iteration 2500, Loss: 0.5547\n",
      "Iteration 2600, Loss: 0.5532\n",
      "Iteration 2700, Loss: 0.5518\n",
      "Iteration 2800, Loss: 0.5504\n",
      "Iteration 2900, Loss: 0.5491\n",
      "Iteration 3000, Loss: 0.5478\n",
      "Iteration 3100, Loss: 0.5466\n",
      "Iteration 3200, Loss: 0.5454\n",
      "Iteration 3300, Loss: 0.5443\n",
      "Iteration 3400, Loss: 0.5432\n",
      "Iteration 3500, Loss: 0.5421\n",
      "Iteration 3600, Loss: 0.5411\n",
      "Iteration 3700, Loss: 0.5400\n",
      "Iteration 3800, Loss: 0.5391\n",
      "Iteration 3900, Loss: 0.5381\n",
      "Iteration 4000, Loss: 0.5372\n",
      "Iteration 4100, Loss: 0.5363\n",
      "Iteration 4200, Loss: 0.5354\n",
      "Iteration 4300, Loss: 0.5346\n",
      "Iteration 4400, Loss: 0.5337\n",
      "Iteration 4500, Loss: 0.5329\n",
      "Iteration 4600, Loss: 0.5321\n",
      "Iteration 4700, Loss: 0.5314\n",
      "Iteration 4800, Loss: 0.5306\n",
      "Iteration 4900, Loss: 0.5299\n",
      "Iteration 0, Loss: 1.0812\n",
      "Iteration 100, Loss: 0.7255\n",
      "Iteration 200, Loss: 0.6755\n",
      "Iteration 300, Loss: 0.6516\n",
      "Iteration 400, Loss: 0.6364\n",
      "Iteration 500, Loss: 0.6254\n",
      "Iteration 600, Loss: 0.6168\n",
      "Iteration 700, Loss: 0.6097\n",
      "Iteration 800, Loss: 0.6037\n",
      "Iteration 900, Loss: 0.5984\n",
      "Iteration 1000, Loss: 0.5937\n",
      "Iteration 1100, Loss: 0.5895\n",
      "Iteration 1200, Loss: 0.5857\n",
      "Iteration 1300, Loss: 0.5822\n",
      "Iteration 1400, Loss: 0.5790\n",
      "Iteration 1500, Loss: 0.5761\n",
      "Iteration 1600, Loss: 0.5733\n",
      "Iteration 1700, Loss: 0.5707\n",
      "Iteration 1800, Loss: 0.5683\n",
      "Iteration 1900, Loss: 0.5660\n",
      "Iteration 2000, Loss: 0.5639\n",
      "Iteration 2100, Loss: 0.5618\n",
      "Iteration 2200, Loss: 0.5599\n",
      "Iteration 2300, Loss: 0.5580\n",
      "Iteration 2400, Loss: 0.5563\n",
      "Iteration 2500, Loss: 0.5546\n",
      "Iteration 2600, Loss: 0.5530\n",
      "Iteration 2700, Loss: 0.5514\n",
      "Iteration 2800, Loss: 0.5500\n",
      "Iteration 2900, Loss: 0.5485\n",
      "Iteration 3000, Loss: 0.5472\n",
      "Iteration 3100, Loss: 0.5458\n",
      "Iteration 3200, Loss: 0.5446\n",
      "Iteration 3300, Loss: 0.5433\n",
      "Iteration 3400, Loss: 0.5421\n",
      "Iteration 3500, Loss: 0.5410\n",
      "Iteration 3600, Loss: 0.5399\n",
      "Iteration 3700, Loss: 0.5388\n",
      "Iteration 3800, Loss: 0.5377\n",
      "Iteration 3900, Loss: 0.5367\n",
      "Iteration 4000, Loss: 0.5357\n",
      "Iteration 4100, Loss: 0.5347\n",
      "Iteration 4200, Loss: 0.5338\n",
      "Iteration 4300, Loss: 0.5329\n",
      "Iteration 4400, Loss: 0.5320\n",
      "Iteration 4500, Loss: 0.5311\n",
      "Iteration 4600, Loss: 0.5302\n",
      "Iteration 4700, Loss: 0.5294\n",
      "Iteration 4800, Loss: 0.5286\n",
      "Iteration 4900, Loss: 0.5278\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7837\n",
      "Iteration 200, Loss: 0.7187\n",
      "Iteration 300, Loss: 0.6887\n",
      "Iteration 400, Loss: 0.6702\n",
      "Iteration 500, Loss: 0.6571\n",
      "Iteration 600, Loss: 0.6471\n",
      "Iteration 700, Loss: 0.6391\n",
      "Iteration 800, Loss: 0.6325\n",
      "Iteration 900, Loss: 0.6268\n",
      "Iteration 1000, Loss: 0.6219\n",
      "Iteration 1100, Loss: 0.6175\n",
      "Iteration 1200, Loss: 0.6136\n",
      "Iteration 1300, Loss: 0.6101\n",
      "Iteration 1400, Loss: 0.6068\n",
      "Iteration 1500, Loss: 0.6039\n",
      "Iteration 1600, Loss: 0.6011\n",
      "Iteration 1700, Loss: 0.5985\n",
      "Iteration 1800, Loss: 0.5961\n",
      "Iteration 1900, Loss: 0.5938\n",
      "Iteration 2000, Loss: 0.5917\n",
      "Iteration 2100, Loss: 0.5896\n",
      "Iteration 2200, Loss: 0.5877\n",
      "Iteration 2300, Loss: 0.5858\n",
      "Iteration 2400, Loss: 0.5841\n",
      "Iteration 2500, Loss: 0.5824\n",
      "Iteration 2600, Loss: 0.5808\n",
      "Iteration 2700, Loss: 0.5792\n",
      "Iteration 2800, Loss: 0.5777\n",
      "Iteration 2900, Loss: 0.5763\n",
      "Iteration 3000, Loss: 0.5749\n",
      "Iteration 3100, Loss: 0.5736\n",
      "Iteration 3200, Loss: 0.5723\n",
      "Iteration 3300, Loss: 0.5710\n",
      "Iteration 3400, Loss: 0.5698\n",
      "Iteration 3500, Loss: 0.5686\n",
      "Iteration 3600, Loss: 0.5675\n",
      "Iteration 3700, Loss: 0.5664\n",
      "Iteration 3800, Loss: 0.5653\n",
      "Iteration 3900, Loss: 0.5643\n",
      "Iteration 4000, Loss: 0.5633\n",
      "Iteration 4100, Loss: 0.5623\n",
      "Iteration 4200, Loss: 0.5613\n",
      "Iteration 4300, Loss: 0.5604\n",
      "Iteration 4400, Loss: 0.5595\n",
      "Iteration 4500, Loss: 0.5586\n",
      "Iteration 4600, Loss: 0.5577\n",
      "Iteration 4700, Loss: 0.5569\n",
      "Iteration 4800, Loss: 0.5560\n",
      "Iteration 4900, Loss: 0.5552\n",
      "Iteration 0, Loss: 1.0890\n",
      "Iteration 100, Loss: 0.7880\n",
      "Iteration 200, Loss: 0.7222\n",
      "Iteration 300, Loss: 0.6919\n",
      "Iteration 400, Loss: 0.6734\n",
      "Iteration 500, Loss: 0.6602\n",
      "Iteration 600, Loss: 0.6500\n",
      "Iteration 700, Loss: 0.6417\n",
      "Iteration 800, Loss: 0.6347\n",
      "Iteration 900, Loss: 0.6287\n",
      "Iteration 1000, Loss: 0.6235\n",
      "Iteration 1100, Loss: 0.6188\n",
      "Iteration 1200, Loss: 0.6146\n",
      "Iteration 1300, Loss: 0.6108\n",
      "Iteration 1400, Loss: 0.6073\n",
      "Iteration 1500, Loss: 0.6041\n",
      "Iteration 1600, Loss: 0.6011\n",
      "Iteration 1700, Loss: 0.5983\n",
      "Iteration 1800, Loss: 0.5956\n",
      "Iteration 1900, Loss: 0.5931\n",
      "Iteration 2000, Loss: 0.5908\n",
      "Iteration 2100, Loss: 0.5886\n",
      "Iteration 2200, Loss: 0.5865\n",
      "Iteration 2300, Loss: 0.5845\n",
      "Iteration 2400, Loss: 0.5825\n",
      "Iteration 2500, Loss: 0.5807\n",
      "Iteration 2600, Loss: 0.5789\n",
      "Iteration 2700, Loss: 0.5772\n",
      "Iteration 2800, Loss: 0.5756\n",
      "Iteration 2900, Loss: 0.5741\n",
      "Iteration 3000, Loss: 0.5726\n",
      "Iteration 3100, Loss: 0.5711\n",
      "Iteration 3200, Loss: 0.5697\n",
      "Iteration 3300, Loss: 0.5683\n",
      "Iteration 3400, Loss: 0.5670\n",
      "Iteration 3500, Loss: 0.5658\n",
      "Iteration 3600, Loss: 0.5646\n",
      "Iteration 3700, Loss: 0.5634\n",
      "Iteration 3800, Loss: 0.5622\n",
      "Iteration 3900, Loss: 0.5611\n",
      "Iteration 4000, Loss: 0.5600\n",
      "Iteration 4100, Loss: 0.5589\n",
      "Iteration 4200, Loss: 0.5579\n",
      "Iteration 4300, Loss: 0.5569\n",
      "Iteration 4400, Loss: 0.5559\n",
      "Iteration 4500, Loss: 0.5549\n",
      "Iteration 4600, Loss: 0.5540\n",
      "Iteration 4700, Loss: 0.5531\n",
      "Iteration 4800, Loss: 0.5522\n",
      "Iteration 4900, Loss: 0.5513\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8597\n",
      "Iteration 200, Loss: 0.7816\n",
      "Iteration 300, Loss: 0.7425\n",
      "Iteration 400, Loss: 0.7185\n",
      "Iteration 500, Loss: 0.7019\n",
      "Iteration 600, Loss: 0.6895\n",
      "Iteration 700, Loss: 0.6798\n",
      "Iteration 800, Loss: 0.6720\n",
      "Iteration 900, Loss: 0.6653\n",
      "Iteration 1000, Loss: 0.6595\n",
      "Iteration 1100, Loss: 0.6545\n",
      "Iteration 1200, Loss: 0.6500\n",
      "Iteration 1300, Loss: 0.6461\n",
      "Iteration 1400, Loss: 0.6424\n",
      "Iteration 1500, Loss: 0.6391\n",
      "Iteration 1600, Loss: 0.6360\n",
      "Iteration 1700, Loss: 0.6332\n",
      "Iteration 1800, Loss: 0.6306\n",
      "Iteration 1900, Loss: 0.6281\n",
      "Iteration 2000, Loss: 0.6258\n",
      "Iteration 2100, Loss: 0.6236\n",
      "Iteration 2200, Loss: 0.6215\n",
      "Iteration 2300, Loss: 0.6196\n",
      "Iteration 2400, Loss: 0.6177\n",
      "Iteration 2500, Loss: 0.6159\n",
      "Iteration 2600, Loss: 0.6141\n",
      "Iteration 2700, Loss: 0.6125\n",
      "Iteration 2800, Loss: 0.6109\n",
      "Iteration 2900, Loss: 0.6093\n",
      "Iteration 3000, Loss: 0.6079\n",
      "Iteration 3100, Loss: 0.6064\n",
      "Iteration 3200, Loss: 0.6050\n",
      "Iteration 3300, Loss: 0.6037\n",
      "Iteration 3400, Loss: 0.6024\n",
      "Iteration 3500, Loss: 0.6011\n",
      "Iteration 3600, Loss: 0.5999\n",
      "Iteration 3700, Loss: 0.5987\n",
      "Iteration 3800, Loss: 0.5976\n",
      "Iteration 3900, Loss: 0.5964\n",
      "Iteration 4000, Loss: 0.5953\n",
      "Iteration 4100, Loss: 0.5943\n",
      "Iteration 4200, Loss: 0.5932\n",
      "Iteration 4300, Loss: 0.5922\n",
      "Iteration 4400, Loss: 0.5912\n",
      "Iteration 4500, Loss: 0.5902\n",
      "Iteration 4600, Loss: 0.5893\n",
      "Iteration 4700, Loss: 0.5884\n",
      "Iteration 4800, Loss: 0.5875\n",
      "Iteration 4900, Loss: 0.5866\n",
      "Iteration 0, Loss: 1.0940\n",
      "Iteration 100, Loss: 0.8634\n",
      "Iteration 200, Loss: 0.7828\n",
      "Iteration 300, Loss: 0.7412\n",
      "Iteration 400, Loss: 0.7159\n",
      "Iteration 500, Loss: 0.6984\n",
      "Iteration 600, Loss: 0.6852\n",
      "Iteration 700, Loss: 0.6746\n",
      "Iteration 800, Loss: 0.6659\n",
      "Iteration 900, Loss: 0.6584\n",
      "Iteration 1000, Loss: 0.6519\n",
      "Iteration 1100, Loss: 0.6462\n",
      "Iteration 1200, Loss: 0.6411\n",
      "Iteration 1300, Loss: 0.6365\n",
      "Iteration 1400, Loss: 0.6324\n",
      "Iteration 1500, Loss: 0.6286\n",
      "Iteration 1600, Loss: 0.6252\n",
      "Iteration 1700, Loss: 0.6220\n",
      "Iteration 1800, Loss: 0.6190\n",
      "Iteration 1900, Loss: 0.6163\n",
      "Iteration 2000, Loss: 0.6137\n",
      "Iteration 2100, Loss: 0.6112\n",
      "Iteration 2200, Loss: 0.6089\n",
      "Iteration 2300, Loss: 0.6068\n",
      "Iteration 2400, Loss: 0.6047\n",
      "Iteration 2500, Loss: 0.6028\n",
      "Iteration 2600, Loss: 0.6009\n",
      "Iteration 2700, Loss: 0.5991\n",
      "Iteration 2800, Loss: 0.5974\n",
      "Iteration 2900, Loss: 0.5958\n",
      "Iteration 3000, Loss: 0.5942\n",
      "Iteration 3100, Loss: 0.5927\n",
      "Iteration 3200, Loss: 0.5912\n",
      "Iteration 3300, Loss: 0.5898\n",
      "Iteration 3400, Loss: 0.5885\n",
      "Iteration 3500, Loss: 0.5872\n",
      "Iteration 3600, Loss: 0.5859\n",
      "Iteration 3700, Loss: 0.5847\n",
      "Iteration 3800, Loss: 0.5835\n",
      "Iteration 3900, Loss: 0.5824\n",
      "Iteration 4000, Loss: 0.5813\n",
      "Iteration 4100, Loss: 0.5802\n",
      "Iteration 4200, Loss: 0.5791\n",
      "Iteration 4300, Loss: 0.5781\n",
      "Iteration 4400, Loss: 0.5771\n",
      "Iteration 4500, Loss: 0.5762\n",
      "Iteration 4600, Loss: 0.5752\n",
      "Iteration 4700, Loss: 0.5743\n",
      "Iteration 4800, Loss: 0.5734\n",
      "Iteration 4900, Loss: 0.5725\n",
      "Iteration 0, Loss: 1.0968\n",
      "Iteration 100, Loss: 0.9709\n",
      "Iteration 200, Loss: 0.9000\n",
      "Iteration 300, Loss: 0.8532\n",
      "Iteration 400, Loss: 0.8200\n",
      "Iteration 500, Loss: 0.7951\n",
      "Iteration 600, Loss: 0.7758\n",
      "Iteration 700, Loss: 0.7605\n",
      "Iteration 800, Loss: 0.7481\n",
      "Iteration 900, Loss: 0.7377\n",
      "Iteration 1000, Loss: 0.7289\n",
      "Iteration 1100, Loss: 0.7214\n",
      "Iteration 1200, Loss: 0.7148\n",
      "Iteration 1300, Loss: 0.7089\n",
      "Iteration 1400, Loss: 0.7037\n",
      "Iteration 1500, Loss: 0.6990\n",
      "Iteration 1600, Loss: 0.6947\n",
      "Iteration 1700, Loss: 0.6907\n",
      "Iteration 1800, Loss: 0.6870\n",
      "Iteration 1900, Loss: 0.6837\n",
      "Iteration 2000, Loss: 0.6805\n",
      "Iteration 2100, Loss: 0.6775\n",
      "Iteration 2200, Loss: 0.6747\n",
      "Iteration 2300, Loss: 0.6721\n",
      "Iteration 2400, Loss: 0.6696\n",
      "Iteration 2500, Loss: 0.6672\n",
      "Iteration 2600, Loss: 0.6649\n",
      "Iteration 2700, Loss: 0.6628\n",
      "Iteration 2800, Loss: 0.6607\n",
      "Iteration 2900, Loss: 0.6588\n",
      "Iteration 3000, Loss: 0.6569\n",
      "Iteration 3100, Loss: 0.6551\n",
      "Iteration 3200, Loss: 0.6533\n",
      "Iteration 3300, Loss: 0.6516\n",
      "Iteration 3400, Loss: 0.6500\n",
      "Iteration 3500, Loss: 0.6485\n",
      "Iteration 3600, Loss: 0.6470\n",
      "Iteration 3700, Loss: 0.6455\n",
      "Iteration 3800, Loss: 0.6441\n",
      "Iteration 3900, Loss: 0.6427\n",
      "Iteration 4000, Loss: 0.6414\n",
      "Iteration 4100, Loss: 0.6401\n",
      "Iteration 4200, Loss: 0.6389\n",
      "Iteration 4300, Loss: 0.6376\n",
      "Iteration 4400, Loss: 0.6365\n",
      "Iteration 4500, Loss: 0.6353\n",
      "Iteration 4600, Loss: 0.6342\n",
      "Iteration 4700, Loss: 0.6331\n",
      "Iteration 4800, Loss: 0.6320\n",
      "Iteration 4900, Loss: 0.6310\n",
      "Iteration 0, Loss: 1.0967\n",
      "Iteration 100, Loss: 0.9628\n",
      "Iteration 200, Loss: 0.8891\n",
      "Iteration 300, Loss: 0.8415\n",
      "Iteration 400, Loss: 0.8082\n",
      "Iteration 500, Loss: 0.7834\n",
      "Iteration 600, Loss: 0.7641\n",
      "Iteration 700, Loss: 0.7488\n",
      "Iteration 800, Loss: 0.7363\n",
      "Iteration 900, Loss: 0.7258\n",
      "Iteration 1000, Loss: 0.7168\n",
      "Iteration 1100, Loss: 0.7091\n",
      "Iteration 1200, Loss: 0.7023\n",
      "Iteration 1300, Loss: 0.6963\n",
      "Iteration 1400, Loss: 0.6910\n",
      "Iteration 1500, Loss: 0.6861\n",
      "Iteration 1600, Loss: 0.6817\n",
      "Iteration 1700, Loss: 0.6776\n",
      "Iteration 1800, Loss: 0.6739\n",
      "Iteration 1900, Loss: 0.6704\n",
      "Iteration 2000, Loss: 0.6672\n",
      "Iteration 2100, Loss: 0.6642\n",
      "Iteration 2200, Loss: 0.6614\n",
      "Iteration 2300, Loss: 0.6588\n",
      "Iteration 2400, Loss: 0.6563\n",
      "Iteration 2500, Loss: 0.6539\n",
      "Iteration 2600, Loss: 0.6517\n",
      "Iteration 2700, Loss: 0.6496\n",
      "Iteration 2800, Loss: 0.6476\n",
      "Iteration 2900, Loss: 0.6457\n",
      "Iteration 3000, Loss: 0.6439\n",
      "Iteration 3100, Loss: 0.6422\n",
      "Iteration 3200, Loss: 0.6405\n",
      "Iteration 3300, Loss: 0.6389\n",
      "Iteration 3400, Loss: 0.6374\n",
      "Iteration 3500, Loss: 0.6359\n",
      "Iteration 3600, Loss: 0.6345\n",
      "Iteration 3700, Loss: 0.6331\n",
      "Iteration 3800, Loss: 0.6318\n",
      "Iteration 3900, Loss: 0.6306\n",
      "Iteration 4000, Loss: 0.6293\n",
      "Iteration 4100, Loss: 0.6282\n",
      "Iteration 4200, Loss: 0.6270\n",
      "Iteration 4300, Loss: 0.6259\n",
      "Iteration 4400, Loss: 0.6248\n",
      "Iteration 4500, Loss: 0.6238\n",
      "Iteration 4600, Loss: 0.6228\n",
      "Iteration 4700, Loss: 0.6218\n",
      "Iteration 4800, Loss: 0.6208\n",
      "Iteration 4900, Loss: 0.6199\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0208\n",
      "Iteration 200, Loss: 0.9673\n",
      "Iteration 300, Loss: 0.9271\n",
      "Iteration 400, Loss: 0.8955\n",
      "Iteration 500, Loss: 0.8700\n",
      "Iteration 600, Loss: 0.8489\n",
      "Iteration 700, Loss: 0.8310\n",
      "Iteration 800, Loss: 0.8158\n",
      "Iteration 900, Loss: 0.8028\n",
      "Iteration 1000, Loss: 0.7914\n",
      "Iteration 1100, Loss: 0.7813\n",
      "Iteration 1200, Loss: 0.7724\n",
      "Iteration 1300, Loss: 0.7645\n",
      "Iteration 1400, Loss: 0.7574\n",
      "Iteration 1500, Loss: 0.7509\n",
      "Iteration 1600, Loss: 0.7451\n",
      "Iteration 1700, Loss: 0.7398\n",
      "Iteration 1800, Loss: 0.7349\n",
      "Iteration 1900, Loss: 0.7304\n",
      "Iteration 2000, Loss: 0.7262\n",
      "Iteration 2100, Loss: 0.7224\n",
      "Iteration 2200, Loss: 0.7188\n",
      "Iteration 2300, Loss: 0.7154\n",
      "Iteration 2400, Loss: 0.7123\n",
      "Iteration 2500, Loss: 0.7093\n",
      "Iteration 2600, Loss: 0.7065\n",
      "Iteration 2700, Loss: 0.7038\n",
      "Iteration 2800, Loss: 0.7013\n",
      "Iteration 2900, Loss: 0.6990\n",
      "Iteration 3000, Loss: 0.6967\n",
      "Iteration 3100, Loss: 0.6945\n",
      "Iteration 3200, Loss: 0.6925\n",
      "Iteration 3300, Loss: 0.6905\n",
      "Iteration 3400, Loss: 0.6886\n",
      "Iteration 3500, Loss: 0.6868\n",
      "Iteration 3600, Loss: 0.6851\n",
      "Iteration 3700, Loss: 0.6834\n",
      "Iteration 3800, Loss: 0.6818\n",
      "Iteration 3900, Loss: 0.6802\n",
      "Iteration 4000, Loss: 0.6787\n",
      "Iteration 4100, Loss: 0.6773\n",
      "Iteration 4200, Loss: 0.6759\n",
      "Iteration 4300, Loss: 0.6745\n",
      "Iteration 4400, Loss: 0.6732\n",
      "Iteration 4500, Loss: 0.6719\n",
      "Iteration 4600, Loss: 0.6707\n",
      "Iteration 4700, Loss: 0.6695\n",
      "Iteration 4800, Loss: 0.6683\n",
      "Iteration 4900, Loss: 0.6672\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0188\n",
      "Iteration 200, Loss: 0.9636\n",
      "Iteration 300, Loss: 0.9222\n",
      "Iteration 400, Loss: 0.8897\n",
      "Iteration 500, Loss: 0.8634\n",
      "Iteration 600, Loss: 0.8417\n",
      "Iteration 700, Loss: 0.8235\n",
      "Iteration 800, Loss: 0.8079\n",
      "Iteration 900, Loss: 0.7945\n",
      "Iteration 1000, Loss: 0.7828\n",
      "Iteration 1100, Loss: 0.7725\n",
      "Iteration 1200, Loss: 0.7634\n",
      "Iteration 1300, Loss: 0.7552\n",
      "Iteration 1400, Loss: 0.7479\n",
      "Iteration 1500, Loss: 0.7413\n",
      "Iteration 1600, Loss: 0.7353\n",
      "Iteration 1700, Loss: 0.7298\n",
      "Iteration 1800, Loss: 0.7247\n",
      "Iteration 1900, Loss: 0.7201\n",
      "Iteration 2000, Loss: 0.7157\n",
      "Iteration 2100, Loss: 0.7117\n",
      "Iteration 2200, Loss: 0.7080\n",
      "Iteration 2300, Loss: 0.7044\n",
      "Iteration 2400, Loss: 0.7011\n",
      "Iteration 2500, Loss: 0.6980\n",
      "Iteration 2600, Loss: 0.6951\n",
      "Iteration 2700, Loss: 0.6923\n",
      "Iteration 2800, Loss: 0.6896\n",
      "Iteration 2900, Loss: 0.6871\n",
      "Iteration 3000, Loss: 0.6847\n",
      "Iteration 3100, Loss: 0.6824\n",
      "Iteration 3200, Loss: 0.6802\n",
      "Iteration 3300, Loss: 0.6781\n",
      "Iteration 3400, Loss: 0.6760\n",
      "Iteration 3500, Loss: 0.6741\n",
      "Iteration 3600, Loss: 0.6722\n",
      "Iteration 3700, Loss: 0.6704\n",
      "Iteration 3800, Loss: 0.6686\n",
      "Iteration 3900, Loss: 0.6669\n",
      "Iteration 4000, Loss: 0.6653\n",
      "Iteration 4100, Loss: 0.6637\n",
      "Iteration 4200, Loss: 0.6622\n",
      "Iteration 4300, Loss: 0.6607\n",
      "Iteration 4400, Loss: 0.6593\n",
      "Iteration 4500, Loss: 0.6579\n",
      "Iteration 4600, Loss: 0.6565\n",
      "Iteration 4700, Loss: 0.6552\n",
      "Iteration 4800, Loss: 0.6539\n",
      "Iteration 4900, Loss: 0.6527\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0542\n",
      "Iteration 200, Loss: 1.0184\n",
      "Iteration 300, Loss: 0.9888\n",
      "Iteration 400, Loss: 0.9639\n",
      "Iteration 500, Loss: 0.9422\n",
      "Iteration 600, Loss: 0.9233\n",
      "Iteration 700, Loss: 0.9065\n",
      "Iteration 800, Loss: 0.8917\n",
      "Iteration 900, Loss: 0.8783\n",
      "Iteration 1000, Loss: 0.8662\n",
      "Iteration 1100, Loss: 0.8553\n",
      "Iteration 1200, Loss: 0.8452\n",
      "Iteration 1300, Loss: 0.8360\n",
      "Iteration 1400, Loss: 0.8276\n",
      "Iteration 1500, Loss: 0.8199\n",
      "Iteration 1600, Loss: 0.8127\n",
      "Iteration 1700, Loss: 0.8060\n",
      "Iteration 1800, Loss: 0.7998\n",
      "Iteration 1900, Loss: 0.7941\n",
      "Iteration 2000, Loss: 0.7887\n",
      "Iteration 2100, Loss: 0.7836\n",
      "Iteration 2200, Loss: 0.7788\n",
      "Iteration 2300, Loss: 0.7744\n",
      "Iteration 2400, Loss: 0.7701\n",
      "Iteration 2500, Loss: 0.7661\n",
      "Iteration 2600, Loss: 0.7624\n",
      "Iteration 2700, Loss: 0.7588\n",
      "Iteration 2800, Loss: 0.7555\n",
      "Iteration 2900, Loss: 0.7522\n",
      "Iteration 3000, Loss: 0.7492\n",
      "Iteration 3100, Loss: 0.7462\n",
      "Iteration 3200, Loss: 0.7435\n",
      "Iteration 3300, Loss: 0.7408\n",
      "Iteration 3400, Loss: 0.7383\n",
      "Iteration 3500, Loss: 0.7358\n",
      "Iteration 3600, Loss: 0.7335\n",
      "Iteration 3700, Loss: 0.7312\n",
      "Iteration 3800, Loss: 0.7291\n",
      "Iteration 3900, Loss: 0.7270\n",
      "Iteration 4000, Loss: 0.7250\n",
      "Iteration 4100, Loss: 0.7231\n",
      "Iteration 4200, Loss: 0.7212\n",
      "Iteration 4300, Loss: 0.7194\n",
      "Iteration 4400, Loss: 0.7177\n",
      "Iteration 4500, Loss: 0.7160\n",
      "Iteration 4600, Loss: 0.7143\n",
      "Iteration 4700, Loss: 0.7128\n",
      "Iteration 4800, Loss: 0.7112\n",
      "Iteration 4900, Loss: 0.7098\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0530\n",
      "Iteration 200, Loss: 1.0166\n",
      "Iteration 300, Loss: 0.9860\n",
      "Iteration 400, Loss: 0.9601\n",
      "Iteration 500, Loss: 0.9377\n",
      "Iteration 600, Loss: 0.9181\n",
      "Iteration 700, Loss: 0.9006\n",
      "Iteration 800, Loss: 0.8851\n",
      "Iteration 900, Loss: 0.8711\n",
      "Iteration 1000, Loss: 0.8585\n",
      "Iteration 1100, Loss: 0.8470\n",
      "Iteration 1200, Loss: 0.8366\n",
      "Iteration 1300, Loss: 0.8270\n",
      "Iteration 1400, Loss: 0.8182\n",
      "Iteration 1500, Loss: 0.8100\n",
      "Iteration 1600, Loss: 0.8025\n",
      "Iteration 1700, Loss: 0.7955\n",
      "Iteration 1800, Loss: 0.7890\n",
      "Iteration 1900, Loss: 0.7829\n",
      "Iteration 2000, Loss: 0.7773\n",
      "Iteration 2100, Loss: 0.7720\n",
      "Iteration 2200, Loss: 0.7670\n",
      "Iteration 2300, Loss: 0.7624\n",
      "Iteration 2400, Loss: 0.7580\n",
      "Iteration 2500, Loss: 0.7538\n",
      "Iteration 2600, Loss: 0.7499\n",
      "Iteration 2700, Loss: 0.7461\n",
      "Iteration 2800, Loss: 0.7426\n",
      "Iteration 2900, Loss: 0.7392\n",
      "Iteration 3000, Loss: 0.7360\n",
      "Iteration 3100, Loss: 0.7329\n",
      "Iteration 3200, Loss: 0.7300\n",
      "Iteration 3300, Loss: 0.7272\n",
      "Iteration 3400, Loss: 0.7245\n",
      "Iteration 3500, Loss: 0.7220\n",
      "Iteration 3600, Loss: 0.7195\n",
      "Iteration 3700, Loss: 0.7171\n",
      "Iteration 3800, Loss: 0.7149\n",
      "Iteration 3900, Loss: 0.7127\n",
      "Iteration 4000, Loss: 0.7106\n",
      "Iteration 4100, Loss: 0.7086\n",
      "Iteration 4200, Loss: 0.7066\n",
      "Iteration 4300, Loss: 0.7047\n",
      "Iteration 4400, Loss: 0.7029\n",
      "Iteration 4500, Loss: 0.7011\n",
      "Iteration 4600, Loss: 0.6994\n",
      "Iteration 4700, Loss: 0.6978\n",
      "Iteration 4800, Loss: 0.6962\n",
      "Iteration 4900, Loss: 0.6946\n",
      "Iteration 0, Loss: 0.9730\n",
      "Iteration 100, Loss: 0.6056\n",
      "Iteration 200, Loss: 0.5742\n",
      "Iteration 300, Loss: 0.5566\n",
      "Iteration 400, Loss: 0.5447\n",
      "Iteration 500, Loss: 0.5358\n",
      "Iteration 600, Loss: 0.5288\n",
      "Iteration 700, Loss: 0.5230\n",
      "Iteration 800, Loss: 0.5180\n",
      "Iteration 900, Loss: 0.5137\n",
      "Iteration 1000, Loss: 0.5099\n",
      "Iteration 1100, Loss: 0.5065\n",
      "Iteration 1200, Loss: 0.5034\n",
      "Iteration 1300, Loss: 0.5006\n",
      "Iteration 1400, Loss: 0.4980\n",
      "Iteration 1500, Loss: 0.4956\n",
      "Iteration 1600, Loss: 0.4934\n",
      "Iteration 1700, Loss: 0.4913\n",
      "Iteration 1800, Loss: 0.4893\n",
      "Iteration 1900, Loss: 0.4875\n",
      "Iteration 2000, Loss: 0.4858\n",
      "Iteration 2100, Loss: 0.4841\n",
      "Iteration 2200, Loss: 0.4826\n",
      "Iteration 2300, Loss: 0.4811\n",
      "Iteration 2400, Loss: 0.4797\n",
      "Iteration 2500, Loss: 0.4784\n",
      "Iteration 2600, Loss: 0.4771\n",
      "Iteration 2700, Loss: 0.4758\n",
      "Iteration 2800, Loss: 0.4747\n",
      "Iteration 2900, Loss: 0.4735\n",
      "Iteration 3000, Loss: 0.4725\n",
      "Iteration 3100, Loss: 0.4714\n",
      "Iteration 3200, Loss: 0.4704\n",
      "Iteration 3300, Loss: 0.4694\n",
      "Iteration 3400, Loss: 0.4685\n",
      "Iteration 3500, Loss: 0.4676\n",
      "Iteration 3600, Loss: 0.4667\n",
      "Iteration 3700, Loss: 0.4659\n",
      "Iteration 3800, Loss: 0.4650\n",
      "Iteration 3900, Loss: 0.4642\n",
      "Iteration 4000, Loss: 0.4634\n",
      "Iteration 4100, Loss: 0.4627\n",
      "Iteration 4200, Loss: 0.4619\n",
      "Iteration 4300, Loss: 0.4612\n",
      "Iteration 4400, Loss: 0.4605\n",
      "Iteration 4500, Loss: 0.4598\n",
      "Iteration 4600, Loss: 0.4592\n",
      "Iteration 4700, Loss: 0.4585\n",
      "Iteration 4800, Loss: 0.4579\n",
      "Iteration 4900, Loss: 0.4573\n",
      "Iteration 5000, Loss: 0.4566\n",
      "Iteration 5100, Loss: 0.4561\n",
      "Iteration 5200, Loss: 0.4555\n",
      "Iteration 5300, Loss: 0.4549\n",
      "Iteration 5400, Loss: 0.4543\n",
      "Iteration 5500, Loss: 0.4538\n",
      "Iteration 5600, Loss: 0.4533\n",
      "Iteration 5700, Loss: 0.4527\n",
      "Iteration 5800, Loss: 0.4522\n",
      "Iteration 5900, Loss: 0.4517\n",
      "Iteration 0, Loss: 0.9613\n",
      "Iteration 100, Loss: 0.5781\n",
      "Iteration 200, Loss: 0.5497\n",
      "Iteration 300, Loss: 0.5337\n",
      "Iteration 400, Loss: 0.5226\n",
      "Iteration 500, Loss: 0.5141\n",
      "Iteration 600, Loss: 0.5073\n",
      "Iteration 700, Loss: 0.5016\n",
      "Iteration 800, Loss: 0.4966\n",
      "Iteration 900, Loss: 0.4922\n",
      "Iteration 1000, Loss: 0.4883\n",
      "Iteration 1100, Loss: 0.4848\n",
      "Iteration 1200, Loss: 0.4816\n",
      "Iteration 1300, Loss: 0.4787\n",
      "Iteration 1400, Loss: 0.4759\n",
      "Iteration 1500, Loss: 0.4734\n",
      "Iteration 1600, Loss: 0.4711\n",
      "Iteration 1700, Loss: 0.4689\n",
      "Iteration 1800, Loss: 0.4668\n",
      "Iteration 1900, Loss: 0.4649\n",
      "Iteration 2000, Loss: 0.4630\n",
      "Iteration 2100, Loss: 0.4613\n",
      "Iteration 2200, Loss: 0.4596\n",
      "Iteration 2300, Loss: 0.4580\n",
      "Iteration 2400, Loss: 0.4565\n",
      "Iteration 2500, Loss: 0.4551\n",
      "Iteration 2600, Loss: 0.4537\n",
      "Iteration 2700, Loss: 0.4524\n",
      "Iteration 2800, Loss: 0.4512\n",
      "Iteration 2900, Loss: 0.4500\n",
      "Iteration 3000, Loss: 0.4488\n",
      "Iteration 3100, Loss: 0.4477\n",
      "Iteration 3200, Loss: 0.4466\n",
      "Iteration 3300, Loss: 0.4456\n",
      "Iteration 3400, Loss: 0.4446\n",
      "Iteration 3500, Loss: 0.4436\n",
      "Iteration 3600, Loss: 0.4427\n",
      "Iteration 3700, Loss: 0.4418\n",
      "Iteration 3800, Loss: 0.4409\n",
      "Iteration 3900, Loss: 0.4400\n",
      "Iteration 4000, Loss: 0.4392\n",
      "Iteration 4100, Loss: 0.4384\n",
      "Iteration 4200, Loss: 0.4376\n",
      "Iteration 4300, Loss: 0.4369\n",
      "Iteration 4400, Loss: 0.4362\n",
      "Iteration 4500, Loss: 0.4355\n",
      "Iteration 4600, Loss: 0.4348\n",
      "Iteration 4700, Loss: 0.4341\n",
      "Iteration 4800, Loss: 0.4334\n",
      "Iteration 4900, Loss: 0.4328\n",
      "Iteration 5000, Loss: 0.4322\n",
      "Iteration 5100, Loss: 0.4316\n",
      "Iteration 5200, Loss: 0.4310\n",
      "Iteration 5300, Loss: 0.4304\n",
      "Iteration 5400, Loss: 0.4298\n",
      "Iteration 5500, Loss: 0.4293\n",
      "Iteration 5600, Loss: 0.4287\n",
      "Iteration 5700, Loss: 0.4282\n",
      "Iteration 5800, Loss: 0.4277\n",
      "Iteration 5900, Loss: 0.4272\n",
      "Iteration 0, Loss: 1.0172\n",
      "Iteration 100, Loss: 0.6069\n",
      "Iteration 200, Loss: 0.5753\n",
      "Iteration 300, Loss: 0.5573\n",
      "Iteration 400, Loss: 0.5447\n",
      "Iteration 500, Loss: 0.5352\n",
      "Iteration 600, Loss: 0.5276\n",
      "Iteration 700, Loss: 0.5213\n",
      "Iteration 800, Loss: 0.5159\n",
      "Iteration 900, Loss: 0.5113\n",
      "Iteration 1000, Loss: 0.5073\n",
      "Iteration 1100, Loss: 0.5037\n",
      "Iteration 1200, Loss: 0.5005\n",
      "Iteration 1300, Loss: 0.4976\n",
      "Iteration 1400, Loss: 0.4949\n",
      "Iteration 1500, Loss: 0.4924\n",
      "Iteration 1600, Loss: 0.4901\n",
      "Iteration 1700, Loss: 0.4880\n",
      "Iteration 1800, Loss: 0.4861\n",
      "Iteration 1900, Loss: 0.4842\n",
      "Iteration 2000, Loss: 0.4825\n",
      "Iteration 2100, Loss: 0.4808\n",
      "Iteration 2200, Loss: 0.4792\n",
      "Iteration 2300, Loss: 0.4778\n",
      "Iteration 2400, Loss: 0.4763\n",
      "Iteration 2500, Loss: 0.4750\n",
      "Iteration 2600, Loss: 0.4737\n",
      "Iteration 2700, Loss: 0.4724\n",
      "Iteration 2800, Loss: 0.4712\n",
      "Iteration 2900, Loss: 0.4701\n",
      "Iteration 3000, Loss: 0.4690\n",
      "Iteration 3100, Loss: 0.4679\n",
      "Iteration 3200, Loss: 0.4669\n",
      "Iteration 3300, Loss: 0.4659\n",
      "Iteration 3400, Loss: 0.4649\n",
      "Iteration 3500, Loss: 0.4640\n",
      "Iteration 3600, Loss: 0.4631\n",
      "Iteration 3700, Loss: 0.4622\n",
      "Iteration 3800, Loss: 0.4613\n",
      "Iteration 3900, Loss: 0.4605\n",
      "Iteration 4000, Loss: 0.4597\n",
      "Iteration 4100, Loss: 0.4589\n",
      "Iteration 4200, Loss: 0.4581\n",
      "Iteration 4300, Loss: 0.4573\n",
      "Iteration 4400, Loss: 0.4566\n",
      "Iteration 4500, Loss: 0.4559\n",
      "Iteration 4600, Loss: 0.4552\n",
      "Iteration 4700, Loss: 0.4545\n",
      "Iteration 4800, Loss: 0.4539\n",
      "Iteration 4900, Loss: 0.4532\n",
      "Iteration 5000, Loss: 0.4525\n",
      "Iteration 5100, Loss: 0.4519\n",
      "Iteration 5200, Loss: 0.4513\n",
      "Iteration 5300, Loss: 0.4507\n",
      "Iteration 5400, Loss: 0.4501\n",
      "Iteration 5500, Loss: 0.4495\n",
      "Iteration 5600, Loss: 0.4490\n",
      "Iteration 5700, Loss: 0.4484\n",
      "Iteration 5800, Loss: 0.4479\n",
      "Iteration 5900, Loss: 0.4473\n",
      "Iteration 0, Loss: 1.0225\n",
      "Iteration 100, Loss: 0.6375\n",
      "Iteration 200, Loss: 0.6072\n",
      "Iteration 300, Loss: 0.5908\n",
      "Iteration 400, Loss: 0.5795\n",
      "Iteration 500, Loss: 0.5710\n",
      "Iteration 600, Loss: 0.5643\n",
      "Iteration 700, Loss: 0.5586\n",
      "Iteration 800, Loss: 0.5538\n",
      "Iteration 900, Loss: 0.5496\n",
      "Iteration 1000, Loss: 0.5458\n",
      "Iteration 1100, Loss: 0.5424\n",
      "Iteration 1200, Loss: 0.5393\n",
      "Iteration 1300, Loss: 0.5364\n",
      "Iteration 1400, Loss: 0.5338\n",
      "Iteration 1500, Loss: 0.5313\n",
      "Iteration 1600, Loss: 0.5290\n",
      "Iteration 1700, Loss: 0.5269\n",
      "Iteration 1800, Loss: 0.5248\n",
      "Iteration 1900, Loss: 0.5229\n",
      "Iteration 2000, Loss: 0.5211\n",
      "Iteration 2100, Loss: 0.5193\n",
      "Iteration 2200, Loss: 0.5177\n",
      "Iteration 2300, Loss: 0.5161\n",
      "Iteration 2400, Loss: 0.5146\n",
      "Iteration 2500, Loss: 0.5131\n",
      "Iteration 2600, Loss: 0.5117\n",
      "Iteration 2700, Loss: 0.5104\n",
      "Iteration 2800, Loss: 0.5091\n",
      "Iteration 2900, Loss: 0.5078\n",
      "Iteration 3000, Loss: 0.5066\n",
      "Iteration 3100, Loss: 0.5055\n",
      "Iteration 3200, Loss: 0.5043\n",
      "Iteration 3300, Loss: 0.5032\n",
      "Iteration 3400, Loss: 0.5022\n",
      "Iteration 3500, Loss: 0.5012\n",
      "Iteration 3600, Loss: 0.5002\n",
      "Iteration 3700, Loss: 0.4992\n",
      "Iteration 3800, Loss: 0.4983\n",
      "Iteration 3900, Loss: 0.4974\n",
      "Iteration 4000, Loss: 0.4965\n",
      "Iteration 4100, Loss: 0.4956\n",
      "Iteration 4200, Loss: 0.4948\n",
      "Iteration 4300, Loss: 0.4939\n",
      "Iteration 4400, Loss: 0.4931\n",
      "Iteration 4500, Loss: 0.4923\n",
      "Iteration 4600, Loss: 0.4916\n",
      "Iteration 4700, Loss: 0.4908\n",
      "Iteration 4800, Loss: 0.4901\n",
      "Iteration 4900, Loss: 0.4894\n",
      "Iteration 5000, Loss: 0.4887\n",
      "Iteration 5100, Loss: 0.4880\n",
      "Iteration 5200, Loss: 0.4873\n",
      "Iteration 5300, Loss: 0.4867\n",
      "Iteration 5400, Loss: 0.4861\n",
      "Iteration 5500, Loss: 0.4854\n",
      "Iteration 5600, Loss: 0.4848\n",
      "Iteration 5700, Loss: 0.4842\n",
      "Iteration 5800, Loss: 0.4836\n",
      "Iteration 5900, Loss: 0.4830\n",
      "Iteration 0, Loss: 1.0527\n",
      "Iteration 100, Loss: 0.6535\n",
      "Iteration 200, Loss: 0.6198\n",
      "Iteration 300, Loss: 0.6023\n",
      "Iteration 400, Loss: 0.5900\n",
      "Iteration 500, Loss: 0.5805\n",
      "Iteration 600, Loss: 0.5727\n",
      "Iteration 700, Loss: 0.5661\n",
      "Iteration 800, Loss: 0.5604\n",
      "Iteration 900, Loss: 0.5553\n",
      "Iteration 1000, Loss: 0.5509\n",
      "Iteration 1100, Loss: 0.5467\n",
      "Iteration 1200, Loss: 0.5429\n",
      "Iteration 1300, Loss: 0.5395\n",
      "Iteration 1400, Loss: 0.5363\n",
      "Iteration 1500, Loss: 0.5334\n",
      "Iteration 1600, Loss: 0.5306\n",
      "Iteration 1700, Loss: 0.5281\n",
      "Iteration 1800, Loss: 0.5257\n",
      "Iteration 1900, Loss: 0.5234\n",
      "Iteration 2000, Loss: 0.5213\n",
      "Iteration 2100, Loss: 0.5193\n",
      "Iteration 2200, Loss: 0.5174\n",
      "Iteration 2300, Loss: 0.5155\n",
      "Iteration 2400, Loss: 0.5138\n",
      "Iteration 2500, Loss: 0.5121\n",
      "Iteration 2600, Loss: 0.5105\n",
      "Iteration 2700, Loss: 0.5090\n",
      "Iteration 2800, Loss: 0.5075\n",
      "Iteration 2900, Loss: 0.5061\n",
      "Iteration 3000, Loss: 0.5047\n",
      "Iteration 3100, Loss: 0.5034\n",
      "Iteration 3200, Loss: 0.5021\n",
      "Iteration 3300, Loss: 0.5009\n",
      "Iteration 3400, Loss: 0.4997\n",
      "Iteration 3500, Loss: 0.4985\n",
      "Iteration 3600, Loss: 0.4974\n",
      "Iteration 3700, Loss: 0.4964\n",
      "Iteration 3800, Loss: 0.4953\n",
      "Iteration 3900, Loss: 0.4943\n",
      "Iteration 4000, Loss: 0.4933\n",
      "Iteration 4100, Loss: 0.4924\n",
      "Iteration 4200, Loss: 0.4914\n",
      "Iteration 4300, Loss: 0.4906\n",
      "Iteration 4400, Loss: 0.4896\n",
      "Iteration 4500, Loss: 0.4888\n",
      "Iteration 4600, Loss: 0.4879\n",
      "Iteration 4700, Loss: 0.4871\n",
      "Iteration 4800, Loss: 0.4863\n",
      "Iteration 4900, Loss: 0.4855\n",
      "Iteration 5000, Loss: 0.4847\n",
      "Iteration 5100, Loss: 0.4840\n",
      "Iteration 5200, Loss: 0.4833\n",
      "Iteration 5300, Loss: 0.4826\n",
      "Iteration 5400, Loss: 0.4819\n",
      "Iteration 5500, Loss: 0.4812\n",
      "Iteration 5600, Loss: 0.4805\n",
      "Iteration 5700, Loss: 0.4798\n",
      "Iteration 5800, Loss: 0.4792\n",
      "Iteration 5900, Loss: 0.4785\n",
      "Iteration 0, Loss: 1.0524\n",
      "Iteration 100, Loss: 0.6594\n",
      "Iteration 200, Loss: 0.6222\n",
      "Iteration 300, Loss: 0.6026\n",
      "Iteration 400, Loss: 0.5893\n",
      "Iteration 500, Loss: 0.5793\n",
      "Iteration 600, Loss: 0.5712\n",
      "Iteration 700, Loss: 0.5645\n",
      "Iteration 800, Loss: 0.5588\n",
      "Iteration 900, Loss: 0.5539\n",
      "Iteration 1000, Loss: 0.5495\n",
      "Iteration 1100, Loss: 0.5457\n",
      "Iteration 1200, Loss: 0.5421\n",
      "Iteration 1300, Loss: 0.5390\n",
      "Iteration 1400, Loss: 0.5361\n",
      "Iteration 1500, Loss: 0.5334\n",
      "Iteration 1600, Loss: 0.5309\n",
      "Iteration 1700, Loss: 0.5285\n",
      "Iteration 1800, Loss: 0.5264\n",
      "Iteration 1900, Loss: 0.5243\n",
      "Iteration 2000, Loss: 0.5224\n",
      "Iteration 2100, Loss: 0.5205\n",
      "Iteration 2200, Loss: 0.5188\n",
      "Iteration 2300, Loss: 0.5171\n",
      "Iteration 2400, Loss: 0.5155\n",
      "Iteration 2500, Loss: 0.5140\n",
      "Iteration 2600, Loss: 0.5126\n",
      "Iteration 2700, Loss: 0.5112\n",
      "Iteration 2800, Loss: 0.5099\n",
      "Iteration 2900, Loss: 0.5086\n",
      "Iteration 3000, Loss: 0.5074\n",
      "Iteration 3100, Loss: 0.5062\n",
      "Iteration 3200, Loss: 0.5050\n",
      "Iteration 3300, Loss: 0.5039\n",
      "Iteration 3400, Loss: 0.5028\n",
      "Iteration 3500, Loss: 0.5018\n",
      "Iteration 3600, Loss: 0.5008\n",
      "Iteration 3700, Loss: 0.4998\n",
      "Iteration 3800, Loss: 0.4988\n",
      "Iteration 3900, Loss: 0.4979\n",
      "Iteration 4000, Loss: 0.4970\n",
      "Iteration 4100, Loss: 0.4961\n",
      "Iteration 4200, Loss: 0.4952\n",
      "Iteration 4300, Loss: 0.4944\n",
      "Iteration 4400, Loss: 0.4936\n",
      "Iteration 4500, Loss: 0.4927\n",
      "Iteration 4600, Loss: 0.4920\n",
      "Iteration 4700, Loss: 0.4912\n",
      "Iteration 4800, Loss: 0.4904\n",
      "Iteration 4900, Loss: 0.4897\n",
      "Iteration 5000, Loss: 0.4890\n",
      "Iteration 5100, Loss: 0.4883\n",
      "Iteration 5200, Loss: 0.4876\n",
      "Iteration 5300, Loss: 0.4870\n",
      "Iteration 5400, Loss: 0.4863\n",
      "Iteration 5500, Loss: 0.4856\n",
      "Iteration 5600, Loss: 0.4850\n",
      "Iteration 5700, Loss: 0.4844\n",
      "Iteration 5800, Loss: 0.4838\n",
      "Iteration 5900, Loss: 0.4832\n",
      "Iteration 0, Loss: 1.0805\n",
      "Iteration 100, Loss: 0.7276\n",
      "Iteration 200, Loss: 0.6810\n",
      "Iteration 300, Loss: 0.6586\n",
      "Iteration 400, Loss: 0.6442\n",
      "Iteration 500, Loss: 0.6337\n",
      "Iteration 600, Loss: 0.6255\n",
      "Iteration 700, Loss: 0.6188\n",
      "Iteration 800, Loss: 0.6131\n",
      "Iteration 900, Loss: 0.6081\n",
      "Iteration 1000, Loss: 0.6037\n",
      "Iteration 1100, Loss: 0.5998\n",
      "Iteration 1200, Loss: 0.5963\n",
      "Iteration 1300, Loss: 0.5930\n",
      "Iteration 1400, Loss: 0.5900\n",
      "Iteration 1500, Loss: 0.5873\n",
      "Iteration 1600, Loss: 0.5847\n",
      "Iteration 1700, Loss: 0.5823\n",
      "Iteration 1800, Loss: 0.5801\n",
      "Iteration 1900, Loss: 0.5779\n",
      "Iteration 2000, Loss: 0.5759\n",
      "Iteration 2100, Loss: 0.5740\n",
      "Iteration 2200, Loss: 0.5722\n",
      "Iteration 2300, Loss: 0.5705\n",
      "Iteration 2400, Loss: 0.5689\n",
      "Iteration 2500, Loss: 0.5673\n",
      "Iteration 2600, Loss: 0.5658\n",
      "Iteration 2700, Loss: 0.5644\n",
      "Iteration 2800, Loss: 0.5630\n",
      "Iteration 2900, Loss: 0.5617\n",
      "Iteration 3000, Loss: 0.5604\n",
      "Iteration 3100, Loss: 0.5592\n",
      "Iteration 3200, Loss: 0.5580\n",
      "Iteration 3300, Loss: 0.5568\n",
      "Iteration 3400, Loss: 0.5557\n",
      "Iteration 3500, Loss: 0.5546\n",
      "Iteration 3600, Loss: 0.5536\n",
      "Iteration 3700, Loss: 0.5526\n",
      "Iteration 3800, Loss: 0.5516\n",
      "Iteration 3900, Loss: 0.5506\n",
      "Iteration 4000, Loss: 0.5497\n",
      "Iteration 4100, Loss: 0.5488\n",
      "Iteration 4200, Loss: 0.5479\n",
      "Iteration 4300, Loss: 0.5471\n",
      "Iteration 4400, Loss: 0.5462\n",
      "Iteration 4500, Loss: 0.5454\n",
      "Iteration 4600, Loss: 0.5446\n",
      "Iteration 4700, Loss: 0.5438\n",
      "Iteration 4800, Loss: 0.5431\n",
      "Iteration 4900, Loss: 0.5423\n",
      "Iteration 5000, Loss: 0.5416\n",
      "Iteration 5100, Loss: 0.5409\n",
      "Iteration 5200, Loss: 0.5402\n",
      "Iteration 5300, Loss: 0.5395\n",
      "Iteration 5400, Loss: 0.5389\n",
      "Iteration 5500, Loss: 0.5382\n",
      "Iteration 5600, Loss: 0.5376\n",
      "Iteration 5700, Loss: 0.5369\n",
      "Iteration 5800, Loss: 0.5363\n",
      "Iteration 5900, Loss: 0.5357\n",
      "Iteration 0, Loss: 1.0807\n",
      "Iteration 100, Loss: 0.7174\n",
      "Iteration 200, Loss: 0.6668\n",
      "Iteration 300, Loss: 0.6426\n",
      "Iteration 400, Loss: 0.6271\n",
      "Iteration 500, Loss: 0.6159\n",
      "Iteration 600, Loss: 0.6071\n",
      "Iteration 700, Loss: 0.5999\n",
      "Iteration 800, Loss: 0.5937\n",
      "Iteration 900, Loss: 0.5884\n",
      "Iteration 1000, Loss: 0.5836\n",
      "Iteration 1100, Loss: 0.5793\n",
      "Iteration 1200, Loss: 0.5755\n",
      "Iteration 1300, Loss: 0.5719\n",
      "Iteration 1400, Loss: 0.5686\n",
      "Iteration 1500, Loss: 0.5656\n",
      "Iteration 1600, Loss: 0.5627\n",
      "Iteration 1700, Loss: 0.5601\n",
      "Iteration 1800, Loss: 0.5576\n",
      "Iteration 1900, Loss: 0.5552\n",
      "Iteration 2000, Loss: 0.5530\n",
      "Iteration 2100, Loss: 0.5509\n",
      "Iteration 2200, Loss: 0.5488\n",
      "Iteration 2300, Loss: 0.5469\n",
      "Iteration 2400, Loss: 0.5451\n",
      "Iteration 2500, Loss: 0.5434\n",
      "Iteration 2600, Loss: 0.5417\n",
      "Iteration 2700, Loss: 0.5401\n",
      "Iteration 2800, Loss: 0.5385\n",
      "Iteration 2900, Loss: 0.5370\n",
      "Iteration 3000, Loss: 0.5356\n",
      "Iteration 3100, Loss: 0.5342\n",
      "Iteration 3200, Loss: 0.5329\n",
      "Iteration 3300, Loss: 0.5316\n",
      "Iteration 3400, Loss: 0.5303\n",
      "Iteration 3500, Loss: 0.5291\n",
      "Iteration 3600, Loss: 0.5279\n",
      "Iteration 3700, Loss: 0.5268\n",
      "Iteration 3800, Loss: 0.5256\n",
      "Iteration 3900, Loss: 0.5246\n",
      "Iteration 4000, Loss: 0.5235\n",
      "Iteration 4100, Loss: 0.5225\n",
      "Iteration 4200, Loss: 0.5215\n",
      "Iteration 4300, Loss: 0.5205\n",
      "Iteration 4400, Loss: 0.5195\n",
      "Iteration 4500, Loss: 0.5186\n",
      "Iteration 4600, Loss: 0.5177\n",
      "Iteration 4700, Loss: 0.5168\n",
      "Iteration 4800, Loss: 0.5159\n",
      "Iteration 4900, Loss: 0.5151\n",
      "Iteration 5000, Loss: 0.5142\n",
      "Iteration 5100, Loss: 0.5134\n",
      "Iteration 5200, Loss: 0.5126\n",
      "Iteration 5300, Loss: 0.5118\n",
      "Iteration 5400, Loss: 0.5111\n",
      "Iteration 5500, Loss: 0.5103\n",
      "Iteration 5600, Loss: 0.5096\n",
      "Iteration 5700, Loss: 0.5088\n",
      "Iteration 5800, Loss: 0.5081\n",
      "Iteration 5900, Loss: 0.5074\n",
      "Iteration 0, Loss: 1.0895\n",
      "Iteration 100, Loss: 0.7879\n",
      "Iteration 200, Loss: 0.7236\n",
      "Iteration 300, Loss: 0.6943\n",
      "Iteration 400, Loss: 0.6766\n",
      "Iteration 500, Loss: 0.6643\n",
      "Iteration 600, Loss: 0.6549\n",
      "Iteration 700, Loss: 0.6474\n",
      "Iteration 800, Loss: 0.6412\n",
      "Iteration 900, Loss: 0.6359\n",
      "Iteration 1000, Loss: 0.6313\n",
      "Iteration 1100, Loss: 0.6272\n",
      "Iteration 1200, Loss: 0.6236\n",
      "Iteration 1300, Loss: 0.6202\n",
      "Iteration 1400, Loss: 0.6171\n",
      "Iteration 1500, Loss: 0.6143\n",
      "Iteration 1600, Loss: 0.6116\n",
      "Iteration 1700, Loss: 0.6092\n",
      "Iteration 1800, Loss: 0.6068\n",
      "Iteration 1900, Loss: 0.6046\n",
      "Iteration 2000, Loss: 0.6025\n",
      "Iteration 2100, Loss: 0.6005\n",
      "Iteration 2200, Loss: 0.5986\n",
      "Iteration 2300, Loss: 0.5968\n",
      "Iteration 2400, Loss: 0.5951\n",
      "Iteration 2500, Loss: 0.5934\n",
      "Iteration 2600, Loss: 0.5918\n",
      "Iteration 2700, Loss: 0.5903\n",
      "Iteration 2800, Loss: 0.5888\n",
      "Iteration 2900, Loss: 0.5874\n",
      "Iteration 3000, Loss: 0.5860\n",
      "Iteration 3100, Loss: 0.5846\n",
      "Iteration 3200, Loss: 0.5834\n",
      "Iteration 3300, Loss: 0.5821\n",
      "Iteration 3400, Loss: 0.5809\n",
      "Iteration 3500, Loss: 0.5797\n",
      "Iteration 3600, Loss: 0.5786\n",
      "Iteration 3700, Loss: 0.5774\n",
      "Iteration 3800, Loss: 0.5763\n",
      "Iteration 3900, Loss: 0.5753\n",
      "Iteration 4000, Loss: 0.5743\n",
      "Iteration 4100, Loss: 0.5733\n",
      "Iteration 4200, Loss: 0.5723\n",
      "Iteration 4300, Loss: 0.5713\n",
      "Iteration 4400, Loss: 0.5704\n",
      "Iteration 4500, Loss: 0.5695\n",
      "Iteration 4600, Loss: 0.5686\n",
      "Iteration 4700, Loss: 0.5677\n",
      "Iteration 4800, Loss: 0.5668\n",
      "Iteration 4900, Loss: 0.5660\n",
      "Iteration 5000, Loss: 0.5652\n",
      "Iteration 5100, Loss: 0.5644\n",
      "Iteration 5200, Loss: 0.5636\n",
      "Iteration 5300, Loss: 0.5628\n",
      "Iteration 5400, Loss: 0.5620\n",
      "Iteration 5500, Loss: 0.5613\n",
      "Iteration 5600, Loss: 0.5606\n",
      "Iteration 5700, Loss: 0.5599\n",
      "Iteration 5800, Loss: 0.5592\n",
      "Iteration 5900, Loss: 0.5585\n",
      "Iteration 0, Loss: 1.0890\n",
      "Iteration 100, Loss: 0.7821\n",
      "Iteration 200, Loss: 0.7154\n",
      "Iteration 300, Loss: 0.6842\n",
      "Iteration 400, Loss: 0.6644\n",
      "Iteration 500, Loss: 0.6501\n",
      "Iteration 600, Loss: 0.6389\n",
      "Iteration 700, Loss: 0.6299\n",
      "Iteration 800, Loss: 0.6224\n",
      "Iteration 900, Loss: 0.6159\n",
      "Iteration 1000, Loss: 0.6103\n",
      "Iteration 1100, Loss: 0.6053\n",
      "Iteration 1200, Loss: 0.6008\n",
      "Iteration 1300, Loss: 0.5968\n",
      "Iteration 1400, Loss: 0.5932\n",
      "Iteration 1500, Loss: 0.5898\n",
      "Iteration 1600, Loss: 0.5867\n",
      "Iteration 1700, Loss: 0.5838\n",
      "Iteration 1800, Loss: 0.5811\n",
      "Iteration 1900, Loss: 0.5786\n",
      "Iteration 2000, Loss: 0.5762\n",
      "Iteration 2100, Loss: 0.5739\n",
      "Iteration 2200, Loss: 0.5718\n",
      "Iteration 2300, Loss: 0.5698\n",
      "Iteration 2400, Loss: 0.5679\n",
      "Iteration 2500, Loss: 0.5661\n",
      "Iteration 2600, Loss: 0.5643\n",
      "Iteration 2700, Loss: 0.5627\n",
      "Iteration 2800, Loss: 0.5611\n",
      "Iteration 2900, Loss: 0.5595\n",
      "Iteration 3000, Loss: 0.5581\n",
      "Iteration 3100, Loss: 0.5567\n",
      "Iteration 3200, Loss: 0.5553\n",
      "Iteration 3300, Loss: 0.5540\n",
      "Iteration 3400, Loss: 0.5527\n",
      "Iteration 3500, Loss: 0.5515\n",
      "Iteration 3600, Loss: 0.5503\n",
      "Iteration 3700, Loss: 0.5492\n",
      "Iteration 3800, Loss: 0.5481\n",
      "Iteration 3900, Loss: 0.5470\n",
      "Iteration 4000, Loss: 0.5460\n",
      "Iteration 4100, Loss: 0.5450\n",
      "Iteration 4200, Loss: 0.5440\n",
      "Iteration 4300, Loss: 0.5431\n",
      "Iteration 4400, Loss: 0.5421\n",
      "Iteration 4500, Loss: 0.5413\n",
      "Iteration 4600, Loss: 0.5404\n",
      "Iteration 4700, Loss: 0.5395\n",
      "Iteration 4800, Loss: 0.5387\n",
      "Iteration 4900, Loss: 0.5379\n",
      "Iteration 5000, Loss: 0.5371\n",
      "Iteration 5100, Loss: 0.5363\n",
      "Iteration 5200, Loss: 0.5356\n",
      "Iteration 5300, Loss: 0.5348\n",
      "Iteration 5400, Loss: 0.5341\n",
      "Iteration 5500, Loss: 0.5334\n",
      "Iteration 5600, Loss: 0.5327\n",
      "Iteration 5700, Loss: 0.5320\n",
      "Iteration 5800, Loss: 0.5314\n",
      "Iteration 5900, Loss: 0.5307\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8584\n",
      "Iteration 200, Loss: 0.7787\n",
      "Iteration 300, Loss: 0.7388\n",
      "Iteration 400, Loss: 0.7151\n",
      "Iteration 500, Loss: 0.6991\n",
      "Iteration 600, Loss: 0.6873\n",
      "Iteration 700, Loss: 0.6780\n",
      "Iteration 800, Loss: 0.6702\n",
      "Iteration 900, Loss: 0.6636\n",
      "Iteration 1000, Loss: 0.6580\n",
      "Iteration 1100, Loss: 0.6530\n",
      "Iteration 1200, Loss: 0.6485\n",
      "Iteration 1300, Loss: 0.6445\n",
      "Iteration 1400, Loss: 0.6408\n",
      "Iteration 1500, Loss: 0.6375\n",
      "Iteration 1600, Loss: 0.6343\n",
      "Iteration 1700, Loss: 0.6314\n",
      "Iteration 1800, Loss: 0.6287\n",
      "Iteration 1900, Loss: 0.6262\n",
      "Iteration 2000, Loss: 0.6238\n",
      "Iteration 2100, Loss: 0.6216\n",
      "Iteration 2200, Loss: 0.6194\n",
      "Iteration 2300, Loss: 0.6174\n",
      "Iteration 2400, Loss: 0.6155\n",
      "Iteration 2500, Loss: 0.6136\n",
      "Iteration 2600, Loss: 0.6118\n",
      "Iteration 2700, Loss: 0.6101\n",
      "Iteration 2800, Loss: 0.6085\n",
      "Iteration 2900, Loss: 0.6070\n",
      "Iteration 3000, Loss: 0.6054\n",
      "Iteration 3100, Loss: 0.6040\n",
      "Iteration 3200, Loss: 0.6026\n",
      "Iteration 3300, Loss: 0.6012\n",
      "Iteration 3400, Loss: 0.5999\n",
      "Iteration 3500, Loss: 0.5986\n",
      "Iteration 3600, Loss: 0.5974\n",
      "Iteration 3700, Loss: 0.5962\n",
      "Iteration 3800, Loss: 0.5950\n",
      "Iteration 3900, Loss: 0.5938\n",
      "Iteration 4000, Loss: 0.5927\n",
      "Iteration 4100, Loss: 0.5917\n",
      "Iteration 4200, Loss: 0.5906\n",
      "Iteration 4300, Loss: 0.5896\n",
      "Iteration 4400, Loss: 0.5886\n",
      "Iteration 4500, Loss: 0.5876\n",
      "Iteration 4600, Loss: 0.5866\n",
      "Iteration 4700, Loss: 0.5857\n",
      "Iteration 4800, Loss: 0.5848\n",
      "Iteration 4900, Loss: 0.5839\n",
      "Iteration 5000, Loss: 0.5830\n",
      "Iteration 5100, Loss: 0.5822\n",
      "Iteration 5200, Loss: 0.5813\n",
      "Iteration 5300, Loss: 0.5805\n",
      "Iteration 5400, Loss: 0.5797\n",
      "Iteration 5500, Loss: 0.5789\n",
      "Iteration 5600, Loss: 0.5782\n",
      "Iteration 5700, Loss: 0.5774\n",
      "Iteration 5800, Loss: 0.5766\n",
      "Iteration 5900, Loss: 0.5759\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8643\n",
      "Iteration 200, Loss: 0.7867\n",
      "Iteration 300, Loss: 0.7461\n",
      "Iteration 400, Loss: 0.7208\n",
      "Iteration 500, Loss: 0.7031\n",
      "Iteration 600, Loss: 0.6896\n",
      "Iteration 700, Loss: 0.6788\n",
      "Iteration 800, Loss: 0.6699\n",
      "Iteration 900, Loss: 0.6623\n",
      "Iteration 1000, Loss: 0.6558\n",
      "Iteration 1100, Loss: 0.6501\n",
      "Iteration 1200, Loss: 0.6450\n",
      "Iteration 1300, Loss: 0.6404\n",
      "Iteration 1400, Loss: 0.6363\n",
      "Iteration 1500, Loss: 0.6325\n",
      "Iteration 1600, Loss: 0.6291\n",
      "Iteration 1700, Loss: 0.6260\n",
      "Iteration 1800, Loss: 0.6230\n",
      "Iteration 1900, Loss: 0.6203\n",
      "Iteration 2000, Loss: 0.6177\n",
      "Iteration 2100, Loss: 0.6153\n",
      "Iteration 2200, Loss: 0.6130\n",
      "Iteration 2300, Loss: 0.6108\n",
      "Iteration 2400, Loss: 0.6088\n",
      "Iteration 2500, Loss: 0.6068\n",
      "Iteration 2600, Loss: 0.6050\n",
      "Iteration 2700, Loss: 0.6032\n",
      "Iteration 2800, Loss: 0.6015\n",
      "Iteration 2900, Loss: 0.5999\n",
      "Iteration 3000, Loss: 0.5983\n",
      "Iteration 3100, Loss: 0.5968\n",
      "Iteration 3200, Loss: 0.5953\n",
      "Iteration 3300, Loss: 0.5939\n",
      "Iteration 3400, Loss: 0.5926\n",
      "Iteration 3500, Loss: 0.5913\n",
      "Iteration 3600, Loss: 0.5900\n",
      "Iteration 3700, Loss: 0.5887\n",
      "Iteration 3800, Loss: 0.5875\n",
      "Iteration 3900, Loss: 0.5864\n",
      "Iteration 4000, Loss: 0.5852\n",
      "Iteration 4100, Loss: 0.5841\n",
      "Iteration 4200, Loss: 0.5830\n",
      "Iteration 4300, Loss: 0.5820\n",
      "Iteration 4400, Loss: 0.5810\n",
      "Iteration 4500, Loss: 0.5800\n",
      "Iteration 4600, Loss: 0.5790\n",
      "Iteration 4700, Loss: 0.5780\n",
      "Iteration 4800, Loss: 0.5771\n",
      "Iteration 4900, Loss: 0.5762\n",
      "Iteration 5000, Loss: 0.5753\n",
      "Iteration 5100, Loss: 0.5744\n",
      "Iteration 5200, Loss: 0.5736\n",
      "Iteration 5300, Loss: 0.5727\n",
      "Iteration 5400, Loss: 0.5719\n",
      "Iteration 5500, Loss: 0.5711\n",
      "Iteration 5600, Loss: 0.5703\n",
      "Iteration 5700, Loss: 0.5696\n",
      "Iteration 5800, Loss: 0.5688\n",
      "Iteration 5900, Loss: 0.5681\n",
      "Iteration 0, Loss: 1.0967\n",
      "Iteration 100, Loss: 0.9641\n",
      "Iteration 200, Loss: 0.8922\n",
      "Iteration 300, Loss: 0.8456\n",
      "Iteration 400, Loss: 0.8127\n",
      "Iteration 500, Loss: 0.7881\n",
      "Iteration 600, Loss: 0.7690\n",
      "Iteration 700, Loss: 0.7537\n",
      "Iteration 800, Loss: 0.7411\n",
      "Iteration 900, Loss: 0.7305\n",
      "Iteration 1000, Loss: 0.7214\n",
      "Iteration 1100, Loss: 0.7136\n",
      "Iteration 1200, Loss: 0.7066\n",
      "Iteration 1300, Loss: 0.7005\n",
      "Iteration 1400, Loss: 0.6949\n",
      "Iteration 1500, Loss: 0.6899\n",
      "Iteration 1600, Loss: 0.6853\n",
      "Iteration 1700, Loss: 0.6811\n",
      "Iteration 1800, Loss: 0.6772\n",
      "Iteration 1900, Loss: 0.6736\n",
      "Iteration 2000, Loss: 0.6702\n",
      "Iteration 2100, Loss: 0.6671\n",
      "Iteration 2200, Loss: 0.6641\n",
      "Iteration 2300, Loss: 0.6614\n",
      "Iteration 2400, Loss: 0.6587\n",
      "Iteration 2500, Loss: 0.6563\n",
      "Iteration 2600, Loss: 0.6539\n",
      "Iteration 2700, Loss: 0.6517\n",
      "Iteration 2800, Loss: 0.6495\n",
      "Iteration 2900, Loss: 0.6475\n",
      "Iteration 3000, Loss: 0.6456\n",
      "Iteration 3100, Loss: 0.6437\n",
      "Iteration 3200, Loss: 0.6419\n",
      "Iteration 3300, Loss: 0.6402\n",
      "Iteration 3400, Loss: 0.6386\n",
      "Iteration 3500, Loss: 0.6370\n",
      "Iteration 3600, Loss: 0.6355\n",
      "Iteration 3700, Loss: 0.6340\n",
      "Iteration 3800, Loss: 0.6326\n",
      "Iteration 3900, Loss: 0.6312\n",
      "Iteration 4000, Loss: 0.6299\n",
      "Iteration 4100, Loss: 0.6286\n",
      "Iteration 4200, Loss: 0.6274\n",
      "Iteration 4300, Loss: 0.6262\n",
      "Iteration 4400, Loss: 0.6250\n",
      "Iteration 4500, Loss: 0.6239\n",
      "Iteration 4600, Loss: 0.6228\n",
      "Iteration 4700, Loss: 0.6217\n",
      "Iteration 4800, Loss: 0.6207\n",
      "Iteration 4900, Loss: 0.6197\n",
      "Iteration 5000, Loss: 0.6187\n",
      "Iteration 5100, Loss: 0.6177\n",
      "Iteration 5200, Loss: 0.6168\n",
      "Iteration 5300, Loss: 0.6159\n",
      "Iteration 5400, Loss: 0.6150\n",
      "Iteration 5500, Loss: 0.6141\n",
      "Iteration 5600, Loss: 0.6132\n",
      "Iteration 5700, Loss: 0.6124\n",
      "Iteration 5800, Loss: 0.6116\n",
      "Iteration 5900, Loss: 0.6108\n",
      "Iteration 0, Loss: 1.0968\n",
      "Iteration 100, Loss: 0.9692\n",
      "Iteration 200, Loss: 0.8964\n",
      "Iteration 300, Loss: 0.8486\n",
      "Iteration 400, Loss: 0.8147\n",
      "Iteration 500, Loss: 0.7897\n",
      "Iteration 600, Loss: 0.7704\n",
      "Iteration 700, Loss: 0.7552\n",
      "Iteration 800, Loss: 0.7429\n",
      "Iteration 900, Loss: 0.7328\n",
      "Iteration 1000, Loss: 0.7243\n",
      "Iteration 1100, Loss: 0.7169\n",
      "Iteration 1200, Loss: 0.7106\n",
      "Iteration 1300, Loss: 0.7050\n",
      "Iteration 1400, Loss: 0.7000\n",
      "Iteration 1500, Loss: 0.6955\n",
      "Iteration 1600, Loss: 0.6914\n",
      "Iteration 1700, Loss: 0.6876\n",
      "Iteration 1800, Loss: 0.6842\n",
      "Iteration 1900, Loss: 0.6810\n",
      "Iteration 2000, Loss: 0.6780\n",
      "Iteration 2100, Loss: 0.6753\n",
      "Iteration 2200, Loss: 0.6726\n",
      "Iteration 2300, Loss: 0.6702\n",
      "Iteration 2400, Loss: 0.6679\n",
      "Iteration 2500, Loss: 0.6656\n",
      "Iteration 2600, Loss: 0.6635\n",
      "Iteration 2700, Loss: 0.6615\n",
      "Iteration 2800, Loss: 0.6596\n",
      "Iteration 2900, Loss: 0.6578\n",
      "Iteration 3000, Loss: 0.6561\n",
      "Iteration 3100, Loss: 0.6544\n",
      "Iteration 3200, Loss: 0.6528\n",
      "Iteration 3300, Loss: 0.6512\n",
      "Iteration 3400, Loss: 0.6497\n",
      "Iteration 3500, Loss: 0.6483\n",
      "Iteration 3600, Loss: 0.6469\n",
      "Iteration 3700, Loss: 0.6455\n",
      "Iteration 3800, Loss: 0.6442\n",
      "Iteration 3900, Loss: 0.6429\n",
      "Iteration 4000, Loss: 0.6417\n",
      "Iteration 4100, Loss: 0.6405\n",
      "Iteration 4200, Loss: 0.6393\n",
      "Iteration 4300, Loss: 0.6382\n",
      "Iteration 4400, Loss: 0.6371\n",
      "Iteration 4500, Loss: 0.6360\n",
      "Iteration 4600, Loss: 0.6350\n",
      "Iteration 4700, Loss: 0.6339\n",
      "Iteration 4800, Loss: 0.6329\n",
      "Iteration 4900, Loss: 0.6320\n",
      "Iteration 5000, Loss: 0.6310\n",
      "Iteration 5100, Loss: 0.6301\n",
      "Iteration 5200, Loss: 0.6292\n",
      "Iteration 5300, Loss: 0.6283\n",
      "Iteration 5400, Loss: 0.6274\n",
      "Iteration 5500, Loss: 0.6265\n",
      "Iteration 5600, Loss: 0.6257\n",
      "Iteration 5700, Loss: 0.6248\n",
      "Iteration 5800, Loss: 0.6240\n",
      "Iteration 5900, Loss: 0.6232\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0237\n",
      "Iteration 200, Loss: 0.9714\n",
      "Iteration 300, Loss: 0.9317\n",
      "Iteration 400, Loss: 0.9002\n",
      "Iteration 500, Loss: 0.8746\n",
      "Iteration 600, Loss: 0.8533\n",
      "Iteration 700, Loss: 0.8353\n",
      "Iteration 800, Loss: 0.8199\n",
      "Iteration 900, Loss: 0.8066\n",
      "Iteration 1000, Loss: 0.7950\n",
      "Iteration 1100, Loss: 0.7848\n",
      "Iteration 1200, Loss: 0.7758\n",
      "Iteration 1300, Loss: 0.7677\n",
      "Iteration 1400, Loss: 0.7605\n",
      "Iteration 1500, Loss: 0.7540\n",
      "Iteration 1600, Loss: 0.7480\n",
      "Iteration 1700, Loss: 0.7426\n",
      "Iteration 1800, Loss: 0.7376\n",
      "Iteration 1900, Loss: 0.7330\n",
      "Iteration 2000, Loss: 0.7287\n",
      "Iteration 2100, Loss: 0.7248\n",
      "Iteration 2200, Loss: 0.7211\n",
      "Iteration 2300, Loss: 0.7177\n",
      "Iteration 2400, Loss: 0.7144\n",
      "Iteration 2500, Loss: 0.7114\n",
      "Iteration 2600, Loss: 0.7085\n",
      "Iteration 2700, Loss: 0.7057\n",
      "Iteration 2800, Loss: 0.7031\n",
      "Iteration 2900, Loss: 0.7007\n",
      "Iteration 3000, Loss: 0.6983\n",
      "Iteration 3100, Loss: 0.6961\n",
      "Iteration 3200, Loss: 0.6939\n",
      "Iteration 3300, Loss: 0.6919\n",
      "Iteration 3400, Loss: 0.6899\n",
      "Iteration 3500, Loss: 0.6880\n",
      "Iteration 3600, Loss: 0.6861\n",
      "Iteration 3700, Loss: 0.6844\n",
      "Iteration 3800, Loss: 0.6827\n",
      "Iteration 3900, Loss: 0.6810\n",
      "Iteration 4000, Loss: 0.6794\n",
      "Iteration 4100, Loss: 0.6779\n",
      "Iteration 4200, Loss: 0.6764\n",
      "Iteration 4300, Loss: 0.6749\n",
      "Iteration 4400, Loss: 0.6735\n",
      "Iteration 4500, Loss: 0.6721\n",
      "Iteration 4600, Loss: 0.6708\n",
      "Iteration 4700, Loss: 0.6695\n",
      "Iteration 4800, Loss: 0.6682\n",
      "Iteration 4900, Loss: 0.6670\n",
      "Iteration 5000, Loss: 0.6658\n",
      "Iteration 5100, Loss: 0.6646\n",
      "Iteration 5200, Loss: 0.6635\n",
      "Iteration 5300, Loss: 0.6624\n",
      "Iteration 5400, Loss: 0.6613\n",
      "Iteration 5500, Loss: 0.6602\n",
      "Iteration 5600, Loss: 0.6592\n",
      "Iteration 5700, Loss: 0.6582\n",
      "Iteration 5800, Loss: 0.6572\n",
      "Iteration 5900, Loss: 0.6562\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0154\n",
      "Iteration 200, Loss: 0.9588\n",
      "Iteration 300, Loss: 0.9168\n",
      "Iteration 400, Loss: 0.8842\n",
      "Iteration 500, Loss: 0.8580\n",
      "Iteration 600, Loss: 0.8363\n",
      "Iteration 700, Loss: 0.8183\n",
      "Iteration 800, Loss: 0.8029\n",
      "Iteration 900, Loss: 0.7897\n",
      "Iteration 1000, Loss: 0.7781\n",
      "Iteration 1100, Loss: 0.7680\n",
      "Iteration 1200, Loss: 0.7590\n",
      "Iteration 1300, Loss: 0.7510\n",
      "Iteration 1400, Loss: 0.7438\n",
      "Iteration 1500, Loss: 0.7373\n",
      "Iteration 1600, Loss: 0.7313\n",
      "Iteration 1700, Loss: 0.7259\n",
      "Iteration 1800, Loss: 0.7209\n",
      "Iteration 1900, Loss: 0.7164\n",
      "Iteration 2000, Loss: 0.7121\n",
      "Iteration 2100, Loss: 0.7082\n",
      "Iteration 2200, Loss: 0.7046\n",
      "Iteration 2300, Loss: 0.7011\n",
      "Iteration 2400, Loss: 0.6979\n",
      "Iteration 2500, Loss: 0.6949\n",
      "Iteration 2600, Loss: 0.6921\n",
      "Iteration 2700, Loss: 0.6894\n",
      "Iteration 2800, Loss: 0.6869\n",
      "Iteration 2900, Loss: 0.6845\n",
      "Iteration 3000, Loss: 0.6822\n",
      "Iteration 3100, Loss: 0.6800\n",
      "Iteration 3200, Loss: 0.6779\n",
      "Iteration 3300, Loss: 0.6759\n",
      "Iteration 3400, Loss: 0.6740\n",
      "Iteration 3500, Loss: 0.6722\n",
      "Iteration 3600, Loss: 0.6704\n",
      "Iteration 3700, Loss: 0.6688\n",
      "Iteration 3800, Loss: 0.6671\n",
      "Iteration 3900, Loss: 0.6656\n",
      "Iteration 4000, Loss: 0.6641\n",
      "Iteration 4100, Loss: 0.6626\n",
      "Iteration 4200, Loss: 0.6612\n",
      "Iteration 4300, Loss: 0.6599\n",
      "Iteration 4400, Loss: 0.6585\n",
      "Iteration 4500, Loss: 0.6573\n",
      "Iteration 4600, Loss: 0.6560\n",
      "Iteration 4700, Loss: 0.6548\n",
      "Iteration 4800, Loss: 0.6537\n",
      "Iteration 4900, Loss: 0.6526\n",
      "Iteration 5000, Loss: 0.6515\n",
      "Iteration 5100, Loss: 0.6504\n",
      "Iteration 5200, Loss: 0.6494\n",
      "Iteration 5300, Loss: 0.6484\n",
      "Iteration 5400, Loss: 0.6474\n",
      "Iteration 5500, Loss: 0.6464\n",
      "Iteration 5600, Loss: 0.6455\n",
      "Iteration 5700, Loss: 0.6446\n",
      "Iteration 5800, Loss: 0.6437\n",
      "Iteration 5900, Loss: 0.6428\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0525\n",
      "Iteration 200, Loss: 1.0156\n",
      "Iteration 300, Loss: 0.9850\n",
      "Iteration 400, Loss: 0.9590\n",
      "Iteration 500, Loss: 0.9365\n",
      "Iteration 600, Loss: 0.9169\n",
      "Iteration 700, Loss: 0.8995\n",
      "Iteration 800, Loss: 0.8841\n",
      "Iteration 900, Loss: 0.8702\n",
      "Iteration 1000, Loss: 0.8578\n",
      "Iteration 1100, Loss: 0.8465\n",
      "Iteration 1200, Loss: 0.8362\n",
      "Iteration 1300, Loss: 0.8268\n",
      "Iteration 1400, Loss: 0.8182\n",
      "Iteration 1500, Loss: 0.8103\n",
      "Iteration 1600, Loss: 0.8030\n",
      "Iteration 1700, Loss: 0.7962\n",
      "Iteration 1800, Loss: 0.7899\n",
      "Iteration 1900, Loss: 0.7841\n",
      "Iteration 2000, Loss: 0.7786\n",
      "Iteration 2100, Loss: 0.7735\n",
      "Iteration 2200, Loss: 0.7687\n",
      "Iteration 2300, Loss: 0.7641\n",
      "Iteration 2400, Loss: 0.7599\n",
      "Iteration 2500, Loss: 0.7559\n",
      "Iteration 2600, Loss: 0.7521\n",
      "Iteration 2700, Loss: 0.7486\n",
      "Iteration 2800, Loss: 0.7452\n",
      "Iteration 2900, Loss: 0.7420\n",
      "Iteration 3000, Loss: 0.7389\n",
      "Iteration 3100, Loss: 0.7360\n",
      "Iteration 3200, Loss: 0.7333\n",
      "Iteration 3300, Loss: 0.7306\n",
      "Iteration 3400, Loss: 0.7281\n",
      "Iteration 3500, Loss: 0.7257\n",
      "Iteration 3600, Loss: 0.7234\n",
      "Iteration 3700, Loss: 0.7211\n",
      "Iteration 3800, Loss: 0.7190\n",
      "Iteration 3900, Loss: 0.7169\n",
      "Iteration 4000, Loss: 0.7150\n",
      "Iteration 4100, Loss: 0.7131\n",
      "Iteration 4200, Loss: 0.7112\n",
      "Iteration 4300, Loss: 0.7095\n",
      "Iteration 4400, Loss: 0.7078\n",
      "Iteration 4500, Loss: 0.7061\n",
      "Iteration 4600, Loss: 0.7045\n",
      "Iteration 4700, Loss: 0.7030\n",
      "Iteration 4800, Loss: 0.7015\n",
      "Iteration 4900, Loss: 0.7000\n",
      "Iteration 5000, Loss: 0.6986\n",
      "Iteration 5100, Loss: 0.6972\n",
      "Iteration 5200, Loss: 0.6959\n",
      "Iteration 5300, Loss: 0.6946\n",
      "Iteration 5400, Loss: 0.6934\n",
      "Iteration 5500, Loss: 0.6922\n",
      "Iteration 5600, Loss: 0.6910\n",
      "Iteration 5700, Loss: 0.6898\n",
      "Iteration 5800, Loss: 0.6887\n",
      "Iteration 5900, Loss: 0.6876\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0551\n",
      "Iteration 200, Loss: 1.0200\n",
      "Iteration 300, Loss: 0.9907\n",
      "Iteration 400, Loss: 0.9655\n",
      "Iteration 500, Loss: 0.9438\n",
      "Iteration 600, Loss: 0.9247\n",
      "Iteration 700, Loss: 0.9077\n",
      "Iteration 800, Loss: 0.8927\n",
      "Iteration 900, Loss: 0.8790\n",
      "Iteration 1000, Loss: 0.8667\n",
      "Iteration 1100, Loss: 0.8554\n",
      "Iteration 1200, Loss: 0.8452\n",
      "Iteration 1300, Loss: 0.8358\n",
      "Iteration 1400, Loss: 0.8271\n",
      "Iteration 1500, Loss: 0.8191\n",
      "Iteration 1600, Loss: 0.8116\n",
      "Iteration 1700, Loss: 0.8047\n",
      "Iteration 1800, Loss: 0.7983\n",
      "Iteration 1900, Loss: 0.7923\n",
      "Iteration 2000, Loss: 0.7866\n",
      "Iteration 2100, Loss: 0.7814\n",
      "Iteration 2200, Loss: 0.7764\n",
      "Iteration 2300, Loss: 0.7717\n",
      "Iteration 2400, Loss: 0.7672\n",
      "Iteration 2500, Loss: 0.7630\n",
      "Iteration 2600, Loss: 0.7591\n",
      "Iteration 2700, Loss: 0.7553\n",
      "Iteration 2800, Loss: 0.7518\n",
      "Iteration 2900, Loss: 0.7483\n",
      "Iteration 3000, Loss: 0.7451\n",
      "Iteration 3100, Loss: 0.7421\n",
      "Iteration 3200, Loss: 0.7391\n",
      "Iteration 3300, Loss: 0.7363\n",
      "Iteration 3400, Loss: 0.7336\n",
      "Iteration 3500, Loss: 0.7310\n",
      "Iteration 3600, Loss: 0.7285\n",
      "Iteration 3700, Loss: 0.7261\n",
      "Iteration 3800, Loss: 0.7238\n",
      "Iteration 3900, Loss: 0.7217\n",
      "Iteration 4000, Loss: 0.7195\n",
      "Iteration 4100, Loss: 0.7175\n",
      "Iteration 4200, Loss: 0.7155\n",
      "Iteration 4300, Loss: 0.7136\n",
      "Iteration 4400, Loss: 0.7117\n",
      "Iteration 4500, Loss: 0.7099\n",
      "Iteration 4600, Loss: 0.7082\n",
      "Iteration 4700, Loss: 0.7065\n",
      "Iteration 4800, Loss: 0.7049\n",
      "Iteration 4900, Loss: 0.7033\n",
      "Iteration 5000, Loss: 0.7017\n",
      "Iteration 5100, Loss: 0.7002\n",
      "Iteration 5200, Loss: 0.6988\n",
      "Iteration 5300, Loss: 0.6974\n",
      "Iteration 5400, Loss: 0.6960\n",
      "Iteration 5500, Loss: 0.6946\n",
      "Iteration 5600, Loss: 0.6933\n",
      "Iteration 5700, Loss: 0.6920\n",
      "Iteration 5800, Loss: 0.6908\n",
      "Iteration 5900, Loss: 0.6896\n",
      "Iteration 0, Loss: 0.9665\n",
      "Iteration 100, Loss: 0.6034\n",
      "Iteration 200, Loss: 0.5750\n",
      "Iteration 300, Loss: 0.5590\n",
      "Iteration 400, Loss: 0.5480\n",
      "Iteration 500, Loss: 0.5398\n",
      "Iteration 600, Loss: 0.5332\n",
      "Iteration 700, Loss: 0.5278\n",
      "Iteration 800, Loss: 0.5231\n",
      "Iteration 900, Loss: 0.5191\n",
      "Iteration 1000, Loss: 0.5156\n",
      "Iteration 1100, Loss: 0.5124\n",
      "Iteration 1200, Loss: 0.5095\n",
      "Iteration 1300, Loss: 0.5068\n",
      "Iteration 1400, Loss: 0.5044\n",
      "Iteration 1500, Loss: 0.5022\n",
      "Iteration 1600, Loss: 0.5001\n",
      "Iteration 1700, Loss: 0.4981\n",
      "Iteration 1800, Loss: 0.4963\n",
      "Iteration 1900, Loss: 0.4945\n",
      "Iteration 2000, Loss: 0.4929\n",
      "Iteration 2100, Loss: 0.4913\n",
      "Iteration 2200, Loss: 0.4898\n",
      "Iteration 2300, Loss: 0.4884\n",
      "Iteration 2400, Loss: 0.4871\n",
      "Iteration 2500, Loss: 0.4858\n",
      "Iteration 2600, Loss: 0.4845\n",
      "Iteration 2700, Loss: 0.4833\n",
      "Iteration 2800, Loss: 0.4822\n",
      "Iteration 2900, Loss: 0.4811\n",
      "Iteration 3000, Loss: 0.4800\n",
      "Iteration 3100, Loss: 0.4790\n",
      "Iteration 3200, Loss: 0.4780\n",
      "Iteration 3300, Loss: 0.4770\n",
      "Iteration 3400, Loss: 0.4761\n",
      "Iteration 3500, Loss: 0.4752\n",
      "Iteration 3600, Loss: 0.4743\n",
      "Iteration 3700, Loss: 0.4734\n",
      "Iteration 3800, Loss: 0.4726\n",
      "Iteration 3900, Loss: 0.4718\n",
      "Iteration 4000, Loss: 0.4710\n",
      "Iteration 4100, Loss: 0.4702\n",
      "Iteration 4200, Loss: 0.4694\n",
      "Iteration 4300, Loss: 0.4687\n",
      "Iteration 4400, Loss: 0.4680\n",
      "Iteration 4500, Loss: 0.4673\n",
      "Iteration 4600, Loss: 0.4666\n",
      "Iteration 4700, Loss: 0.4659\n",
      "Iteration 4800, Loss: 0.4653\n",
      "Iteration 4900, Loss: 0.4646\n",
      "Iteration 5000, Loss: 0.4640\n",
      "Iteration 5100, Loss: 0.4634\n",
      "Iteration 5200, Loss: 0.4628\n",
      "Iteration 5300, Loss: 0.4622\n",
      "Iteration 5400, Loss: 0.4616\n",
      "Iteration 5500, Loss: 0.4611\n",
      "Iteration 5600, Loss: 0.4605\n",
      "Iteration 5700, Loss: 0.4600\n",
      "Iteration 5800, Loss: 0.4594\n",
      "Iteration 5900, Loss: 0.4589\n",
      "Iteration 6000, Loss: 0.4584\n",
      "Iteration 6100, Loss: 0.4579\n",
      "Iteration 6200, Loss: 0.4574\n",
      "Iteration 6300, Loss: 0.4569\n",
      "Iteration 6400, Loss: 0.4564\n",
      "Iteration 6500, Loss: 0.4559\n",
      "Iteration 6600, Loss: 0.4555\n",
      "Iteration 6700, Loss: 0.4550\n",
      "Iteration 6800, Loss: 0.4545\n",
      "Iteration 6900, Loss: 0.4541\n",
      "Iteration 0, Loss: 0.9672\n",
      "Iteration 100, Loss: 0.5771\n",
      "Iteration 200, Loss: 0.5449\n",
      "Iteration 300, Loss: 0.5270\n",
      "Iteration 400, Loss: 0.5147\n",
      "Iteration 500, Loss: 0.5053\n",
      "Iteration 600, Loss: 0.4977\n",
      "Iteration 700, Loss: 0.4914\n",
      "Iteration 800, Loss: 0.4859\n",
      "Iteration 900, Loss: 0.4811\n",
      "Iteration 1000, Loss: 0.4769\n",
      "Iteration 1100, Loss: 0.4731\n",
      "Iteration 1200, Loss: 0.4696\n",
      "Iteration 1300, Loss: 0.4665\n",
      "Iteration 1400, Loss: 0.4636\n",
      "Iteration 1500, Loss: 0.4610\n",
      "Iteration 1600, Loss: 0.4585\n",
      "Iteration 1700, Loss: 0.4562\n",
      "Iteration 1800, Loss: 0.4541\n",
      "Iteration 1900, Loss: 0.4521\n",
      "Iteration 2000, Loss: 0.4502\n",
      "Iteration 2100, Loss: 0.4484\n",
      "Iteration 2200, Loss: 0.4467\n",
      "Iteration 2300, Loss: 0.4451\n",
      "Iteration 2400, Loss: 0.4436\n",
      "Iteration 2500, Loss: 0.4422\n",
      "Iteration 2600, Loss: 0.4408\n",
      "Iteration 2700, Loss: 0.4395\n",
      "Iteration 2800, Loss: 0.4382\n",
      "Iteration 2900, Loss: 0.4370\n",
      "Iteration 3000, Loss: 0.4359\n",
      "Iteration 3100, Loss: 0.4348\n",
      "Iteration 3200, Loss: 0.4337\n",
      "Iteration 3300, Loss: 0.4327\n",
      "Iteration 3400, Loss: 0.4317\n",
      "Iteration 3500, Loss: 0.4308\n",
      "Iteration 3600, Loss: 0.4299\n",
      "Iteration 3700, Loss: 0.4290\n",
      "Iteration 3800, Loss: 0.4281\n",
      "Iteration 3900, Loss: 0.4273\n",
      "Iteration 4000, Loss: 0.4265\n",
      "Iteration 4100, Loss: 0.4257\n",
      "Iteration 4200, Loss: 0.4249\n",
      "Iteration 4300, Loss: 0.4242\n",
      "Iteration 4400, Loss: 0.4235\n",
      "Iteration 4500, Loss: 0.4228\n",
      "Iteration 4600, Loss: 0.4221\n",
      "Iteration 4700, Loss: 0.4215\n",
      "Iteration 4800, Loss: 0.4208\n",
      "Iteration 4900, Loss: 0.4202\n",
      "Iteration 5000, Loss: 0.4196\n",
      "Iteration 5100, Loss: 0.4190\n",
      "Iteration 5200, Loss: 0.4184\n",
      "Iteration 5300, Loss: 0.4178\n",
      "Iteration 5400, Loss: 0.4173\n",
      "Iteration 5500, Loss: 0.4167\n",
      "Iteration 5600, Loss: 0.4162\n",
      "Iteration 5700, Loss: 0.4156\n",
      "Iteration 5800, Loss: 0.4151\n",
      "Iteration 5900, Loss: 0.4146\n",
      "Iteration 6000, Loss: 0.4141\n",
      "Iteration 6100, Loss: 0.4136\n",
      "Iteration 6200, Loss: 0.4132\n",
      "Iteration 6300, Loss: 0.4127\n",
      "Iteration 6400, Loss: 0.4122\n",
      "Iteration 6500, Loss: 0.4118\n",
      "Iteration 6600, Loss: 0.4113\n",
      "Iteration 6700, Loss: 0.4109\n",
      "Iteration 6800, Loss: 0.4105\n",
      "Iteration 6900, Loss: 0.4101\n",
      "Iteration 0, Loss: 1.0280\n",
      "Iteration 100, Loss: 0.6426\n",
      "Iteration 200, Loss: 0.6092\n",
      "Iteration 300, Loss: 0.5902\n",
      "Iteration 400, Loss: 0.5769\n",
      "Iteration 500, Loss: 0.5669\n",
      "Iteration 600, Loss: 0.5590\n",
      "Iteration 700, Loss: 0.5524\n",
      "Iteration 800, Loss: 0.5469\n",
      "Iteration 900, Loss: 0.5421\n",
      "Iteration 1000, Loss: 0.5379\n",
      "Iteration 1100, Loss: 0.5342\n",
      "Iteration 1200, Loss: 0.5308\n",
      "Iteration 1300, Loss: 0.5277\n",
      "Iteration 1400, Loss: 0.5249\n",
      "Iteration 1500, Loss: 0.5223\n",
      "Iteration 1600, Loss: 0.5199\n",
      "Iteration 1700, Loss: 0.5177\n",
      "Iteration 1800, Loss: 0.5156\n",
      "Iteration 1900, Loss: 0.5137\n",
      "Iteration 2000, Loss: 0.5119\n",
      "Iteration 2100, Loss: 0.5101\n",
      "Iteration 2200, Loss: 0.5085\n",
      "Iteration 2300, Loss: 0.5069\n",
      "Iteration 2400, Loss: 0.5054\n",
      "Iteration 2500, Loss: 0.5040\n",
      "Iteration 2600, Loss: 0.5026\n",
      "Iteration 2700, Loss: 0.5013\n",
      "Iteration 2800, Loss: 0.5000\n",
      "Iteration 2900, Loss: 0.4988\n",
      "Iteration 3000, Loss: 0.4976\n",
      "Iteration 3100, Loss: 0.4965\n",
      "Iteration 3200, Loss: 0.4954\n",
      "Iteration 3300, Loss: 0.4944\n",
      "Iteration 3400, Loss: 0.4934\n",
      "Iteration 3500, Loss: 0.4924\n",
      "Iteration 3600, Loss: 0.4914\n",
      "Iteration 3700, Loss: 0.4905\n",
      "Iteration 3800, Loss: 0.4896\n",
      "Iteration 3900, Loss: 0.4887\n",
      "Iteration 4000, Loss: 0.4879\n",
      "Iteration 4100, Loss: 0.4870\n",
      "Iteration 4200, Loss: 0.4862\n",
      "Iteration 4300, Loss: 0.4854\n",
      "Iteration 4400, Loss: 0.4847\n",
      "Iteration 4500, Loss: 0.4839\n",
      "Iteration 4600, Loss: 0.4832\n",
      "Iteration 4700, Loss: 0.4825\n",
      "Iteration 4800, Loss: 0.4818\n",
      "Iteration 4900, Loss: 0.4811\n",
      "Iteration 5000, Loss: 0.4804\n",
      "Iteration 5100, Loss: 0.4798\n",
      "Iteration 5200, Loss: 0.4791\n",
      "Iteration 5300, Loss: 0.4785\n",
      "Iteration 5400, Loss: 0.4779\n",
      "Iteration 5500, Loss: 0.4773\n",
      "Iteration 5600, Loss: 0.4767\n",
      "Iteration 5700, Loss: 0.4761\n",
      "Iteration 5800, Loss: 0.4755\n",
      "Iteration 5900, Loss: 0.4750\n",
      "Iteration 6000, Loss: 0.4744\n",
      "Iteration 6100, Loss: 0.4739\n",
      "Iteration 6200, Loss: 0.4733\n",
      "Iteration 6300, Loss: 0.4728\n",
      "Iteration 6400, Loss: 0.4723\n",
      "Iteration 6500, Loss: 0.4718\n",
      "Iteration 6600, Loss: 0.4713\n",
      "Iteration 6700, Loss: 0.4708\n",
      "Iteration 6800, Loss: 0.4703\n",
      "Iteration 6900, Loss: 0.4698\n",
      "Iteration 0, Loss: 1.0123\n",
      "Iteration 100, Loss: 0.6018\n",
      "Iteration 200, Loss: 0.5727\n",
      "Iteration 300, Loss: 0.5571\n",
      "Iteration 400, Loss: 0.5464\n",
      "Iteration 500, Loss: 0.5383\n",
      "Iteration 600, Loss: 0.5319\n",
      "Iteration 700, Loss: 0.5265\n",
      "Iteration 800, Loss: 0.5219\n",
      "Iteration 900, Loss: 0.5178\n",
      "Iteration 1000, Loss: 0.5142\n",
      "Iteration 1100, Loss: 0.5109\n",
      "Iteration 1200, Loss: 0.5080\n",
      "Iteration 1300, Loss: 0.5053\n",
      "Iteration 1400, Loss: 0.5027\n",
      "Iteration 1500, Loss: 0.5004\n",
      "Iteration 1600, Loss: 0.4982\n",
      "Iteration 1700, Loss: 0.4961\n",
      "Iteration 1800, Loss: 0.4942\n",
      "Iteration 1900, Loss: 0.4924\n",
      "Iteration 2000, Loss: 0.4906\n",
      "Iteration 2100, Loss: 0.4889\n",
      "Iteration 2200, Loss: 0.4874\n",
      "Iteration 2300, Loss: 0.4858\n",
      "Iteration 2400, Loss: 0.4844\n",
      "Iteration 2500, Loss: 0.4830\n",
      "Iteration 2600, Loss: 0.4816\n",
      "Iteration 2700, Loss: 0.4803\n",
      "Iteration 2800, Loss: 0.4791\n",
      "Iteration 2900, Loss: 0.4779\n",
      "Iteration 3000, Loss: 0.4767\n",
      "Iteration 3100, Loss: 0.4756\n",
      "Iteration 3200, Loss: 0.4744\n",
      "Iteration 3300, Loss: 0.4734\n",
      "Iteration 3400, Loss: 0.4723\n",
      "Iteration 3500, Loss: 0.4713\n",
      "Iteration 3600, Loss: 0.4704\n",
      "Iteration 3700, Loss: 0.4694\n",
      "Iteration 3800, Loss: 0.4685\n",
      "Iteration 3900, Loss: 0.4676\n",
      "Iteration 4000, Loss: 0.4667\n",
      "Iteration 4100, Loss: 0.4658\n",
      "Iteration 4200, Loss: 0.4650\n",
      "Iteration 4300, Loss: 0.4642\n",
      "Iteration 4400, Loss: 0.4633\n",
      "Iteration 4500, Loss: 0.4626\n",
      "Iteration 4600, Loss: 0.4618\n",
      "Iteration 4700, Loss: 0.4610\n",
      "Iteration 4800, Loss: 0.4603\n",
      "Iteration 4900, Loss: 0.4596\n",
      "Iteration 5000, Loss: 0.4589\n",
      "Iteration 5100, Loss: 0.4582\n",
      "Iteration 5200, Loss: 0.4575\n",
      "Iteration 5300, Loss: 0.4568\n",
      "Iteration 5400, Loss: 0.4562\n",
      "Iteration 5500, Loss: 0.4555\n",
      "Iteration 5600, Loss: 0.4549\n",
      "Iteration 5700, Loss: 0.4543\n",
      "Iteration 5800, Loss: 0.4537\n",
      "Iteration 5900, Loss: 0.4531\n",
      "Iteration 6000, Loss: 0.4525\n",
      "Iteration 6100, Loss: 0.4519\n",
      "Iteration 6200, Loss: 0.4513\n",
      "Iteration 6300, Loss: 0.4508\n",
      "Iteration 6400, Loss: 0.4502\n",
      "Iteration 6500, Loss: 0.4497\n",
      "Iteration 6600, Loss: 0.4491\n",
      "Iteration 6700, Loss: 0.4486\n",
      "Iteration 6800, Loss: 0.4481\n",
      "Iteration 6900, Loss: 0.4476\n",
      "Iteration 0, Loss: 1.0573\n",
      "Iteration 100, Loss: 0.6550\n",
      "Iteration 200, Loss: 0.6161\n",
      "Iteration 300, Loss: 0.5964\n",
      "Iteration 400, Loss: 0.5830\n",
      "Iteration 500, Loss: 0.5732\n",
      "Iteration 600, Loss: 0.5652\n",
      "Iteration 700, Loss: 0.5587\n",
      "Iteration 800, Loss: 0.5531\n",
      "Iteration 900, Loss: 0.5482\n",
      "Iteration 1000, Loss: 0.5439\n",
      "Iteration 1100, Loss: 0.5402\n",
      "Iteration 1200, Loss: 0.5367\n",
      "Iteration 1300, Loss: 0.5336\n",
      "Iteration 1400, Loss: 0.5307\n",
      "Iteration 1500, Loss: 0.5280\n",
      "Iteration 1600, Loss: 0.5256\n",
      "Iteration 1700, Loss: 0.5232\n",
      "Iteration 1800, Loss: 0.5211\n",
      "Iteration 1900, Loss: 0.5191\n",
      "Iteration 2000, Loss: 0.5172\n",
      "Iteration 2100, Loss: 0.5153\n",
      "Iteration 2200, Loss: 0.5136\n",
      "Iteration 2300, Loss: 0.5119\n",
      "Iteration 2400, Loss: 0.5104\n",
      "Iteration 2500, Loss: 0.5089\n",
      "Iteration 2600, Loss: 0.5074\n",
      "Iteration 2700, Loss: 0.5061\n",
      "Iteration 2800, Loss: 0.5047\n",
      "Iteration 2900, Loss: 0.5035\n",
      "Iteration 3000, Loss: 0.5022\n",
      "Iteration 3100, Loss: 0.5010\n",
      "Iteration 3200, Loss: 0.4999\n",
      "Iteration 3300, Loss: 0.4988\n",
      "Iteration 3400, Loss: 0.4977\n",
      "Iteration 3500, Loss: 0.4967\n",
      "Iteration 3600, Loss: 0.4958\n",
      "Iteration 3700, Loss: 0.4947\n",
      "Iteration 3800, Loss: 0.4937\n",
      "Iteration 3900, Loss: 0.4928\n",
      "Iteration 4000, Loss: 0.4919\n",
      "Iteration 4100, Loss: 0.4910\n",
      "Iteration 4200, Loss: 0.4902\n",
      "Iteration 4300, Loss: 0.4893\n",
      "Iteration 4400, Loss: 0.4885\n",
      "Iteration 4500, Loss: 0.4877\n",
      "Iteration 4600, Loss: 0.4870\n",
      "Iteration 4700, Loss: 0.4862\n",
      "Iteration 4800, Loss: 0.4855\n",
      "Iteration 4900, Loss: 0.4848\n",
      "Iteration 5000, Loss: 0.4840\n",
      "Iteration 5100, Loss: 0.4833\n",
      "Iteration 5200, Loss: 0.4827\n",
      "Iteration 5300, Loss: 0.4820\n",
      "Iteration 5400, Loss: 0.4813\n",
      "Iteration 5500, Loss: 0.4807\n",
      "Iteration 5600, Loss: 0.4801\n",
      "Iteration 5700, Loss: 0.4794\n",
      "Iteration 5800, Loss: 0.4788\n",
      "Iteration 5900, Loss: 0.4782\n",
      "Iteration 6000, Loss: 0.4777\n",
      "Iteration 6100, Loss: 0.4771\n",
      "Iteration 6200, Loss: 0.4765\n",
      "Iteration 6300, Loss: 0.4760\n",
      "Iteration 6400, Loss: 0.4754\n",
      "Iteration 6500, Loss: 0.4749\n",
      "Iteration 6600, Loss: 0.4743\n",
      "Iteration 6700, Loss: 0.4738\n",
      "Iteration 6800, Loss: 0.4733\n",
      "Iteration 6900, Loss: 0.4728\n",
      "Iteration 0, Loss: 1.0504\n",
      "Iteration 100, Loss: 0.6559\n",
      "Iteration 200, Loss: 0.6241\n",
      "Iteration 300, Loss: 0.6069\n",
      "Iteration 400, Loss: 0.5948\n",
      "Iteration 500, Loss: 0.5856\n",
      "Iteration 600, Loss: 0.5779\n",
      "Iteration 700, Loss: 0.5715\n",
      "Iteration 800, Loss: 0.5660\n",
      "Iteration 900, Loss: 0.5612\n",
      "Iteration 1000, Loss: 0.5569\n",
      "Iteration 1100, Loss: 0.5531\n",
      "Iteration 1200, Loss: 0.5496\n",
      "Iteration 1300, Loss: 0.5464\n",
      "Iteration 1400, Loss: 0.5435\n",
      "Iteration 1500, Loss: 0.5407\n",
      "Iteration 1600, Loss: 0.5382\n",
      "Iteration 1700, Loss: 0.5358\n",
      "Iteration 1800, Loss: 0.5336\n",
      "Iteration 1900, Loss: 0.5315\n",
      "Iteration 2000, Loss: 0.5295\n",
      "Iteration 2100, Loss: 0.5277\n",
      "Iteration 2200, Loss: 0.5259\n",
      "Iteration 2300, Loss: 0.5243\n",
      "Iteration 2400, Loss: 0.5227\n",
      "Iteration 2500, Loss: 0.5211\n",
      "Iteration 2600, Loss: 0.5196\n",
      "Iteration 2700, Loss: 0.5182\n",
      "Iteration 2800, Loss: 0.5168\n",
      "Iteration 2900, Loss: 0.5155\n",
      "Iteration 3000, Loss: 0.5143\n",
      "Iteration 3100, Loss: 0.5131\n",
      "Iteration 3200, Loss: 0.5119\n",
      "Iteration 3300, Loss: 0.5107\n",
      "Iteration 3400, Loss: 0.5097\n",
      "Iteration 3500, Loss: 0.5086\n",
      "Iteration 3600, Loss: 0.5076\n",
      "Iteration 3700, Loss: 0.5065\n",
      "Iteration 3800, Loss: 0.5056\n",
      "Iteration 3900, Loss: 0.5046\n",
      "Iteration 4000, Loss: 0.5037\n",
      "Iteration 4100, Loss: 0.5028\n",
      "Iteration 4200, Loss: 0.5019\n",
      "Iteration 4300, Loss: 0.5011\n",
      "Iteration 4400, Loss: 0.5002\n",
      "Iteration 4500, Loss: 0.4994\n",
      "Iteration 4600, Loss: 0.4987\n",
      "Iteration 4700, Loss: 0.4979\n",
      "Iteration 4800, Loss: 0.4971\n",
      "Iteration 4900, Loss: 0.4964\n",
      "Iteration 5000, Loss: 0.4956\n",
      "Iteration 5100, Loss: 0.4949\n",
      "Iteration 5200, Loss: 0.4942\n",
      "Iteration 5300, Loss: 0.4936\n",
      "Iteration 5400, Loss: 0.4929\n",
      "Iteration 5500, Loss: 0.4922\n",
      "Iteration 5600, Loss: 0.4916\n",
      "Iteration 5700, Loss: 0.4909\n",
      "Iteration 5800, Loss: 0.4903\n",
      "Iteration 5900, Loss: 0.4897\n",
      "Iteration 6000, Loss: 0.4891\n",
      "Iteration 6100, Loss: 0.4886\n",
      "Iteration 6200, Loss: 0.4880\n",
      "Iteration 6300, Loss: 0.4874\n",
      "Iteration 6400, Loss: 0.4868\n",
      "Iteration 6500, Loss: 0.4863\n",
      "Iteration 6600, Loss: 0.4857\n",
      "Iteration 6700, Loss: 0.4852\n",
      "Iteration 6800, Loss: 0.4847\n",
      "Iteration 6900, Loss: 0.4842\n",
      "Iteration 0, Loss: 1.0814\n",
      "Iteration 100, Loss: 0.7360\n",
      "Iteration 200, Loss: 0.6880\n",
      "Iteration 300, Loss: 0.6649\n",
      "Iteration 400, Loss: 0.6501\n",
      "Iteration 500, Loss: 0.6393\n",
      "Iteration 600, Loss: 0.6309\n",
      "Iteration 700, Loss: 0.6239\n",
      "Iteration 800, Loss: 0.6180\n",
      "Iteration 900, Loss: 0.6128\n",
      "Iteration 1000, Loss: 0.6082\n",
      "Iteration 1100, Loss: 0.6041\n",
      "Iteration 1200, Loss: 0.6003\n",
      "Iteration 1300, Loss: 0.5969\n",
      "Iteration 1400, Loss: 0.5937\n",
      "Iteration 1500, Loss: 0.5908\n",
      "Iteration 1600, Loss: 0.5880\n",
      "Iteration 1700, Loss: 0.5855\n",
      "Iteration 1800, Loss: 0.5831\n",
      "Iteration 1900, Loss: 0.5808\n",
      "Iteration 2000, Loss: 0.5787\n",
      "Iteration 2100, Loss: 0.5767\n",
      "Iteration 2200, Loss: 0.5748\n",
      "Iteration 2300, Loss: 0.5730\n",
      "Iteration 2400, Loss: 0.5713\n",
      "Iteration 2500, Loss: 0.5696\n",
      "Iteration 2600, Loss: 0.5681\n",
      "Iteration 2700, Loss: 0.5666\n",
      "Iteration 2800, Loss: 0.5651\n",
      "Iteration 2900, Loss: 0.5637\n",
      "Iteration 3000, Loss: 0.5624\n",
      "Iteration 3100, Loss: 0.5611\n",
      "Iteration 3200, Loss: 0.5599\n",
      "Iteration 3300, Loss: 0.5587\n",
      "Iteration 3400, Loss: 0.5576\n",
      "Iteration 3500, Loss: 0.5564\n",
      "Iteration 3600, Loss: 0.5554\n",
      "Iteration 3700, Loss: 0.5543\n",
      "Iteration 3800, Loss: 0.5533\n",
      "Iteration 3900, Loss: 0.5523\n",
      "Iteration 4000, Loss: 0.5514\n",
      "Iteration 4100, Loss: 0.5505\n",
      "Iteration 4200, Loss: 0.5496\n",
      "Iteration 4300, Loss: 0.5487\n",
      "Iteration 4400, Loss: 0.5478\n",
      "Iteration 4500, Loss: 0.5470\n",
      "Iteration 4600, Loss: 0.5462\n",
      "Iteration 4700, Loss: 0.5454\n",
      "Iteration 4800, Loss: 0.5446\n",
      "Iteration 4900, Loss: 0.5439\n",
      "Iteration 5000, Loss: 0.5432\n",
      "Iteration 5100, Loss: 0.5424\n",
      "Iteration 5200, Loss: 0.5417\n",
      "Iteration 5300, Loss: 0.5410\n",
      "Iteration 5400, Loss: 0.5404\n",
      "Iteration 5500, Loss: 0.5397\n",
      "Iteration 5600, Loss: 0.5391\n",
      "Iteration 5700, Loss: 0.5384\n",
      "Iteration 5800, Loss: 0.5378\n",
      "Iteration 5900, Loss: 0.5372\n",
      "Iteration 6000, Loss: 0.5366\n",
      "Iteration 6100, Loss: 0.5360\n",
      "Iteration 6200, Loss: 0.5354\n",
      "Iteration 6300, Loss: 0.5349\n",
      "Iteration 6400, Loss: 0.5343\n",
      "Iteration 6500, Loss: 0.5338\n",
      "Iteration 6600, Loss: 0.5332\n",
      "Iteration 6700, Loss: 0.5327\n",
      "Iteration 6800, Loss: 0.5322\n",
      "Iteration 6900, Loss: 0.5317\n",
      "Iteration 0, Loss: 1.0795\n",
      "Iteration 100, Loss: 0.7074\n",
      "Iteration 200, Loss: 0.6581\n",
      "Iteration 300, Loss: 0.6346\n",
      "Iteration 400, Loss: 0.6194\n",
      "Iteration 500, Loss: 0.6082\n",
      "Iteration 600, Loss: 0.5994\n",
      "Iteration 700, Loss: 0.5922\n",
      "Iteration 800, Loss: 0.5860\n",
      "Iteration 900, Loss: 0.5807\n",
      "Iteration 1000, Loss: 0.5759\n",
      "Iteration 1100, Loss: 0.5717\n",
      "Iteration 1200, Loss: 0.5678\n",
      "Iteration 1300, Loss: 0.5643\n",
      "Iteration 1400, Loss: 0.5610\n",
      "Iteration 1500, Loss: 0.5580\n",
      "Iteration 1600, Loss: 0.5552\n",
      "Iteration 1700, Loss: 0.5526\n",
      "Iteration 1800, Loss: 0.5501\n",
      "Iteration 1900, Loss: 0.5478\n",
      "Iteration 2000, Loss: 0.5456\n",
      "Iteration 2100, Loss: 0.5435\n",
      "Iteration 2200, Loss: 0.5416\n",
      "Iteration 2300, Loss: 0.5397\n",
      "Iteration 2400, Loss: 0.5379\n",
      "Iteration 2500, Loss: 0.5361\n",
      "Iteration 2600, Loss: 0.5345\n",
      "Iteration 2700, Loss: 0.5328\n",
      "Iteration 2800, Loss: 0.5313\n",
      "Iteration 2900, Loss: 0.5298\n",
      "Iteration 3000, Loss: 0.5284\n",
      "Iteration 3100, Loss: 0.5270\n",
      "Iteration 3200, Loss: 0.5256\n",
      "Iteration 3300, Loss: 0.5243\n",
      "Iteration 3400, Loss: 0.5231\n",
      "Iteration 3500, Loss: 0.5218\n",
      "Iteration 3600, Loss: 0.5206\n",
      "Iteration 3700, Loss: 0.5195\n",
      "Iteration 3800, Loss: 0.5184\n",
      "Iteration 3900, Loss: 0.5173\n",
      "Iteration 4000, Loss: 0.5162\n",
      "Iteration 4100, Loss: 0.5151\n",
      "Iteration 4200, Loss: 0.5141\n",
      "Iteration 4300, Loss: 0.5131\n",
      "Iteration 4400, Loss: 0.5121\n",
      "Iteration 4500, Loss: 0.5112\n",
      "Iteration 4600, Loss: 0.5102\n",
      "Iteration 4700, Loss: 0.5093\n",
      "Iteration 4800, Loss: 0.5084\n",
      "Iteration 4900, Loss: 0.5076\n",
      "Iteration 5000, Loss: 0.5067\n",
      "Iteration 5100, Loss: 0.5059\n",
      "Iteration 5200, Loss: 0.5050\n",
      "Iteration 5300, Loss: 0.5042\n",
      "Iteration 5400, Loss: 0.5034\n",
      "Iteration 5500, Loss: 0.5027\n",
      "Iteration 5600, Loss: 0.5019\n",
      "Iteration 5700, Loss: 0.5011\n",
      "Iteration 5800, Loss: 0.5004\n",
      "Iteration 5900, Loss: 0.4997\n",
      "Iteration 6000, Loss: 0.4990\n",
      "Iteration 6100, Loss: 0.4983\n",
      "Iteration 6200, Loss: 0.4976\n",
      "Iteration 6300, Loss: 0.4969\n",
      "Iteration 6400, Loss: 0.4962\n",
      "Iteration 6500, Loss: 0.4956\n",
      "Iteration 6600, Loss: 0.4949\n",
      "Iteration 6700, Loss: 0.4943\n",
      "Iteration 6800, Loss: 0.4937\n",
      "Iteration 6900, Loss: 0.4931\n",
      "Iteration 0, Loss: 1.0888\n",
      "Iteration 100, Loss: 0.7805\n",
      "Iteration 200, Loss: 0.7151\n",
      "Iteration 300, Loss: 0.6846\n",
      "Iteration 400, Loss: 0.6656\n",
      "Iteration 500, Loss: 0.6522\n",
      "Iteration 600, Loss: 0.6419\n",
      "Iteration 700, Loss: 0.6336\n",
      "Iteration 800, Loss: 0.6267\n",
      "Iteration 900, Loss: 0.6209\n",
      "Iteration 1000, Loss: 0.6158\n",
      "Iteration 1100, Loss: 0.6112\n",
      "Iteration 1200, Loss: 0.6072\n",
      "Iteration 1300, Loss: 0.6035\n",
      "Iteration 1400, Loss: 0.6001\n",
      "Iteration 1500, Loss: 0.5970\n",
      "Iteration 1600, Loss: 0.5941\n",
      "Iteration 1700, Loss: 0.5914\n",
      "Iteration 1800, Loss: 0.5888\n",
      "Iteration 1900, Loss: 0.5864\n",
      "Iteration 2000, Loss: 0.5841\n",
      "Iteration 2100, Loss: 0.5820\n",
      "Iteration 2200, Loss: 0.5799\n",
      "Iteration 2300, Loss: 0.5780\n",
      "Iteration 2400, Loss: 0.5761\n",
      "Iteration 2500, Loss: 0.5743\n",
      "Iteration 2600, Loss: 0.5726\n",
      "Iteration 2700, Loss: 0.5710\n",
      "Iteration 2800, Loss: 0.5694\n",
      "Iteration 2900, Loss: 0.5679\n",
      "Iteration 3000, Loss: 0.5664\n",
      "Iteration 3100, Loss: 0.5650\n",
      "Iteration 3200, Loss: 0.5636\n",
      "Iteration 3300, Loss: 0.5623\n",
      "Iteration 3400, Loss: 0.5610\n",
      "Iteration 3500, Loss: 0.5598\n",
      "Iteration 3600, Loss: 0.5586\n",
      "Iteration 3700, Loss: 0.5574\n",
      "Iteration 3800, Loss: 0.5563\n",
      "Iteration 3900, Loss: 0.5552\n",
      "Iteration 4000, Loss: 0.5541\n",
      "Iteration 4100, Loss: 0.5531\n",
      "Iteration 4200, Loss: 0.5521\n",
      "Iteration 4300, Loss: 0.5511\n",
      "Iteration 4400, Loss: 0.5501\n",
      "Iteration 4500, Loss: 0.5492\n",
      "Iteration 4600, Loss: 0.5483\n",
      "Iteration 4700, Loss: 0.5474\n",
      "Iteration 4800, Loss: 0.5465\n",
      "Iteration 4900, Loss: 0.5457\n",
      "Iteration 5000, Loss: 0.5448\n",
      "Iteration 5100, Loss: 0.5440\n",
      "Iteration 5200, Loss: 0.5432\n",
      "Iteration 5300, Loss: 0.5425\n",
      "Iteration 5400, Loss: 0.5417\n",
      "Iteration 5500, Loss: 0.5410\n",
      "Iteration 5600, Loss: 0.5402\n",
      "Iteration 5700, Loss: 0.5395\n",
      "Iteration 5800, Loss: 0.5388\n",
      "Iteration 5900, Loss: 0.5381\n",
      "Iteration 6000, Loss: 0.5374\n",
      "Iteration 6100, Loss: 0.5368\n",
      "Iteration 6200, Loss: 0.5361\n",
      "Iteration 6300, Loss: 0.5355\n",
      "Iteration 6400, Loss: 0.5349\n",
      "Iteration 6500, Loss: 0.5343\n",
      "Iteration 6600, Loss: 0.5337\n",
      "Iteration 6700, Loss: 0.5331\n",
      "Iteration 6800, Loss: 0.5325\n",
      "Iteration 6900, Loss: 0.5319\n",
      "Iteration 0, Loss: 1.0897\n",
      "Iteration 100, Loss: 0.7918\n",
      "Iteration 200, Loss: 0.7260\n",
      "Iteration 300, Loss: 0.6965\n",
      "Iteration 400, Loss: 0.6785\n",
      "Iteration 500, Loss: 0.6657\n",
      "Iteration 600, Loss: 0.6560\n",
      "Iteration 700, Loss: 0.6480\n",
      "Iteration 800, Loss: 0.6414\n",
      "Iteration 900, Loss: 0.6357\n",
      "Iteration 1000, Loss: 0.6307\n",
      "Iteration 1100, Loss: 0.6263\n",
      "Iteration 1200, Loss: 0.6223\n",
      "Iteration 1300, Loss: 0.6187\n",
      "Iteration 1400, Loss: 0.6154\n",
      "Iteration 1500, Loss: 0.6124\n",
      "Iteration 1600, Loss: 0.6095\n",
      "Iteration 1700, Loss: 0.6068\n",
      "Iteration 1800, Loss: 0.6043\n",
      "Iteration 1900, Loss: 0.6020\n",
      "Iteration 2000, Loss: 0.5998\n",
      "Iteration 2100, Loss: 0.5977\n",
      "Iteration 2200, Loss: 0.5957\n",
      "Iteration 2300, Loss: 0.5938\n",
      "Iteration 2400, Loss: 0.5920\n",
      "Iteration 2500, Loss: 0.5903\n",
      "Iteration 2600, Loss: 0.5886\n",
      "Iteration 2700, Loss: 0.5870\n",
      "Iteration 2800, Loss: 0.5855\n",
      "Iteration 2900, Loss: 0.5840\n",
      "Iteration 3000, Loss: 0.5826\n",
      "Iteration 3100, Loss: 0.5812\n",
      "Iteration 3200, Loss: 0.5799\n",
      "Iteration 3300, Loss: 0.5786\n",
      "Iteration 3400, Loss: 0.5774\n",
      "Iteration 3500, Loss: 0.5762\n",
      "Iteration 3600, Loss: 0.5751\n",
      "Iteration 3700, Loss: 0.5739\n",
      "Iteration 3800, Loss: 0.5728\n",
      "Iteration 3900, Loss: 0.5718\n",
      "Iteration 4000, Loss: 0.5708\n",
      "Iteration 4100, Loss: 0.5698\n",
      "Iteration 4200, Loss: 0.5688\n",
      "Iteration 4300, Loss: 0.5678\n",
      "Iteration 4400, Loss: 0.5669\n",
      "Iteration 4500, Loss: 0.5660\n",
      "Iteration 4600, Loss: 0.5651\n",
      "Iteration 4700, Loss: 0.5642\n",
      "Iteration 4800, Loss: 0.5634\n",
      "Iteration 4900, Loss: 0.5626\n",
      "Iteration 5000, Loss: 0.5618\n",
      "Iteration 5100, Loss: 0.5610\n",
      "Iteration 5200, Loss: 0.5602\n",
      "Iteration 5300, Loss: 0.5595\n",
      "Iteration 5400, Loss: 0.5587\n",
      "Iteration 5500, Loss: 0.5580\n",
      "Iteration 5600, Loss: 0.5573\n",
      "Iteration 5700, Loss: 0.5566\n",
      "Iteration 5800, Loss: 0.5559\n",
      "Iteration 5900, Loss: 0.5552\n",
      "Iteration 6000, Loss: 0.5546\n",
      "Iteration 6100, Loss: 0.5539\n",
      "Iteration 6200, Loss: 0.5533\n",
      "Iteration 6300, Loss: 0.5526\n",
      "Iteration 6400, Loss: 0.5520\n",
      "Iteration 6500, Loss: 0.5514\n",
      "Iteration 6600, Loss: 0.5508\n",
      "Iteration 6700, Loss: 0.5502\n",
      "Iteration 6800, Loss: 0.5497\n",
      "Iteration 6900, Loss: 0.5491\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8688\n",
      "Iteration 200, Loss: 0.7926\n",
      "Iteration 300, Loss: 0.7535\n",
      "Iteration 400, Loss: 0.7292\n",
      "Iteration 500, Loss: 0.7122\n",
      "Iteration 600, Loss: 0.6993\n",
      "Iteration 700, Loss: 0.6893\n",
      "Iteration 800, Loss: 0.6809\n",
      "Iteration 900, Loss: 0.6739\n",
      "Iteration 1000, Loss: 0.6677\n",
      "Iteration 1100, Loss: 0.6623\n",
      "Iteration 1200, Loss: 0.6575\n",
      "Iteration 1300, Loss: 0.6531\n",
      "Iteration 1400, Loss: 0.6492\n",
      "Iteration 1500, Loss: 0.6456\n",
      "Iteration 1600, Loss: 0.6422\n",
      "Iteration 1700, Loss: 0.6392\n",
      "Iteration 1800, Loss: 0.6363\n",
      "Iteration 1900, Loss: 0.6336\n",
      "Iteration 2000, Loss: 0.6310\n",
      "Iteration 2100, Loss: 0.6286\n",
      "Iteration 2200, Loss: 0.6264\n",
      "Iteration 2300, Loss: 0.6242\n",
      "Iteration 2400, Loss: 0.6222\n",
      "Iteration 2500, Loss: 0.6202\n",
      "Iteration 2600, Loss: 0.6184\n",
      "Iteration 2700, Loss: 0.6166\n",
      "Iteration 2800, Loss: 0.6148\n",
      "Iteration 2900, Loss: 0.6132\n",
      "Iteration 3000, Loss: 0.6116\n",
      "Iteration 3100, Loss: 0.6101\n",
      "Iteration 3200, Loss: 0.6086\n",
      "Iteration 3300, Loss: 0.6072\n",
      "Iteration 3400, Loss: 0.6058\n",
      "Iteration 3500, Loss: 0.6044\n",
      "Iteration 3600, Loss: 0.6032\n",
      "Iteration 3700, Loss: 0.6019\n",
      "Iteration 3800, Loss: 0.6007\n",
      "Iteration 3900, Loss: 0.5995\n",
      "Iteration 4000, Loss: 0.5983\n",
      "Iteration 4100, Loss: 0.5972\n",
      "Iteration 4200, Loss: 0.5961\n",
      "Iteration 4300, Loss: 0.5950\n",
      "Iteration 4400, Loss: 0.5940\n",
      "Iteration 4500, Loss: 0.5929\n",
      "Iteration 4600, Loss: 0.5919\n",
      "Iteration 4700, Loss: 0.5910\n",
      "Iteration 4800, Loss: 0.5900\n",
      "Iteration 4900, Loss: 0.5891\n",
      "Iteration 5000, Loss: 0.5882\n",
      "Iteration 5100, Loss: 0.5873\n",
      "Iteration 5200, Loss: 0.5864\n",
      "Iteration 5300, Loss: 0.5856\n",
      "Iteration 5400, Loss: 0.5848\n",
      "Iteration 5500, Loss: 0.5839\n",
      "Iteration 5600, Loss: 0.5831\n",
      "Iteration 5700, Loss: 0.5823\n",
      "Iteration 5800, Loss: 0.5816\n",
      "Iteration 5900, Loss: 0.5808\n",
      "Iteration 6000, Loss: 0.5801\n",
      "Iteration 6100, Loss: 0.5794\n",
      "Iteration 6200, Loss: 0.5786\n",
      "Iteration 6300, Loss: 0.5779\n",
      "Iteration 6400, Loss: 0.5772\n",
      "Iteration 6500, Loss: 0.5766\n",
      "Iteration 6600, Loss: 0.5759\n",
      "Iteration 6700, Loss: 0.5753\n",
      "Iteration 6800, Loss: 0.5746\n",
      "Iteration 6900, Loss: 0.5740\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8543\n",
      "Iteration 200, Loss: 0.7735\n",
      "Iteration 300, Loss: 0.7324\n",
      "Iteration 400, Loss: 0.7076\n",
      "Iteration 500, Loss: 0.6906\n",
      "Iteration 600, Loss: 0.6779\n",
      "Iteration 700, Loss: 0.6679\n",
      "Iteration 800, Loss: 0.6597\n",
      "Iteration 900, Loss: 0.6528\n",
      "Iteration 1000, Loss: 0.6468\n",
      "Iteration 1100, Loss: 0.6416\n",
      "Iteration 1200, Loss: 0.6369\n",
      "Iteration 1300, Loss: 0.6328\n",
      "Iteration 1400, Loss: 0.6290\n",
      "Iteration 1500, Loss: 0.6256\n",
      "Iteration 1600, Loss: 0.6225\n",
      "Iteration 1700, Loss: 0.6196\n",
      "Iteration 1800, Loss: 0.6169\n",
      "Iteration 1900, Loss: 0.6144\n",
      "Iteration 2000, Loss: 0.6121\n",
      "Iteration 2100, Loss: 0.6099\n",
      "Iteration 2200, Loss: 0.6078\n",
      "Iteration 2300, Loss: 0.6059\n",
      "Iteration 2400, Loss: 0.6040\n",
      "Iteration 2500, Loss: 0.6022\n",
      "Iteration 2600, Loss: 0.6005\n",
      "Iteration 2700, Loss: 0.5988\n",
      "Iteration 2800, Loss: 0.5973\n",
      "Iteration 2900, Loss: 0.5958\n",
      "Iteration 3000, Loss: 0.5943\n",
      "Iteration 3100, Loss: 0.5929\n",
      "Iteration 3200, Loss: 0.5916\n",
      "Iteration 3300, Loss: 0.5903\n",
      "Iteration 3400, Loss: 0.5890\n",
      "Iteration 3500, Loss: 0.5878\n",
      "Iteration 3600, Loss: 0.5866\n",
      "Iteration 3700, Loss: 0.5855\n",
      "Iteration 3800, Loss: 0.5843\n",
      "Iteration 3900, Loss: 0.5833\n",
      "Iteration 4000, Loss: 0.5822\n",
      "Iteration 4100, Loss: 0.5812\n",
      "Iteration 4200, Loss: 0.5802\n",
      "Iteration 4300, Loss: 0.5792\n",
      "Iteration 4400, Loss: 0.5782\n",
      "Iteration 4500, Loss: 0.5773\n",
      "Iteration 4600, Loss: 0.5764\n",
      "Iteration 4700, Loss: 0.5755\n",
      "Iteration 4800, Loss: 0.5747\n",
      "Iteration 4900, Loss: 0.5738\n",
      "Iteration 5000, Loss: 0.5730\n",
      "Iteration 5100, Loss: 0.5722\n",
      "Iteration 5200, Loss: 0.5714\n",
      "Iteration 5300, Loss: 0.5706\n",
      "Iteration 5400, Loss: 0.5698\n",
      "Iteration 5500, Loss: 0.5691\n",
      "Iteration 5600, Loss: 0.5683\n",
      "Iteration 5700, Loss: 0.5676\n",
      "Iteration 5800, Loss: 0.5669\n",
      "Iteration 5900, Loss: 0.5662\n",
      "Iteration 6000, Loss: 0.5655\n",
      "Iteration 6100, Loss: 0.5649\n",
      "Iteration 6200, Loss: 0.5642\n",
      "Iteration 6300, Loss: 0.5635\n",
      "Iteration 6400, Loss: 0.5629\n",
      "Iteration 6500, Loss: 0.5623\n",
      "Iteration 6600, Loss: 0.5617\n",
      "Iteration 6700, Loss: 0.5611\n",
      "Iteration 6800, Loss: 0.5605\n",
      "Iteration 6900, Loss: 0.5599\n",
      "Iteration 0, Loss: 1.0967\n",
      "Iteration 100, Loss: 0.9665\n",
      "Iteration 200, Loss: 0.8949\n",
      "Iteration 300, Loss: 0.8485\n",
      "Iteration 400, Loss: 0.8158\n",
      "Iteration 500, Loss: 0.7916\n",
      "Iteration 600, Loss: 0.7728\n",
      "Iteration 700, Loss: 0.7578\n",
      "Iteration 800, Loss: 0.7456\n",
      "Iteration 900, Loss: 0.7353\n",
      "Iteration 1000, Loss: 0.7266\n",
      "Iteration 1100, Loss: 0.7191\n",
      "Iteration 1200, Loss: 0.7125\n",
      "Iteration 1300, Loss: 0.7066\n",
      "Iteration 1400, Loss: 0.7014\n",
      "Iteration 1500, Loss: 0.6967\n",
      "Iteration 1600, Loss: 0.6924\n",
      "Iteration 1700, Loss: 0.6884\n",
      "Iteration 1800, Loss: 0.6848\n",
      "Iteration 1900, Loss: 0.6814\n",
      "Iteration 2000, Loss: 0.6783\n",
      "Iteration 2100, Loss: 0.6753\n",
      "Iteration 2200, Loss: 0.6726\n",
      "Iteration 2300, Loss: 0.6700\n",
      "Iteration 2400, Loss: 0.6676\n",
      "Iteration 2500, Loss: 0.6653\n",
      "Iteration 2600, Loss: 0.6631\n",
      "Iteration 2700, Loss: 0.6610\n",
      "Iteration 2800, Loss: 0.6590\n",
      "Iteration 2900, Loss: 0.6571\n",
      "Iteration 3000, Loss: 0.6553\n",
      "Iteration 3100, Loss: 0.6536\n",
      "Iteration 3200, Loss: 0.6520\n",
      "Iteration 3300, Loss: 0.6504\n",
      "Iteration 3400, Loss: 0.6488\n",
      "Iteration 3500, Loss: 0.6474\n",
      "Iteration 3600, Loss: 0.6460\n",
      "Iteration 3700, Loss: 0.6446\n",
      "Iteration 3800, Loss: 0.6433\n",
      "Iteration 3900, Loss: 0.6420\n",
      "Iteration 4000, Loss: 0.6408\n",
      "Iteration 4100, Loss: 0.6396\n",
      "Iteration 4200, Loss: 0.6384\n",
      "Iteration 4300, Loss: 0.6373\n",
      "Iteration 4400, Loss: 0.6362\n",
      "Iteration 4500, Loss: 0.6351\n",
      "Iteration 4600, Loss: 0.6341\n",
      "Iteration 4700, Loss: 0.6331\n",
      "Iteration 4800, Loss: 0.6321\n",
      "Iteration 4900, Loss: 0.6312\n",
      "Iteration 5000, Loss: 0.6302\n",
      "Iteration 5100, Loss: 0.6293\n",
      "Iteration 5200, Loss: 0.6284\n",
      "Iteration 5300, Loss: 0.6276\n",
      "Iteration 5400, Loss: 0.6267\n",
      "Iteration 5500, Loss: 0.6259\n",
      "Iteration 5600, Loss: 0.6251\n",
      "Iteration 5700, Loss: 0.6243\n",
      "Iteration 5800, Loss: 0.6235\n",
      "Iteration 5900, Loss: 0.6227\n",
      "Iteration 6000, Loss: 0.6220\n",
      "Iteration 6100, Loss: 0.6212\n",
      "Iteration 6200, Loss: 0.6205\n",
      "Iteration 6300, Loss: 0.6198\n",
      "Iteration 6400, Loss: 0.6191\n",
      "Iteration 6500, Loss: 0.6184\n",
      "Iteration 6600, Loss: 0.6178\n",
      "Iteration 6700, Loss: 0.6171\n",
      "Iteration 6800, Loss: 0.6165\n",
      "Iteration 6900, Loss: 0.6158\n",
      "Iteration 0, Loss: 1.0968\n",
      "Iteration 100, Loss: 0.9675\n",
      "Iteration 200, Loss: 0.8948\n",
      "Iteration 300, Loss: 0.8469\n",
      "Iteration 400, Loss: 0.8128\n",
      "Iteration 500, Loss: 0.7874\n",
      "Iteration 600, Loss: 0.7677\n",
      "Iteration 700, Loss: 0.7520\n",
      "Iteration 800, Loss: 0.7392\n",
      "Iteration 900, Loss: 0.7285\n",
      "Iteration 1000, Loss: 0.7195\n",
      "Iteration 1100, Loss: 0.7117\n",
      "Iteration 1200, Loss: 0.7050\n",
      "Iteration 1300, Loss: 0.6990\n",
      "Iteration 1400, Loss: 0.6936\n",
      "Iteration 1500, Loss: 0.6887\n",
      "Iteration 1600, Loss: 0.6843\n",
      "Iteration 1700, Loss: 0.6803\n",
      "Iteration 1800, Loss: 0.6765\n",
      "Iteration 1900, Loss: 0.6731\n",
      "Iteration 2000, Loss: 0.6699\n",
      "Iteration 2100, Loss: 0.6668\n",
      "Iteration 2200, Loss: 0.6640\n",
      "Iteration 2300, Loss: 0.6613\n",
      "Iteration 2400, Loss: 0.6588\n",
      "Iteration 2500, Loss: 0.6564\n",
      "Iteration 2600, Loss: 0.6541\n",
      "Iteration 2700, Loss: 0.6520\n",
      "Iteration 2800, Loss: 0.6499\n",
      "Iteration 2900, Loss: 0.6479\n",
      "Iteration 3000, Loss: 0.6461\n",
      "Iteration 3100, Loss: 0.6442\n",
      "Iteration 3200, Loss: 0.6425\n",
      "Iteration 3300, Loss: 0.6408\n",
      "Iteration 3400, Loss: 0.6392\n",
      "Iteration 3500, Loss: 0.6376\n",
      "Iteration 3600, Loss: 0.6361\n",
      "Iteration 3700, Loss: 0.6347\n",
      "Iteration 3800, Loss: 0.6333\n",
      "Iteration 3900, Loss: 0.6319\n",
      "Iteration 4000, Loss: 0.6306\n",
      "Iteration 4100, Loss: 0.6293\n",
      "Iteration 4200, Loss: 0.6281\n",
      "Iteration 4300, Loss: 0.6268\n",
      "Iteration 4400, Loss: 0.6257\n",
      "Iteration 4500, Loss: 0.6245\n",
      "Iteration 4600, Loss: 0.6234\n",
      "Iteration 4700, Loss: 0.6223\n",
      "Iteration 4800, Loss: 0.6213\n",
      "Iteration 4900, Loss: 0.6202\n",
      "Iteration 5000, Loss: 0.6192\n",
      "Iteration 5100, Loss: 0.6182\n",
      "Iteration 5200, Loss: 0.6173\n",
      "Iteration 5300, Loss: 0.6163\n",
      "Iteration 5400, Loss: 0.6154\n",
      "Iteration 5500, Loss: 0.6145\n",
      "Iteration 5600, Loss: 0.6136\n",
      "Iteration 5700, Loss: 0.6127\n",
      "Iteration 5800, Loss: 0.6119\n",
      "Iteration 5900, Loss: 0.6111\n",
      "Iteration 6000, Loss: 0.6102\n",
      "Iteration 6100, Loss: 0.6094\n",
      "Iteration 6200, Loss: 0.6086\n",
      "Iteration 6300, Loss: 0.6079\n",
      "Iteration 6400, Loss: 0.6071\n",
      "Iteration 6500, Loss: 0.6064\n",
      "Iteration 6600, Loss: 0.6056\n",
      "Iteration 6700, Loss: 0.6049\n",
      "Iteration 6800, Loss: 0.6042\n",
      "Iteration 6900, Loss: 0.6035\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0193\n",
      "Iteration 200, Loss: 0.9650\n",
      "Iteration 300, Loss: 0.9242\n",
      "Iteration 400, Loss: 0.8923\n",
      "Iteration 500, Loss: 0.8666\n",
      "Iteration 600, Loss: 0.8453\n",
      "Iteration 700, Loss: 0.8275\n",
      "Iteration 800, Loss: 0.8122\n",
      "Iteration 900, Loss: 0.7991\n",
      "Iteration 1000, Loss: 0.7876\n",
      "Iteration 1100, Loss: 0.7776\n",
      "Iteration 1200, Loss: 0.7687\n",
      "Iteration 1300, Loss: 0.7608\n",
      "Iteration 1400, Loss: 0.7537\n",
      "Iteration 1500, Loss: 0.7473\n",
      "Iteration 1600, Loss: 0.7415\n",
      "Iteration 1700, Loss: 0.7361\n",
      "Iteration 1800, Loss: 0.7313\n",
      "Iteration 1900, Loss: 0.7268\n",
      "Iteration 2000, Loss: 0.7226\n",
      "Iteration 2100, Loss: 0.7187\n",
      "Iteration 2200, Loss: 0.7151\n",
      "Iteration 2300, Loss: 0.7117\n",
      "Iteration 2400, Loss: 0.7086\n",
      "Iteration 2500, Loss: 0.7056\n",
      "Iteration 2600, Loss: 0.7028\n",
      "Iteration 2700, Loss: 0.7001\n",
      "Iteration 2800, Loss: 0.6975\n",
      "Iteration 2900, Loss: 0.6951\n",
      "Iteration 3000, Loss: 0.6928\n",
      "Iteration 3100, Loss: 0.6906\n",
      "Iteration 3200, Loss: 0.6885\n",
      "Iteration 3300, Loss: 0.6865\n",
      "Iteration 3400, Loss: 0.6846\n",
      "Iteration 3500, Loss: 0.6827\n",
      "Iteration 3600, Loss: 0.6809\n",
      "Iteration 3700, Loss: 0.6792\n",
      "Iteration 3800, Loss: 0.6776\n",
      "Iteration 3900, Loss: 0.6759\n",
      "Iteration 4000, Loss: 0.6744\n",
      "Iteration 4100, Loss: 0.6729\n",
      "Iteration 4200, Loss: 0.6714\n",
      "Iteration 4300, Loss: 0.6700\n",
      "Iteration 4400, Loss: 0.6687\n",
      "Iteration 4500, Loss: 0.6673\n",
      "Iteration 4600, Loss: 0.6661\n",
      "Iteration 4700, Loss: 0.6648\n",
      "Iteration 4800, Loss: 0.6636\n",
      "Iteration 4900, Loss: 0.6624\n",
      "Iteration 5000, Loss: 0.6613\n",
      "Iteration 5100, Loss: 0.6601\n",
      "Iteration 5200, Loss: 0.6591\n",
      "Iteration 5300, Loss: 0.6580\n",
      "Iteration 5400, Loss: 0.6570\n",
      "Iteration 5500, Loss: 0.6559\n",
      "Iteration 5600, Loss: 0.6550\n",
      "Iteration 5700, Loss: 0.6540\n",
      "Iteration 5800, Loss: 0.6530\n",
      "Iteration 5900, Loss: 0.6521\n",
      "Iteration 6000, Loss: 0.6512\n",
      "Iteration 6100, Loss: 0.6503\n",
      "Iteration 6200, Loss: 0.6495\n",
      "Iteration 6300, Loss: 0.6486\n",
      "Iteration 6400, Loss: 0.6478\n",
      "Iteration 6500, Loss: 0.6470\n",
      "Iteration 6600, Loss: 0.6462\n",
      "Iteration 6700, Loss: 0.6454\n",
      "Iteration 6800, Loss: 0.6446\n",
      "Iteration 6900, Loss: 0.6439\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0204\n",
      "Iteration 200, Loss: 0.9664\n",
      "Iteration 300, Loss: 0.9256\n",
      "Iteration 400, Loss: 0.8936\n",
      "Iteration 500, Loss: 0.8677\n",
      "Iteration 600, Loss: 0.8461\n",
      "Iteration 700, Loss: 0.8280\n",
      "Iteration 800, Loss: 0.8125\n",
      "Iteration 900, Loss: 0.7992\n",
      "Iteration 1000, Loss: 0.7876\n",
      "Iteration 1100, Loss: 0.7774\n",
      "Iteration 1200, Loss: 0.7683\n",
      "Iteration 1300, Loss: 0.7602\n",
      "Iteration 1400, Loss: 0.7529\n",
      "Iteration 1500, Loss: 0.7463\n",
      "Iteration 1600, Loss: 0.7403\n",
      "Iteration 1700, Loss: 0.7348\n",
      "Iteration 1800, Loss: 0.7298\n",
      "Iteration 1900, Loss: 0.7251\n",
      "Iteration 2000, Loss: 0.7208\n",
      "Iteration 2100, Loss: 0.7169\n",
      "Iteration 2200, Loss: 0.7131\n",
      "Iteration 2300, Loss: 0.7096\n",
      "Iteration 2400, Loss: 0.7064\n",
      "Iteration 2500, Loss: 0.7033\n",
      "Iteration 2600, Loss: 0.7004\n",
      "Iteration 2700, Loss: 0.6977\n",
      "Iteration 2800, Loss: 0.6950\n",
      "Iteration 2900, Loss: 0.6926\n",
      "Iteration 3000, Loss: 0.6902\n",
      "Iteration 3100, Loss: 0.6880\n",
      "Iteration 3200, Loss: 0.6858\n",
      "Iteration 3300, Loss: 0.6837\n",
      "Iteration 3400, Loss: 0.6818\n",
      "Iteration 3500, Loss: 0.6798\n",
      "Iteration 3600, Loss: 0.6780\n",
      "Iteration 3700, Loss: 0.6762\n",
      "Iteration 3800, Loss: 0.6745\n",
      "Iteration 3900, Loss: 0.6729\n",
      "Iteration 4000, Loss: 0.6713\n",
      "Iteration 4100, Loss: 0.6698\n",
      "Iteration 4200, Loss: 0.6683\n",
      "Iteration 4300, Loss: 0.6669\n",
      "Iteration 4400, Loss: 0.6655\n",
      "Iteration 4500, Loss: 0.6641\n",
      "Iteration 4600, Loss: 0.6628\n",
      "Iteration 4700, Loss: 0.6615\n",
      "Iteration 4800, Loss: 0.6603\n",
      "Iteration 4900, Loss: 0.6591\n",
      "Iteration 5000, Loss: 0.6579\n",
      "Iteration 5100, Loss: 0.6568\n",
      "Iteration 5200, Loss: 0.6557\n",
      "Iteration 5300, Loss: 0.6546\n",
      "Iteration 5400, Loss: 0.6535\n",
      "Iteration 5500, Loss: 0.6525\n",
      "Iteration 5600, Loss: 0.6515\n",
      "Iteration 5700, Loss: 0.6505\n",
      "Iteration 5800, Loss: 0.6495\n",
      "Iteration 5900, Loss: 0.6485\n",
      "Iteration 6000, Loss: 0.6476\n",
      "Iteration 6100, Loss: 0.6467\n",
      "Iteration 6200, Loss: 0.6458\n",
      "Iteration 6300, Loss: 0.6449\n",
      "Iteration 6400, Loss: 0.6441\n",
      "Iteration 6500, Loss: 0.6432\n",
      "Iteration 6600, Loss: 0.6424\n",
      "Iteration 6700, Loss: 0.6416\n",
      "Iteration 6800, Loss: 0.6408\n",
      "Iteration 6900, Loss: 0.6400\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0540\n",
      "Iteration 200, Loss: 1.0184\n",
      "Iteration 300, Loss: 0.9888\n",
      "Iteration 400, Loss: 0.9637\n",
      "Iteration 500, Loss: 0.9419\n",
      "Iteration 600, Loss: 0.9229\n",
      "Iteration 700, Loss: 0.9060\n",
      "Iteration 800, Loss: 0.8909\n",
      "Iteration 900, Loss: 0.8773\n",
      "Iteration 1000, Loss: 0.8650\n",
      "Iteration 1100, Loss: 0.8539\n",
      "Iteration 1200, Loss: 0.8437\n",
      "Iteration 1300, Loss: 0.8344\n",
      "Iteration 1400, Loss: 0.8258\n",
      "Iteration 1500, Loss: 0.8179\n",
      "Iteration 1600, Loss: 0.8105\n",
      "Iteration 1700, Loss: 0.8037\n",
      "Iteration 1800, Loss: 0.7973\n",
      "Iteration 1900, Loss: 0.7914\n",
      "Iteration 2000, Loss: 0.7858\n",
      "Iteration 2100, Loss: 0.7805\n",
      "Iteration 2200, Loss: 0.7756\n",
      "Iteration 2300, Loss: 0.7710\n",
      "Iteration 2400, Loss: 0.7666\n",
      "Iteration 2500, Loss: 0.7625\n",
      "Iteration 2600, Loss: 0.7586\n",
      "Iteration 2700, Loss: 0.7549\n",
      "Iteration 2800, Loss: 0.7513\n",
      "Iteration 2900, Loss: 0.7480\n",
      "Iteration 3000, Loss: 0.7448\n",
      "Iteration 3100, Loss: 0.7418\n",
      "Iteration 3200, Loss: 0.7389\n",
      "Iteration 3300, Loss: 0.7361\n",
      "Iteration 3400, Loss: 0.7334\n",
      "Iteration 3500, Loss: 0.7308\n",
      "Iteration 3600, Loss: 0.7283\n",
      "Iteration 3700, Loss: 0.7260\n",
      "Iteration 3800, Loss: 0.7237\n",
      "Iteration 3900, Loss: 0.7215\n",
      "Iteration 4000, Loss: 0.7194\n",
      "Iteration 4100, Loss: 0.7173\n",
      "Iteration 4200, Loss: 0.7153\n",
      "Iteration 4300, Loss: 0.7134\n",
      "Iteration 4400, Loss: 0.7116\n",
      "Iteration 4500, Loss: 0.7098\n",
      "Iteration 4600, Loss: 0.7080\n",
      "Iteration 4700, Loss: 0.7064\n",
      "Iteration 4800, Loss: 0.7047\n",
      "Iteration 4900, Loss: 0.7031\n",
      "Iteration 5000, Loss: 0.7016\n",
      "Iteration 5100, Loss: 0.7001\n",
      "Iteration 5200, Loss: 0.6986\n",
      "Iteration 5300, Loss: 0.6972\n",
      "Iteration 5400, Loss: 0.6958\n",
      "Iteration 5500, Loss: 0.6945\n",
      "Iteration 5600, Loss: 0.6931\n",
      "Iteration 5700, Loss: 0.6918\n",
      "Iteration 5800, Loss: 0.6906\n",
      "Iteration 5900, Loss: 0.6893\n",
      "Iteration 6000, Loss: 0.6881\n",
      "Iteration 6100, Loss: 0.6870\n",
      "Iteration 6200, Loss: 0.6858\n",
      "Iteration 6300, Loss: 0.6847\n",
      "Iteration 6400, Loss: 0.6836\n",
      "Iteration 6500, Loss: 0.6825\n",
      "Iteration 6600, Loss: 0.6814\n",
      "Iteration 6700, Loss: 0.6804\n",
      "Iteration 6800, Loss: 0.6794\n",
      "Iteration 6900, Loss: 0.6784\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0532\n",
      "Iteration 200, Loss: 1.0167\n",
      "Iteration 300, Loss: 0.9862\n",
      "Iteration 400, Loss: 0.9602\n",
      "Iteration 500, Loss: 0.9379\n",
      "Iteration 600, Loss: 0.9184\n",
      "Iteration 700, Loss: 0.9011\n",
      "Iteration 800, Loss: 0.8857\n",
      "Iteration 900, Loss: 0.8718\n",
      "Iteration 1000, Loss: 0.8594\n",
      "Iteration 1100, Loss: 0.8481\n",
      "Iteration 1200, Loss: 0.8379\n",
      "Iteration 1300, Loss: 0.8286\n",
      "Iteration 1400, Loss: 0.8200\n",
      "Iteration 1500, Loss: 0.8120\n",
      "Iteration 1600, Loss: 0.8047\n",
      "Iteration 1700, Loss: 0.7978\n",
      "Iteration 1800, Loss: 0.7915\n",
      "Iteration 1900, Loss: 0.7857\n",
      "Iteration 2000, Loss: 0.7802\n",
      "Iteration 2100, Loss: 0.7751\n",
      "Iteration 2200, Loss: 0.7703\n",
      "Iteration 2300, Loss: 0.7657\n",
      "Iteration 2400, Loss: 0.7615\n",
      "Iteration 2500, Loss: 0.7575\n",
      "Iteration 2600, Loss: 0.7538\n",
      "Iteration 2700, Loss: 0.7502\n",
      "Iteration 2800, Loss: 0.7468\n",
      "Iteration 2900, Loss: 0.7436\n",
      "Iteration 3000, Loss: 0.7406\n",
      "Iteration 3100, Loss: 0.7377\n",
      "Iteration 3200, Loss: 0.7349\n",
      "Iteration 3300, Loss: 0.7322\n",
      "Iteration 3400, Loss: 0.7297\n",
      "Iteration 3500, Loss: 0.7273\n",
      "Iteration 3600, Loss: 0.7250\n",
      "Iteration 3700, Loss: 0.7228\n",
      "Iteration 3800, Loss: 0.7207\n",
      "Iteration 3900, Loss: 0.7187\n",
      "Iteration 4000, Loss: 0.7167\n",
      "Iteration 4100, Loss: 0.7148\n",
      "Iteration 4200, Loss: 0.7130\n",
      "Iteration 4300, Loss: 0.7113\n",
      "Iteration 4400, Loss: 0.7096\n",
      "Iteration 4500, Loss: 0.7080\n",
      "Iteration 4600, Loss: 0.7064\n",
      "Iteration 4700, Loss: 0.7048\n",
      "Iteration 4800, Loss: 0.7034\n",
      "Iteration 4900, Loss: 0.7019\n",
      "Iteration 5000, Loss: 0.7005\n",
      "Iteration 5100, Loss: 0.6992\n",
      "Iteration 5200, Loss: 0.6979\n",
      "Iteration 5300, Loss: 0.6966\n",
      "Iteration 5400, Loss: 0.6954\n",
      "Iteration 5500, Loss: 0.6942\n",
      "Iteration 5600, Loss: 0.6930\n",
      "Iteration 5700, Loss: 0.6918\n",
      "Iteration 5800, Loss: 0.6907\n",
      "Iteration 5900, Loss: 0.6896\n",
      "Iteration 6000, Loss: 0.6885\n",
      "Iteration 6100, Loss: 0.6875\n",
      "Iteration 6200, Loss: 0.6865\n",
      "Iteration 6300, Loss: 0.6855\n",
      "Iteration 6400, Loss: 0.6845\n",
      "Iteration 6500, Loss: 0.6836\n",
      "Iteration 6600, Loss: 0.6827\n",
      "Iteration 6700, Loss: 0.6818\n",
      "Iteration 6800, Loss: 0.6809\n",
      "Iteration 6900, Loss: 0.6800\n",
      "Iteration 0, Loss: 0.9695\n",
      "Iteration 100, Loss: 0.6003\n",
      "Iteration 200, Loss: 0.5709\n",
      "Iteration 300, Loss: 0.5538\n",
      "Iteration 400, Loss: 0.5416\n",
      "Iteration 500, Loss: 0.5322\n",
      "Iteration 600, Loss: 0.5245\n",
      "Iteration 700, Loss: 0.5181\n",
      "Iteration 800, Loss: 0.5125\n",
      "Iteration 900, Loss: 0.5077\n",
      "Iteration 1000, Loss: 0.5035\n",
      "Iteration 1100, Loss: 0.4996\n",
      "Iteration 1200, Loss: 0.4962\n",
      "Iteration 1300, Loss: 0.4931\n",
      "Iteration 1400, Loss: 0.4902\n",
      "Iteration 1500, Loss: 0.4875\n",
      "Iteration 1600, Loss: 0.4851\n",
      "Iteration 1700, Loss: 0.4828\n",
      "Iteration 1800, Loss: 0.4806\n",
      "Iteration 1900, Loss: 0.4786\n",
      "Iteration 2000, Loss: 0.4768\n",
      "Iteration 2100, Loss: 0.4750\n",
      "Iteration 2200, Loss: 0.4733\n",
      "Iteration 2300, Loss: 0.4717\n",
      "Iteration 2400, Loss: 0.4702\n",
      "Iteration 2500, Loss: 0.4688\n",
      "Iteration 2600, Loss: 0.4674\n",
      "Iteration 2700, Loss: 0.4661\n",
      "Iteration 2800, Loss: 0.4648\n",
      "Iteration 2900, Loss: 0.4636\n",
      "Iteration 3000, Loss: 0.4625\n",
      "Iteration 3100, Loss: 0.4614\n",
      "Iteration 3200, Loss: 0.4603\n",
      "Iteration 3300, Loss: 0.4593\n",
      "Iteration 3400, Loss: 0.4583\n",
      "Iteration 3500, Loss: 0.4573\n",
      "Iteration 3600, Loss: 0.4564\n",
      "Iteration 3700, Loss: 0.4555\n",
      "Iteration 3800, Loss: 0.4547\n",
      "Iteration 3900, Loss: 0.4538\n",
      "Iteration 4000, Loss: 0.4530\n",
      "Iteration 4100, Loss: 0.4522\n",
      "Iteration 4200, Loss: 0.4515\n",
      "Iteration 4300, Loss: 0.4507\n",
      "Iteration 4400, Loss: 0.4500\n",
      "Iteration 4500, Loss: 0.4493\n",
      "Iteration 4600, Loss: 0.4486\n",
      "Iteration 4700, Loss: 0.4479\n",
      "Iteration 4800, Loss: 0.4473\n",
      "Iteration 4900, Loss: 0.4467\n",
      "Iteration 5000, Loss: 0.4460\n",
      "Iteration 5100, Loss: 0.4454\n",
      "Iteration 5200, Loss: 0.4448\n",
      "Iteration 5300, Loss: 0.4443\n",
      "Iteration 5400, Loss: 0.4437\n",
      "Iteration 5500, Loss: 0.4431\n",
      "Iteration 5600, Loss: 0.4426\n",
      "Iteration 5700, Loss: 0.4421\n",
      "Iteration 5800, Loss: 0.4415\n",
      "Iteration 5900, Loss: 0.4410\n",
      "Iteration 6000, Loss: 0.4405\n",
      "Iteration 6100, Loss: 0.4400\n",
      "Iteration 6200, Loss: 0.4396\n",
      "Iteration 6300, Loss: 0.4391\n",
      "Iteration 6400, Loss: 0.4386\n",
      "Iteration 6500, Loss: 0.4382\n",
      "Iteration 6600, Loss: 0.4377\n",
      "Iteration 6700, Loss: 0.4373\n",
      "Iteration 6800, Loss: 0.4368\n",
      "Iteration 6900, Loss: 0.4364\n",
      "Iteration 7000, Loss: 0.4360\n",
      "Iteration 7100, Loss: 0.4356\n",
      "Iteration 7200, Loss: 0.4352\n",
      "Iteration 7300, Loss: 0.4348\n",
      "Iteration 7400, Loss: 0.4344\n",
      "Iteration 7500, Loss: 0.4340\n",
      "Iteration 7600, Loss: 0.4336\n",
      "Iteration 7700, Loss: 0.4332\n",
      "Iteration 7800, Loss: 0.4329\n",
      "Iteration 7900, Loss: 0.4325\n",
      "Iteration 0, Loss: 0.9661\n",
      "Iteration 100, Loss: 0.5829\n",
      "Iteration 200, Loss: 0.5529\n",
      "Iteration 300, Loss: 0.5364\n",
      "Iteration 400, Loss: 0.5254\n",
      "Iteration 500, Loss: 0.5173\n",
      "Iteration 600, Loss: 0.5108\n",
      "Iteration 700, Loss: 0.5055\n",
      "Iteration 800, Loss: 0.5010\n",
      "Iteration 900, Loss: 0.4971\n",
      "Iteration 1000, Loss: 0.4937\n",
      "Iteration 1100, Loss: 0.4906\n",
      "Iteration 1200, Loss: 0.4878\n",
      "Iteration 1300, Loss: 0.4852\n",
      "Iteration 1400, Loss: 0.4828\n",
      "Iteration 1500, Loss: 0.4806\n",
      "Iteration 1600, Loss: 0.4786\n",
      "Iteration 1700, Loss: 0.4767\n",
      "Iteration 1800, Loss: 0.4748\n",
      "Iteration 1900, Loss: 0.4731\n",
      "Iteration 2000, Loss: 0.4715\n",
      "Iteration 2100, Loss: 0.4700\n",
      "Iteration 2200, Loss: 0.4686\n",
      "Iteration 2300, Loss: 0.4672\n",
      "Iteration 2400, Loss: 0.4658\n",
      "Iteration 2500, Loss: 0.4646\n",
      "Iteration 2600, Loss: 0.4634\n",
      "Iteration 2700, Loss: 0.4622\n",
      "Iteration 2800, Loss: 0.4611\n",
      "Iteration 2900, Loss: 0.4600\n",
      "Iteration 3000, Loss: 0.4589\n",
      "Iteration 3100, Loss: 0.4579\n",
      "Iteration 3200, Loss: 0.4569\n",
      "Iteration 3300, Loss: 0.4560\n",
      "Iteration 3400, Loss: 0.4551\n",
      "Iteration 3500, Loss: 0.4542\n",
      "Iteration 3600, Loss: 0.4533\n",
      "Iteration 3700, Loss: 0.4525\n",
      "Iteration 3800, Loss: 0.4517\n",
      "Iteration 3900, Loss: 0.4509\n",
      "Iteration 4000, Loss: 0.4501\n",
      "Iteration 4100, Loss: 0.4494\n",
      "Iteration 4200, Loss: 0.4487\n",
      "Iteration 4300, Loss: 0.4480\n",
      "Iteration 4400, Loss: 0.4473\n",
      "Iteration 4500, Loss: 0.4466\n",
      "Iteration 4600, Loss: 0.4460\n",
      "Iteration 4700, Loss: 0.4453\n",
      "Iteration 4800, Loss: 0.4447\n",
      "Iteration 4900, Loss: 0.4441\n",
      "Iteration 5000, Loss: 0.4435\n",
      "Iteration 5100, Loss: 0.4429\n",
      "Iteration 5200, Loss: 0.4423\n",
      "Iteration 5300, Loss: 0.4417\n",
      "Iteration 5400, Loss: 0.4412\n",
      "Iteration 5500, Loss: 0.4407\n",
      "Iteration 5600, Loss: 0.4401\n",
      "Iteration 5700, Loss: 0.4396\n",
      "Iteration 5800, Loss: 0.4391\n",
      "Iteration 5900, Loss: 0.4386\n",
      "Iteration 6000, Loss: 0.4381\n",
      "Iteration 6100, Loss: 0.4376\n",
      "Iteration 6200, Loss: 0.4372\n",
      "Iteration 6300, Loss: 0.4367\n",
      "Iteration 6400, Loss: 0.4362\n",
      "Iteration 6500, Loss: 0.4358\n",
      "Iteration 6600, Loss: 0.4353\n",
      "Iteration 6700, Loss: 0.4349\n",
      "Iteration 6800, Loss: 0.4345\n",
      "Iteration 6900, Loss: 0.4341\n",
      "Iteration 7000, Loss: 0.4336\n",
      "Iteration 7100, Loss: 0.4332\n",
      "Iteration 7200, Loss: 0.4328\n",
      "Iteration 7300, Loss: 0.4324\n",
      "Iteration 7400, Loss: 0.4320\n",
      "Iteration 7500, Loss: 0.4317\n",
      "Iteration 7600, Loss: 0.4313\n",
      "Iteration 7700, Loss: 0.4309\n",
      "Iteration 7800, Loss: 0.4305\n",
      "Iteration 7900, Loss: 0.4302\n",
      "Iteration 0, Loss: 1.0165\n",
      "Iteration 100, Loss: 0.6192\n",
      "Iteration 200, Loss: 0.5886\n",
      "Iteration 300, Loss: 0.5711\n",
      "Iteration 400, Loss: 0.5589\n",
      "Iteration 500, Loss: 0.5495\n",
      "Iteration 600, Loss: 0.5420\n",
      "Iteration 700, Loss: 0.5357\n",
      "Iteration 800, Loss: 0.5302\n",
      "Iteration 900, Loss: 0.5255\n",
      "Iteration 1000, Loss: 0.5213\n",
      "Iteration 1100, Loss: 0.5176\n",
      "Iteration 1200, Loss: 0.5142\n",
      "Iteration 1300, Loss: 0.5111\n",
      "Iteration 1400, Loss: 0.5082\n",
      "Iteration 1500, Loss: 0.5056\n",
      "Iteration 1600, Loss: 0.5032\n",
      "Iteration 1700, Loss: 0.5009\n",
      "Iteration 1800, Loss: 0.4988\n",
      "Iteration 1900, Loss: 0.4967\n",
      "Iteration 2000, Loss: 0.4948\n",
      "Iteration 2100, Loss: 0.4931\n",
      "Iteration 2200, Loss: 0.4913\n",
      "Iteration 2300, Loss: 0.4897\n",
      "Iteration 2400, Loss: 0.4882\n",
      "Iteration 2500, Loss: 0.4867\n",
      "Iteration 2600, Loss: 0.4853\n",
      "Iteration 2700, Loss: 0.4839\n",
      "Iteration 2800, Loss: 0.4826\n",
      "Iteration 2900, Loss: 0.4814\n",
      "Iteration 3000, Loss: 0.4802\n",
      "Iteration 3100, Loss: 0.4791\n",
      "Iteration 3200, Loss: 0.4779\n",
      "Iteration 3300, Loss: 0.4769\n",
      "Iteration 3400, Loss: 0.4758\n",
      "Iteration 3500, Loss: 0.4748\n",
      "Iteration 3600, Loss: 0.4739\n",
      "Iteration 3700, Loss: 0.4729\n",
      "Iteration 3800, Loss: 0.4720\n",
      "Iteration 3900, Loss: 0.4711\n",
      "Iteration 4000, Loss: 0.4703\n",
      "Iteration 4100, Loss: 0.4694\n",
      "Iteration 4200, Loss: 0.4686\n",
      "Iteration 4300, Loss: 0.4678\n",
      "Iteration 4400, Loss: 0.4671\n",
      "Iteration 4500, Loss: 0.4663\n",
      "Iteration 4600, Loss: 0.4656\n",
      "Iteration 4700, Loss: 0.4649\n",
      "Iteration 4800, Loss: 0.4642\n",
      "Iteration 4900, Loss: 0.4635\n",
      "Iteration 5000, Loss: 0.4628\n",
      "Iteration 5100, Loss: 0.4622\n",
      "Iteration 5200, Loss: 0.4615\n",
      "Iteration 5300, Loss: 0.4609\n",
      "Iteration 5400, Loss: 0.4603\n",
      "Iteration 5500, Loss: 0.4597\n",
      "Iteration 5600, Loss: 0.4591\n",
      "Iteration 5700, Loss: 0.4585\n",
      "Iteration 5800, Loss: 0.4580\n",
      "Iteration 5900, Loss: 0.4574\n",
      "Iteration 6000, Loss: 0.4569\n",
      "Iteration 6100, Loss: 0.4563\n",
      "Iteration 6200, Loss: 0.4558\n",
      "Iteration 6300, Loss: 0.4553\n",
      "Iteration 6400, Loss: 0.4548\n",
      "Iteration 6500, Loss: 0.4543\n",
      "Iteration 6600, Loss: 0.4538\n",
      "Iteration 6700, Loss: 0.4534\n",
      "Iteration 6800, Loss: 0.4529\n",
      "Iteration 6900, Loss: 0.4524\n",
      "Iteration 7000, Loss: 0.4520\n",
      "Iteration 7100, Loss: 0.4515\n",
      "Iteration 7200, Loss: 0.4511\n",
      "Iteration 7300, Loss: 0.4506\n",
      "Iteration 7400, Loss: 0.4502\n",
      "Iteration 7500, Loss: 0.4498\n",
      "Iteration 7600, Loss: 0.4494\n",
      "Iteration 7700, Loss: 0.4490\n",
      "Iteration 7800, Loss: 0.4486\n",
      "Iteration 7900, Loss: 0.4482\n",
      "Iteration 0, Loss: 1.0187\n",
      "Iteration 100, Loss: 0.6258\n",
      "Iteration 200, Loss: 0.5940\n",
      "Iteration 300, Loss: 0.5768\n",
      "Iteration 400, Loss: 0.5653\n",
      "Iteration 500, Loss: 0.5566\n",
      "Iteration 600, Loss: 0.5497\n",
      "Iteration 700, Loss: 0.5440\n",
      "Iteration 800, Loss: 0.5392\n",
      "Iteration 900, Loss: 0.5350\n",
      "Iteration 1000, Loss: 0.5312\n",
      "Iteration 1100, Loss: 0.5279\n",
      "Iteration 1200, Loss: 0.5249\n",
      "Iteration 1300, Loss: 0.5221\n",
      "Iteration 1400, Loss: 0.5196\n",
      "Iteration 1500, Loss: 0.5172\n",
      "Iteration 1600, Loss: 0.5151\n",
      "Iteration 1700, Loss: 0.5130\n",
      "Iteration 1800, Loss: 0.5111\n",
      "Iteration 1900, Loss: 0.5093\n",
      "Iteration 2000, Loss: 0.5076\n",
      "Iteration 2100, Loss: 0.5060\n",
      "Iteration 2200, Loss: 0.5045\n",
      "Iteration 2300, Loss: 0.5030\n",
      "Iteration 2400, Loss: 0.5016\n",
      "Iteration 2500, Loss: 0.5003\n",
      "Iteration 2600, Loss: 0.4990\n",
      "Iteration 2700, Loss: 0.4977\n",
      "Iteration 2800, Loss: 0.4966\n",
      "Iteration 2900, Loss: 0.4954\n",
      "Iteration 3000, Loss: 0.4943\n",
      "Iteration 3100, Loss: 0.4933\n",
      "Iteration 3200, Loss: 0.4922\n",
      "Iteration 3300, Loss: 0.4912\n",
      "Iteration 3400, Loss: 0.4903\n",
      "Iteration 3500, Loss: 0.4893\n",
      "Iteration 3600, Loss: 0.4884\n",
      "Iteration 3700, Loss: 0.4875\n",
      "Iteration 3800, Loss: 0.4867\n",
      "Iteration 3900, Loss: 0.4859\n",
      "Iteration 4000, Loss: 0.4851\n",
      "Iteration 4100, Loss: 0.4843\n",
      "Iteration 4200, Loss: 0.4835\n",
      "Iteration 4300, Loss: 0.4827\n",
      "Iteration 4400, Loss: 0.4820\n",
      "Iteration 4500, Loss: 0.4813\n",
      "Iteration 4600, Loss: 0.4806\n",
      "Iteration 4700, Loss: 0.4799\n",
      "Iteration 4800, Loss: 0.4792\n",
      "Iteration 4900, Loss: 0.4786\n",
      "Iteration 5000, Loss: 0.4779\n",
      "Iteration 5100, Loss: 0.4773\n",
      "Iteration 5200, Loss: 0.4767\n",
      "Iteration 5300, Loss: 0.4761\n",
      "Iteration 5400, Loss: 0.4755\n",
      "Iteration 5500, Loss: 0.4749\n",
      "Iteration 5600, Loss: 0.4743\n",
      "Iteration 5700, Loss: 0.4738\n",
      "Iteration 5800, Loss: 0.4732\n",
      "Iteration 5900, Loss: 0.4727\n",
      "Iteration 6000, Loss: 0.4722\n",
      "Iteration 6100, Loss: 0.4716\n",
      "Iteration 6200, Loss: 0.4711\n",
      "Iteration 6300, Loss: 0.4706\n",
      "Iteration 6400, Loss: 0.4701\n",
      "Iteration 6500, Loss: 0.4696\n",
      "Iteration 6600, Loss: 0.4692\n",
      "Iteration 6700, Loss: 0.4687\n",
      "Iteration 6800, Loss: 0.4682\n",
      "Iteration 6900, Loss: 0.4678\n",
      "Iteration 7000, Loss: 0.4673\n",
      "Iteration 7100, Loss: 0.4669\n",
      "Iteration 7200, Loss: 0.4664\n",
      "Iteration 7300, Loss: 0.4660\n",
      "Iteration 7400, Loss: 0.4656\n",
      "Iteration 7500, Loss: 0.4652\n",
      "Iteration 7600, Loss: 0.4648\n",
      "Iteration 7700, Loss: 0.4643\n",
      "Iteration 7800, Loss: 0.4639\n",
      "Iteration 7900, Loss: 0.4636\n",
      "Iteration 0, Loss: 1.0525\n",
      "Iteration 100, Loss: 0.6392\n",
      "Iteration 200, Loss: 0.6058\n",
      "Iteration 300, Loss: 0.5894\n",
      "Iteration 400, Loss: 0.5786\n",
      "Iteration 500, Loss: 0.5704\n",
      "Iteration 600, Loss: 0.5638\n",
      "Iteration 700, Loss: 0.5581\n",
      "Iteration 800, Loss: 0.5533\n",
      "Iteration 900, Loss: 0.5490\n",
      "Iteration 1000, Loss: 0.5452\n",
      "Iteration 1100, Loss: 0.5418\n",
      "Iteration 1200, Loss: 0.5387\n",
      "Iteration 1300, Loss: 0.5357\n",
      "Iteration 1400, Loss: 0.5331\n",
      "Iteration 1500, Loss: 0.5306\n",
      "Iteration 1600, Loss: 0.5282\n",
      "Iteration 1700, Loss: 0.5261\n",
      "Iteration 1800, Loss: 0.5239\n",
      "Iteration 1900, Loss: 0.5220\n",
      "Iteration 2000, Loss: 0.5201\n",
      "Iteration 2100, Loss: 0.5184\n",
      "Iteration 2200, Loss: 0.5167\n",
      "Iteration 2300, Loss: 0.5150\n",
      "Iteration 2400, Loss: 0.5135\n",
      "Iteration 2500, Loss: 0.5120\n",
      "Iteration 2600, Loss: 0.5106\n",
      "Iteration 2700, Loss: 0.5092\n",
      "Iteration 2800, Loss: 0.5079\n",
      "Iteration 2900, Loss: 0.5066\n",
      "Iteration 3000, Loss: 0.5054\n",
      "Iteration 3100, Loss: 0.5042\n",
      "Iteration 3200, Loss: 0.5030\n",
      "Iteration 3300, Loss: 0.5019\n",
      "Iteration 3400, Loss: 0.5008\n",
      "Iteration 3500, Loss: 0.4997\n",
      "Iteration 3600, Loss: 0.4987\n",
      "Iteration 3700, Loss: 0.4977\n",
      "Iteration 3800, Loss: 0.4968\n",
      "Iteration 3900, Loss: 0.4957\n",
      "Iteration 4000, Loss: 0.4948\n",
      "Iteration 4100, Loss: 0.4939\n",
      "Iteration 4200, Loss: 0.4930\n",
      "Iteration 4300, Loss: 0.4922\n",
      "Iteration 4400, Loss: 0.4913\n",
      "Iteration 4500, Loss: 0.4905\n",
      "Iteration 4600, Loss: 0.4897\n",
      "Iteration 4700, Loss: 0.4889\n",
      "Iteration 4800, Loss: 0.4881\n",
      "Iteration 4900, Loss: 0.4874\n",
      "Iteration 5000, Loss: 0.4867\n",
      "Iteration 5100, Loss: 0.4859\n",
      "Iteration 5200, Loss: 0.4852\n",
      "Iteration 5300, Loss: 0.4845\n",
      "Iteration 5400, Loss: 0.4838\n",
      "Iteration 5500, Loss: 0.4831\n",
      "Iteration 5600, Loss: 0.4825\n",
      "Iteration 5700, Loss: 0.4819\n",
      "Iteration 5800, Loss: 0.4812\n",
      "Iteration 5900, Loss: 0.4806\n",
      "Iteration 6000, Loss: 0.4800\n",
      "Iteration 6100, Loss: 0.4794\n",
      "Iteration 6200, Loss: 0.4788\n",
      "Iteration 6300, Loss: 0.4782\n",
      "Iteration 6400, Loss: 0.4777\n",
      "Iteration 6500, Loss: 0.4771\n",
      "Iteration 6600, Loss: 0.4765\n",
      "Iteration 6700, Loss: 0.4760\n",
      "Iteration 6800, Loss: 0.4754\n",
      "Iteration 6900, Loss: 0.4749\n",
      "Iteration 7000, Loss: 0.4744\n",
      "Iteration 7100, Loss: 0.4739\n",
      "Iteration 7200, Loss: 0.4734\n",
      "Iteration 7300, Loss: 0.4729\n",
      "Iteration 7400, Loss: 0.4724\n",
      "Iteration 7500, Loss: 0.4719\n",
      "Iteration 7600, Loss: 0.4715\n",
      "Iteration 7700, Loss: 0.4710\n",
      "Iteration 7800, Loss: 0.4705\n",
      "Iteration 7900, Loss: 0.4701\n",
      "Iteration 0, Loss: 1.0563\n",
      "Iteration 100, Loss: 0.6718\n",
      "Iteration 200, Loss: 0.6348\n",
      "Iteration 300, Loss: 0.6140\n",
      "Iteration 400, Loss: 0.5997\n",
      "Iteration 500, Loss: 0.5889\n",
      "Iteration 600, Loss: 0.5800\n",
      "Iteration 700, Loss: 0.5729\n",
      "Iteration 800, Loss: 0.5668\n",
      "Iteration 900, Loss: 0.5615\n",
      "Iteration 1000, Loss: 0.5567\n",
      "Iteration 1100, Loss: 0.5525\n",
      "Iteration 1200, Loss: 0.5487\n",
      "Iteration 1300, Loss: 0.5453\n",
      "Iteration 1400, Loss: 0.5421\n",
      "Iteration 1500, Loss: 0.5391\n",
      "Iteration 1600, Loss: 0.5364\n",
      "Iteration 1700, Loss: 0.5339\n",
      "Iteration 1800, Loss: 0.5315\n",
      "Iteration 1900, Loss: 0.5293\n",
      "Iteration 2000, Loss: 0.5271\n",
      "Iteration 2100, Loss: 0.5251\n",
      "Iteration 2200, Loss: 0.5232\n",
      "Iteration 2300, Loss: 0.5215\n",
      "Iteration 2400, Loss: 0.5197\n",
      "Iteration 2500, Loss: 0.5181\n",
      "Iteration 2600, Loss: 0.5166\n",
      "Iteration 2700, Loss: 0.5150\n",
      "Iteration 2800, Loss: 0.5136\n",
      "Iteration 2900, Loss: 0.5122\n",
      "Iteration 3000, Loss: 0.5109\n",
      "Iteration 3100, Loss: 0.5096\n",
      "Iteration 3200, Loss: 0.5083\n",
      "Iteration 3300, Loss: 0.5071\n",
      "Iteration 3400, Loss: 0.5060\n",
      "Iteration 3500, Loss: 0.5049\n",
      "Iteration 3600, Loss: 0.5038\n",
      "Iteration 3700, Loss: 0.5028\n",
      "Iteration 3800, Loss: 0.5018\n",
      "Iteration 3900, Loss: 0.5007\n",
      "Iteration 4000, Loss: 0.4998\n",
      "Iteration 4100, Loss: 0.4988\n",
      "Iteration 4200, Loss: 0.4980\n",
      "Iteration 4300, Loss: 0.4970\n",
      "Iteration 4400, Loss: 0.4961\n",
      "Iteration 4500, Loss: 0.4953\n",
      "Iteration 4600, Loss: 0.4945\n",
      "Iteration 4700, Loss: 0.4937\n",
      "Iteration 4800, Loss: 0.4930\n",
      "Iteration 4900, Loss: 0.4922\n",
      "Iteration 5000, Loss: 0.4914\n",
      "Iteration 5100, Loss: 0.4907\n",
      "Iteration 5200, Loss: 0.4900\n",
      "Iteration 5300, Loss: 0.4893\n",
      "Iteration 5400, Loss: 0.4886\n",
      "Iteration 5500, Loss: 0.4879\n",
      "Iteration 5600, Loss: 0.4873\n",
      "Iteration 5700, Loss: 0.4866\n",
      "Iteration 5800, Loss: 0.4860\n",
      "Iteration 5900, Loss: 0.4854\n",
      "Iteration 6000, Loss: 0.4849\n",
      "Iteration 6100, Loss: 0.4843\n",
      "Iteration 6200, Loss: 0.4837\n",
      "Iteration 6300, Loss: 0.4831\n",
      "Iteration 6400, Loss: 0.4825\n",
      "Iteration 6500, Loss: 0.4820\n",
      "Iteration 6600, Loss: 0.4815\n",
      "Iteration 6700, Loss: 0.4809\n",
      "Iteration 6800, Loss: 0.4805\n",
      "Iteration 6900, Loss: 0.4799\n",
      "Iteration 7000, Loss: 0.4794\n",
      "Iteration 7100, Loss: 0.4789\n",
      "Iteration 7200, Loss: 0.4784\n",
      "Iteration 7300, Loss: 0.4779\n",
      "Iteration 7400, Loss: 0.4775\n",
      "Iteration 7500, Loss: 0.4770\n",
      "Iteration 7600, Loss: 0.4766\n",
      "Iteration 7700, Loss: 0.4761\n",
      "Iteration 7800, Loss: 0.4757\n",
      "Iteration 7900, Loss: 0.4752\n",
      "Iteration 0, Loss: 1.0813\n",
      "Iteration 100, Loss: 0.7406\n",
      "Iteration 200, Loss: 0.6949\n",
      "Iteration 300, Loss: 0.6728\n",
      "Iteration 400, Loss: 0.6583\n",
      "Iteration 500, Loss: 0.6475\n",
      "Iteration 600, Loss: 0.6389\n",
      "Iteration 700, Loss: 0.6316\n",
      "Iteration 800, Loss: 0.6254\n",
      "Iteration 900, Loss: 0.6199\n",
      "Iteration 1000, Loss: 0.6150\n",
      "Iteration 1100, Loss: 0.6106\n",
      "Iteration 1200, Loss: 0.6066\n",
      "Iteration 1300, Loss: 0.6029\n",
      "Iteration 1400, Loss: 0.5995\n",
      "Iteration 1500, Loss: 0.5964\n",
      "Iteration 1600, Loss: 0.5935\n",
      "Iteration 1700, Loss: 0.5908\n",
      "Iteration 1800, Loss: 0.5882\n",
      "Iteration 1900, Loss: 0.5859\n",
      "Iteration 2000, Loss: 0.5836\n",
      "Iteration 2100, Loss: 0.5815\n",
      "Iteration 2200, Loss: 0.5795\n",
      "Iteration 2300, Loss: 0.5775\n",
      "Iteration 2400, Loss: 0.5757\n",
      "Iteration 2500, Loss: 0.5740\n",
      "Iteration 2600, Loss: 0.5723\n",
      "Iteration 2700, Loss: 0.5707\n",
      "Iteration 2800, Loss: 0.5692\n",
      "Iteration 2900, Loss: 0.5677\n",
      "Iteration 3000, Loss: 0.5663\n",
      "Iteration 3100, Loss: 0.5649\n",
      "Iteration 3200, Loss: 0.5636\n",
      "Iteration 3300, Loss: 0.5623\n",
      "Iteration 3400, Loss: 0.5611\n",
      "Iteration 3500, Loss: 0.5599\n",
      "Iteration 3600, Loss: 0.5588\n",
      "Iteration 3700, Loss: 0.5577\n",
      "Iteration 3800, Loss: 0.5566\n",
      "Iteration 3900, Loss: 0.5556\n",
      "Iteration 4000, Loss: 0.5545\n",
      "Iteration 4100, Loss: 0.5535\n",
      "Iteration 4200, Loss: 0.5526\n",
      "Iteration 4300, Loss: 0.5516\n",
      "Iteration 4400, Loss: 0.5507\n",
      "Iteration 4500, Loss: 0.5498\n",
      "Iteration 4600, Loss: 0.5490\n",
      "Iteration 4700, Loss: 0.5481\n",
      "Iteration 4800, Loss: 0.5473\n",
      "Iteration 4900, Loss: 0.5465\n",
      "Iteration 5000, Loss: 0.5457\n",
      "Iteration 5100, Loss: 0.5449\n",
      "Iteration 5200, Loss: 0.5442\n",
      "Iteration 5300, Loss: 0.5434\n",
      "Iteration 5400, Loss: 0.5427\n",
      "Iteration 5500, Loss: 0.5420\n",
      "Iteration 5600, Loss: 0.5413\n",
      "Iteration 5700, Loss: 0.5406\n",
      "Iteration 5800, Loss: 0.5400\n",
      "Iteration 5900, Loss: 0.5393\n",
      "Iteration 6000, Loss: 0.5387\n",
      "Iteration 6100, Loss: 0.5380\n",
      "Iteration 6200, Loss: 0.5374\n",
      "Iteration 6300, Loss: 0.5368\n",
      "Iteration 6400, Loss: 0.5362\n",
      "Iteration 6500, Loss: 0.5356\n",
      "Iteration 6600, Loss: 0.5350\n",
      "Iteration 6700, Loss: 0.5345\n",
      "Iteration 6800, Loss: 0.5339\n",
      "Iteration 6900, Loss: 0.5334\n",
      "Iteration 7000, Loss: 0.5328\n",
      "Iteration 7100, Loss: 0.5323\n",
      "Iteration 7200, Loss: 0.5318\n",
      "Iteration 7300, Loss: 0.5312\n",
      "Iteration 7400, Loss: 0.5307\n",
      "Iteration 7500, Loss: 0.5302\n",
      "Iteration 7600, Loss: 0.5297\n",
      "Iteration 7700, Loss: 0.5293\n",
      "Iteration 7800, Loss: 0.5288\n",
      "Iteration 7900, Loss: 0.5283\n",
      "Iteration 0, Loss: 1.0799\n",
      "Iteration 100, Loss: 0.7022\n",
      "Iteration 200, Loss: 0.6497\n",
      "Iteration 300, Loss: 0.6245\n",
      "Iteration 400, Loss: 0.6086\n",
      "Iteration 500, Loss: 0.5973\n",
      "Iteration 600, Loss: 0.5886\n",
      "Iteration 700, Loss: 0.5814\n",
      "Iteration 800, Loss: 0.5754\n",
      "Iteration 900, Loss: 0.5701\n",
      "Iteration 1000, Loss: 0.5655\n",
      "Iteration 1100, Loss: 0.5613\n",
      "Iteration 1200, Loss: 0.5575\n",
      "Iteration 1300, Loss: 0.5540\n",
      "Iteration 1400, Loss: 0.5508\n",
      "Iteration 1500, Loss: 0.5478\n",
      "Iteration 1600, Loss: 0.5450\n",
      "Iteration 1700, Loss: 0.5424\n",
      "Iteration 1800, Loss: 0.5399\n",
      "Iteration 1900, Loss: 0.5376\n",
      "Iteration 2000, Loss: 0.5354\n",
      "Iteration 2100, Loss: 0.5333\n",
      "Iteration 2200, Loss: 0.5314\n",
      "Iteration 2300, Loss: 0.5295\n",
      "Iteration 2400, Loss: 0.5277\n",
      "Iteration 2500, Loss: 0.5259\n",
      "Iteration 2600, Loss: 0.5243\n",
      "Iteration 2700, Loss: 0.5227\n",
      "Iteration 2800, Loss: 0.5212\n",
      "Iteration 2900, Loss: 0.5197\n",
      "Iteration 3000, Loss: 0.5183\n",
      "Iteration 3100, Loss: 0.5170\n",
      "Iteration 3200, Loss: 0.5157\n",
      "Iteration 3300, Loss: 0.5144\n",
      "Iteration 3400, Loss: 0.5132\n",
      "Iteration 3500, Loss: 0.5120\n",
      "Iteration 3600, Loss: 0.5109\n",
      "Iteration 3700, Loss: 0.5098\n",
      "Iteration 3800, Loss: 0.5087\n",
      "Iteration 3900, Loss: 0.5077\n",
      "Iteration 4000, Loss: 0.5067\n",
      "Iteration 4100, Loss: 0.5057\n",
      "Iteration 4200, Loss: 0.5047\n",
      "Iteration 4300, Loss: 0.5038\n",
      "Iteration 4400, Loss: 0.5029\n",
      "Iteration 4500, Loss: 0.5020\n",
      "Iteration 4600, Loss: 0.5011\n",
      "Iteration 4700, Loss: 0.5003\n",
      "Iteration 4800, Loss: 0.4995\n",
      "Iteration 4900, Loss: 0.4987\n",
      "Iteration 5000, Loss: 0.4979\n",
      "Iteration 5100, Loss: 0.4971\n",
      "Iteration 5200, Loss: 0.4964\n",
      "Iteration 5300, Loss: 0.4957\n",
      "Iteration 5400, Loss: 0.4949\n",
      "Iteration 5500, Loss: 0.4942\n",
      "Iteration 5600, Loss: 0.4936\n",
      "Iteration 5700, Loss: 0.4929\n",
      "Iteration 5800, Loss: 0.4922\n",
      "Iteration 5900, Loss: 0.4916\n",
      "Iteration 6000, Loss: 0.4910\n",
      "Iteration 6100, Loss: 0.4903\n",
      "Iteration 6200, Loss: 0.4897\n",
      "Iteration 6300, Loss: 0.4891\n",
      "Iteration 6400, Loss: 0.4885\n",
      "Iteration 6500, Loss: 0.4880\n",
      "Iteration 6600, Loss: 0.4874\n",
      "Iteration 6700, Loss: 0.4868\n",
      "Iteration 6800, Loss: 0.4863\n",
      "Iteration 6900, Loss: 0.4858\n",
      "Iteration 7000, Loss: 0.4852\n",
      "Iteration 7100, Loss: 0.4847\n",
      "Iteration 7200, Loss: 0.4842\n",
      "Iteration 7300, Loss: 0.4837\n",
      "Iteration 7400, Loss: 0.4832\n",
      "Iteration 7500, Loss: 0.4827\n",
      "Iteration 7600, Loss: 0.4822\n",
      "Iteration 7700, Loss: 0.4818\n",
      "Iteration 7800, Loss: 0.4813\n",
      "Iteration 7900, Loss: 0.4808\n",
      "Iteration 0, Loss: 1.0894\n",
      "Iteration 100, Loss: 0.7880\n",
      "Iteration 200, Loss: 0.7198\n",
      "Iteration 300, Loss: 0.6879\n",
      "Iteration 400, Loss: 0.6681\n",
      "Iteration 500, Loss: 0.6539\n",
      "Iteration 600, Loss: 0.6429\n",
      "Iteration 700, Loss: 0.6341\n",
      "Iteration 800, Loss: 0.6268\n",
      "Iteration 900, Loss: 0.6205\n",
      "Iteration 1000, Loss: 0.6151\n",
      "Iteration 1100, Loss: 0.6102\n",
      "Iteration 1200, Loss: 0.6059\n",
      "Iteration 1300, Loss: 0.6020\n",
      "Iteration 1400, Loss: 0.5984\n",
      "Iteration 1500, Loss: 0.5951\n",
      "Iteration 1600, Loss: 0.5920\n",
      "Iteration 1700, Loss: 0.5892\n",
      "Iteration 1800, Loss: 0.5865\n",
      "Iteration 1900, Loss: 0.5840\n",
      "Iteration 2000, Loss: 0.5816\n",
      "Iteration 2100, Loss: 0.5793\n",
      "Iteration 2200, Loss: 0.5772\n",
      "Iteration 2300, Loss: 0.5752\n",
      "Iteration 2400, Loss: 0.5732\n",
      "Iteration 2500, Loss: 0.5714\n",
      "Iteration 2600, Loss: 0.5696\n",
      "Iteration 2700, Loss: 0.5679\n",
      "Iteration 2800, Loss: 0.5663\n",
      "Iteration 2900, Loss: 0.5647\n",
      "Iteration 3000, Loss: 0.5632\n",
      "Iteration 3100, Loss: 0.5618\n",
      "Iteration 3200, Loss: 0.5604\n",
      "Iteration 3300, Loss: 0.5590\n",
      "Iteration 3400, Loss: 0.5577\n",
      "Iteration 3500, Loss: 0.5564\n",
      "Iteration 3600, Loss: 0.5552\n",
      "Iteration 3700, Loss: 0.5540\n",
      "Iteration 3800, Loss: 0.5529\n",
      "Iteration 3900, Loss: 0.5517\n",
      "Iteration 4000, Loss: 0.5506\n",
      "Iteration 4100, Loss: 0.5496\n",
      "Iteration 4200, Loss: 0.5486\n",
      "Iteration 4300, Loss: 0.5476\n",
      "Iteration 4400, Loss: 0.5466\n",
      "Iteration 4500, Loss: 0.5456\n",
      "Iteration 4600, Loss: 0.5447\n",
      "Iteration 4700, Loss: 0.5438\n",
      "Iteration 4800, Loss: 0.5429\n",
      "Iteration 4900, Loss: 0.5421\n",
      "Iteration 5000, Loss: 0.5412\n",
      "Iteration 5100, Loss: 0.5404\n",
      "Iteration 5200, Loss: 0.5396\n",
      "Iteration 5300, Loss: 0.5388\n",
      "Iteration 5400, Loss: 0.5380\n",
      "Iteration 5500, Loss: 0.5373\n",
      "Iteration 5600, Loss: 0.5365\n",
      "Iteration 5700, Loss: 0.5358\n",
      "Iteration 5800, Loss: 0.5351\n",
      "Iteration 5900, Loss: 0.5344\n",
      "Iteration 6000, Loss: 0.5337\n",
      "Iteration 6100, Loss: 0.5331\n",
      "Iteration 6200, Loss: 0.5324\n",
      "Iteration 6300, Loss: 0.5318\n",
      "Iteration 6400, Loss: 0.5312\n",
      "Iteration 6500, Loss: 0.5305\n",
      "Iteration 6600, Loss: 0.5299\n",
      "Iteration 6700, Loss: 0.5293\n",
      "Iteration 6800, Loss: 0.5287\n",
      "Iteration 6900, Loss: 0.5282\n",
      "Iteration 7000, Loss: 0.5276\n",
      "Iteration 7100, Loss: 0.5270\n",
      "Iteration 7200, Loss: 0.5265\n",
      "Iteration 7300, Loss: 0.5259\n",
      "Iteration 7400, Loss: 0.5254\n",
      "Iteration 7500, Loss: 0.5249\n",
      "Iteration 7600, Loss: 0.5244\n",
      "Iteration 7700, Loss: 0.5239\n",
      "Iteration 7800, Loss: 0.5233\n",
      "Iteration 7900, Loss: 0.5229\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7855\n",
      "Iteration 200, Loss: 0.7234\n",
      "Iteration 300, Loss: 0.6954\n",
      "Iteration 400, Loss: 0.6783\n",
      "Iteration 500, Loss: 0.6663\n",
      "Iteration 600, Loss: 0.6571\n",
      "Iteration 700, Loss: 0.6497\n",
      "Iteration 800, Loss: 0.6435\n",
      "Iteration 900, Loss: 0.6382\n",
      "Iteration 1000, Loss: 0.6336\n",
      "Iteration 1100, Loss: 0.6294\n",
      "Iteration 1200, Loss: 0.6257\n",
      "Iteration 1300, Loss: 0.6224\n",
      "Iteration 1400, Loss: 0.6192\n",
      "Iteration 1500, Loss: 0.6163\n",
      "Iteration 1600, Loss: 0.6136\n",
      "Iteration 1700, Loss: 0.6111\n",
      "Iteration 1800, Loss: 0.6087\n",
      "Iteration 1900, Loss: 0.6064\n",
      "Iteration 2000, Loss: 0.6043\n",
      "Iteration 2100, Loss: 0.6023\n",
      "Iteration 2200, Loss: 0.6003\n",
      "Iteration 2300, Loss: 0.5985\n",
      "Iteration 2400, Loss: 0.5967\n",
      "Iteration 2500, Loss: 0.5950\n",
      "Iteration 2600, Loss: 0.5934\n",
      "Iteration 2700, Loss: 0.5918\n",
      "Iteration 2800, Loss: 0.5903\n",
      "Iteration 2900, Loss: 0.5888\n",
      "Iteration 3000, Loss: 0.5874\n",
      "Iteration 3100, Loss: 0.5860\n",
      "Iteration 3200, Loss: 0.5847\n",
      "Iteration 3300, Loss: 0.5834\n",
      "Iteration 3400, Loss: 0.5822\n",
      "Iteration 3500, Loss: 0.5810\n",
      "Iteration 3600, Loss: 0.5798\n",
      "Iteration 3700, Loss: 0.5787\n",
      "Iteration 3800, Loss: 0.5776\n",
      "Iteration 3900, Loss: 0.5765\n",
      "Iteration 4000, Loss: 0.5754\n",
      "Iteration 4100, Loss: 0.5744\n",
      "Iteration 4200, Loss: 0.5734\n",
      "Iteration 4300, Loss: 0.5724\n",
      "Iteration 4400, Loss: 0.5715\n",
      "Iteration 4500, Loss: 0.5705\n",
      "Iteration 4600, Loss: 0.5696\n",
      "Iteration 4700, Loss: 0.5687\n",
      "Iteration 4800, Loss: 0.5679\n",
      "Iteration 4900, Loss: 0.5670\n",
      "Iteration 5000, Loss: 0.5662\n",
      "Iteration 5100, Loss: 0.5654\n",
      "Iteration 5200, Loss: 0.5646\n",
      "Iteration 5300, Loss: 0.5638\n",
      "Iteration 5400, Loss: 0.5630\n",
      "Iteration 5500, Loss: 0.5622\n",
      "Iteration 5600, Loss: 0.5615\n",
      "Iteration 5700, Loss: 0.5608\n",
      "Iteration 5800, Loss: 0.5600\n",
      "Iteration 5900, Loss: 0.5593\n",
      "Iteration 6000, Loss: 0.5587\n",
      "Iteration 6100, Loss: 0.5580\n",
      "Iteration 6200, Loss: 0.5573\n",
      "Iteration 6300, Loss: 0.5567\n",
      "Iteration 6400, Loss: 0.5560\n",
      "Iteration 6500, Loss: 0.5554\n",
      "Iteration 6600, Loss: 0.5547\n",
      "Iteration 6700, Loss: 0.5541\n",
      "Iteration 6800, Loss: 0.5535\n",
      "Iteration 6900, Loss: 0.5529\n",
      "Iteration 7000, Loss: 0.5523\n",
      "Iteration 7100, Loss: 0.5518\n",
      "Iteration 7200, Loss: 0.5512\n",
      "Iteration 7300, Loss: 0.5506\n",
      "Iteration 7400, Loss: 0.5501\n",
      "Iteration 7500, Loss: 0.5495\n",
      "Iteration 7600, Loss: 0.5490\n",
      "Iteration 7700, Loss: 0.5485\n",
      "Iteration 7800, Loss: 0.5479\n",
      "Iteration 7900, Loss: 0.5474\n",
      "Iteration 0, Loss: 1.0935\n",
      "Iteration 100, Loss: 0.8553\n",
      "Iteration 200, Loss: 0.7779\n",
      "Iteration 300, Loss: 0.7380\n",
      "Iteration 400, Loss: 0.7134\n",
      "Iteration 500, Loss: 0.6964\n",
      "Iteration 600, Loss: 0.6836\n",
      "Iteration 700, Loss: 0.6736\n",
      "Iteration 800, Loss: 0.6652\n",
      "Iteration 900, Loss: 0.6582\n",
      "Iteration 1000, Loss: 0.6521\n",
      "Iteration 1100, Loss: 0.6468\n",
      "Iteration 1200, Loss: 0.6420\n",
      "Iteration 1300, Loss: 0.6378\n",
      "Iteration 1400, Loss: 0.6339\n",
      "Iteration 1500, Loss: 0.6303\n",
      "Iteration 1600, Loss: 0.6271\n",
      "Iteration 1700, Loss: 0.6241\n",
      "Iteration 1800, Loss: 0.6213\n",
      "Iteration 1900, Loss: 0.6187\n",
      "Iteration 2000, Loss: 0.6162\n",
      "Iteration 2100, Loss: 0.6139\n",
      "Iteration 2200, Loss: 0.6117\n",
      "Iteration 2300, Loss: 0.6097\n",
      "Iteration 2400, Loss: 0.6077\n",
      "Iteration 2500, Loss: 0.6058\n",
      "Iteration 2600, Loss: 0.6040\n",
      "Iteration 2700, Loss: 0.6023\n",
      "Iteration 2800, Loss: 0.6006\n",
      "Iteration 2900, Loss: 0.5990\n",
      "Iteration 3000, Loss: 0.5975\n",
      "Iteration 3100, Loss: 0.5960\n",
      "Iteration 3200, Loss: 0.5946\n",
      "Iteration 3300, Loss: 0.5932\n",
      "Iteration 3400, Loss: 0.5919\n",
      "Iteration 3500, Loss: 0.5906\n",
      "Iteration 3600, Loss: 0.5894\n",
      "Iteration 3700, Loss: 0.5882\n",
      "Iteration 3800, Loss: 0.5870\n",
      "Iteration 3900, Loss: 0.5859\n",
      "Iteration 4000, Loss: 0.5848\n",
      "Iteration 4100, Loss: 0.5837\n",
      "Iteration 4200, Loss: 0.5827\n",
      "Iteration 4300, Loss: 0.5816\n",
      "Iteration 4400, Loss: 0.5807\n",
      "Iteration 4500, Loss: 0.5797\n",
      "Iteration 4600, Loss: 0.5788\n",
      "Iteration 4700, Loss: 0.5778\n",
      "Iteration 4800, Loss: 0.5769\n",
      "Iteration 4900, Loss: 0.5761\n",
      "Iteration 5000, Loss: 0.5752\n",
      "Iteration 5100, Loss: 0.5744\n",
      "Iteration 5200, Loss: 0.5735\n",
      "Iteration 5300, Loss: 0.5727\n",
      "Iteration 5400, Loss: 0.5719\n",
      "Iteration 5500, Loss: 0.5712\n",
      "Iteration 5600, Loss: 0.5704\n",
      "Iteration 5700, Loss: 0.5697\n",
      "Iteration 5800, Loss: 0.5690\n",
      "Iteration 5900, Loss: 0.5683\n",
      "Iteration 6000, Loss: 0.5676\n",
      "Iteration 6100, Loss: 0.5669\n",
      "Iteration 6200, Loss: 0.5662\n",
      "Iteration 6300, Loss: 0.5655\n",
      "Iteration 6400, Loss: 0.5649\n",
      "Iteration 6500, Loss: 0.5643\n",
      "Iteration 6600, Loss: 0.5636\n",
      "Iteration 6700, Loss: 0.5630\n",
      "Iteration 6800, Loss: 0.5624\n",
      "Iteration 6900, Loss: 0.5618\n",
      "Iteration 7000, Loss: 0.5612\n",
      "Iteration 7100, Loss: 0.5606\n",
      "Iteration 7200, Loss: 0.5601\n",
      "Iteration 7300, Loss: 0.5595\n",
      "Iteration 7400, Loss: 0.5590\n",
      "Iteration 7500, Loss: 0.5584\n",
      "Iteration 7600, Loss: 0.5579\n",
      "Iteration 7700, Loss: 0.5574\n",
      "Iteration 7800, Loss: 0.5568\n",
      "Iteration 7900, Loss: 0.5563\n",
      "Iteration 0, Loss: 1.0940\n",
      "Iteration 100, Loss: 0.8667\n",
      "Iteration 200, Loss: 0.7873\n",
      "Iteration 300, Loss: 0.7461\n",
      "Iteration 400, Loss: 0.7211\n",
      "Iteration 500, Loss: 0.7040\n",
      "Iteration 600, Loss: 0.6913\n",
      "Iteration 700, Loss: 0.6814\n",
      "Iteration 800, Loss: 0.6732\n",
      "Iteration 900, Loss: 0.6663\n",
      "Iteration 1000, Loss: 0.6604\n",
      "Iteration 1100, Loss: 0.6552\n",
      "Iteration 1200, Loss: 0.6505\n",
      "Iteration 1300, Loss: 0.6464\n",
      "Iteration 1400, Loss: 0.6426\n",
      "Iteration 1500, Loss: 0.6392\n",
      "Iteration 1600, Loss: 0.6360\n",
      "Iteration 1700, Loss: 0.6331\n",
      "Iteration 1800, Loss: 0.6304\n",
      "Iteration 1900, Loss: 0.6279\n",
      "Iteration 2000, Loss: 0.6255\n",
      "Iteration 2100, Loss: 0.6233\n",
      "Iteration 2200, Loss: 0.6211\n",
      "Iteration 2300, Loss: 0.6191\n",
      "Iteration 2400, Loss: 0.6172\n",
      "Iteration 2500, Loss: 0.6154\n",
      "Iteration 2600, Loss: 0.6136\n",
      "Iteration 2700, Loss: 0.6119\n",
      "Iteration 2800, Loss: 0.6103\n",
      "Iteration 2900, Loss: 0.6088\n",
      "Iteration 3000, Loss: 0.6073\n",
      "Iteration 3100, Loss: 0.6058\n",
      "Iteration 3200, Loss: 0.6044\n",
      "Iteration 3300, Loss: 0.6031\n",
      "Iteration 3400, Loss: 0.6018\n",
      "Iteration 3500, Loss: 0.6005\n",
      "Iteration 3600, Loss: 0.5993\n",
      "Iteration 3700, Loss: 0.5981\n",
      "Iteration 3800, Loss: 0.5970\n",
      "Iteration 3900, Loss: 0.5958\n",
      "Iteration 4000, Loss: 0.5947\n",
      "Iteration 4100, Loss: 0.5937\n",
      "Iteration 4200, Loss: 0.5926\n",
      "Iteration 4300, Loss: 0.5916\n",
      "Iteration 4400, Loss: 0.5906\n",
      "Iteration 4500, Loss: 0.5896\n",
      "Iteration 4600, Loss: 0.5887\n",
      "Iteration 4700, Loss: 0.5878\n",
      "Iteration 4800, Loss: 0.5869\n",
      "Iteration 4900, Loss: 0.5860\n",
      "Iteration 5000, Loss: 0.5851\n",
      "Iteration 5100, Loss: 0.5843\n",
      "Iteration 5200, Loss: 0.5834\n",
      "Iteration 5300, Loss: 0.5826\n",
      "Iteration 5400, Loss: 0.5818\n",
      "Iteration 5500, Loss: 0.5810\n",
      "Iteration 5600, Loss: 0.5802\n",
      "Iteration 5700, Loss: 0.5795\n",
      "Iteration 5800, Loss: 0.5787\n",
      "Iteration 5900, Loss: 0.5780\n",
      "Iteration 6000, Loss: 0.5773\n",
      "Iteration 6100, Loss: 0.5766\n",
      "Iteration 6200, Loss: 0.5759\n",
      "Iteration 6300, Loss: 0.5752\n",
      "Iteration 6400, Loss: 0.5745\n",
      "Iteration 6500, Loss: 0.5739\n",
      "Iteration 6600, Loss: 0.5732\n",
      "Iteration 6700, Loss: 0.5726\n",
      "Iteration 6800, Loss: 0.5719\n",
      "Iteration 6900, Loss: 0.5713\n",
      "Iteration 7000, Loss: 0.5707\n",
      "Iteration 7100, Loss: 0.5701\n",
      "Iteration 7200, Loss: 0.5695\n",
      "Iteration 7300, Loss: 0.5689\n",
      "Iteration 7400, Loss: 0.5683\n",
      "Iteration 7500, Loss: 0.5678\n",
      "Iteration 7600, Loss: 0.5672\n",
      "Iteration 7700, Loss: 0.5667\n",
      "Iteration 7800, Loss: 0.5661\n",
      "Iteration 7900, Loss: 0.5656\n",
      "Iteration 0, Loss: 1.0967\n",
      "Iteration 100, Loss: 0.9669\n",
      "Iteration 200, Loss: 0.8948\n",
      "Iteration 300, Loss: 0.8477\n",
      "Iteration 400, Loss: 0.8142\n",
      "Iteration 500, Loss: 0.7892\n",
      "Iteration 600, Loss: 0.7698\n",
      "Iteration 700, Loss: 0.7542\n",
      "Iteration 800, Loss: 0.7414\n",
      "Iteration 900, Loss: 0.7306\n",
      "Iteration 1000, Loss: 0.7215\n",
      "Iteration 1100, Loss: 0.7135\n",
      "Iteration 1200, Loss: 0.7065\n",
      "Iteration 1300, Loss: 0.7003\n",
      "Iteration 1400, Loss: 0.6948\n",
      "Iteration 1500, Loss: 0.6897\n",
      "Iteration 1600, Loss: 0.6851\n",
      "Iteration 1700, Loss: 0.6809\n",
      "Iteration 1800, Loss: 0.6770\n",
      "Iteration 1900, Loss: 0.6734\n",
      "Iteration 2000, Loss: 0.6700\n",
      "Iteration 2100, Loss: 0.6669\n",
      "Iteration 2200, Loss: 0.6639\n",
      "Iteration 2300, Loss: 0.6611\n",
      "Iteration 2400, Loss: 0.6585\n",
      "Iteration 2500, Loss: 0.6560\n",
      "Iteration 2600, Loss: 0.6536\n",
      "Iteration 2700, Loss: 0.6513\n",
      "Iteration 2800, Loss: 0.6492\n",
      "Iteration 2900, Loss: 0.6471\n",
      "Iteration 3000, Loss: 0.6451\n",
      "Iteration 3100, Loss: 0.6433\n",
      "Iteration 3200, Loss: 0.6414\n",
      "Iteration 3300, Loss: 0.6397\n",
      "Iteration 3400, Loss: 0.6380\n",
      "Iteration 3500, Loss: 0.6364\n",
      "Iteration 3600, Loss: 0.6349\n",
      "Iteration 3700, Loss: 0.6334\n",
      "Iteration 3800, Loss: 0.6319\n",
      "Iteration 3900, Loss: 0.6305\n",
      "Iteration 4000, Loss: 0.6291\n",
      "Iteration 4100, Loss: 0.6278\n",
      "Iteration 4200, Loss: 0.6265\n",
      "Iteration 4300, Loss: 0.6253\n",
      "Iteration 4400, Loss: 0.6241\n",
      "Iteration 4500, Loss: 0.6229\n",
      "Iteration 4600, Loss: 0.6217\n",
      "Iteration 4700, Loss: 0.6206\n",
      "Iteration 4800, Loss: 0.6195\n",
      "Iteration 4900, Loss: 0.6185\n",
      "Iteration 5000, Loss: 0.6174\n",
      "Iteration 5100, Loss: 0.6164\n",
      "Iteration 5200, Loss: 0.6154\n",
      "Iteration 5300, Loss: 0.6145\n",
      "Iteration 5400, Loss: 0.6135\n",
      "Iteration 5500, Loss: 0.6126\n",
      "Iteration 5600, Loss: 0.6117\n",
      "Iteration 5700, Loss: 0.6108\n",
      "Iteration 5800, Loss: 0.6099\n",
      "Iteration 5900, Loss: 0.6091\n",
      "Iteration 6000, Loss: 0.6082\n",
      "Iteration 6100, Loss: 0.6074\n",
      "Iteration 6200, Loss: 0.6066\n",
      "Iteration 6300, Loss: 0.6058\n",
      "Iteration 6400, Loss: 0.6051\n",
      "Iteration 6500, Loss: 0.6043\n",
      "Iteration 6600, Loss: 0.6035\n",
      "Iteration 6700, Loss: 0.6028\n",
      "Iteration 6800, Loss: 0.6021\n",
      "Iteration 6900, Loss: 0.6014\n",
      "Iteration 7000, Loss: 0.6007\n",
      "Iteration 7100, Loss: 0.6000\n",
      "Iteration 7200, Loss: 0.5993\n",
      "Iteration 7300, Loss: 0.5986\n",
      "Iteration 7400, Loss: 0.5980\n",
      "Iteration 7500, Loss: 0.5973\n",
      "Iteration 7600, Loss: 0.5967\n",
      "Iteration 7700, Loss: 0.5960\n",
      "Iteration 7800, Loss: 0.5954\n",
      "Iteration 7900, Loss: 0.5948\n",
      "Iteration 0, Loss: 1.0968\n",
      "Iteration 100, Loss: 0.9670\n",
      "Iteration 200, Loss: 0.8948\n",
      "Iteration 300, Loss: 0.8477\n",
      "Iteration 400, Loss: 0.8144\n",
      "Iteration 500, Loss: 0.7896\n",
      "Iteration 600, Loss: 0.7706\n",
      "Iteration 700, Loss: 0.7555\n",
      "Iteration 800, Loss: 0.7433\n",
      "Iteration 900, Loss: 0.7331\n",
      "Iteration 1000, Loss: 0.7245\n",
      "Iteration 1100, Loss: 0.7170\n",
      "Iteration 1200, Loss: 0.7105\n",
      "Iteration 1300, Loss: 0.7048\n",
      "Iteration 1400, Loss: 0.6996\n",
      "Iteration 1500, Loss: 0.6949\n",
      "Iteration 1600, Loss: 0.6907\n",
      "Iteration 1700, Loss: 0.6868\n",
      "Iteration 1800, Loss: 0.6831\n",
      "Iteration 1900, Loss: 0.6798\n",
      "Iteration 2000, Loss: 0.6767\n",
      "Iteration 2100, Loss: 0.6737\n",
      "Iteration 2200, Loss: 0.6710\n",
      "Iteration 2300, Loss: 0.6684\n",
      "Iteration 2400, Loss: 0.6659\n",
      "Iteration 2500, Loss: 0.6636\n",
      "Iteration 2600, Loss: 0.6614\n",
      "Iteration 2700, Loss: 0.6593\n",
      "Iteration 2800, Loss: 0.6573\n",
      "Iteration 2900, Loss: 0.6554\n",
      "Iteration 3000, Loss: 0.6536\n",
      "Iteration 3100, Loss: 0.6518\n",
      "Iteration 3200, Loss: 0.6501\n",
      "Iteration 3300, Loss: 0.6485\n",
      "Iteration 3400, Loss: 0.6470\n",
      "Iteration 3500, Loss: 0.6455\n",
      "Iteration 3600, Loss: 0.6441\n",
      "Iteration 3700, Loss: 0.6427\n",
      "Iteration 3800, Loss: 0.6413\n",
      "Iteration 3900, Loss: 0.6400\n",
      "Iteration 4000, Loss: 0.6388\n",
      "Iteration 4100, Loss: 0.6375\n",
      "Iteration 4200, Loss: 0.6364\n",
      "Iteration 4300, Loss: 0.6352\n",
      "Iteration 4400, Loss: 0.6341\n",
      "Iteration 4500, Loss: 0.6330\n",
      "Iteration 4600, Loss: 0.6320\n",
      "Iteration 4700, Loss: 0.6309\n",
      "Iteration 4800, Loss: 0.6299\n",
      "Iteration 4900, Loss: 0.6290\n",
      "Iteration 5000, Loss: 0.6280\n",
      "Iteration 5100, Loss: 0.6271\n",
      "Iteration 5200, Loss: 0.6262\n",
      "Iteration 5300, Loss: 0.6253\n",
      "Iteration 5400, Loss: 0.6245\n",
      "Iteration 5500, Loss: 0.6236\n",
      "Iteration 5600, Loss: 0.6228\n",
      "Iteration 5700, Loss: 0.6220\n",
      "Iteration 5800, Loss: 0.6212\n",
      "Iteration 5900, Loss: 0.6204\n",
      "Iteration 6000, Loss: 0.6197\n",
      "Iteration 6100, Loss: 0.6190\n",
      "Iteration 6200, Loss: 0.6182\n",
      "Iteration 6300, Loss: 0.6175\n",
      "Iteration 6400, Loss: 0.6168\n",
      "Iteration 6500, Loss: 0.6161\n",
      "Iteration 6600, Loss: 0.6155\n",
      "Iteration 6700, Loss: 0.6148\n",
      "Iteration 6800, Loss: 0.6142\n",
      "Iteration 6900, Loss: 0.6135\n",
      "Iteration 7000, Loss: 0.6129\n",
      "Iteration 7100, Loss: 0.6123\n",
      "Iteration 7200, Loss: 0.6117\n",
      "Iteration 7300, Loss: 0.6111\n",
      "Iteration 7400, Loss: 0.6105\n",
      "Iteration 7500, Loss: 0.6099\n",
      "Iteration 7600, Loss: 0.6093\n",
      "Iteration 7700, Loss: 0.6088\n",
      "Iteration 7800, Loss: 0.6082\n",
      "Iteration 7900, Loss: 0.6077\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0230\n",
      "Iteration 200, Loss: 0.9702\n",
      "Iteration 300, Loss: 0.9302\n",
      "Iteration 400, Loss: 0.8985\n",
      "Iteration 500, Loss: 0.8728\n",
      "Iteration 600, Loss: 0.8514\n",
      "Iteration 700, Loss: 0.8333\n",
      "Iteration 800, Loss: 0.8179\n",
      "Iteration 900, Loss: 0.8046\n",
      "Iteration 1000, Loss: 0.7930\n",
      "Iteration 1100, Loss: 0.7829\n",
      "Iteration 1200, Loss: 0.7739\n",
      "Iteration 1300, Loss: 0.7660\n",
      "Iteration 1400, Loss: 0.7588\n",
      "Iteration 1500, Loss: 0.7524\n",
      "Iteration 1600, Loss: 0.7465\n",
      "Iteration 1700, Loss: 0.7412\n",
      "Iteration 1800, Loss: 0.7363\n",
      "Iteration 1900, Loss: 0.7319\n",
      "Iteration 2000, Loss: 0.7277\n",
      "Iteration 2100, Loss: 0.7238\n",
      "Iteration 2200, Loss: 0.7202\n",
      "Iteration 2300, Loss: 0.7169\n",
      "Iteration 2400, Loss: 0.7137\n",
      "Iteration 2500, Loss: 0.7108\n",
      "Iteration 2600, Loss: 0.7080\n",
      "Iteration 2700, Loss: 0.7053\n",
      "Iteration 2800, Loss: 0.7028\n",
      "Iteration 2900, Loss: 0.7004\n",
      "Iteration 3000, Loss: 0.6981\n",
      "Iteration 3100, Loss: 0.6960\n",
      "Iteration 3200, Loss: 0.6939\n",
      "Iteration 3300, Loss: 0.6919\n",
      "Iteration 3400, Loss: 0.6900\n",
      "Iteration 3500, Loss: 0.6881\n",
      "Iteration 3600, Loss: 0.6864\n",
      "Iteration 3700, Loss: 0.6847\n",
      "Iteration 3800, Loss: 0.6830\n",
      "Iteration 3900, Loss: 0.6814\n",
      "Iteration 4000, Loss: 0.6799\n",
      "Iteration 4100, Loss: 0.6784\n",
      "Iteration 4200, Loss: 0.6770\n",
      "Iteration 4300, Loss: 0.6756\n",
      "Iteration 4400, Loss: 0.6742\n",
      "Iteration 4500, Loss: 0.6729\n",
      "Iteration 4600, Loss: 0.6717\n",
      "Iteration 4700, Loss: 0.6704\n",
      "Iteration 4800, Loss: 0.6692\n",
      "Iteration 4900, Loss: 0.6681\n",
      "Iteration 5000, Loss: 0.6669\n",
      "Iteration 5100, Loss: 0.6658\n",
      "Iteration 5200, Loss: 0.6647\n",
      "Iteration 5300, Loss: 0.6637\n",
      "Iteration 5400, Loss: 0.6626\n",
      "Iteration 5500, Loss: 0.6616\n",
      "Iteration 5600, Loss: 0.6606\n",
      "Iteration 5700, Loss: 0.6597\n",
      "Iteration 5800, Loss: 0.6587\n",
      "Iteration 5900, Loss: 0.6578\n",
      "Iteration 6000, Loss: 0.6569\n",
      "Iteration 6100, Loss: 0.6560\n",
      "Iteration 6200, Loss: 0.6552\n",
      "Iteration 6300, Loss: 0.6543\n",
      "Iteration 6400, Loss: 0.6535\n",
      "Iteration 6500, Loss: 0.6527\n",
      "Iteration 6600, Loss: 0.6519\n",
      "Iteration 6700, Loss: 0.6511\n",
      "Iteration 6800, Loss: 0.6503\n",
      "Iteration 6900, Loss: 0.6496\n",
      "Iteration 7000, Loss: 0.6488\n",
      "Iteration 7100, Loss: 0.6481\n",
      "Iteration 7200, Loss: 0.6474\n",
      "Iteration 7300, Loss: 0.6467\n",
      "Iteration 7400, Loss: 0.6460\n",
      "Iteration 7500, Loss: 0.6453\n",
      "Iteration 7600, Loss: 0.6447\n",
      "Iteration 7700, Loss: 0.6440\n",
      "Iteration 7800, Loss: 0.6434\n",
      "Iteration 7900, Loss: 0.6427\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0162\n",
      "Iteration 200, Loss: 0.9600\n",
      "Iteration 300, Loss: 0.9181\n",
      "Iteration 400, Loss: 0.8855\n",
      "Iteration 500, Loss: 0.8593\n",
      "Iteration 600, Loss: 0.8378\n",
      "Iteration 700, Loss: 0.8196\n",
      "Iteration 800, Loss: 0.8042\n",
      "Iteration 900, Loss: 0.7910\n",
      "Iteration 1000, Loss: 0.7794\n",
      "Iteration 1100, Loss: 0.7693\n",
      "Iteration 1200, Loss: 0.7602\n",
      "Iteration 1300, Loss: 0.7522\n",
      "Iteration 1400, Loss: 0.7449\n",
      "Iteration 1500, Loss: 0.7383\n",
      "Iteration 1600, Loss: 0.7323\n",
      "Iteration 1700, Loss: 0.7268\n",
      "Iteration 1800, Loss: 0.7217\n",
      "Iteration 1900, Loss: 0.7171\n",
      "Iteration 2000, Loss: 0.7127\n",
      "Iteration 2100, Loss: 0.7087\n",
      "Iteration 2200, Loss: 0.7050\n",
      "Iteration 2300, Loss: 0.7014\n",
      "Iteration 2400, Loss: 0.6981\n",
      "Iteration 2500, Loss: 0.6950\n",
      "Iteration 2600, Loss: 0.6921\n",
      "Iteration 2700, Loss: 0.6893\n",
      "Iteration 2800, Loss: 0.6867\n",
      "Iteration 2900, Loss: 0.6841\n",
      "Iteration 3000, Loss: 0.6817\n",
      "Iteration 3100, Loss: 0.6795\n",
      "Iteration 3200, Loss: 0.6773\n",
      "Iteration 3300, Loss: 0.6752\n",
      "Iteration 3400, Loss: 0.6732\n",
      "Iteration 3500, Loss: 0.6713\n",
      "Iteration 3600, Loss: 0.6694\n",
      "Iteration 3700, Loss: 0.6676\n",
      "Iteration 3800, Loss: 0.6659\n",
      "Iteration 3900, Loss: 0.6643\n",
      "Iteration 4000, Loss: 0.6627\n",
      "Iteration 4100, Loss: 0.6611\n",
      "Iteration 4200, Loss: 0.6596\n",
      "Iteration 4300, Loss: 0.6582\n",
      "Iteration 4400, Loss: 0.6568\n",
      "Iteration 4500, Loss: 0.6554\n",
      "Iteration 4600, Loss: 0.6541\n",
      "Iteration 4700, Loss: 0.6528\n",
      "Iteration 4800, Loss: 0.6516\n",
      "Iteration 4900, Loss: 0.6504\n",
      "Iteration 5000, Loss: 0.6492\n",
      "Iteration 5100, Loss: 0.6480\n",
      "Iteration 5200, Loss: 0.6469\n",
      "Iteration 5300, Loss: 0.6458\n",
      "Iteration 5400, Loss: 0.6448\n",
      "Iteration 5500, Loss: 0.6437\n",
      "Iteration 5600, Loss: 0.6427\n",
      "Iteration 5700, Loss: 0.6417\n",
      "Iteration 5800, Loss: 0.6408\n",
      "Iteration 5900, Loss: 0.6398\n",
      "Iteration 6000, Loss: 0.6389\n",
      "Iteration 6100, Loss: 0.6380\n",
      "Iteration 6200, Loss: 0.6371\n",
      "Iteration 6300, Loss: 0.6363\n",
      "Iteration 6400, Loss: 0.6354\n",
      "Iteration 6500, Loss: 0.6346\n",
      "Iteration 6600, Loss: 0.6338\n",
      "Iteration 6700, Loss: 0.6330\n",
      "Iteration 6800, Loss: 0.6322\n",
      "Iteration 6900, Loss: 0.6314\n",
      "Iteration 7000, Loss: 0.6307\n",
      "Iteration 7100, Loss: 0.6299\n",
      "Iteration 7200, Loss: 0.6292\n",
      "Iteration 7300, Loss: 0.6285\n",
      "Iteration 7400, Loss: 0.6278\n",
      "Iteration 7500, Loss: 0.6271\n",
      "Iteration 7600, Loss: 0.6264\n",
      "Iteration 7700, Loss: 0.6258\n",
      "Iteration 7800, Loss: 0.6251\n",
      "Iteration 7900, Loss: 0.6245\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0541\n",
      "Iteration 200, Loss: 1.0178\n",
      "Iteration 300, Loss: 0.9872\n",
      "Iteration 400, Loss: 0.9610\n",
      "Iteration 500, Loss: 0.9382\n",
      "Iteration 600, Loss: 0.9181\n",
      "Iteration 700, Loss: 0.9003\n",
      "Iteration 800, Loss: 0.8845\n",
      "Iteration 900, Loss: 0.8702\n",
      "Iteration 1000, Loss: 0.8573\n",
      "Iteration 1100, Loss: 0.8455\n",
      "Iteration 1200, Loss: 0.8349\n",
      "Iteration 1300, Loss: 0.8251\n",
      "Iteration 1400, Loss: 0.8161\n",
      "Iteration 1500, Loss: 0.8078\n",
      "Iteration 1600, Loss: 0.8001\n",
      "Iteration 1700, Loss: 0.7930\n",
      "Iteration 1800, Loss: 0.7863\n",
      "Iteration 1900, Loss: 0.7801\n",
      "Iteration 2000, Loss: 0.7743\n",
      "Iteration 2100, Loss: 0.7689\n",
      "Iteration 2200, Loss: 0.7638\n",
      "Iteration 2300, Loss: 0.7590\n",
      "Iteration 2400, Loss: 0.7545\n",
      "Iteration 2500, Loss: 0.7503\n",
      "Iteration 2600, Loss: 0.7463\n",
      "Iteration 2700, Loss: 0.7425\n",
      "Iteration 2800, Loss: 0.7388\n",
      "Iteration 2900, Loss: 0.7354\n",
      "Iteration 3000, Loss: 0.7322\n",
      "Iteration 3100, Loss: 0.7291\n",
      "Iteration 3200, Loss: 0.7261\n",
      "Iteration 3300, Loss: 0.7232\n",
      "Iteration 3400, Loss: 0.7206\n",
      "Iteration 3500, Loss: 0.7179\n",
      "Iteration 3600, Loss: 0.7155\n",
      "Iteration 3700, Loss: 0.7130\n",
      "Iteration 3800, Loss: 0.7108\n",
      "Iteration 3900, Loss: 0.7085\n",
      "Iteration 4000, Loss: 0.7064\n",
      "Iteration 4100, Loss: 0.7043\n",
      "Iteration 4200, Loss: 0.7023\n",
      "Iteration 4300, Loss: 0.7004\n",
      "Iteration 4400, Loss: 0.6985\n",
      "Iteration 4500, Loss: 0.6967\n",
      "Iteration 4600, Loss: 0.6950\n",
      "Iteration 4700, Loss: 0.6933\n",
      "Iteration 4800, Loss: 0.6916\n",
      "Iteration 4900, Loss: 0.6900\n",
      "Iteration 5000, Loss: 0.6885\n",
      "Iteration 5100, Loss: 0.6870\n",
      "Iteration 5200, Loss: 0.6855\n",
      "Iteration 5300, Loss: 0.6840\n",
      "Iteration 5400, Loss: 0.6826\n",
      "Iteration 5500, Loss: 0.6813\n",
      "Iteration 5600, Loss: 0.6800\n",
      "Iteration 5700, Loss: 0.6787\n",
      "Iteration 5800, Loss: 0.6774\n",
      "Iteration 5900, Loss: 0.6761\n",
      "Iteration 6000, Loss: 0.6749\n",
      "Iteration 6100, Loss: 0.6738\n",
      "Iteration 6200, Loss: 0.6726\n",
      "Iteration 6300, Loss: 0.6715\n",
      "Iteration 6400, Loss: 0.6704\n",
      "Iteration 6500, Loss: 0.6693\n",
      "Iteration 6600, Loss: 0.6682\n",
      "Iteration 6700, Loss: 0.6672\n",
      "Iteration 6800, Loss: 0.6662\n",
      "Iteration 6900, Loss: 0.6651\n",
      "Iteration 7000, Loss: 0.6642\n",
      "Iteration 7100, Loss: 0.6632\n",
      "Iteration 7200, Loss: 0.6622\n",
      "Iteration 7300, Loss: 0.6613\n",
      "Iteration 7400, Loss: 0.6604\n",
      "Iteration 7500, Loss: 0.6595\n",
      "Iteration 7600, Loss: 0.6586\n",
      "Iteration 7700, Loss: 0.6577\n",
      "Iteration 7800, Loss: 0.6569\n",
      "Iteration 7900, Loss: 0.6560\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0523\n",
      "Iteration 200, Loss: 1.0161\n",
      "Iteration 300, Loss: 0.9862\n",
      "Iteration 400, Loss: 0.9612\n",
      "Iteration 500, Loss: 0.9395\n",
      "Iteration 600, Loss: 0.9206\n",
      "Iteration 700, Loss: 0.9040\n",
      "Iteration 800, Loss: 0.8892\n",
      "Iteration 900, Loss: 0.8760\n",
      "Iteration 1000, Loss: 0.8641\n",
      "Iteration 1100, Loss: 0.8533\n",
      "Iteration 1200, Loss: 0.8435\n",
      "Iteration 1300, Loss: 0.8345\n",
      "Iteration 1400, Loss: 0.8263\n",
      "Iteration 1500, Loss: 0.8188\n",
      "Iteration 1600, Loss: 0.8118\n",
      "Iteration 1700, Loss: 0.8053\n",
      "Iteration 1800, Loss: 0.7993\n",
      "Iteration 1900, Loss: 0.7936\n",
      "Iteration 2000, Loss: 0.7884\n",
      "Iteration 2100, Loss: 0.7835\n",
      "Iteration 2200, Loss: 0.7789\n",
      "Iteration 2300, Loss: 0.7746\n",
      "Iteration 2400, Loss: 0.7705\n",
      "Iteration 2500, Loss: 0.7666\n",
      "Iteration 2600, Loss: 0.7629\n",
      "Iteration 2700, Loss: 0.7595\n",
      "Iteration 2800, Loss: 0.7562\n",
      "Iteration 2900, Loss: 0.7531\n",
      "Iteration 3000, Loss: 0.7501\n",
      "Iteration 3100, Loss: 0.7473\n",
      "Iteration 3200, Loss: 0.7446\n",
      "Iteration 3300, Loss: 0.7420\n",
      "Iteration 3400, Loss: 0.7396\n",
      "Iteration 3500, Loss: 0.7373\n",
      "Iteration 3600, Loss: 0.7350\n",
      "Iteration 3700, Loss: 0.7329\n",
      "Iteration 3800, Loss: 0.7308\n",
      "Iteration 3900, Loss: 0.7288\n",
      "Iteration 4000, Loss: 0.7268\n",
      "Iteration 4100, Loss: 0.7250\n",
      "Iteration 4200, Loss: 0.7232\n",
      "Iteration 4300, Loss: 0.7215\n",
      "Iteration 4400, Loss: 0.7198\n",
      "Iteration 4500, Loss: 0.7182\n",
      "Iteration 4600, Loss: 0.7166\n",
      "Iteration 4700, Loss: 0.7151\n",
      "Iteration 4800, Loss: 0.7136\n",
      "Iteration 4900, Loss: 0.7122\n",
      "Iteration 5000, Loss: 0.7108\n",
      "Iteration 5100, Loss: 0.7095\n",
      "Iteration 5200, Loss: 0.7082\n",
      "Iteration 5300, Loss: 0.7069\n",
      "Iteration 5400, Loss: 0.7057\n",
      "Iteration 5500, Loss: 0.7045\n",
      "Iteration 5600, Loss: 0.7033\n",
      "Iteration 5700, Loss: 0.7022\n",
      "Iteration 5800, Loss: 0.7011\n",
      "Iteration 5900, Loss: 0.7000\n",
      "Iteration 6000, Loss: 0.6989\n",
      "Iteration 6100, Loss: 0.6979\n",
      "Iteration 6200, Loss: 0.6969\n",
      "Iteration 6300, Loss: 0.6959\n",
      "Iteration 6400, Loss: 0.6949\n",
      "Iteration 6500, Loss: 0.6940\n",
      "Iteration 6600, Loss: 0.6931\n",
      "Iteration 6700, Loss: 0.6922\n",
      "Iteration 6800, Loss: 0.6913\n",
      "Iteration 6900, Loss: 0.6904\n",
      "Iteration 7000, Loss: 0.6896\n",
      "Iteration 7100, Loss: 0.6887\n",
      "Iteration 7200, Loss: 0.6879\n",
      "Iteration 7300, Loss: 0.6871\n",
      "Iteration 7400, Loss: 0.6863\n",
      "Iteration 7500, Loss: 0.6855\n",
      "Iteration 7600, Loss: 0.6847\n",
      "Iteration 7700, Loss: 0.6840\n",
      "Iteration 7800, Loss: 0.6833\n",
      "Iteration 7900, Loss: 0.6825\n",
      "Iteration 0, Loss: 0.9596\n",
      "Iteration 100, Loss: 0.5833\n",
      "Iteration 200, Loss: 0.5554\n",
      "Iteration 300, Loss: 0.5399\n",
      "Iteration 400, Loss: 0.5293\n",
      "Iteration 500, Loss: 0.5212\n",
      "Iteration 600, Loss: 0.5146\n",
      "Iteration 700, Loss: 0.5091\n",
      "Iteration 800, Loss: 0.5043\n",
      "Iteration 900, Loss: 0.5001\n",
      "Iteration 1000, Loss: 0.4963\n",
      "Iteration 1100, Loss: 0.4929\n",
      "Iteration 1200, Loss: 0.4898\n",
      "Iteration 1300, Loss: 0.4869\n",
      "Iteration 1400, Loss: 0.4843\n",
      "Iteration 1500, Loss: 0.4818\n",
      "Iteration 1600, Loss: 0.4796\n",
      "Iteration 1700, Loss: 0.4774\n",
      "Iteration 1800, Loss: 0.4754\n",
      "Iteration 1900, Loss: 0.4735\n",
      "Iteration 2000, Loss: 0.4717\n",
      "Iteration 2100, Loss: 0.4699\n",
      "Iteration 2200, Loss: 0.4683\n",
      "Iteration 2300, Loss: 0.4668\n",
      "Iteration 2400, Loss: 0.4653\n",
      "Iteration 2500, Loss: 0.4638\n",
      "Iteration 2600, Loss: 0.4625\n",
      "Iteration 2700, Loss: 0.4612\n",
      "Iteration 2800, Loss: 0.4599\n",
      "Iteration 2900, Loss: 0.4587\n",
      "Iteration 3000, Loss: 0.4575\n",
      "Iteration 3100, Loss: 0.4564\n",
      "Iteration 3200, Loss: 0.4553\n",
      "Iteration 3300, Loss: 0.4542\n",
      "Iteration 3400, Loss: 0.4532\n",
      "Iteration 3500, Loss: 0.4522\n",
      "Iteration 3600, Loss: 0.4513\n",
      "Iteration 3700, Loss: 0.4503\n",
      "Iteration 3800, Loss: 0.4494\n",
      "Iteration 3900, Loss: 0.4486\n",
      "Iteration 4000, Loss: 0.4477\n",
      "Iteration 4100, Loss: 0.4469\n",
      "Iteration 4200, Loss: 0.4461\n",
      "Iteration 4300, Loss: 0.4453\n",
      "Iteration 4400, Loss: 0.4445\n",
      "Iteration 4500, Loss: 0.4438\n",
      "Iteration 4600, Loss: 0.4430\n",
      "Iteration 4700, Loss: 0.4423\n",
      "Iteration 4800, Loss: 0.4416\n",
      "Iteration 4900, Loss: 0.4409\n",
      "Iteration 5000, Loss: 0.4403\n",
      "Iteration 5100, Loss: 0.4396\n",
      "Iteration 5200, Loss: 0.4390\n",
      "Iteration 5300, Loss: 0.4383\n",
      "Iteration 5400, Loss: 0.4377\n",
      "Iteration 5500, Loss: 0.4371\n",
      "Iteration 5600, Loss: 0.4365\n",
      "Iteration 5700, Loss: 0.4360\n",
      "Iteration 5800, Loss: 0.4354\n",
      "Iteration 5900, Loss: 0.4348\n",
      "Iteration 6000, Loss: 0.4343\n",
      "Iteration 6100, Loss: 0.4338\n",
      "Iteration 6200, Loss: 0.4332\n",
      "Iteration 6300, Loss: 0.4327\n",
      "Iteration 6400, Loss: 0.4322\n",
      "Iteration 6500, Loss: 0.4317\n",
      "Iteration 6600, Loss: 0.4312\n",
      "Iteration 6700, Loss: 0.4307\n",
      "Iteration 6800, Loss: 0.4303\n",
      "Iteration 6900, Loss: 0.4298\n",
      "Iteration 7000, Loss: 0.4293\n",
      "Iteration 7100, Loss: 0.4289\n",
      "Iteration 7200, Loss: 0.4284\n",
      "Iteration 7300, Loss: 0.4280\n",
      "Iteration 7400, Loss: 0.4276\n",
      "Iteration 7500, Loss: 0.4271\n",
      "Iteration 7600, Loss: 0.4267\n",
      "Iteration 7700, Loss: 0.4263\n",
      "Iteration 7800, Loss: 0.4259\n",
      "Iteration 7900, Loss: 0.4255\n",
      "Iteration 8000, Loss: 0.4251\n",
      "Iteration 8100, Loss: 0.4247\n",
      "Iteration 8200, Loss: 0.4243\n",
      "Iteration 8300, Loss: 0.4239\n",
      "Iteration 8400, Loss: 0.4236\n",
      "Iteration 8500, Loss: 0.4232\n",
      "Iteration 8600, Loss: 0.4228\n",
      "Iteration 8700, Loss: 0.4225\n",
      "Iteration 8800, Loss: 0.4221\n",
      "Iteration 8900, Loss: 0.4217\n",
      "Iteration 0, Loss: 0.9764\n",
      "Iteration 100, Loss: 0.6033\n",
      "Iteration 200, Loss: 0.5725\n",
      "Iteration 300, Loss: 0.5552\n",
      "Iteration 400, Loss: 0.5432\n",
      "Iteration 500, Loss: 0.5341\n",
      "Iteration 600, Loss: 0.5267\n",
      "Iteration 700, Loss: 0.5206\n",
      "Iteration 800, Loss: 0.5153\n",
      "Iteration 900, Loss: 0.5108\n",
      "Iteration 1000, Loss: 0.5068\n",
      "Iteration 1100, Loss: 0.5032\n",
      "Iteration 1200, Loss: 0.4999\n",
      "Iteration 1300, Loss: 0.4970\n",
      "Iteration 1400, Loss: 0.4943\n",
      "Iteration 1500, Loss: 0.4918\n",
      "Iteration 1600, Loss: 0.4895\n",
      "Iteration 1700, Loss: 0.4873\n",
      "Iteration 1800, Loss: 0.4853\n",
      "Iteration 1900, Loss: 0.4834\n",
      "Iteration 2000, Loss: 0.4816\n",
      "Iteration 2100, Loss: 0.4799\n",
      "Iteration 2200, Loss: 0.4783\n",
      "Iteration 2300, Loss: 0.4768\n",
      "Iteration 2400, Loss: 0.4754\n",
      "Iteration 2500, Loss: 0.4740\n",
      "Iteration 2600, Loss: 0.4727\n",
      "Iteration 2700, Loss: 0.4715\n",
      "Iteration 2800, Loss: 0.4703\n",
      "Iteration 2900, Loss: 0.4691\n",
      "Iteration 3000, Loss: 0.4680\n",
      "Iteration 3100, Loss: 0.4669\n",
      "Iteration 3200, Loss: 0.4659\n",
      "Iteration 3300, Loss: 0.4649\n",
      "Iteration 3400, Loss: 0.4640\n",
      "Iteration 3500, Loss: 0.4630\n",
      "Iteration 3600, Loss: 0.4621\n",
      "Iteration 3700, Loss: 0.4613\n",
      "Iteration 3800, Loss: 0.4604\n",
      "Iteration 3900, Loss: 0.4596\n",
      "Iteration 4000, Loss: 0.4588\n",
      "Iteration 4100, Loss: 0.4580\n",
      "Iteration 4200, Loss: 0.4573\n",
      "Iteration 4300, Loss: 0.4566\n",
      "Iteration 4400, Loss: 0.4558\n",
      "Iteration 4500, Loss: 0.4552\n",
      "Iteration 4600, Loss: 0.4545\n",
      "Iteration 4700, Loss: 0.4538\n",
      "Iteration 4800, Loss: 0.4532\n",
      "Iteration 4900, Loss: 0.4525\n",
      "Iteration 5000, Loss: 0.4519\n",
      "Iteration 5100, Loss: 0.4513\n",
      "Iteration 5200, Loss: 0.4507\n",
      "Iteration 5300, Loss: 0.4502\n",
      "Iteration 5400, Loss: 0.4496\n",
      "Iteration 5500, Loss: 0.4490\n",
      "Iteration 5600, Loss: 0.4485\n",
      "Iteration 5700, Loss: 0.4480\n",
      "Iteration 5800, Loss: 0.4474\n",
      "Iteration 5900, Loss: 0.4469\n",
      "Iteration 6000, Loss: 0.4464\n",
      "Iteration 6100, Loss: 0.4459\n",
      "Iteration 6200, Loss: 0.4454\n",
      "Iteration 6300, Loss: 0.4450\n",
      "Iteration 6400, Loss: 0.4445\n",
      "Iteration 6500, Loss: 0.4440\n",
      "Iteration 6600, Loss: 0.4436\n",
      "Iteration 6700, Loss: 0.4431\n",
      "Iteration 6800, Loss: 0.4427\n",
      "Iteration 6900, Loss: 0.4423\n",
      "Iteration 7000, Loss: 0.4418\n",
      "Iteration 7100, Loss: 0.4414\n",
      "Iteration 7200, Loss: 0.4410\n",
      "Iteration 7300, Loss: 0.4406\n",
      "Iteration 7400, Loss: 0.4402\n",
      "Iteration 7500, Loss: 0.4398\n",
      "Iteration 7600, Loss: 0.4394\n",
      "Iteration 7700, Loss: 0.4390\n",
      "Iteration 7800, Loss: 0.4387\n",
      "Iteration 7900, Loss: 0.4383\n",
      "Iteration 8000, Loss: 0.4379\n",
      "Iteration 8100, Loss: 0.4376\n",
      "Iteration 8200, Loss: 0.4372\n",
      "Iteration 8300, Loss: 0.4369\n",
      "Iteration 8400, Loss: 0.4365\n",
      "Iteration 8500, Loss: 0.4362\n",
      "Iteration 8600, Loss: 0.4358\n",
      "Iteration 8700, Loss: 0.4355\n",
      "Iteration 8800, Loss: 0.4352\n",
      "Iteration 8900, Loss: 0.4348\n",
      "Iteration 0, Loss: 1.0288\n",
      "Iteration 100, Loss: 0.6370\n",
      "Iteration 200, Loss: 0.6064\n",
      "Iteration 300, Loss: 0.5899\n",
      "Iteration 400, Loss: 0.5788\n",
      "Iteration 500, Loss: 0.5703\n",
      "Iteration 600, Loss: 0.5636\n",
      "Iteration 700, Loss: 0.5581\n",
      "Iteration 800, Loss: 0.5533\n",
      "Iteration 900, Loss: 0.5492\n",
      "Iteration 1000, Loss: 0.5456\n",
      "Iteration 1100, Loss: 0.5423\n",
      "Iteration 1200, Loss: 0.5393\n",
      "Iteration 1300, Loss: 0.5366\n",
      "Iteration 1400, Loss: 0.5341\n",
      "Iteration 1500, Loss: 0.5318\n",
      "Iteration 1600, Loss: 0.5297\n",
      "Iteration 1700, Loss: 0.5277\n",
      "Iteration 1800, Loss: 0.5258\n",
      "Iteration 1900, Loss: 0.5240\n",
      "Iteration 2000, Loss: 0.5223\n",
      "Iteration 2100, Loss: 0.5207\n",
      "Iteration 2200, Loss: 0.5192\n",
      "Iteration 2300, Loss: 0.5177\n",
      "Iteration 2400, Loss: 0.5163\n",
      "Iteration 2500, Loss: 0.5150\n",
      "Iteration 2600, Loss: 0.5137\n",
      "Iteration 2700, Loss: 0.5124\n",
      "Iteration 2800, Loss: 0.5112\n",
      "Iteration 2900, Loss: 0.5101\n",
      "Iteration 3000, Loss: 0.5090\n",
      "Iteration 3100, Loss: 0.5079\n",
      "Iteration 3200, Loss: 0.5068\n",
      "Iteration 3300, Loss: 0.5058\n",
      "Iteration 3400, Loss: 0.5049\n",
      "Iteration 3500, Loss: 0.5039\n",
      "Iteration 3600, Loss: 0.5030\n",
      "Iteration 3700, Loss: 0.5021\n",
      "Iteration 3800, Loss: 0.5012\n",
      "Iteration 3900, Loss: 0.5003\n",
      "Iteration 4000, Loss: 0.4995\n",
      "Iteration 4100, Loss: 0.4987\n",
      "Iteration 4200, Loss: 0.4979\n",
      "Iteration 4300, Loss: 0.4972\n",
      "Iteration 4400, Loss: 0.4964\n",
      "Iteration 4500, Loss: 0.4957\n",
      "Iteration 4600, Loss: 0.4949\n",
      "Iteration 4700, Loss: 0.4942\n",
      "Iteration 4800, Loss: 0.4936\n",
      "Iteration 4900, Loss: 0.4929\n",
      "Iteration 5000, Loss: 0.4922\n",
      "Iteration 5100, Loss: 0.4916\n",
      "Iteration 5200, Loss: 0.4909\n",
      "Iteration 5300, Loss: 0.4903\n",
      "Iteration 5400, Loss: 0.4897\n",
      "Iteration 5500, Loss: 0.4891\n",
      "Iteration 5600, Loss: 0.4885\n",
      "Iteration 5700, Loss: 0.4880\n",
      "Iteration 5800, Loss: 0.4874\n",
      "Iteration 5900, Loss: 0.4868\n",
      "Iteration 6000, Loss: 0.4863\n",
      "Iteration 6100, Loss: 0.4858\n",
      "Iteration 6200, Loss: 0.4852\n",
      "Iteration 6300, Loss: 0.4847\n",
      "Iteration 6400, Loss: 0.4842\n",
      "Iteration 6500, Loss: 0.4837\n",
      "Iteration 6600, Loss: 0.4832\n",
      "Iteration 6700, Loss: 0.4827\n",
      "Iteration 6800, Loss: 0.4822\n",
      "Iteration 6900, Loss: 0.4818\n",
      "Iteration 7000, Loss: 0.4813\n",
      "Iteration 7100, Loss: 0.4808\n",
      "Iteration 7200, Loss: 0.4804\n",
      "Iteration 7300, Loss: 0.4799\n",
      "Iteration 7400, Loss: 0.4795\n",
      "Iteration 7500, Loss: 0.4791\n",
      "Iteration 7600, Loss: 0.4786\n",
      "Iteration 7700, Loss: 0.4782\n",
      "Iteration 7800, Loss: 0.4778\n",
      "Iteration 7900, Loss: 0.4774\n",
      "Iteration 8000, Loss: 0.4770\n",
      "Iteration 8100, Loss: 0.4766\n",
      "Iteration 8200, Loss: 0.4762\n",
      "Iteration 8300, Loss: 0.4758\n",
      "Iteration 8400, Loss: 0.4754\n",
      "Iteration 8500, Loss: 0.4751\n",
      "Iteration 8600, Loss: 0.4747\n",
      "Iteration 8700, Loss: 0.4743\n",
      "Iteration 8800, Loss: 0.4739\n",
      "Iteration 8900, Loss: 0.4736\n",
      "Iteration 0, Loss: 1.0136\n",
      "Iteration 100, Loss: 0.6058\n",
      "Iteration 200, Loss: 0.5744\n",
      "Iteration 300, Loss: 0.5563\n",
      "Iteration 400, Loss: 0.5436\n",
      "Iteration 500, Loss: 0.5338\n",
      "Iteration 600, Loss: 0.5260\n",
      "Iteration 700, Loss: 0.5194\n",
      "Iteration 800, Loss: 0.5138\n",
      "Iteration 900, Loss: 0.5088\n",
      "Iteration 1000, Loss: 0.5045\n",
      "Iteration 1100, Loss: 0.5006\n",
      "Iteration 1200, Loss: 0.4970\n",
      "Iteration 1300, Loss: 0.4938\n",
      "Iteration 1400, Loss: 0.4909\n",
      "Iteration 1500, Loss: 0.4881\n",
      "Iteration 1600, Loss: 0.4856\n",
      "Iteration 1700, Loss: 0.4832\n",
      "Iteration 1800, Loss: 0.4810\n",
      "Iteration 1900, Loss: 0.4789\n",
      "Iteration 2000, Loss: 0.4770\n",
      "Iteration 2100, Loss: 0.4751\n",
      "Iteration 2200, Loss: 0.4733\n",
      "Iteration 2300, Loss: 0.4717\n",
      "Iteration 2400, Loss: 0.4701\n",
      "Iteration 2500, Loss: 0.4686\n",
      "Iteration 2600, Loss: 0.4671\n",
      "Iteration 2700, Loss: 0.4657\n",
      "Iteration 2800, Loss: 0.4644\n",
      "Iteration 2900, Loss: 0.4631\n",
      "Iteration 3000, Loss: 0.4619\n",
      "Iteration 3100, Loss: 0.4607\n",
      "Iteration 3200, Loss: 0.4596\n",
      "Iteration 3300, Loss: 0.4585\n",
      "Iteration 3400, Loss: 0.4574\n",
      "Iteration 3500, Loss: 0.4564\n",
      "Iteration 3600, Loss: 0.4554\n",
      "Iteration 3700, Loss: 0.4544\n",
      "Iteration 3800, Loss: 0.4535\n",
      "Iteration 3900, Loss: 0.4526\n",
      "Iteration 4000, Loss: 0.4517\n",
      "Iteration 4100, Loss: 0.4508\n",
      "Iteration 4200, Loss: 0.4500\n",
      "Iteration 4300, Loss: 0.4492\n",
      "Iteration 4400, Loss: 0.4484\n",
      "Iteration 4500, Loss: 0.4476\n",
      "Iteration 4600, Loss: 0.4468\n",
      "Iteration 4700, Loss: 0.4461\n",
      "Iteration 4800, Loss: 0.4454\n",
      "Iteration 4900, Loss: 0.4447\n",
      "Iteration 5000, Loss: 0.4440\n",
      "Iteration 5100, Loss: 0.4433\n",
      "Iteration 5200, Loss: 0.4427\n",
      "Iteration 5300, Loss: 0.4420\n",
      "Iteration 5400, Loss: 0.4414\n",
      "Iteration 5500, Loss: 0.4408\n",
      "Iteration 5600, Loss: 0.4402\n",
      "Iteration 5700, Loss: 0.4396\n",
      "Iteration 5800, Loss: 0.4390\n",
      "Iteration 5900, Loss: 0.4384\n",
      "Iteration 6000, Loss: 0.4378\n",
      "Iteration 6100, Loss: 0.4373\n",
      "Iteration 6200, Loss: 0.4368\n",
      "Iteration 6300, Loss: 0.4362\n",
      "Iteration 6400, Loss: 0.4357\n",
      "Iteration 6500, Loss: 0.4352\n",
      "Iteration 6600, Loss: 0.4347\n",
      "Iteration 6700, Loss: 0.4342\n",
      "Iteration 6800, Loss: 0.4337\n",
      "Iteration 6900, Loss: 0.4333\n",
      "Iteration 7000, Loss: 0.4328\n",
      "Iteration 7100, Loss: 0.4323\n",
      "Iteration 7200, Loss: 0.4319\n",
      "Iteration 7300, Loss: 0.4314\n",
      "Iteration 7400, Loss: 0.4310\n",
      "Iteration 7500, Loss: 0.4306\n",
      "Iteration 7600, Loss: 0.4301\n",
      "Iteration 7700, Loss: 0.4297\n",
      "Iteration 7800, Loss: 0.4293\n",
      "Iteration 7900, Loss: 0.4289\n",
      "Iteration 8000, Loss: 0.4285\n",
      "Iteration 8100, Loss: 0.4281\n",
      "Iteration 8200, Loss: 0.4277\n",
      "Iteration 8300, Loss: 0.4273\n",
      "Iteration 8400, Loss: 0.4269\n",
      "Iteration 8500, Loss: 0.4266\n",
      "Iteration 8600, Loss: 0.4262\n",
      "Iteration 8700, Loss: 0.4258\n",
      "Iteration 8800, Loss: 0.4255\n",
      "Iteration 8900, Loss: 0.4251\n",
      "Iteration 0, Loss: 1.0581\n",
      "Iteration 100, Loss: 0.6709\n",
      "Iteration 200, Loss: 0.6381\n",
      "Iteration 300, Loss: 0.6205\n",
      "Iteration 400, Loss: 0.6086\n",
      "Iteration 500, Loss: 0.5995\n",
      "Iteration 600, Loss: 0.5923\n",
      "Iteration 700, Loss: 0.5862\n",
      "Iteration 800, Loss: 0.5812\n",
      "Iteration 900, Loss: 0.5766\n",
      "Iteration 1000, Loss: 0.5727\n",
      "Iteration 1100, Loss: 0.5692\n",
      "Iteration 1200, Loss: 0.5659\n",
      "Iteration 1300, Loss: 0.5629\n",
      "Iteration 1400, Loss: 0.5602\n",
      "Iteration 1500, Loss: 0.5576\n",
      "Iteration 1600, Loss: 0.5552\n",
      "Iteration 1700, Loss: 0.5530\n",
      "Iteration 1800, Loss: 0.5511\n",
      "Iteration 1900, Loss: 0.5490\n",
      "Iteration 2000, Loss: 0.5471\n",
      "Iteration 2100, Loss: 0.5454\n",
      "Iteration 2200, Loss: 0.5437\n",
      "Iteration 2300, Loss: 0.5421\n",
      "Iteration 2400, Loss: 0.5406\n",
      "Iteration 2500, Loss: 0.5391\n",
      "Iteration 2600, Loss: 0.5377\n",
      "Iteration 2700, Loss: 0.5364\n",
      "Iteration 2800, Loss: 0.5351\n",
      "Iteration 2900, Loss: 0.5338\n",
      "Iteration 3000, Loss: 0.5327\n",
      "Iteration 3100, Loss: 0.5314\n",
      "Iteration 3200, Loss: 0.5303\n",
      "Iteration 3300, Loss: 0.5292\n",
      "Iteration 3400, Loss: 0.5282\n",
      "Iteration 3500, Loss: 0.5271\n",
      "Iteration 3600, Loss: 0.5262\n",
      "Iteration 3700, Loss: 0.5251\n",
      "Iteration 3800, Loss: 0.5242\n",
      "Iteration 3900, Loss: 0.5233\n",
      "Iteration 4000, Loss: 0.5224\n",
      "Iteration 4100, Loss: 0.5215\n",
      "Iteration 4200, Loss: 0.5206\n",
      "Iteration 4300, Loss: 0.5198\n",
      "Iteration 4400, Loss: 0.5190\n",
      "Iteration 4500, Loss: 0.5182\n",
      "Iteration 4600, Loss: 0.5174\n",
      "Iteration 4700, Loss: 0.5166\n",
      "Iteration 4800, Loss: 0.5159\n",
      "Iteration 4900, Loss: 0.5152\n",
      "Iteration 5000, Loss: 0.5144\n",
      "Iteration 5100, Loss: 0.5137\n",
      "Iteration 5200, Loss: 0.5130\n",
      "Iteration 5300, Loss: 0.5124\n",
      "Iteration 5400, Loss: 0.5118\n",
      "Iteration 5500, Loss: 0.5111\n",
      "Iteration 5600, Loss: 0.5104\n",
      "Iteration 5700, Loss: 0.5098\n",
      "Iteration 5800, Loss: 0.5092\n",
      "Iteration 5900, Loss: 0.5086\n",
      "Iteration 6000, Loss: 0.5080\n",
      "Iteration 6100, Loss: 0.5074\n",
      "Iteration 6200, Loss: 0.5069\n",
      "Iteration 6300, Loss: 0.5063\n",
      "Iteration 6400, Loss: 0.5057\n",
      "Iteration 6500, Loss: 0.5052\n",
      "Iteration 6600, Loss: 0.5047\n",
      "Iteration 6700, Loss: 0.5041\n",
      "Iteration 6800, Loss: 0.5037\n",
      "Iteration 6900, Loss: 0.5031\n",
      "Iteration 7000, Loss: 0.5026\n",
      "Iteration 7100, Loss: 0.5021\n",
      "Iteration 7200, Loss: 0.5016\n",
      "Iteration 7300, Loss: 0.5012\n",
      "Iteration 7400, Loss: 0.5007\n",
      "Iteration 7500, Loss: 0.5002\n",
      "Iteration 7600, Loss: 0.4997\n",
      "Iteration 7700, Loss: 0.4993\n",
      "Iteration 7800, Loss: 0.4988\n",
      "Iteration 7900, Loss: 0.4984\n",
      "Iteration 8000, Loss: 0.4979\n",
      "Iteration 8100, Loss: 0.4975\n",
      "Iteration 8200, Loss: 0.4971\n",
      "Iteration 8300, Loss: 0.4967\n",
      "Iteration 8400, Loss: 0.4962\n",
      "Iteration 8500, Loss: 0.4958\n",
      "Iteration 8600, Loss: 0.4954\n",
      "Iteration 8700, Loss: 0.4950\n",
      "Iteration 8800, Loss: 0.4946\n",
      "Iteration 8900, Loss: 0.4942\n",
      "Iteration 0, Loss: 1.0520\n",
      "Iteration 100, Loss: 0.6404\n",
      "Iteration 200, Loss: 0.6018\n",
      "Iteration 300, Loss: 0.5821\n",
      "Iteration 400, Loss: 0.5685\n",
      "Iteration 500, Loss: 0.5580\n",
      "Iteration 600, Loss: 0.5495\n",
      "Iteration 700, Loss: 0.5424\n",
      "Iteration 800, Loss: 0.5362\n",
      "Iteration 900, Loss: 0.5308\n",
      "Iteration 1000, Loss: 0.5260\n",
      "Iteration 1100, Loss: 0.5219\n",
      "Iteration 1200, Loss: 0.5179\n",
      "Iteration 1300, Loss: 0.5144\n",
      "Iteration 1400, Loss: 0.5111\n",
      "Iteration 1500, Loss: 0.5081\n",
      "Iteration 1600, Loss: 0.5054\n",
      "Iteration 1700, Loss: 0.5028\n",
      "Iteration 1800, Loss: 0.5004\n",
      "Iteration 1900, Loss: 0.4981\n",
      "Iteration 2000, Loss: 0.4959\n",
      "Iteration 2100, Loss: 0.4939\n",
      "Iteration 2200, Loss: 0.4920\n",
      "Iteration 2300, Loss: 0.4902\n",
      "Iteration 2400, Loss: 0.4884\n",
      "Iteration 2500, Loss: 0.4868\n",
      "Iteration 2600, Loss: 0.4852\n",
      "Iteration 2700, Loss: 0.4837\n",
      "Iteration 2800, Loss: 0.4822\n",
      "Iteration 2900, Loss: 0.4808\n",
      "Iteration 3000, Loss: 0.4795\n",
      "Iteration 3100, Loss: 0.4782\n",
      "Iteration 3200, Loss: 0.4770\n",
      "Iteration 3300, Loss: 0.4758\n",
      "Iteration 3400, Loss: 0.4746\n",
      "Iteration 3500, Loss: 0.4735\n",
      "Iteration 3600, Loss: 0.4724\n",
      "Iteration 3700, Loss: 0.4713\n",
      "Iteration 3800, Loss: 0.4703\n",
      "Iteration 3900, Loss: 0.4693\n",
      "Iteration 4000, Loss: 0.4683\n",
      "Iteration 4100, Loss: 0.4674\n",
      "Iteration 4200, Loss: 0.4665\n",
      "Iteration 4300, Loss: 0.4656\n",
      "Iteration 4400, Loss: 0.4648\n",
      "Iteration 4500, Loss: 0.4639\n",
      "Iteration 4600, Loss: 0.4631\n",
      "Iteration 4700, Loss: 0.4623\n",
      "Iteration 4800, Loss: 0.4616\n",
      "Iteration 4900, Loss: 0.4608\n",
      "Iteration 5000, Loss: 0.4601\n",
      "Iteration 5100, Loss: 0.4593\n",
      "Iteration 5200, Loss: 0.4586\n",
      "Iteration 5300, Loss: 0.4579\n",
      "Iteration 5400, Loss: 0.4573\n",
      "Iteration 5500, Loss: 0.4566\n",
      "Iteration 5600, Loss: 0.4560\n",
      "Iteration 5700, Loss: 0.4553\n",
      "Iteration 5800, Loss: 0.4547\n",
      "Iteration 5900, Loss: 0.4541\n",
      "Iteration 6000, Loss: 0.4535\n",
      "Iteration 6100, Loss: 0.4529\n",
      "Iteration 6200, Loss: 0.4523\n",
      "Iteration 6300, Loss: 0.4518\n",
      "Iteration 6400, Loss: 0.4512\n",
      "Iteration 6500, Loss: 0.4507\n",
      "Iteration 6600, Loss: 0.4501\n",
      "Iteration 6700, Loss: 0.4496\n",
      "Iteration 6800, Loss: 0.4491\n",
      "Iteration 6900, Loss: 0.4486\n",
      "Iteration 7000, Loss: 0.4481\n",
      "Iteration 7100, Loss: 0.4476\n",
      "Iteration 7200, Loss: 0.4471\n",
      "Iteration 7300, Loss: 0.4467\n",
      "Iteration 7400, Loss: 0.4462\n",
      "Iteration 7500, Loss: 0.4458\n",
      "Iteration 7600, Loss: 0.4453\n",
      "Iteration 7700, Loss: 0.4449\n",
      "Iteration 7800, Loss: 0.4444\n",
      "Iteration 7900, Loss: 0.4440\n",
      "Iteration 8000, Loss: 0.4436\n",
      "Iteration 8100, Loss: 0.4432\n",
      "Iteration 8200, Loss: 0.4428\n",
      "Iteration 8300, Loss: 0.4424\n",
      "Iteration 8400, Loss: 0.4420\n",
      "Iteration 8500, Loss: 0.4416\n",
      "Iteration 8600, Loss: 0.4412\n",
      "Iteration 8700, Loss: 0.4408\n",
      "Iteration 8800, Loss: 0.4404\n",
      "Iteration 8900, Loss: 0.4401\n",
      "Iteration 0, Loss: 1.0804\n",
      "Iteration 100, Loss: 0.7167\n",
      "Iteration 200, Loss: 0.6663\n",
      "Iteration 300, Loss: 0.6427\n",
      "Iteration 400, Loss: 0.6281\n",
      "Iteration 500, Loss: 0.6178\n",
      "Iteration 600, Loss: 0.6100\n",
      "Iteration 700, Loss: 0.6036\n",
      "Iteration 800, Loss: 0.5983\n",
      "Iteration 900, Loss: 0.5936\n",
      "Iteration 1000, Loss: 0.5895\n",
      "Iteration 1100, Loss: 0.5858\n",
      "Iteration 1200, Loss: 0.5825\n",
      "Iteration 1300, Loss: 0.5794\n",
      "Iteration 1400, Loss: 0.5766\n",
      "Iteration 1500, Loss: 0.5739\n",
      "Iteration 1600, Loss: 0.5715\n",
      "Iteration 1700, Loss: 0.5692\n",
      "Iteration 1800, Loss: 0.5670\n",
      "Iteration 1900, Loss: 0.5650\n",
      "Iteration 2000, Loss: 0.5631\n",
      "Iteration 2100, Loss: 0.5612\n",
      "Iteration 2200, Loss: 0.5595\n",
      "Iteration 2300, Loss: 0.5578\n",
      "Iteration 2400, Loss: 0.5562\n",
      "Iteration 2500, Loss: 0.5547\n",
      "Iteration 2600, Loss: 0.5533\n",
      "Iteration 2700, Loss: 0.5519\n",
      "Iteration 2800, Loss: 0.5505\n",
      "Iteration 2900, Loss: 0.5492\n",
      "Iteration 3000, Loss: 0.5480\n",
      "Iteration 3100, Loss: 0.5468\n",
      "Iteration 3200, Loss: 0.5456\n",
      "Iteration 3300, Loss: 0.5445\n",
      "Iteration 3400, Loss: 0.5434\n",
      "Iteration 3500, Loss: 0.5423\n",
      "Iteration 3600, Loss: 0.5413\n",
      "Iteration 3700, Loss: 0.5403\n",
      "Iteration 3800, Loss: 0.5393\n",
      "Iteration 3900, Loss: 0.5384\n",
      "Iteration 4000, Loss: 0.5375\n",
      "Iteration 4100, Loss: 0.5366\n",
      "Iteration 4200, Loss: 0.5357\n",
      "Iteration 4300, Loss: 0.5348\n",
      "Iteration 4400, Loss: 0.5340\n",
      "Iteration 4500, Loss: 0.5332\n",
      "Iteration 4600, Loss: 0.5324\n",
      "Iteration 4700, Loss: 0.5316\n",
      "Iteration 4800, Loss: 0.5308\n",
      "Iteration 4900, Loss: 0.5301\n",
      "Iteration 5000, Loss: 0.5294\n",
      "Iteration 5100, Loss: 0.5286\n",
      "Iteration 5200, Loss: 0.5279\n",
      "Iteration 5300, Loss: 0.5273\n",
      "Iteration 5400, Loss: 0.5266\n",
      "Iteration 5500, Loss: 0.5259\n",
      "Iteration 5600, Loss: 0.5253\n",
      "Iteration 5700, Loss: 0.5246\n",
      "Iteration 5800, Loss: 0.5240\n",
      "Iteration 5900, Loss: 0.5234\n",
      "Iteration 6000, Loss: 0.5228\n",
      "Iteration 6100, Loss: 0.5222\n",
      "Iteration 6200, Loss: 0.5216\n",
      "Iteration 6300, Loss: 0.5210\n",
      "Iteration 6400, Loss: 0.5204\n",
      "Iteration 6500, Loss: 0.5198\n",
      "Iteration 6600, Loss: 0.5193\n",
      "Iteration 6700, Loss: 0.5187\n",
      "Iteration 6800, Loss: 0.5182\n",
      "Iteration 6900, Loss: 0.5177\n",
      "Iteration 7000, Loss: 0.5172\n",
      "Iteration 7100, Loss: 0.5166\n",
      "Iteration 7200, Loss: 0.5161\n",
      "Iteration 7300, Loss: 0.5156\n",
      "Iteration 7400, Loss: 0.5151\n",
      "Iteration 7500, Loss: 0.5147\n",
      "Iteration 7600, Loss: 0.5142\n",
      "Iteration 7700, Loss: 0.5137\n",
      "Iteration 7800, Loss: 0.5132\n",
      "Iteration 7900, Loss: 0.5128\n",
      "Iteration 8000, Loss: 0.5123\n",
      "Iteration 8100, Loss: 0.5119\n",
      "Iteration 8200, Loss: 0.5114\n",
      "Iteration 8300, Loss: 0.5110\n",
      "Iteration 8400, Loss: 0.5105\n",
      "Iteration 8500, Loss: 0.5101\n",
      "Iteration 8600, Loss: 0.5097\n",
      "Iteration 8700, Loss: 0.5093\n",
      "Iteration 8800, Loss: 0.5089\n",
      "Iteration 8900, Loss: 0.5084\n",
      "Iteration 0, Loss: 1.0806\n",
      "Iteration 100, Loss: 0.7260\n",
      "Iteration 200, Loss: 0.6778\n",
      "Iteration 300, Loss: 0.6538\n",
      "Iteration 400, Loss: 0.6380\n",
      "Iteration 500, Loss: 0.6263\n",
      "Iteration 600, Loss: 0.6171\n",
      "Iteration 700, Loss: 0.6095\n",
      "Iteration 800, Loss: 0.6030\n",
      "Iteration 900, Loss: 0.5974\n",
      "Iteration 1000, Loss: 0.5924\n",
      "Iteration 1100, Loss: 0.5879\n",
      "Iteration 1200, Loss: 0.5839\n",
      "Iteration 1300, Loss: 0.5802\n",
      "Iteration 1400, Loss: 0.5768\n",
      "Iteration 1500, Loss: 0.5737\n",
      "Iteration 1600, Loss: 0.5708\n",
      "Iteration 1700, Loss: 0.5681\n",
      "Iteration 1800, Loss: 0.5656\n",
      "Iteration 1900, Loss: 0.5632\n",
      "Iteration 2000, Loss: 0.5610\n",
      "Iteration 2100, Loss: 0.5589\n",
      "Iteration 2200, Loss: 0.5569\n",
      "Iteration 2300, Loss: 0.5550\n",
      "Iteration 2400, Loss: 0.5532\n",
      "Iteration 2500, Loss: 0.5514\n",
      "Iteration 2600, Loss: 0.5498\n",
      "Iteration 2700, Loss: 0.5482\n",
      "Iteration 2800, Loss: 0.5467\n",
      "Iteration 2900, Loss: 0.5452\n",
      "Iteration 3000, Loss: 0.5439\n",
      "Iteration 3100, Loss: 0.5425\n",
      "Iteration 3200, Loss: 0.5412\n",
      "Iteration 3300, Loss: 0.5400\n",
      "Iteration 3400, Loss: 0.5388\n",
      "Iteration 3500, Loss: 0.5376\n",
      "Iteration 3600, Loss: 0.5365\n",
      "Iteration 3700, Loss: 0.5354\n",
      "Iteration 3800, Loss: 0.5343\n",
      "Iteration 3900, Loss: 0.5333\n",
      "Iteration 4000, Loss: 0.5323\n",
      "Iteration 4100, Loss: 0.5313\n",
      "Iteration 4200, Loss: 0.5304\n",
      "Iteration 4300, Loss: 0.5295\n",
      "Iteration 4400, Loss: 0.5286\n",
      "Iteration 4500, Loss: 0.5277\n",
      "Iteration 4600, Loss: 0.5269\n",
      "Iteration 4700, Loss: 0.5260\n",
      "Iteration 4800, Loss: 0.5252\n",
      "Iteration 4900, Loss: 0.5244\n",
      "Iteration 5000, Loss: 0.5237\n",
      "Iteration 5100, Loss: 0.5229\n",
      "Iteration 5200, Loss: 0.5222\n",
      "Iteration 5300, Loss: 0.5215\n",
      "Iteration 5400, Loss: 0.5208\n",
      "Iteration 5500, Loss: 0.5201\n",
      "Iteration 5600, Loss: 0.5194\n",
      "Iteration 5700, Loss: 0.5187\n",
      "Iteration 5800, Loss: 0.5181\n",
      "Iteration 5900, Loss: 0.5174\n",
      "Iteration 6000, Loss: 0.5168\n",
      "Iteration 6100, Loss: 0.5162\n",
      "Iteration 6200, Loss: 0.5156\n",
      "Iteration 6300, Loss: 0.5150\n",
      "Iteration 6400, Loss: 0.5144\n",
      "Iteration 6500, Loss: 0.5138\n",
      "Iteration 6600, Loss: 0.5133\n",
      "Iteration 6700, Loss: 0.5127\n",
      "Iteration 6800, Loss: 0.5122\n",
      "Iteration 6900, Loss: 0.5116\n",
      "Iteration 7000, Loss: 0.5111\n",
      "Iteration 7100, Loss: 0.5106\n",
      "Iteration 7200, Loss: 0.5101\n",
      "Iteration 7300, Loss: 0.5096\n",
      "Iteration 7400, Loss: 0.5091\n",
      "Iteration 7500, Loss: 0.5086\n",
      "Iteration 7600, Loss: 0.5081\n",
      "Iteration 7700, Loss: 0.5077\n",
      "Iteration 7800, Loss: 0.5072\n",
      "Iteration 7900, Loss: 0.5067\n",
      "Iteration 8000, Loss: 0.5063\n",
      "Iteration 8100, Loss: 0.5058\n",
      "Iteration 8200, Loss: 0.5054\n",
      "Iteration 8300, Loss: 0.5049\n",
      "Iteration 8400, Loss: 0.5045\n",
      "Iteration 8500, Loss: 0.5041\n",
      "Iteration 8600, Loss: 0.5037\n",
      "Iteration 8700, Loss: 0.5033\n",
      "Iteration 8800, Loss: 0.5028\n",
      "Iteration 8900, Loss: 0.5024\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7845\n",
      "Iteration 200, Loss: 0.7210\n",
      "Iteration 300, Loss: 0.6922\n",
      "Iteration 400, Loss: 0.6746\n",
      "Iteration 500, Loss: 0.6621\n",
      "Iteration 600, Loss: 0.6524\n",
      "Iteration 700, Loss: 0.6447\n",
      "Iteration 800, Loss: 0.6382\n",
      "Iteration 900, Loss: 0.6327\n",
      "Iteration 1000, Loss: 0.6278\n",
      "Iteration 1100, Loss: 0.6235\n",
      "Iteration 1200, Loss: 0.6197\n",
      "Iteration 1300, Loss: 0.6162\n",
      "Iteration 1400, Loss: 0.6130\n",
      "Iteration 1500, Loss: 0.6101\n",
      "Iteration 1600, Loss: 0.6073\n",
      "Iteration 1700, Loss: 0.6047\n",
      "Iteration 1800, Loss: 0.6023\n",
      "Iteration 1900, Loss: 0.6001\n",
      "Iteration 2000, Loss: 0.5979\n",
      "Iteration 2100, Loss: 0.5959\n",
      "Iteration 2200, Loss: 0.5940\n",
      "Iteration 2300, Loss: 0.5922\n",
      "Iteration 2400, Loss: 0.5904\n",
      "Iteration 2500, Loss: 0.5887\n",
      "Iteration 2600, Loss: 0.5871\n",
      "Iteration 2700, Loss: 0.5856\n",
      "Iteration 2800, Loss: 0.5841\n",
      "Iteration 2900, Loss: 0.5827\n",
      "Iteration 3000, Loss: 0.5813\n",
      "Iteration 3100, Loss: 0.5800\n",
      "Iteration 3200, Loss: 0.5787\n",
      "Iteration 3300, Loss: 0.5775\n",
      "Iteration 3400, Loss: 0.5763\n",
      "Iteration 3500, Loss: 0.5751\n",
      "Iteration 3600, Loss: 0.5740\n",
      "Iteration 3700, Loss: 0.5729\n",
      "Iteration 3800, Loss: 0.5718\n",
      "Iteration 3900, Loss: 0.5707\n",
      "Iteration 4000, Loss: 0.5697\n",
      "Iteration 4100, Loss: 0.5688\n",
      "Iteration 4200, Loss: 0.5678\n",
      "Iteration 4300, Loss: 0.5669\n",
      "Iteration 4400, Loss: 0.5659\n",
      "Iteration 4500, Loss: 0.5650\n",
      "Iteration 4600, Loss: 0.5642\n",
      "Iteration 4700, Loss: 0.5633\n",
      "Iteration 4800, Loss: 0.5625\n",
      "Iteration 4900, Loss: 0.5617\n",
      "Iteration 5000, Loss: 0.5609\n",
      "Iteration 5100, Loss: 0.5601\n",
      "Iteration 5200, Loss: 0.5593\n",
      "Iteration 5300, Loss: 0.5586\n",
      "Iteration 5400, Loss: 0.5578\n",
      "Iteration 5500, Loss: 0.5571\n",
      "Iteration 5600, Loss: 0.5564\n",
      "Iteration 5700, Loss: 0.5557\n",
      "Iteration 5800, Loss: 0.5550\n",
      "Iteration 5900, Loss: 0.5543\n",
      "Iteration 6000, Loss: 0.5537\n",
      "Iteration 6100, Loss: 0.5530\n",
      "Iteration 6200, Loss: 0.5524\n",
      "Iteration 6300, Loss: 0.5517\n",
      "Iteration 6400, Loss: 0.5511\n",
      "Iteration 6500, Loss: 0.5505\n",
      "Iteration 6600, Loss: 0.5499\n",
      "Iteration 6700, Loss: 0.5493\n",
      "Iteration 6800, Loss: 0.5487\n",
      "Iteration 6900, Loss: 0.5482\n",
      "Iteration 7000, Loss: 0.5476\n",
      "Iteration 7100, Loss: 0.5470\n",
      "Iteration 7200, Loss: 0.5465\n",
      "Iteration 7300, Loss: 0.5459\n",
      "Iteration 7400, Loss: 0.5454\n",
      "Iteration 7500, Loss: 0.5449\n",
      "Iteration 7600, Loss: 0.5443\n",
      "Iteration 7700, Loss: 0.5438\n",
      "Iteration 7800, Loss: 0.5433\n",
      "Iteration 7900, Loss: 0.5428\n",
      "Iteration 8000, Loss: 0.5423\n",
      "Iteration 8100, Loss: 0.5418\n",
      "Iteration 8200, Loss: 0.5413\n",
      "Iteration 8300, Loss: 0.5409\n",
      "Iteration 8400, Loss: 0.5404\n",
      "Iteration 8500, Loss: 0.5399\n",
      "Iteration 8600, Loss: 0.5395\n",
      "Iteration 8700, Loss: 0.5390\n",
      "Iteration 8800, Loss: 0.5386\n",
      "Iteration 8900, Loss: 0.5381\n",
      "Iteration 0, Loss: 1.0895\n",
      "Iteration 100, Loss: 0.7881\n",
      "Iteration 200, Loss: 0.7210\n",
      "Iteration 300, Loss: 0.6896\n",
      "Iteration 400, Loss: 0.6699\n",
      "Iteration 500, Loss: 0.6559\n",
      "Iteration 600, Loss: 0.6451\n",
      "Iteration 700, Loss: 0.6364\n",
      "Iteration 800, Loss: 0.6291\n",
      "Iteration 900, Loss: 0.6229\n",
      "Iteration 1000, Loss: 0.6174\n",
      "Iteration 1100, Loss: 0.6126\n",
      "Iteration 1200, Loss: 0.6083\n",
      "Iteration 1300, Loss: 0.6043\n",
      "Iteration 1400, Loss: 0.6007\n",
      "Iteration 1500, Loss: 0.5974\n",
      "Iteration 1600, Loss: 0.5943\n",
      "Iteration 1700, Loss: 0.5914\n",
      "Iteration 1800, Loss: 0.5887\n",
      "Iteration 1900, Loss: 0.5862\n",
      "Iteration 2000, Loss: 0.5838\n",
      "Iteration 2100, Loss: 0.5815\n",
      "Iteration 2200, Loss: 0.5794\n",
      "Iteration 2300, Loss: 0.5773\n",
      "Iteration 2400, Loss: 0.5754\n",
      "Iteration 2500, Loss: 0.5735\n",
      "Iteration 2600, Loss: 0.5717\n",
      "Iteration 2700, Loss: 0.5700\n",
      "Iteration 2800, Loss: 0.5684\n",
      "Iteration 2900, Loss: 0.5668\n",
      "Iteration 3000, Loss: 0.5653\n",
      "Iteration 3100, Loss: 0.5638\n",
      "Iteration 3200, Loss: 0.5624\n",
      "Iteration 3300, Loss: 0.5610\n",
      "Iteration 3400, Loss: 0.5597\n",
      "Iteration 3500, Loss: 0.5584\n",
      "Iteration 3600, Loss: 0.5572\n",
      "Iteration 3700, Loss: 0.5560\n",
      "Iteration 3800, Loss: 0.5548\n",
      "Iteration 3900, Loss: 0.5537\n",
      "Iteration 4000, Loss: 0.5526\n",
      "Iteration 4100, Loss: 0.5515\n",
      "Iteration 4200, Loss: 0.5505\n",
      "Iteration 4300, Loss: 0.5495\n",
      "Iteration 4400, Loss: 0.5485\n",
      "Iteration 4500, Loss: 0.5476\n",
      "Iteration 4600, Loss: 0.5466\n",
      "Iteration 4700, Loss: 0.5457\n",
      "Iteration 4800, Loss: 0.5448\n",
      "Iteration 4900, Loss: 0.5440\n",
      "Iteration 5000, Loss: 0.5431\n",
      "Iteration 5100, Loss: 0.5423\n",
      "Iteration 5200, Loss: 0.5415\n",
      "Iteration 5300, Loss: 0.5407\n",
      "Iteration 5400, Loss: 0.5399\n",
      "Iteration 5500, Loss: 0.5392\n",
      "Iteration 5600, Loss: 0.5384\n",
      "Iteration 5700, Loss: 0.5377\n",
      "Iteration 5800, Loss: 0.5370\n",
      "Iteration 5900, Loss: 0.5363\n",
      "Iteration 6000, Loss: 0.5356\n",
      "Iteration 6100, Loss: 0.5349\n",
      "Iteration 6200, Loss: 0.5342\n",
      "Iteration 6300, Loss: 0.5336\n",
      "Iteration 6400, Loss: 0.5330\n",
      "Iteration 6500, Loss: 0.5323\n",
      "Iteration 6600, Loss: 0.5317\n",
      "Iteration 6700, Loss: 0.5311\n",
      "Iteration 6800, Loss: 0.5305\n",
      "Iteration 6900, Loss: 0.5300\n",
      "Iteration 7000, Loss: 0.5294\n",
      "Iteration 7100, Loss: 0.5288\n",
      "Iteration 7200, Loss: 0.5283\n",
      "Iteration 7300, Loss: 0.5277\n",
      "Iteration 7400, Loss: 0.5272\n",
      "Iteration 7500, Loss: 0.5266\n",
      "Iteration 7600, Loss: 0.5261\n",
      "Iteration 7700, Loss: 0.5256\n",
      "Iteration 7800, Loss: 0.5251\n",
      "Iteration 7900, Loss: 0.5246\n",
      "Iteration 8000, Loss: 0.5241\n",
      "Iteration 8100, Loss: 0.5236\n",
      "Iteration 8200, Loss: 0.5232\n",
      "Iteration 8300, Loss: 0.5227\n",
      "Iteration 8400, Loss: 0.5222\n",
      "Iteration 8500, Loss: 0.5218\n",
      "Iteration 8600, Loss: 0.5213\n",
      "Iteration 8700, Loss: 0.5209\n",
      "Iteration 8800, Loss: 0.5204\n",
      "Iteration 8900, Loss: 0.5200\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8577\n",
      "Iteration 200, Loss: 0.7770\n",
      "Iteration 300, Loss: 0.7356\n",
      "Iteration 400, Loss: 0.7097\n",
      "Iteration 500, Loss: 0.6918\n",
      "Iteration 600, Loss: 0.6784\n",
      "Iteration 700, Loss: 0.6678\n",
      "Iteration 800, Loss: 0.6590\n",
      "Iteration 900, Loss: 0.6515\n",
      "Iteration 1000, Loss: 0.6451\n",
      "Iteration 1100, Loss: 0.6395\n",
      "Iteration 1200, Loss: 0.6345\n",
      "Iteration 1300, Loss: 0.6299\n",
      "Iteration 1400, Loss: 0.6258\n",
      "Iteration 1500, Loss: 0.6221\n",
      "Iteration 1600, Loss: 0.6186\n",
      "Iteration 1700, Loss: 0.6154\n",
      "Iteration 1800, Loss: 0.6124\n",
      "Iteration 1900, Loss: 0.6097\n",
      "Iteration 2000, Loss: 0.6071\n",
      "Iteration 2100, Loss: 0.6046\n",
      "Iteration 2200, Loss: 0.6023\n",
      "Iteration 2300, Loss: 0.6001\n",
      "Iteration 2400, Loss: 0.5980\n",
      "Iteration 2500, Loss: 0.5960\n",
      "Iteration 2600, Loss: 0.5941\n",
      "Iteration 2700, Loss: 0.5924\n",
      "Iteration 2800, Loss: 0.5906\n",
      "Iteration 2900, Loss: 0.5890\n",
      "Iteration 3000, Loss: 0.5874\n",
      "Iteration 3100, Loss: 0.5859\n",
      "Iteration 3200, Loss: 0.5844\n",
      "Iteration 3300, Loss: 0.5830\n",
      "Iteration 3400, Loss: 0.5816\n",
      "Iteration 3500, Loss: 0.5803\n",
      "Iteration 3600, Loss: 0.5790\n",
      "Iteration 3700, Loss: 0.5777\n",
      "Iteration 3800, Loss: 0.5765\n",
      "Iteration 3900, Loss: 0.5754\n",
      "Iteration 4000, Loss: 0.5742\n",
      "Iteration 4100, Loss: 0.5731\n",
      "Iteration 4200, Loss: 0.5720\n",
      "Iteration 4300, Loss: 0.5710\n",
      "Iteration 4400, Loss: 0.5699\n",
      "Iteration 4500, Loss: 0.5689\n",
      "Iteration 4600, Loss: 0.5680\n",
      "Iteration 4700, Loss: 0.5670\n",
      "Iteration 4800, Loss: 0.5661\n",
      "Iteration 4900, Loss: 0.5652\n",
      "Iteration 5000, Loss: 0.5643\n",
      "Iteration 5100, Loss: 0.5634\n",
      "Iteration 5200, Loss: 0.5625\n",
      "Iteration 5300, Loss: 0.5617\n",
      "Iteration 5400, Loss: 0.5609\n",
      "Iteration 5500, Loss: 0.5601\n",
      "Iteration 5600, Loss: 0.5593\n",
      "Iteration 5700, Loss: 0.5585\n",
      "Iteration 5800, Loss: 0.5577\n",
      "Iteration 5900, Loss: 0.5570\n",
      "Iteration 6000, Loss: 0.5563\n",
      "Iteration 6100, Loss: 0.5555\n",
      "Iteration 6200, Loss: 0.5548\n",
      "Iteration 6300, Loss: 0.5541\n",
      "Iteration 6400, Loss: 0.5534\n",
      "Iteration 6500, Loss: 0.5528\n",
      "Iteration 6600, Loss: 0.5521\n",
      "Iteration 6700, Loss: 0.5514\n",
      "Iteration 6800, Loss: 0.5508\n",
      "Iteration 6900, Loss: 0.5502\n",
      "Iteration 7000, Loss: 0.5496\n",
      "Iteration 7100, Loss: 0.5490\n",
      "Iteration 7200, Loss: 0.5484\n",
      "Iteration 7300, Loss: 0.5478\n",
      "Iteration 7400, Loss: 0.5472\n",
      "Iteration 7500, Loss: 0.5466\n",
      "Iteration 7600, Loss: 0.5460\n",
      "Iteration 7700, Loss: 0.5454\n",
      "Iteration 7800, Loss: 0.5449\n",
      "Iteration 7900, Loss: 0.5443\n",
      "Iteration 8000, Loss: 0.5438\n",
      "Iteration 8100, Loss: 0.5433\n",
      "Iteration 8200, Loss: 0.5427\n",
      "Iteration 8300, Loss: 0.5422\n",
      "Iteration 8400, Loss: 0.5417\n",
      "Iteration 8500, Loss: 0.5412\n",
      "Iteration 8600, Loss: 0.5407\n",
      "Iteration 8700, Loss: 0.5402\n",
      "Iteration 8800, Loss: 0.5397\n",
      "Iteration 8900, Loss: 0.5392\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8655\n",
      "Iteration 200, Loss: 0.7890\n",
      "Iteration 300, Loss: 0.7506\n",
      "Iteration 400, Loss: 0.7273\n",
      "Iteration 500, Loss: 0.7115\n",
      "Iteration 600, Loss: 0.6996\n",
      "Iteration 700, Loss: 0.6903\n",
      "Iteration 800, Loss: 0.6826\n",
      "Iteration 900, Loss: 0.6762\n",
      "Iteration 1000, Loss: 0.6705\n",
      "Iteration 1100, Loss: 0.6656\n",
      "Iteration 1200, Loss: 0.6613\n",
      "Iteration 1300, Loss: 0.6573\n",
      "Iteration 1400, Loss: 0.6537\n",
      "Iteration 1500, Loss: 0.6505\n",
      "Iteration 1600, Loss: 0.6475\n",
      "Iteration 1700, Loss: 0.6447\n",
      "Iteration 1800, Loss: 0.6420\n",
      "Iteration 1900, Loss: 0.6396\n",
      "Iteration 2000, Loss: 0.6372\n",
      "Iteration 2100, Loss: 0.6350\n",
      "Iteration 2200, Loss: 0.6330\n",
      "Iteration 2300, Loss: 0.6310\n",
      "Iteration 2400, Loss: 0.6291\n",
      "Iteration 2500, Loss: 0.6273\n",
      "Iteration 2600, Loss: 0.6255\n",
      "Iteration 2700, Loss: 0.6238\n",
      "Iteration 2800, Loss: 0.6222\n",
      "Iteration 2900, Loss: 0.6207\n",
      "Iteration 3000, Loss: 0.6191\n",
      "Iteration 3100, Loss: 0.6177\n",
      "Iteration 3200, Loss: 0.6163\n",
      "Iteration 3300, Loss: 0.6149\n",
      "Iteration 3400, Loss: 0.6136\n",
      "Iteration 3500, Loss: 0.6124\n",
      "Iteration 3600, Loss: 0.6111\n",
      "Iteration 3700, Loss: 0.6099\n",
      "Iteration 3800, Loss: 0.6088\n",
      "Iteration 3900, Loss: 0.6076\n",
      "Iteration 4000, Loss: 0.6065\n",
      "Iteration 4100, Loss: 0.6054\n",
      "Iteration 4200, Loss: 0.6044\n",
      "Iteration 4300, Loss: 0.6034\n",
      "Iteration 4400, Loss: 0.6024\n",
      "Iteration 4500, Loss: 0.6014\n",
      "Iteration 4600, Loss: 0.6004\n",
      "Iteration 4700, Loss: 0.5995\n",
      "Iteration 4800, Loss: 0.5986\n",
      "Iteration 4900, Loss: 0.5977\n",
      "Iteration 5000, Loss: 0.5968\n",
      "Iteration 5100, Loss: 0.5960\n",
      "Iteration 5200, Loss: 0.5951\n",
      "Iteration 5300, Loss: 0.5943\n",
      "Iteration 5400, Loss: 0.5935\n",
      "Iteration 5500, Loss: 0.5927\n",
      "Iteration 5600, Loss: 0.5920\n",
      "Iteration 5700, Loss: 0.5912\n",
      "Iteration 5800, Loss: 0.5905\n",
      "Iteration 5900, Loss: 0.5897\n",
      "Iteration 6000, Loss: 0.5890\n",
      "Iteration 6100, Loss: 0.5883\n",
      "Iteration 6200, Loss: 0.5877\n",
      "Iteration 6300, Loss: 0.5870\n",
      "Iteration 6400, Loss: 0.5863\n",
      "Iteration 6500, Loss: 0.5857\n",
      "Iteration 6600, Loss: 0.5850\n",
      "Iteration 6700, Loss: 0.5844\n",
      "Iteration 6800, Loss: 0.5838\n",
      "Iteration 6900, Loss: 0.5832\n",
      "Iteration 7000, Loss: 0.5826\n",
      "Iteration 7100, Loss: 0.5820\n",
      "Iteration 7200, Loss: 0.5814\n",
      "Iteration 7300, Loss: 0.5809\n",
      "Iteration 7400, Loss: 0.5803\n",
      "Iteration 7500, Loss: 0.5798\n",
      "Iteration 7600, Loss: 0.5792\n",
      "Iteration 7700, Loss: 0.5787\n",
      "Iteration 7800, Loss: 0.5782\n",
      "Iteration 7900, Loss: 0.5777\n",
      "Iteration 8000, Loss: 0.5772\n",
      "Iteration 8100, Loss: 0.5767\n",
      "Iteration 8200, Loss: 0.5762\n",
      "Iteration 8300, Loss: 0.5757\n",
      "Iteration 8400, Loss: 0.5752\n",
      "Iteration 8500, Loss: 0.5747\n",
      "Iteration 8600, Loss: 0.5743\n",
      "Iteration 8700, Loss: 0.5738\n",
      "Iteration 8800, Loss: 0.5734\n",
      "Iteration 8900, Loss: 0.5729\n",
      "Iteration 0, Loss: 1.0967\n",
      "Iteration 100, Loss: 0.9641\n",
      "Iteration 200, Loss: 0.8894\n",
      "Iteration 300, Loss: 0.8401\n",
      "Iteration 400, Loss: 0.8048\n",
      "Iteration 500, Loss: 0.7784\n",
      "Iteration 600, Loss: 0.7579\n",
      "Iteration 700, Loss: 0.7416\n",
      "Iteration 800, Loss: 0.7283\n",
      "Iteration 900, Loss: 0.7172\n",
      "Iteration 1000, Loss: 0.7077\n",
      "Iteration 1100, Loss: 0.6996\n",
      "Iteration 1200, Loss: 0.6925\n",
      "Iteration 1300, Loss: 0.6862\n",
      "Iteration 1400, Loss: 0.6805\n",
      "Iteration 1500, Loss: 0.6754\n",
      "Iteration 1600, Loss: 0.6708\n",
      "Iteration 1700, Loss: 0.6666\n",
      "Iteration 1800, Loss: 0.6627\n",
      "Iteration 1900, Loss: 0.6591\n",
      "Iteration 2000, Loss: 0.6558\n",
      "Iteration 2100, Loss: 0.6526\n",
      "Iteration 2200, Loss: 0.6497\n",
      "Iteration 2300, Loss: 0.6470\n",
      "Iteration 2400, Loss: 0.6444\n",
      "Iteration 2500, Loss: 0.6420\n",
      "Iteration 2600, Loss: 0.6397\n",
      "Iteration 2700, Loss: 0.6375\n",
      "Iteration 2800, Loss: 0.6354\n",
      "Iteration 2900, Loss: 0.6334\n",
      "Iteration 3000, Loss: 0.6315\n",
      "Iteration 3100, Loss: 0.6297\n",
      "Iteration 3200, Loss: 0.6280\n",
      "Iteration 3300, Loss: 0.6263\n",
      "Iteration 3400, Loss: 0.6247\n",
      "Iteration 3500, Loss: 0.6232\n",
      "Iteration 3600, Loss: 0.6217\n",
      "Iteration 3700, Loss: 0.6203\n",
      "Iteration 3800, Loss: 0.6189\n",
      "Iteration 3900, Loss: 0.6176\n",
      "Iteration 4000, Loss: 0.6163\n",
      "Iteration 4100, Loss: 0.6150\n",
      "Iteration 4200, Loss: 0.6138\n",
      "Iteration 4300, Loss: 0.6127\n",
      "Iteration 4400, Loss: 0.6115\n",
      "Iteration 4500, Loss: 0.6104\n",
      "Iteration 4600, Loss: 0.6094\n",
      "Iteration 4700, Loss: 0.6083\n",
      "Iteration 4800, Loss: 0.6073\n",
      "Iteration 4900, Loss: 0.6063\n",
      "Iteration 5000, Loss: 0.6054\n",
      "Iteration 5100, Loss: 0.6044\n",
      "Iteration 5200, Loss: 0.6035\n",
      "Iteration 5300, Loss: 0.6026\n",
      "Iteration 5400, Loss: 0.6018\n",
      "Iteration 5500, Loss: 0.6009\n",
      "Iteration 5600, Loss: 0.6001\n",
      "Iteration 5700, Loss: 0.5993\n",
      "Iteration 5800, Loss: 0.5985\n",
      "Iteration 5900, Loss: 0.5977\n",
      "Iteration 6000, Loss: 0.5969\n",
      "Iteration 6100, Loss: 0.5961\n",
      "Iteration 6200, Loss: 0.5954\n",
      "Iteration 6300, Loss: 0.5947\n",
      "Iteration 6400, Loss: 0.5940\n",
      "Iteration 6500, Loss: 0.5933\n",
      "Iteration 6600, Loss: 0.5926\n",
      "Iteration 6700, Loss: 0.5919\n",
      "Iteration 6800, Loss: 0.5913\n",
      "Iteration 6900, Loss: 0.5906\n",
      "Iteration 7000, Loss: 0.5900\n",
      "Iteration 7100, Loss: 0.5893\n",
      "Iteration 7200, Loss: 0.5887\n",
      "Iteration 7300, Loss: 0.5881\n",
      "Iteration 7400, Loss: 0.5875\n",
      "Iteration 7500, Loss: 0.5869\n",
      "Iteration 7600, Loss: 0.5863\n",
      "Iteration 7700, Loss: 0.5858\n",
      "Iteration 7800, Loss: 0.5852\n",
      "Iteration 7900, Loss: 0.5846\n",
      "Iteration 8000, Loss: 0.5841\n",
      "Iteration 8100, Loss: 0.5835\n",
      "Iteration 8200, Loss: 0.5830\n",
      "Iteration 8300, Loss: 0.5825\n",
      "Iteration 8400, Loss: 0.5820\n",
      "Iteration 8500, Loss: 0.5815\n",
      "Iteration 8600, Loss: 0.5809\n",
      "Iteration 8700, Loss: 0.5804\n",
      "Iteration 8800, Loss: 0.5799\n",
      "Iteration 8900, Loss: 0.5795\n",
      "Iteration 0, Loss: 1.0967\n",
      "Iteration 100, Loss: 0.9681\n",
      "Iteration 200, Loss: 0.8975\n",
      "Iteration 300, Loss: 0.8518\n",
      "Iteration 400, Loss: 0.8198\n",
      "Iteration 500, Loss: 0.7961\n",
      "Iteration 600, Loss: 0.7778\n",
      "Iteration 700, Loss: 0.7633\n",
      "Iteration 800, Loss: 0.7514\n",
      "Iteration 900, Loss: 0.7415\n",
      "Iteration 1000, Loss: 0.7331\n",
      "Iteration 1100, Loss: 0.7258\n",
      "Iteration 1200, Loss: 0.7194\n",
      "Iteration 1300, Loss: 0.7138\n",
      "Iteration 1400, Loss: 0.7087\n",
      "Iteration 1500, Loss: 0.7041\n",
      "Iteration 1600, Loss: 0.6999\n",
      "Iteration 1700, Loss: 0.6961\n",
      "Iteration 1800, Loss: 0.6925\n",
      "Iteration 1900, Loss: 0.6892\n",
      "Iteration 2000, Loss: 0.6861\n",
      "Iteration 2100, Loss: 0.6832\n",
      "Iteration 2200, Loss: 0.6805\n",
      "Iteration 2300, Loss: 0.6779\n",
      "Iteration 2400, Loss: 0.6755\n",
      "Iteration 2500, Loss: 0.6732\n",
      "Iteration 2600, Loss: 0.6710\n",
      "Iteration 2700, Loss: 0.6689\n",
      "Iteration 2800, Loss: 0.6669\n",
      "Iteration 2900, Loss: 0.6650\n",
      "Iteration 3000, Loss: 0.6632\n",
      "Iteration 3100, Loss: 0.6614\n",
      "Iteration 3200, Loss: 0.6597\n",
      "Iteration 3300, Loss: 0.6581\n",
      "Iteration 3400, Loss: 0.6565\n",
      "Iteration 3500, Loss: 0.6550\n",
      "Iteration 3600, Loss: 0.6536\n",
      "Iteration 3700, Loss: 0.6522\n",
      "Iteration 3800, Loss: 0.6508\n",
      "Iteration 3900, Loss: 0.6495\n",
      "Iteration 4000, Loss: 0.6482\n",
      "Iteration 4100, Loss: 0.6470\n",
      "Iteration 4200, Loss: 0.6458\n",
      "Iteration 4300, Loss: 0.6446\n",
      "Iteration 4400, Loss: 0.6435\n",
      "Iteration 4500, Loss: 0.6424\n",
      "Iteration 4600, Loss: 0.6413\n",
      "Iteration 4700, Loss: 0.6402\n",
      "Iteration 4800, Loss: 0.6392\n",
      "Iteration 4900, Loss: 0.6382\n",
      "Iteration 5000, Loss: 0.6372\n",
      "Iteration 5100, Loss: 0.6363\n",
      "Iteration 5200, Loss: 0.6353\n",
      "Iteration 5300, Loss: 0.6344\n",
      "Iteration 5400, Loss: 0.6335\n",
      "Iteration 5500, Loss: 0.6327\n",
      "Iteration 5600, Loss: 0.6318\n",
      "Iteration 5700, Loss: 0.6310\n",
      "Iteration 5800, Loss: 0.6302\n",
      "Iteration 5900, Loss: 0.6293\n",
      "Iteration 6000, Loss: 0.6286\n",
      "Iteration 6100, Loss: 0.6278\n",
      "Iteration 6200, Loss: 0.6270\n",
      "Iteration 6300, Loss: 0.6263\n",
      "Iteration 6400, Loss: 0.6255\n",
      "Iteration 6500, Loss: 0.6248\n",
      "Iteration 6600, Loss: 0.6241\n",
      "Iteration 6700, Loss: 0.6234\n",
      "Iteration 6800, Loss: 0.6227\n",
      "Iteration 6900, Loss: 0.6221\n",
      "Iteration 7000, Loss: 0.6214\n",
      "Iteration 7100, Loss: 0.6208\n",
      "Iteration 7200, Loss: 0.6201\n",
      "Iteration 7300, Loss: 0.6195\n",
      "Iteration 7400, Loss: 0.6189\n",
      "Iteration 7500, Loss: 0.6182\n",
      "Iteration 7600, Loss: 0.6176\n",
      "Iteration 7700, Loss: 0.6170\n",
      "Iteration 7800, Loss: 0.6165\n",
      "Iteration 7900, Loss: 0.6159\n",
      "Iteration 8000, Loss: 0.6153\n",
      "Iteration 8100, Loss: 0.6147\n",
      "Iteration 8200, Loss: 0.6142\n",
      "Iteration 8300, Loss: 0.6136\n",
      "Iteration 8400, Loss: 0.6131\n",
      "Iteration 8500, Loss: 0.6126\n",
      "Iteration 8600, Loss: 0.6120\n",
      "Iteration 8700, Loss: 0.6115\n",
      "Iteration 8800, Loss: 0.6110\n",
      "Iteration 8900, Loss: 0.6105\n",
      "Iteration 0, Loss: 1.0975\n",
      "Iteration 100, Loss: 1.0137\n",
      "Iteration 200, Loss: 0.9572\n",
      "Iteration 300, Loss: 0.9157\n",
      "Iteration 400, Loss: 0.8835\n",
      "Iteration 500, Loss: 0.8578\n",
      "Iteration 600, Loss: 0.8368\n",
      "Iteration 700, Loss: 0.8192\n",
      "Iteration 800, Loss: 0.8043\n",
      "Iteration 900, Loss: 0.7914\n",
      "Iteration 1000, Loss: 0.7801\n",
      "Iteration 1100, Loss: 0.7702\n",
      "Iteration 1200, Loss: 0.7614\n",
      "Iteration 1300, Loss: 0.7535\n",
      "Iteration 1400, Loss: 0.7463\n",
      "Iteration 1500, Loss: 0.7398\n",
      "Iteration 1600, Loss: 0.7339\n",
      "Iteration 1700, Loss: 0.7285\n",
      "Iteration 1800, Loss: 0.7235\n",
      "Iteration 1900, Loss: 0.7188\n",
      "Iteration 2000, Loss: 0.7145\n",
      "Iteration 2100, Loss: 0.7104\n",
      "Iteration 2200, Loss: 0.7066\n",
      "Iteration 2300, Loss: 0.7031\n",
      "Iteration 2400, Loss: 0.6998\n",
      "Iteration 2500, Loss: 0.6966\n",
      "Iteration 2600, Loss: 0.6936\n",
      "Iteration 2700, Loss: 0.6908\n",
      "Iteration 2800, Loss: 0.6881\n",
      "Iteration 2900, Loss: 0.6855\n",
      "Iteration 3000, Loss: 0.6831\n",
      "Iteration 3100, Loss: 0.6808\n",
      "Iteration 3200, Loss: 0.6785\n",
      "Iteration 3300, Loss: 0.6764\n",
      "Iteration 3400, Loss: 0.6744\n",
      "Iteration 3500, Loss: 0.6724\n",
      "Iteration 3600, Loss: 0.6705\n",
      "Iteration 3700, Loss: 0.6687\n",
      "Iteration 3800, Loss: 0.6669\n",
      "Iteration 3900, Loss: 0.6652\n",
      "Iteration 4000, Loss: 0.6636\n",
      "Iteration 4100, Loss: 0.6620\n",
      "Iteration 4200, Loss: 0.6605\n",
      "Iteration 4300, Loss: 0.6590\n",
      "Iteration 4400, Loss: 0.6576\n",
      "Iteration 4500, Loss: 0.6562\n",
      "Iteration 4600, Loss: 0.6548\n",
      "Iteration 4700, Loss: 0.6535\n",
      "Iteration 4800, Loss: 0.6522\n",
      "Iteration 4900, Loss: 0.6510\n",
      "Iteration 5000, Loss: 0.6498\n",
      "Iteration 5100, Loss: 0.6486\n",
      "Iteration 5200, Loss: 0.6475\n",
      "Iteration 5300, Loss: 0.6464\n",
      "Iteration 5400, Loss: 0.6453\n",
      "Iteration 5500, Loss: 0.6442\n",
      "Iteration 5600, Loss: 0.6432\n",
      "Iteration 5700, Loss: 0.6422\n",
      "Iteration 5800, Loss: 0.6412\n",
      "Iteration 5900, Loss: 0.6402\n",
      "Iteration 6000, Loss: 0.6393\n",
      "Iteration 6100, Loss: 0.6384\n",
      "Iteration 6200, Loss: 0.6375\n",
      "Iteration 6300, Loss: 0.6366\n",
      "Iteration 6400, Loss: 0.6357\n",
      "Iteration 6500, Loss: 0.6349\n",
      "Iteration 6600, Loss: 0.6341\n",
      "Iteration 6700, Loss: 0.6332\n",
      "Iteration 6800, Loss: 0.6324\n",
      "Iteration 6900, Loss: 0.6317\n",
      "Iteration 7000, Loss: 0.6309\n",
      "Iteration 7100, Loss: 0.6301\n",
      "Iteration 7200, Loss: 0.6294\n",
      "Iteration 7300, Loss: 0.6287\n",
      "Iteration 7400, Loss: 0.6279\n",
      "Iteration 7500, Loss: 0.6272\n",
      "Iteration 7600, Loss: 0.6266\n",
      "Iteration 7700, Loss: 0.6259\n",
      "Iteration 7800, Loss: 0.6252\n",
      "Iteration 7900, Loss: 0.6246\n",
      "Iteration 8000, Loss: 0.6239\n",
      "Iteration 8100, Loss: 0.6233\n",
      "Iteration 8200, Loss: 0.6227\n",
      "Iteration 8300, Loss: 0.6220\n",
      "Iteration 8400, Loss: 0.6214\n",
      "Iteration 8500, Loss: 0.6208\n",
      "Iteration 8600, Loss: 0.6202\n",
      "Iteration 8700, Loss: 0.6197\n",
      "Iteration 8800, Loss: 0.6191\n",
      "Iteration 8900, Loss: 0.6185\n",
      "Iteration 0, Loss: 1.0978\n",
      "Iteration 100, Loss: 1.0251\n",
      "Iteration 200, Loss: 0.9723\n",
      "Iteration 300, Loss: 0.9318\n",
      "Iteration 400, Loss: 0.8996\n",
      "Iteration 500, Loss: 0.8733\n",
      "Iteration 600, Loss: 0.8513\n",
      "Iteration 700, Loss: 0.8329\n",
      "Iteration 800, Loss: 0.8171\n",
      "Iteration 900, Loss: 0.8035\n",
      "Iteration 1000, Loss: 0.7917\n",
      "Iteration 1100, Loss: 0.7814\n",
      "Iteration 1200, Loss: 0.7724\n",
      "Iteration 1300, Loss: 0.7643\n",
      "Iteration 1400, Loss: 0.7571\n",
      "Iteration 1500, Loss: 0.7506\n",
      "Iteration 1600, Loss: 0.7447\n",
      "Iteration 1700, Loss: 0.7394\n",
      "Iteration 1800, Loss: 0.7346\n",
      "Iteration 1900, Loss: 0.7301\n",
      "Iteration 2000, Loss: 0.7260\n",
      "Iteration 2100, Loss: 0.7222\n",
      "Iteration 2200, Loss: 0.7186\n",
      "Iteration 2300, Loss: 0.7153\n",
      "Iteration 2400, Loss: 0.7122\n",
      "Iteration 2500, Loss: 0.7092\n",
      "Iteration 2600, Loss: 0.7065\n",
      "Iteration 2700, Loss: 0.7039\n",
      "Iteration 2800, Loss: 0.7014\n",
      "Iteration 2900, Loss: 0.6990\n",
      "Iteration 3000, Loss: 0.6968\n",
      "Iteration 3100, Loss: 0.6946\n",
      "Iteration 3200, Loss: 0.6926\n",
      "Iteration 3300, Loss: 0.6906\n",
      "Iteration 3400, Loss: 0.6887\n",
      "Iteration 3500, Loss: 0.6869\n",
      "Iteration 3600, Loss: 0.6852\n",
      "Iteration 3700, Loss: 0.6835\n",
      "Iteration 3800, Loss: 0.6818\n",
      "Iteration 3900, Loss: 0.6803\n",
      "Iteration 4000, Loss: 0.6788\n",
      "Iteration 4100, Loss: 0.6773\n",
      "Iteration 4200, Loss: 0.6758\n",
      "Iteration 4300, Loss: 0.6745\n",
      "Iteration 4400, Loss: 0.6731\n",
      "Iteration 4500, Loss: 0.6718\n",
      "Iteration 4600, Loss: 0.6705\n",
      "Iteration 4700, Loss: 0.6693\n",
      "Iteration 4800, Loss: 0.6681\n",
      "Iteration 4900, Loss: 0.6669\n",
      "Iteration 5000, Loss: 0.6658\n",
      "Iteration 5100, Loss: 0.6647\n",
      "Iteration 5200, Loss: 0.6636\n",
      "Iteration 5300, Loss: 0.6625\n",
      "Iteration 5400, Loss: 0.6615\n",
      "Iteration 5500, Loss: 0.6605\n",
      "Iteration 5600, Loss: 0.6595\n",
      "Iteration 5700, Loss: 0.6585\n",
      "Iteration 5800, Loss: 0.6576\n",
      "Iteration 5900, Loss: 0.6566\n",
      "Iteration 6000, Loss: 0.6557\n",
      "Iteration 6100, Loss: 0.6548\n",
      "Iteration 6200, Loss: 0.6540\n",
      "Iteration 6300, Loss: 0.6531\n",
      "Iteration 6400, Loss: 0.6523\n",
      "Iteration 6500, Loss: 0.6515\n",
      "Iteration 6600, Loss: 0.6507\n",
      "Iteration 6700, Loss: 0.6499\n",
      "Iteration 6800, Loss: 0.6491\n",
      "Iteration 6900, Loss: 0.6483\n",
      "Iteration 7000, Loss: 0.6476\n",
      "Iteration 7100, Loss: 0.6468\n",
      "Iteration 7200, Loss: 0.6461\n",
      "Iteration 7300, Loss: 0.6454\n",
      "Iteration 7400, Loss: 0.6447\n",
      "Iteration 7500, Loss: 0.6440\n",
      "Iteration 7600, Loss: 0.6433\n",
      "Iteration 7700, Loss: 0.6427\n",
      "Iteration 7800, Loss: 0.6420\n",
      "Iteration 7900, Loss: 0.6413\n",
      "Iteration 8000, Loss: 0.6407\n",
      "Iteration 8100, Loss: 0.6401\n",
      "Iteration 8200, Loss: 0.6395\n",
      "Iteration 8300, Loss: 0.6389\n",
      "Iteration 8400, Loss: 0.6383\n",
      "Iteration 8500, Loss: 0.6377\n",
      "Iteration 8600, Loss: 0.6371\n",
      "Iteration 8700, Loss: 0.6365\n",
      "Iteration 8800, Loss: 0.6359\n",
      "Iteration 8900, Loss: 0.6354\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0523\n",
      "Iteration 200, Loss: 1.0152\n",
      "Iteration 300, Loss: 0.9845\n",
      "Iteration 400, Loss: 0.9587\n",
      "Iteration 500, Loss: 0.9367\n",
      "Iteration 600, Loss: 0.9173\n",
      "Iteration 700, Loss: 0.9004\n",
      "Iteration 800, Loss: 0.8852\n",
      "Iteration 900, Loss: 0.8716\n",
      "Iteration 1000, Loss: 0.8595\n",
      "Iteration 1100, Loss: 0.8484\n",
      "Iteration 1200, Loss: 0.8384\n",
      "Iteration 1300, Loss: 0.8293\n",
      "Iteration 1400, Loss: 0.8210\n",
      "Iteration 1500, Loss: 0.8132\n",
      "Iteration 1600, Loss: 0.8060\n",
      "Iteration 1700, Loss: 0.7994\n",
      "Iteration 1800, Loss: 0.7933\n",
      "Iteration 1900, Loss: 0.7876\n",
      "Iteration 2000, Loss: 0.7822\n",
      "Iteration 2100, Loss: 0.7772\n",
      "Iteration 2200, Loss: 0.7725\n",
      "Iteration 2300, Loss: 0.7681\n",
      "Iteration 2400, Loss: 0.7639\n",
      "Iteration 2500, Loss: 0.7600\n",
      "Iteration 2600, Loss: 0.7563\n",
      "Iteration 2700, Loss: 0.7527\n",
      "Iteration 2800, Loss: 0.7494\n",
      "Iteration 2900, Loss: 0.7462\n",
      "Iteration 3000, Loss: 0.7431\n",
      "Iteration 3100, Loss: 0.7402\n",
      "Iteration 3200, Loss: 0.7375\n",
      "Iteration 3300, Loss: 0.7348\n",
      "Iteration 3400, Loss: 0.7323\n",
      "Iteration 3500, Loss: 0.7298\n",
      "Iteration 3600, Loss: 0.7275\n",
      "Iteration 3700, Loss: 0.7253\n",
      "Iteration 3800, Loss: 0.7231\n",
      "Iteration 3900, Loss: 0.7211\n",
      "Iteration 4000, Loss: 0.7191\n",
      "Iteration 4100, Loss: 0.7171\n",
      "Iteration 4200, Loss: 0.7153\n",
      "Iteration 4300, Loss: 0.7135\n",
      "Iteration 4400, Loss: 0.7117\n",
      "Iteration 4500, Loss: 0.7101\n",
      "Iteration 4600, Loss: 0.7084\n",
      "Iteration 4700, Loss: 0.7068\n",
      "Iteration 4800, Loss: 0.7053\n",
      "Iteration 4900, Loss: 0.7038\n",
      "Iteration 5000, Loss: 0.7024\n",
      "Iteration 5100, Loss: 0.7010\n",
      "Iteration 5200, Loss: 0.6996\n",
      "Iteration 5300, Loss: 0.6983\n",
      "Iteration 5400, Loss: 0.6970\n",
      "Iteration 5500, Loss: 0.6957\n",
      "Iteration 5600, Loss: 0.6945\n",
      "Iteration 5700, Loss: 0.6933\n",
      "Iteration 5800, Loss: 0.6921\n",
      "Iteration 5900, Loss: 0.6910\n",
      "Iteration 6000, Loss: 0.6899\n",
      "Iteration 6100, Loss: 0.6888\n",
      "Iteration 6200, Loss: 0.6877\n",
      "Iteration 6300, Loss: 0.6866\n",
      "Iteration 6400, Loss: 0.6856\n",
      "Iteration 6500, Loss: 0.6846\n",
      "Iteration 6600, Loss: 0.6836\n",
      "Iteration 6700, Loss: 0.6826\n",
      "Iteration 6800, Loss: 0.6817\n",
      "Iteration 6900, Loss: 0.6808\n",
      "Iteration 7000, Loss: 0.6799\n",
      "Iteration 7100, Loss: 0.6790\n",
      "Iteration 7200, Loss: 0.6781\n",
      "Iteration 7300, Loss: 0.6773\n",
      "Iteration 7400, Loss: 0.6764\n",
      "Iteration 7500, Loss: 0.6756\n",
      "Iteration 7600, Loss: 0.6748\n",
      "Iteration 7700, Loss: 0.6740\n",
      "Iteration 7800, Loss: 0.6732\n",
      "Iteration 7900, Loss: 0.6724\n",
      "Iteration 8000, Loss: 0.6716\n",
      "Iteration 8100, Loss: 0.6709\n",
      "Iteration 8200, Loss: 0.6701\n",
      "Iteration 8300, Loss: 0.6694\n",
      "Iteration 8400, Loss: 0.6687\n",
      "Iteration 8500, Loss: 0.6680\n",
      "Iteration 8600, Loss: 0.6673\n",
      "Iteration 8700, Loss: 0.6666\n",
      "Iteration 8800, Loss: 0.6659\n",
      "Iteration 8900, Loss: 0.6653\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0553\n",
      "Iteration 200, Loss: 1.0199\n",
      "Iteration 300, Loss: 0.9902\n",
      "Iteration 400, Loss: 0.9649\n",
      "Iteration 500, Loss: 0.9428\n",
      "Iteration 600, Loss: 0.9235\n",
      "Iteration 700, Loss: 0.9062\n",
      "Iteration 800, Loss: 0.8908\n",
      "Iteration 900, Loss: 0.8769\n",
      "Iteration 1000, Loss: 0.8644\n",
      "Iteration 1100, Loss: 0.8529\n",
      "Iteration 1200, Loss: 0.8425\n",
      "Iteration 1300, Loss: 0.8329\n",
      "Iteration 1400, Loss: 0.8241\n",
      "Iteration 1500, Loss: 0.8159\n",
      "Iteration 1600, Loss: 0.8084\n",
      "Iteration 1700, Loss: 0.8013\n",
      "Iteration 1800, Loss: 0.7949\n",
      "Iteration 1900, Loss: 0.7888\n",
      "Iteration 2000, Loss: 0.7831\n",
      "Iteration 2100, Loss: 0.7779\n",
      "Iteration 2200, Loss: 0.7729\n",
      "Iteration 2300, Loss: 0.7682\n",
      "Iteration 2400, Loss: 0.7638\n",
      "Iteration 2500, Loss: 0.7597\n",
      "Iteration 2600, Loss: 0.7558\n",
      "Iteration 2700, Loss: 0.7521\n",
      "Iteration 2800, Loss: 0.7486\n",
      "Iteration 2900, Loss: 0.7453\n",
      "Iteration 3000, Loss: 0.7421\n",
      "Iteration 3100, Loss: 0.7391\n",
      "Iteration 3200, Loss: 0.7362\n",
      "Iteration 3300, Loss: 0.7334\n",
      "Iteration 3400, Loss: 0.7308\n",
      "Iteration 3500, Loss: 0.7283\n",
      "Iteration 3600, Loss: 0.7259\n",
      "Iteration 3700, Loss: 0.7235\n",
      "Iteration 3800, Loss: 0.7213\n",
      "Iteration 3900, Loss: 0.7192\n",
      "Iteration 4000, Loss: 0.7171\n",
      "Iteration 4100, Loss: 0.7151\n",
      "Iteration 4200, Loss: 0.7132\n",
      "Iteration 4300, Loss: 0.7113\n",
      "Iteration 4400, Loss: 0.7095\n",
      "Iteration 4500, Loss: 0.7078\n",
      "Iteration 4600, Loss: 0.7062\n",
      "Iteration 4700, Loss: 0.7045\n",
      "Iteration 4800, Loss: 0.7029\n",
      "Iteration 4900, Loss: 0.7014\n",
      "Iteration 5000, Loss: 0.6999\n",
      "Iteration 5100, Loss: 0.6985\n",
      "Iteration 5200, Loss: 0.6971\n",
      "Iteration 5300, Loss: 0.6957\n",
      "Iteration 5400, Loss: 0.6944\n",
      "Iteration 5500, Loss: 0.6931\n",
      "Iteration 5600, Loss: 0.6919\n",
      "Iteration 5700, Loss: 0.6907\n",
      "Iteration 5800, Loss: 0.6895\n",
      "Iteration 5900, Loss: 0.6883\n",
      "Iteration 6000, Loss: 0.6872\n",
      "Iteration 6100, Loss: 0.6860\n",
      "Iteration 6200, Loss: 0.6850\n",
      "Iteration 6300, Loss: 0.6839\n",
      "Iteration 6400, Loss: 0.6829\n",
      "Iteration 6500, Loss: 0.6818\n",
      "Iteration 6600, Loss: 0.6808\n",
      "Iteration 6700, Loss: 0.6799\n",
      "Iteration 6800, Loss: 0.6789\n",
      "Iteration 6900, Loss: 0.6780\n",
      "Iteration 7000, Loss: 0.6771\n",
      "Iteration 7100, Loss: 0.6762\n",
      "Iteration 7200, Loss: 0.6753\n",
      "Iteration 7300, Loss: 0.6744\n",
      "Iteration 7400, Loss: 0.6736\n",
      "Iteration 7500, Loss: 0.6727\n",
      "Iteration 7600, Loss: 0.6719\n",
      "Iteration 7700, Loss: 0.6711\n",
      "Iteration 7800, Loss: 0.6704\n",
      "Iteration 7900, Loss: 0.6696\n",
      "Iteration 8000, Loss: 0.6688\n",
      "Iteration 8100, Loss: 0.6681\n",
      "Iteration 8200, Loss: 0.6673\n",
      "Iteration 8300, Loss: 0.6666\n",
      "Iteration 8400, Loss: 0.6659\n",
      "Iteration 8500, Loss: 0.6652\n",
      "Iteration 8600, Loss: 0.6645\n",
      "Iteration 8700, Loss: 0.6638\n",
      "Iteration 8800, Loss: 0.6632\n",
      "Iteration 8900, Loss: 0.6625\n",
      "Iteration 0, Loss: 0.9677\n",
      "Iteration 100, Loss: 0.5994\n",
      "Iteration 200, Loss: 0.5687\n",
      "Iteration 300, Loss: 0.5514\n",
      "Iteration 400, Loss: 0.5394\n",
      "Iteration 500, Loss: 0.5302\n",
      "Iteration 600, Loss: 0.5228\n",
      "Iteration 700, Loss: 0.5165\n",
      "Iteration 800, Loss: 0.5112\n",
      "Iteration 900, Loss: 0.5065\n",
      "Iteration 1000, Loss: 0.5024\n",
      "Iteration 1100, Loss: 0.4987\n",
      "Iteration 1200, Loss: 0.4953\n",
      "Iteration 1300, Loss: 0.4922\n",
      "Iteration 1400, Loss: 0.4894\n",
      "Iteration 1500, Loss: 0.4868\n",
      "Iteration 1600, Loss: 0.4844\n",
      "Iteration 1700, Loss: 0.4821\n",
      "Iteration 1800, Loss: 0.4800\n",
      "Iteration 1900, Loss: 0.4781\n",
      "Iteration 2000, Loss: 0.4762\n",
      "Iteration 2100, Loss: 0.4745\n",
      "Iteration 2200, Loss: 0.4728\n",
      "Iteration 2300, Loss: 0.4712\n",
      "Iteration 2400, Loss: 0.4697\n",
      "Iteration 2500, Loss: 0.4683\n",
      "Iteration 2600, Loss: 0.4670\n",
      "Iteration 2700, Loss: 0.4657\n",
      "Iteration 2800, Loss: 0.4645\n",
      "Iteration 2900, Loss: 0.4633\n",
      "Iteration 3000, Loss: 0.4621\n",
      "Iteration 3100, Loss: 0.4611\n",
      "Iteration 3200, Loss: 0.4600\n",
      "Iteration 3300, Loss: 0.4590\n",
      "Iteration 3400, Loss: 0.4580\n",
      "Iteration 3500, Loss: 0.4571\n",
      "Iteration 3600, Loss: 0.4562\n",
      "Iteration 3700, Loss: 0.4553\n",
      "Iteration 3800, Loss: 0.4545\n",
      "Iteration 3900, Loss: 0.4536\n",
      "Iteration 4000, Loss: 0.4528\n",
      "Iteration 4100, Loss: 0.4521\n",
      "Iteration 4200, Loss: 0.4513\n",
      "Iteration 4300, Loss: 0.4506\n",
      "Iteration 4400, Loss: 0.4499\n",
      "Iteration 4500, Loss: 0.4492\n",
      "Iteration 4600, Loss: 0.4485\n",
      "Iteration 4700, Loss: 0.4478\n",
      "Iteration 4800, Loss: 0.4472\n",
      "Iteration 4900, Loss: 0.4466\n",
      "Iteration 5000, Loss: 0.4460\n",
      "Iteration 5100, Loss: 0.4454\n",
      "Iteration 5200, Loss: 0.4448\n",
      "Iteration 5300, Loss: 0.4442\n",
      "Iteration 5400, Loss: 0.4437\n",
      "Iteration 5500, Loss: 0.4431\n",
      "Iteration 5600, Loss: 0.4426\n",
      "Iteration 5700, Loss: 0.4420\n",
      "Iteration 5800, Loss: 0.4415\n",
      "Iteration 5900, Loss: 0.4410\n",
      "Iteration 6000, Loss: 0.4405\n",
      "Iteration 6100, Loss: 0.4401\n",
      "Iteration 6200, Loss: 0.4396\n",
      "Iteration 6300, Loss: 0.4391\n",
      "Iteration 6400, Loss: 0.4387\n",
      "Iteration 6500, Loss: 0.4382\n",
      "Iteration 6600, Loss: 0.4378\n",
      "Iteration 6700, Loss: 0.4374\n",
      "Iteration 6800, Loss: 0.4369\n",
      "Iteration 6900, Loss: 0.4365\n",
      "Iteration 7000, Loss: 0.4361\n",
      "Iteration 7100, Loss: 0.4357\n",
      "Iteration 7200, Loss: 0.4353\n",
      "Iteration 7300, Loss: 0.4349\n",
      "Iteration 7400, Loss: 0.4346\n",
      "Iteration 7500, Loss: 0.4342\n",
      "Iteration 7600, Loss: 0.4338\n",
      "Iteration 7700, Loss: 0.4335\n",
      "Iteration 7800, Loss: 0.4331\n",
      "Iteration 7900, Loss: 0.4327\n",
      "Iteration 8000, Loss: 0.4324\n",
      "Iteration 8100, Loss: 0.4321\n",
      "Iteration 8200, Loss: 0.4317\n",
      "Iteration 8300, Loss: 0.4314\n",
      "Iteration 8400, Loss: 0.4310\n",
      "Iteration 8500, Loss: 0.4307\n",
      "Iteration 8600, Loss: 0.4304\n",
      "Iteration 8700, Loss: 0.4301\n",
      "Iteration 8800, Loss: 0.4298\n",
      "Iteration 8900, Loss: 0.4295\n",
      "Iteration 9000, Loss: 0.4292\n",
      "Iteration 9100, Loss: 0.4289\n",
      "Iteration 9200, Loss: 0.4286\n",
      "Iteration 9300, Loss: 0.4283\n",
      "Iteration 9400, Loss: 0.4280\n",
      "Iteration 9500, Loss: 0.4277\n",
      "Iteration 9600, Loss: 0.4274\n",
      "Iteration 9700, Loss: 0.4271\n",
      "Iteration 9800, Loss: 0.4269\n",
      "Iteration 9900, Loss: 0.4266\n",
      "Iteration 0, Loss: 0.9670\n",
      "Iteration 100, Loss: 0.5842\n",
      "Iteration 200, Loss: 0.5549\n",
      "Iteration 300, Loss: 0.5387\n",
      "Iteration 400, Loss: 0.5277\n",
      "Iteration 500, Loss: 0.5194\n",
      "Iteration 600, Loss: 0.5128\n",
      "Iteration 700, Loss: 0.5073\n",
      "Iteration 800, Loss: 0.5026\n",
      "Iteration 900, Loss: 0.4985\n",
      "Iteration 1000, Loss: 0.4948\n",
      "Iteration 1100, Loss: 0.4916\n",
      "Iteration 1200, Loss: 0.4886\n",
      "Iteration 1300, Loss: 0.4859\n",
      "Iteration 1400, Loss: 0.4834\n",
      "Iteration 1500, Loss: 0.4811\n",
      "Iteration 1600, Loss: 0.4790\n",
      "Iteration 1700, Loss: 0.4769\n",
      "Iteration 1800, Loss: 0.4751\n",
      "Iteration 1900, Loss: 0.4733\n",
      "Iteration 2000, Loss: 0.4716\n",
      "Iteration 2100, Loss: 0.4700\n",
      "Iteration 2200, Loss: 0.4686\n",
      "Iteration 2300, Loss: 0.4671\n",
      "Iteration 2400, Loss: 0.4658\n",
      "Iteration 2500, Loss: 0.4645\n",
      "Iteration 2600, Loss: 0.4632\n",
      "Iteration 2700, Loss: 0.4620\n",
      "Iteration 2800, Loss: 0.4609\n",
      "Iteration 2900, Loss: 0.4598\n",
      "Iteration 3000, Loss: 0.4587\n",
      "Iteration 3100, Loss: 0.4577\n",
      "Iteration 3200, Loss: 0.4567\n",
      "Iteration 3300, Loss: 0.4558\n",
      "Iteration 3400, Loss: 0.4549\n",
      "Iteration 3500, Loss: 0.4540\n",
      "Iteration 3600, Loss: 0.4531\n",
      "Iteration 3700, Loss: 0.4523\n",
      "Iteration 3800, Loss: 0.4514\n",
      "Iteration 3900, Loss: 0.4507\n",
      "Iteration 4000, Loss: 0.4499\n",
      "Iteration 4100, Loss: 0.4491\n",
      "Iteration 4200, Loss: 0.4484\n",
      "Iteration 4300, Loss: 0.4477\n",
      "Iteration 4400, Loss: 0.4470\n",
      "Iteration 4500, Loss: 0.4463\n",
      "Iteration 4600, Loss: 0.4457\n",
      "Iteration 4700, Loss: 0.4450\n",
      "Iteration 4800, Loss: 0.4444\n",
      "Iteration 4900, Loss: 0.4438\n",
      "Iteration 5000, Loss: 0.4432\n",
      "Iteration 5100, Loss: 0.4426\n",
      "Iteration 5200, Loss: 0.4420\n",
      "Iteration 5300, Loss: 0.4414\n",
      "Iteration 5400, Loss: 0.4409\n",
      "Iteration 5500, Loss: 0.4404\n",
      "Iteration 5600, Loss: 0.4398\n",
      "Iteration 5700, Loss: 0.4393\n",
      "Iteration 5800, Loss: 0.4388\n",
      "Iteration 5900, Loss: 0.4383\n",
      "Iteration 6000, Loss: 0.4378\n",
      "Iteration 6100, Loss: 0.4373\n",
      "Iteration 6200, Loss: 0.4368\n",
      "Iteration 6300, Loss: 0.4364\n",
      "Iteration 6400, Loss: 0.4359\n",
      "Iteration 6500, Loss: 0.4354\n",
      "Iteration 6600, Loss: 0.4350\n",
      "Iteration 6700, Loss: 0.4346\n",
      "Iteration 6800, Loss: 0.4341\n",
      "Iteration 6900, Loss: 0.4337\n",
      "Iteration 7000, Loss: 0.4333\n",
      "Iteration 7100, Loss: 0.4329\n",
      "Iteration 7200, Loss: 0.4325\n",
      "Iteration 7300, Loss: 0.4321\n",
      "Iteration 7400, Loss: 0.4317\n",
      "Iteration 7500, Loss: 0.4313\n",
      "Iteration 7600, Loss: 0.4309\n",
      "Iteration 7700, Loss: 0.4305\n",
      "Iteration 7800, Loss: 0.4301\n",
      "Iteration 7900, Loss: 0.4298\n",
      "Iteration 8000, Loss: 0.4294\n",
      "Iteration 8100, Loss: 0.4291\n",
      "Iteration 8200, Loss: 0.4287\n",
      "Iteration 8300, Loss: 0.4284\n",
      "Iteration 8400, Loss: 0.4280\n",
      "Iteration 8500, Loss: 0.4277\n",
      "Iteration 8600, Loss: 0.4273\n",
      "Iteration 8700, Loss: 0.4270\n",
      "Iteration 8800, Loss: 0.4267\n",
      "Iteration 8900, Loss: 0.4264\n",
      "Iteration 9000, Loss: 0.4260\n",
      "Iteration 9100, Loss: 0.4257\n",
      "Iteration 9200, Loss: 0.4254\n",
      "Iteration 9300, Loss: 0.4251\n",
      "Iteration 9400, Loss: 0.4248\n",
      "Iteration 9500, Loss: 0.4245\n",
      "Iteration 9600, Loss: 0.4242\n",
      "Iteration 9700, Loss: 0.4239\n",
      "Iteration 9800, Loss: 0.4236\n",
      "Iteration 9900, Loss: 0.4233\n",
      "Iteration 0, Loss: 1.0176\n",
      "Iteration 100, Loss: 0.6171\n",
      "Iteration 200, Loss: 0.5864\n",
      "Iteration 300, Loss: 0.5688\n",
      "Iteration 400, Loss: 0.5564\n",
      "Iteration 500, Loss: 0.5469\n",
      "Iteration 600, Loss: 0.5393\n",
      "Iteration 700, Loss: 0.5330\n",
      "Iteration 800, Loss: 0.5276\n",
      "Iteration 900, Loss: 0.5229\n",
      "Iteration 1000, Loss: 0.5188\n",
      "Iteration 1100, Loss: 0.5151\n",
      "Iteration 1200, Loss: 0.5117\n",
      "Iteration 1300, Loss: 0.5087\n",
      "Iteration 1400, Loss: 0.5058\n",
      "Iteration 1500, Loss: 0.5032\n",
      "Iteration 1600, Loss: 0.5008\n",
      "Iteration 1700, Loss: 0.4985\n",
      "Iteration 1800, Loss: 0.4964\n",
      "Iteration 1900, Loss: 0.4944\n",
      "Iteration 2000, Loss: 0.4925\n",
      "Iteration 2100, Loss: 0.4906\n",
      "Iteration 2200, Loss: 0.4889\n",
      "Iteration 2300, Loss: 0.4873\n",
      "Iteration 2400, Loss: 0.4858\n",
      "Iteration 2500, Loss: 0.4843\n",
      "Iteration 2600, Loss: 0.4829\n",
      "Iteration 2700, Loss: 0.4815\n",
      "Iteration 2800, Loss: 0.4802\n",
      "Iteration 2900, Loss: 0.4789\n",
      "Iteration 3000, Loss: 0.4777\n",
      "Iteration 3100, Loss: 0.4765\n",
      "Iteration 3200, Loss: 0.4754\n",
      "Iteration 3300, Loss: 0.4743\n",
      "Iteration 3400, Loss: 0.4733\n",
      "Iteration 3500, Loss: 0.4723\n",
      "Iteration 3600, Loss: 0.4713\n",
      "Iteration 3700, Loss: 0.4703\n",
      "Iteration 3800, Loss: 0.4694\n",
      "Iteration 3900, Loss: 0.4685\n",
      "Iteration 4000, Loss: 0.4676\n",
      "Iteration 4100, Loss: 0.4668\n",
      "Iteration 4200, Loss: 0.4660\n",
      "Iteration 4300, Loss: 0.4652\n",
      "Iteration 4400, Loss: 0.4644\n",
      "Iteration 4500, Loss: 0.4636\n",
      "Iteration 4600, Loss: 0.4629\n",
      "Iteration 4700, Loss: 0.4621\n",
      "Iteration 4800, Loss: 0.4614\n",
      "Iteration 4900, Loss: 0.4607\n",
      "Iteration 5000, Loss: 0.4601\n",
      "Iteration 5100, Loss: 0.4594\n",
      "Iteration 5200, Loss: 0.4587\n",
      "Iteration 5300, Loss: 0.4581\n",
      "Iteration 5400, Loss: 0.4575\n",
      "Iteration 5500, Loss: 0.4569\n",
      "Iteration 5600, Loss: 0.4563\n",
      "Iteration 5700, Loss: 0.4557\n",
      "Iteration 5800, Loss: 0.4551\n",
      "Iteration 5900, Loss: 0.4545\n",
      "Iteration 6000, Loss: 0.4540\n",
      "Iteration 6100, Loss: 0.4534\n",
      "Iteration 6200, Loss: 0.4529\n",
      "Iteration 6300, Loss: 0.4524\n",
      "Iteration 6400, Loss: 0.4519\n",
      "Iteration 6500, Loss: 0.4514\n",
      "Iteration 6600, Loss: 0.4509\n",
      "Iteration 6700, Loss: 0.4504\n",
      "Iteration 6800, Loss: 0.4499\n",
      "Iteration 6900, Loss: 0.4494\n",
      "Iteration 7000, Loss: 0.4490\n",
      "Iteration 7100, Loss: 0.4485\n",
      "Iteration 7200, Loss: 0.4480\n",
      "Iteration 7300, Loss: 0.4476\n",
      "Iteration 7400, Loss: 0.4472\n",
      "Iteration 7500, Loss: 0.4467\n",
      "Iteration 7600, Loss: 0.4463\n",
      "Iteration 7700, Loss: 0.4459\n",
      "Iteration 7800, Loss: 0.4455\n",
      "Iteration 7900, Loss: 0.4451\n",
      "Iteration 8000, Loss: 0.4447\n",
      "Iteration 8100, Loss: 0.4443\n",
      "Iteration 8200, Loss: 0.4439\n",
      "Iteration 8300, Loss: 0.4435\n",
      "Iteration 8400, Loss: 0.4431\n",
      "Iteration 8500, Loss: 0.4427\n",
      "Iteration 8600, Loss: 0.4424\n",
      "Iteration 8700, Loss: 0.4420\n",
      "Iteration 8800, Loss: 0.4416\n",
      "Iteration 8900, Loss: 0.4413\n",
      "Iteration 9000, Loss: 0.4409\n",
      "Iteration 9100, Loss: 0.4406\n",
      "Iteration 9200, Loss: 0.4402\n",
      "Iteration 9300, Loss: 0.4399\n",
      "Iteration 9400, Loss: 0.4396\n",
      "Iteration 9500, Loss: 0.4392\n",
      "Iteration 9600, Loss: 0.4389\n",
      "Iteration 9700, Loss: 0.4386\n",
      "Iteration 9800, Loss: 0.4383\n",
      "Iteration 9900, Loss: 0.4380\n",
      "Iteration 0, Loss: 1.0228\n",
      "Iteration 100, Loss: 0.6264\n",
      "Iteration 200, Loss: 0.5948\n",
      "Iteration 300, Loss: 0.5775\n",
      "Iteration 400, Loss: 0.5657\n",
      "Iteration 500, Loss: 0.5567\n",
      "Iteration 600, Loss: 0.5495\n",
      "Iteration 700, Loss: 0.5435\n",
      "Iteration 800, Loss: 0.5383\n",
      "Iteration 900, Loss: 0.5338\n",
      "Iteration 1000, Loss: 0.5298\n",
      "Iteration 1100, Loss: 0.5261\n",
      "Iteration 1200, Loss: 0.5228\n",
      "Iteration 1300, Loss: 0.5197\n",
      "Iteration 1400, Loss: 0.5169\n",
      "Iteration 1500, Loss: 0.5143\n",
      "Iteration 1600, Loss: 0.5119\n",
      "Iteration 1700, Loss: 0.5096\n",
      "Iteration 1800, Loss: 0.5074\n",
      "Iteration 1900, Loss: 0.5053\n",
      "Iteration 2000, Loss: 0.5034\n",
      "Iteration 2100, Loss: 0.5015\n",
      "Iteration 2200, Loss: 0.4998\n",
      "Iteration 2300, Loss: 0.4982\n",
      "Iteration 2400, Loss: 0.4965\n",
      "Iteration 2500, Loss: 0.4950\n",
      "Iteration 2600, Loss: 0.4935\n",
      "Iteration 2700, Loss: 0.4921\n",
      "Iteration 2800, Loss: 0.4907\n",
      "Iteration 2900, Loss: 0.4893\n",
      "Iteration 3000, Loss: 0.4881\n",
      "Iteration 3100, Loss: 0.4868\n",
      "Iteration 3200, Loss: 0.4856\n",
      "Iteration 3300, Loss: 0.4845\n",
      "Iteration 3400, Loss: 0.4833\n",
      "Iteration 3500, Loss: 0.4822\n",
      "Iteration 3600, Loss: 0.4812\n",
      "Iteration 3700, Loss: 0.4801\n",
      "Iteration 3800, Loss: 0.4791\n",
      "Iteration 3900, Loss: 0.4781\n",
      "Iteration 4000, Loss: 0.4772\n",
      "Iteration 4100, Loss: 0.4762\n",
      "Iteration 4200, Loss: 0.4753\n",
      "Iteration 4300, Loss: 0.4744\n",
      "Iteration 4400, Loss: 0.4736\n",
      "Iteration 4500, Loss: 0.4727\n",
      "Iteration 4600, Loss: 0.4719\n",
      "Iteration 4700, Loss: 0.4711\n",
      "Iteration 4800, Loss: 0.4703\n",
      "Iteration 4900, Loss: 0.4695\n",
      "Iteration 5000, Loss: 0.4688\n",
      "Iteration 5100, Loss: 0.4680\n",
      "Iteration 5200, Loss: 0.4673\n",
      "Iteration 5300, Loss: 0.4666\n",
      "Iteration 5400, Loss: 0.4659\n",
      "Iteration 5500, Loss: 0.4652\n",
      "Iteration 5600, Loss: 0.4645\n",
      "Iteration 5700, Loss: 0.4639\n",
      "Iteration 5800, Loss: 0.4632\n",
      "Iteration 5900, Loss: 0.4626\n",
      "Iteration 6000, Loss: 0.4619\n",
      "Iteration 6100, Loss: 0.4613\n",
      "Iteration 6200, Loss: 0.4607\n",
      "Iteration 6300, Loss: 0.4601\n",
      "Iteration 6400, Loss: 0.4596\n",
      "Iteration 6500, Loss: 0.4590\n",
      "Iteration 6600, Loss: 0.4584\n",
      "Iteration 6700, Loss: 0.4579\n",
      "Iteration 6800, Loss: 0.4573\n",
      "Iteration 6900, Loss: 0.4568\n",
      "Iteration 7000, Loss: 0.4563\n",
      "Iteration 7100, Loss: 0.4558\n",
      "Iteration 7200, Loss: 0.4552\n",
      "Iteration 7300, Loss: 0.4547\n",
      "Iteration 7400, Loss: 0.4542\n",
      "Iteration 7500, Loss: 0.4538\n",
      "Iteration 7600, Loss: 0.4533\n",
      "Iteration 7700, Loss: 0.4528\n",
      "Iteration 7800, Loss: 0.4523\n",
      "Iteration 7900, Loss: 0.4519\n",
      "Iteration 8000, Loss: 0.4514\n",
      "Iteration 8100, Loss: 0.4510\n",
      "Iteration 8200, Loss: 0.4505\n",
      "Iteration 8300, Loss: 0.4501\n",
      "Iteration 8400, Loss: 0.4496\n",
      "Iteration 8500, Loss: 0.4492\n",
      "Iteration 8600, Loss: 0.4488\n",
      "Iteration 8700, Loss: 0.4484\n",
      "Iteration 8800, Loss: 0.4480\n",
      "Iteration 8900, Loss: 0.4476\n",
      "Iteration 9000, Loss: 0.4472\n",
      "Iteration 9100, Loss: 0.4468\n",
      "Iteration 9200, Loss: 0.4464\n",
      "Iteration 9300, Loss: 0.4460\n",
      "Iteration 9400, Loss: 0.4456\n",
      "Iteration 9500, Loss: 0.4453\n",
      "Iteration 9600, Loss: 0.4449\n",
      "Iteration 9700, Loss: 0.4445\n",
      "Iteration 9800, Loss: 0.4442\n",
      "Iteration 9900, Loss: 0.4438\n",
      "Iteration 0, Loss: 1.0571\n",
      "Iteration 100, Loss: 0.6550\n",
      "Iteration 200, Loss: 0.6174\n",
      "Iteration 300, Loss: 0.5972\n",
      "Iteration 400, Loss: 0.5834\n",
      "Iteration 500, Loss: 0.5729\n",
      "Iteration 600, Loss: 0.5645\n",
      "Iteration 700, Loss: 0.5574\n",
      "Iteration 800, Loss: 0.5514\n",
      "Iteration 900, Loss: 0.5462\n",
      "Iteration 1000, Loss: 0.5416\n",
      "Iteration 1100, Loss: 0.5374\n",
      "Iteration 1200, Loss: 0.5338\n",
      "Iteration 1300, Loss: 0.5303\n",
      "Iteration 1400, Loss: 0.5272\n",
      "Iteration 1500, Loss: 0.5244\n",
      "Iteration 1600, Loss: 0.5218\n",
      "Iteration 1700, Loss: 0.5194\n",
      "Iteration 1800, Loss: 0.5171\n",
      "Iteration 1900, Loss: 0.5150\n",
      "Iteration 2000, Loss: 0.5129\n",
      "Iteration 2100, Loss: 0.5111\n",
      "Iteration 2200, Loss: 0.5093\n",
      "Iteration 2300, Loss: 0.5076\n",
      "Iteration 2400, Loss: 0.5060\n",
      "Iteration 2500, Loss: 0.5045\n",
      "Iteration 2600, Loss: 0.5031\n",
      "Iteration 2700, Loss: 0.5017\n",
      "Iteration 2800, Loss: 0.5004\n",
      "Iteration 2900, Loss: 0.4991\n",
      "Iteration 3000, Loss: 0.4979\n",
      "Iteration 3100, Loss: 0.4967\n",
      "Iteration 3200, Loss: 0.4956\n",
      "Iteration 3300, Loss: 0.4945\n",
      "Iteration 3400, Loss: 0.4935\n",
      "Iteration 3500, Loss: 0.4925\n",
      "Iteration 3600, Loss: 0.4915\n",
      "Iteration 3700, Loss: 0.4906\n",
      "Iteration 3800, Loss: 0.4897\n",
      "Iteration 3900, Loss: 0.4888\n",
      "Iteration 4000, Loss: 0.4879\n",
      "Iteration 4100, Loss: 0.4871\n",
      "Iteration 4200, Loss: 0.4863\n",
      "Iteration 4300, Loss: 0.4855\n",
      "Iteration 4400, Loss: 0.4848\n",
      "Iteration 4500, Loss: 0.4840\n",
      "Iteration 4600, Loss: 0.4833\n",
      "Iteration 4700, Loss: 0.4826\n",
      "Iteration 4800, Loss: 0.4819\n",
      "Iteration 4900, Loss: 0.4812\n",
      "Iteration 5000, Loss: 0.4805\n",
      "Iteration 5100, Loss: 0.4799\n",
      "Iteration 5200, Loss: 0.4793\n",
      "Iteration 5300, Loss: 0.4786\n",
      "Iteration 5400, Loss: 0.4780\n",
      "Iteration 5500, Loss: 0.4775\n",
      "Iteration 5600, Loss: 0.4769\n",
      "Iteration 5700, Loss: 0.4763\n",
      "Iteration 5800, Loss: 0.4758\n",
      "Iteration 5900, Loss: 0.4752\n",
      "Iteration 6000, Loss: 0.4747\n",
      "Iteration 6100, Loss: 0.4742\n",
      "Iteration 6200, Loss: 0.4737\n",
      "Iteration 6300, Loss: 0.4732\n",
      "Iteration 6400, Loss: 0.4727\n",
      "Iteration 6500, Loss: 0.4722\n",
      "Iteration 6600, Loss: 0.4717\n",
      "Iteration 6700, Loss: 0.4712\n",
      "Iteration 6800, Loss: 0.4708\n",
      "Iteration 6900, Loss: 0.4703\n",
      "Iteration 7000, Loss: 0.4699\n",
      "Iteration 7100, Loss: 0.4694\n",
      "Iteration 7200, Loss: 0.4690\n",
      "Iteration 7300, Loss: 0.4686\n",
      "Iteration 7400, Loss: 0.4682\n",
      "Iteration 7500, Loss: 0.4677\n",
      "Iteration 7600, Loss: 0.4673\n",
      "Iteration 7700, Loss: 0.4669\n",
      "Iteration 7800, Loss: 0.4666\n",
      "Iteration 7900, Loss: 0.4662\n",
      "Iteration 8000, Loss: 0.4658\n",
      "Iteration 8100, Loss: 0.4654\n",
      "Iteration 8200, Loss: 0.4650\n",
      "Iteration 8300, Loss: 0.4647\n",
      "Iteration 8400, Loss: 0.4643\n",
      "Iteration 8500, Loss: 0.4639\n",
      "Iteration 8600, Loss: 0.4636\n",
      "Iteration 8700, Loss: 0.4632\n",
      "Iteration 8800, Loss: 0.4629\n",
      "Iteration 8900, Loss: 0.4625\n",
      "Iteration 9000, Loss: 0.4622\n",
      "Iteration 9100, Loss: 0.4619\n",
      "Iteration 9200, Loss: 0.4616\n",
      "Iteration 9300, Loss: 0.4612\n",
      "Iteration 9400, Loss: 0.4609\n",
      "Iteration 9500, Loss: 0.4606\n",
      "Iteration 9600, Loss: 0.4603\n",
      "Iteration 9700, Loss: 0.4600\n",
      "Iteration 9800, Loss: 0.4597\n",
      "Iteration 9900, Loss: 0.4593\n",
      "Iteration 0, Loss: 1.0518\n",
      "Iteration 100, Loss: 0.6542\n",
      "Iteration 200, Loss: 0.6215\n",
      "Iteration 300, Loss: 0.6046\n",
      "Iteration 400, Loss: 0.5932\n",
      "Iteration 500, Loss: 0.5844\n",
      "Iteration 600, Loss: 0.5774\n",
      "Iteration 700, Loss: 0.5715\n",
      "Iteration 800, Loss: 0.5665\n",
      "Iteration 900, Loss: 0.5621\n",
      "Iteration 1000, Loss: 0.5582\n",
      "Iteration 1100, Loss: 0.5547\n",
      "Iteration 1200, Loss: 0.5516\n",
      "Iteration 1300, Loss: 0.5487\n",
      "Iteration 1400, Loss: 0.5461\n",
      "Iteration 1500, Loss: 0.5436\n",
      "Iteration 1600, Loss: 0.5414\n",
      "Iteration 1700, Loss: 0.5392\n",
      "Iteration 1800, Loss: 0.5371\n",
      "Iteration 1900, Loss: 0.5352\n",
      "Iteration 2000, Loss: 0.5334\n",
      "Iteration 2100, Loss: 0.5317\n",
      "Iteration 2200, Loss: 0.5300\n",
      "Iteration 2300, Loss: 0.5285\n",
      "Iteration 2400, Loss: 0.5269\n",
      "Iteration 2500, Loss: 0.5255\n",
      "Iteration 2600, Loss: 0.5240\n",
      "Iteration 2700, Loss: 0.5227\n",
      "Iteration 2800, Loss: 0.5214\n",
      "Iteration 2900, Loss: 0.5201\n",
      "Iteration 3000, Loss: 0.5189\n",
      "Iteration 3100, Loss: 0.5177\n",
      "Iteration 3200, Loss: 0.5166\n",
      "Iteration 3300, Loss: 0.5155\n",
      "Iteration 3400, Loss: 0.5144\n",
      "Iteration 3500, Loss: 0.5133\n",
      "Iteration 3600, Loss: 0.5123\n",
      "Iteration 3700, Loss: 0.5113\n",
      "Iteration 3800, Loss: 0.5103\n",
      "Iteration 3900, Loss: 0.5094\n",
      "Iteration 4000, Loss: 0.5085\n",
      "Iteration 4100, Loss: 0.5075\n",
      "Iteration 4200, Loss: 0.5066\n",
      "Iteration 4300, Loss: 0.5058\n",
      "Iteration 4400, Loss: 0.5049\n",
      "Iteration 4500, Loss: 0.5041\n",
      "Iteration 4600, Loss: 0.5033\n",
      "Iteration 4700, Loss: 0.5025\n",
      "Iteration 4800, Loss: 0.5017\n",
      "Iteration 4900, Loss: 0.5010\n",
      "Iteration 5000, Loss: 0.5002\n",
      "Iteration 5100, Loss: 0.4995\n",
      "Iteration 5200, Loss: 0.4988\n",
      "Iteration 5300, Loss: 0.4980\n",
      "Iteration 5400, Loss: 0.4973\n",
      "Iteration 5500, Loss: 0.4967\n",
      "Iteration 5600, Loss: 0.4960\n",
      "Iteration 5700, Loss: 0.4954\n",
      "Iteration 5800, Loss: 0.4947\n",
      "Iteration 5900, Loss: 0.4941\n",
      "Iteration 6000, Loss: 0.4935\n",
      "Iteration 6100, Loss: 0.4929\n",
      "Iteration 6200, Loss: 0.4922\n",
      "Iteration 6300, Loss: 0.4917\n",
      "Iteration 6400, Loss: 0.4911\n",
      "Iteration 6500, Loss: 0.4905\n",
      "Iteration 6600, Loss: 0.4899\n",
      "Iteration 6700, Loss: 0.4894\n",
      "Iteration 6800, Loss: 0.4888\n",
      "Iteration 6900, Loss: 0.4883\n",
      "Iteration 7000, Loss: 0.4878\n",
      "Iteration 7100, Loss: 0.4872\n",
      "Iteration 7200, Loss: 0.4867\n",
      "Iteration 7300, Loss: 0.4862\n",
      "Iteration 7400, Loss: 0.4857\n",
      "Iteration 7500, Loss: 0.4852\n",
      "Iteration 7600, Loss: 0.4847\n",
      "Iteration 7700, Loss: 0.4843\n",
      "Iteration 7800, Loss: 0.4838\n",
      "Iteration 7900, Loss: 0.4833\n",
      "Iteration 8000, Loss: 0.4829\n",
      "Iteration 8100, Loss: 0.4824\n",
      "Iteration 8200, Loss: 0.4820\n",
      "Iteration 8300, Loss: 0.4815\n",
      "Iteration 8400, Loss: 0.4811\n",
      "Iteration 8500, Loss: 0.4806\n",
      "Iteration 8600, Loss: 0.4802\n",
      "Iteration 8700, Loss: 0.4798\n",
      "Iteration 8800, Loss: 0.4794\n",
      "Iteration 8900, Loss: 0.4790\n",
      "Iteration 9000, Loss: 0.4786\n",
      "Iteration 9100, Loss: 0.4782\n",
      "Iteration 9200, Loss: 0.4778\n",
      "Iteration 9300, Loss: 0.4774\n",
      "Iteration 9400, Loss: 0.4770\n",
      "Iteration 9500, Loss: 0.4766\n",
      "Iteration 9600, Loss: 0.4762\n",
      "Iteration 9700, Loss: 0.4759\n",
      "Iteration 9800, Loss: 0.4755\n",
      "Iteration 9900, Loss: 0.4751\n",
      "Iteration 0, Loss: 1.0802\n",
      "Iteration 100, Loss: 0.7141\n",
      "Iteration 200, Loss: 0.6663\n",
      "Iteration 300, Loss: 0.6437\n",
      "Iteration 400, Loss: 0.6293\n",
      "Iteration 500, Loss: 0.6190\n",
      "Iteration 600, Loss: 0.6108\n",
      "Iteration 700, Loss: 0.6041\n",
      "Iteration 800, Loss: 0.5983\n",
      "Iteration 900, Loss: 0.5932\n",
      "Iteration 1000, Loss: 0.5887\n",
      "Iteration 1100, Loss: 0.5846\n",
      "Iteration 1200, Loss: 0.5809\n",
      "Iteration 1300, Loss: 0.5775\n",
      "Iteration 1400, Loss: 0.5743\n",
      "Iteration 1500, Loss: 0.5714\n",
      "Iteration 1600, Loss: 0.5686\n",
      "Iteration 1700, Loss: 0.5661\n",
      "Iteration 1800, Loss: 0.5636\n",
      "Iteration 1900, Loss: 0.5614\n",
      "Iteration 2000, Loss: 0.5592\n",
      "Iteration 2100, Loss: 0.5571\n",
      "Iteration 2200, Loss: 0.5552\n",
      "Iteration 2300, Loss: 0.5533\n",
      "Iteration 2400, Loss: 0.5515\n",
      "Iteration 2500, Loss: 0.5498\n",
      "Iteration 2600, Loss: 0.5482\n",
      "Iteration 2700, Loss: 0.5466\n",
      "Iteration 2800, Loss: 0.5451\n",
      "Iteration 2900, Loss: 0.5437\n",
      "Iteration 3000, Loss: 0.5423\n",
      "Iteration 3100, Loss: 0.5409\n",
      "Iteration 3200, Loss: 0.5396\n",
      "Iteration 3300, Loss: 0.5383\n",
      "Iteration 3400, Loss: 0.5371\n",
      "Iteration 3500, Loss: 0.5359\n",
      "Iteration 3600, Loss: 0.5348\n",
      "Iteration 3700, Loss: 0.5336\n",
      "Iteration 3800, Loss: 0.5326\n",
      "Iteration 3900, Loss: 0.5315\n",
      "Iteration 4000, Loss: 0.5305\n",
      "Iteration 4100, Loss: 0.5294\n",
      "Iteration 4200, Loss: 0.5285\n",
      "Iteration 4300, Loss: 0.5275\n",
      "Iteration 4400, Loss: 0.5266\n",
      "Iteration 4500, Loss: 0.5257\n",
      "Iteration 4600, Loss: 0.5248\n",
      "Iteration 4700, Loss: 0.5239\n",
      "Iteration 4800, Loss: 0.5230\n",
      "Iteration 4900, Loss: 0.5222\n",
      "Iteration 5000, Loss: 0.5214\n",
      "Iteration 5100, Loss: 0.5206\n",
      "Iteration 5200, Loss: 0.5198\n",
      "Iteration 5300, Loss: 0.5191\n",
      "Iteration 5400, Loss: 0.5183\n",
      "Iteration 5500, Loss: 0.5176\n",
      "Iteration 5600, Loss: 0.5168\n",
      "Iteration 5700, Loss: 0.5161\n",
      "Iteration 5800, Loss: 0.5154\n",
      "Iteration 5900, Loss: 0.5148\n",
      "Iteration 6000, Loss: 0.5141\n",
      "Iteration 6100, Loss: 0.5134\n",
      "Iteration 6200, Loss: 0.5128\n",
      "Iteration 6300, Loss: 0.5121\n",
      "Iteration 6400, Loss: 0.5115\n",
      "Iteration 6500, Loss: 0.5109\n",
      "Iteration 6600, Loss: 0.5103\n",
      "Iteration 6700, Loss: 0.5097\n",
      "Iteration 6800, Loss: 0.5091\n",
      "Iteration 6900, Loss: 0.5085\n",
      "Iteration 7000, Loss: 0.5080\n",
      "Iteration 7100, Loss: 0.5074\n",
      "Iteration 7200, Loss: 0.5069\n",
      "Iteration 7300, Loss: 0.5063\n",
      "Iteration 7400, Loss: 0.5058\n",
      "Iteration 7500, Loss: 0.5053\n",
      "Iteration 7600, Loss: 0.5047\n",
      "Iteration 7700, Loss: 0.5042\n",
      "Iteration 7800, Loss: 0.5037\n",
      "Iteration 7900, Loss: 0.5032\n",
      "Iteration 8000, Loss: 0.5027\n",
      "Iteration 8100, Loss: 0.5023\n",
      "Iteration 8200, Loss: 0.5018\n",
      "Iteration 8300, Loss: 0.5013\n",
      "Iteration 8400, Loss: 0.5009\n",
      "Iteration 8500, Loss: 0.5004\n",
      "Iteration 8600, Loss: 0.4999\n",
      "Iteration 8700, Loss: 0.4995\n",
      "Iteration 8800, Loss: 0.4991\n",
      "Iteration 8900, Loss: 0.4986\n",
      "Iteration 9000, Loss: 0.4982\n",
      "Iteration 9100, Loss: 0.4978\n",
      "Iteration 9200, Loss: 0.4974\n",
      "Iteration 9300, Loss: 0.4969\n",
      "Iteration 9400, Loss: 0.4965\n",
      "Iteration 9500, Loss: 0.4961\n",
      "Iteration 9600, Loss: 0.4957\n",
      "Iteration 9700, Loss: 0.4953\n",
      "Iteration 9800, Loss: 0.4949\n",
      "Iteration 9900, Loss: 0.4946\n",
      "Iteration 0, Loss: 1.0808\n",
      "Iteration 100, Loss: 0.7287\n",
      "Iteration 200, Loss: 0.6788\n",
      "Iteration 300, Loss: 0.6545\n",
      "Iteration 400, Loss: 0.6388\n",
      "Iteration 500, Loss: 0.6274\n",
      "Iteration 600, Loss: 0.6184\n",
      "Iteration 700, Loss: 0.6111\n",
      "Iteration 800, Loss: 0.6049\n",
      "Iteration 900, Loss: 0.5995\n",
      "Iteration 1000, Loss: 0.5947\n",
      "Iteration 1100, Loss: 0.5904\n",
      "Iteration 1200, Loss: 0.5866\n",
      "Iteration 1300, Loss: 0.5831\n",
      "Iteration 1400, Loss: 0.5799\n",
      "Iteration 1500, Loss: 0.5769\n",
      "Iteration 1600, Loss: 0.5741\n",
      "Iteration 1700, Loss: 0.5716\n",
      "Iteration 1800, Loss: 0.5692\n",
      "Iteration 1900, Loss: 0.5669\n",
      "Iteration 2000, Loss: 0.5648\n",
      "Iteration 2100, Loss: 0.5628\n",
      "Iteration 2200, Loss: 0.5609\n",
      "Iteration 2300, Loss: 0.5591\n",
      "Iteration 2400, Loss: 0.5573\n",
      "Iteration 2500, Loss: 0.5557\n",
      "Iteration 2600, Loss: 0.5541\n",
      "Iteration 2700, Loss: 0.5526\n",
      "Iteration 2800, Loss: 0.5512\n",
      "Iteration 2900, Loss: 0.5498\n",
      "Iteration 3000, Loss: 0.5485\n",
      "Iteration 3100, Loss: 0.5472\n",
      "Iteration 3200, Loss: 0.5459\n",
      "Iteration 3300, Loss: 0.5447\n",
      "Iteration 3400, Loss: 0.5436\n",
      "Iteration 3500, Loss: 0.5425\n",
      "Iteration 3600, Loss: 0.5414\n",
      "Iteration 3700, Loss: 0.5403\n",
      "Iteration 3800, Loss: 0.5393\n",
      "Iteration 3900, Loss: 0.5383\n",
      "Iteration 4000, Loss: 0.5373\n",
      "Iteration 4100, Loss: 0.5364\n",
      "Iteration 4200, Loss: 0.5355\n",
      "Iteration 4300, Loss: 0.5346\n",
      "Iteration 4400, Loss: 0.5337\n",
      "Iteration 4500, Loss: 0.5329\n",
      "Iteration 4600, Loss: 0.5321\n",
      "Iteration 4700, Loss: 0.5312\n",
      "Iteration 4800, Loss: 0.5305\n",
      "Iteration 4900, Loss: 0.5297\n",
      "Iteration 5000, Loss: 0.5289\n",
      "Iteration 5100, Loss: 0.5282\n",
      "Iteration 5200, Loss: 0.5275\n",
      "Iteration 5300, Loss: 0.5268\n",
      "Iteration 5400, Loss: 0.5261\n",
      "Iteration 5500, Loss: 0.5254\n",
      "Iteration 5600, Loss: 0.5247\n",
      "Iteration 5700, Loss: 0.5241\n",
      "Iteration 5800, Loss: 0.5234\n",
      "Iteration 5900, Loss: 0.5228\n",
      "Iteration 6000, Loss: 0.5222\n",
      "Iteration 6100, Loss: 0.5216\n",
      "Iteration 6200, Loss: 0.5210\n",
      "Iteration 6300, Loss: 0.5204\n",
      "Iteration 6400, Loss: 0.5198\n",
      "Iteration 6500, Loss: 0.5192\n",
      "Iteration 6600, Loss: 0.5187\n",
      "Iteration 6700, Loss: 0.5181\n",
      "Iteration 6800, Loss: 0.5176\n",
      "Iteration 6900, Loss: 0.5170\n",
      "Iteration 7000, Loss: 0.5165\n",
      "Iteration 7100, Loss: 0.5160\n",
      "Iteration 7200, Loss: 0.5155\n",
      "Iteration 7300, Loss: 0.5150\n",
      "Iteration 7400, Loss: 0.5145\n",
      "Iteration 7500, Loss: 0.5140\n",
      "Iteration 7600, Loss: 0.5135\n",
      "Iteration 7700, Loss: 0.5131\n",
      "Iteration 7800, Loss: 0.5126\n",
      "Iteration 7900, Loss: 0.5121\n",
      "Iteration 8000, Loss: 0.5117\n",
      "Iteration 8100, Loss: 0.5112\n",
      "Iteration 8200, Loss: 0.5108\n",
      "Iteration 8300, Loss: 0.5103\n",
      "Iteration 8400, Loss: 0.5099\n",
      "Iteration 8500, Loss: 0.5095\n",
      "Iteration 8600, Loss: 0.5091\n",
      "Iteration 8700, Loss: 0.5087\n",
      "Iteration 8800, Loss: 0.5082\n",
      "Iteration 8900, Loss: 0.5078\n",
      "Iteration 9000, Loss: 0.5074\n",
      "Iteration 9100, Loss: 0.5070\n",
      "Iteration 9200, Loss: 0.5067\n",
      "Iteration 9300, Loss: 0.5063\n",
      "Iteration 9400, Loss: 0.5059\n",
      "Iteration 9500, Loss: 0.5055\n",
      "Iteration 9600, Loss: 0.5051\n",
      "Iteration 9700, Loss: 0.5048\n",
      "Iteration 9800, Loss: 0.5044\n",
      "Iteration 9900, Loss: 0.5040\n",
      "Iteration 0, Loss: 1.0887\n",
      "Iteration 100, Loss: 0.7852\n",
      "Iteration 200, Loss: 0.7200\n",
      "Iteration 300, Loss: 0.6888\n",
      "Iteration 400, Loss: 0.6692\n",
      "Iteration 500, Loss: 0.6551\n",
      "Iteration 600, Loss: 0.6443\n",
      "Iteration 700, Loss: 0.6356\n",
      "Iteration 800, Loss: 0.6284\n",
      "Iteration 900, Loss: 0.6223\n",
      "Iteration 1000, Loss: 0.6169\n",
      "Iteration 1100, Loss: 0.6122\n",
      "Iteration 1200, Loss: 0.6081\n",
      "Iteration 1300, Loss: 0.6043\n",
      "Iteration 1400, Loss: 0.6008\n",
      "Iteration 1500, Loss: 0.5976\n",
      "Iteration 1600, Loss: 0.5947\n",
      "Iteration 1700, Loss: 0.5919\n",
      "Iteration 1800, Loss: 0.5894\n",
      "Iteration 1900, Loss: 0.5870\n",
      "Iteration 2000, Loss: 0.5847\n",
      "Iteration 2100, Loss: 0.5825\n",
      "Iteration 2200, Loss: 0.5805\n",
      "Iteration 2300, Loss: 0.5786\n",
      "Iteration 2400, Loss: 0.5767\n",
      "Iteration 2500, Loss: 0.5749\n",
      "Iteration 2600, Loss: 0.5732\n",
      "Iteration 2700, Loss: 0.5716\n",
      "Iteration 2800, Loss: 0.5700\n",
      "Iteration 2900, Loss: 0.5685\n",
      "Iteration 3000, Loss: 0.5671\n",
      "Iteration 3100, Loss: 0.5656\n",
      "Iteration 3200, Loss: 0.5643\n",
      "Iteration 3300, Loss: 0.5630\n",
      "Iteration 3400, Loss: 0.5617\n",
      "Iteration 3500, Loss: 0.5605\n",
      "Iteration 3600, Loss: 0.5593\n",
      "Iteration 3700, Loss: 0.5581\n",
      "Iteration 3800, Loss: 0.5570\n",
      "Iteration 3900, Loss: 0.5559\n",
      "Iteration 4000, Loss: 0.5548\n",
      "Iteration 4100, Loss: 0.5538\n",
      "Iteration 4200, Loss: 0.5528\n",
      "Iteration 4300, Loss: 0.5518\n",
      "Iteration 4400, Loss: 0.5508\n",
      "Iteration 4500, Loss: 0.5499\n",
      "Iteration 4600, Loss: 0.5489\n",
      "Iteration 4700, Loss: 0.5480\n",
      "Iteration 4800, Loss: 0.5472\n",
      "Iteration 4900, Loss: 0.5463\n",
      "Iteration 5000, Loss: 0.5455\n",
      "Iteration 5100, Loss: 0.5446\n",
      "Iteration 5200, Loss: 0.5438\n",
      "Iteration 5300, Loss: 0.5430\n",
      "Iteration 5400, Loss: 0.5423\n",
      "Iteration 5500, Loss: 0.5415\n",
      "Iteration 5600, Loss: 0.5408\n",
      "Iteration 5700, Loss: 0.5400\n",
      "Iteration 5800, Loss: 0.5393\n",
      "Iteration 5900, Loss: 0.5386\n",
      "Iteration 6000, Loss: 0.5379\n",
      "Iteration 6100, Loss: 0.5373\n",
      "Iteration 6200, Loss: 0.5366\n",
      "Iteration 6300, Loss: 0.5359\n",
      "Iteration 6400, Loss: 0.5353\n",
      "Iteration 6500, Loss: 0.5347\n",
      "Iteration 6600, Loss: 0.5340\n",
      "Iteration 6700, Loss: 0.5334\n",
      "Iteration 6800, Loss: 0.5328\n",
      "Iteration 6900, Loss: 0.5322\n",
      "Iteration 7000, Loss: 0.5317\n",
      "Iteration 7100, Loss: 0.5311\n",
      "Iteration 7200, Loss: 0.5305\n",
      "Iteration 7300, Loss: 0.5300\n",
      "Iteration 7400, Loss: 0.5294\n",
      "Iteration 7500, Loss: 0.5289\n",
      "Iteration 7600, Loss: 0.5284\n",
      "Iteration 7700, Loss: 0.5278\n",
      "Iteration 7800, Loss: 0.5273\n",
      "Iteration 7900, Loss: 0.5268\n",
      "Iteration 8000, Loss: 0.5263\n",
      "Iteration 8100, Loss: 0.5258\n",
      "Iteration 8200, Loss: 0.5253\n",
      "Iteration 8300, Loss: 0.5248\n",
      "Iteration 8400, Loss: 0.5244\n",
      "Iteration 8500, Loss: 0.5239\n",
      "Iteration 8600, Loss: 0.5234\n",
      "Iteration 8700, Loss: 0.5230\n",
      "Iteration 8800, Loss: 0.5225\n",
      "Iteration 8900, Loss: 0.5221\n",
      "Iteration 9000, Loss: 0.5216\n",
      "Iteration 9100, Loss: 0.5212\n",
      "Iteration 9200, Loss: 0.5207\n",
      "Iteration 9300, Loss: 0.5203\n",
      "Iteration 9400, Loss: 0.5199\n",
      "Iteration 9500, Loss: 0.5195\n",
      "Iteration 9600, Loss: 0.5191\n",
      "Iteration 9700, Loss: 0.5187\n",
      "Iteration 9800, Loss: 0.5183\n",
      "Iteration 9900, Loss: 0.5179\n",
      "Iteration 0, Loss: 1.0897\n",
      "Iteration 100, Loss: 0.7864\n",
      "Iteration 200, Loss: 0.7213\n",
      "Iteration 300, Loss: 0.6921\n",
      "Iteration 400, Loss: 0.6743\n",
      "Iteration 500, Loss: 0.6617\n",
      "Iteration 600, Loss: 0.6520\n",
      "Iteration 700, Loss: 0.6441\n",
      "Iteration 800, Loss: 0.6375\n",
      "Iteration 900, Loss: 0.6318\n",
      "Iteration 1000, Loss: 0.6268\n",
      "Iteration 1100, Loss: 0.6224\n",
      "Iteration 1200, Loss: 0.6183\n",
      "Iteration 1300, Loss: 0.6147\n",
      "Iteration 1400, Loss: 0.6113\n",
      "Iteration 1500, Loss: 0.6081\n",
      "Iteration 1600, Loss: 0.6052\n",
      "Iteration 1700, Loss: 0.6025\n",
      "Iteration 1800, Loss: 0.5999\n",
      "Iteration 1900, Loss: 0.5975\n",
      "Iteration 2000, Loss: 0.5952\n",
      "Iteration 2100, Loss: 0.5931\n",
      "Iteration 2200, Loss: 0.5910\n",
      "Iteration 2300, Loss: 0.5890\n",
      "Iteration 2400, Loss: 0.5872\n",
      "Iteration 2500, Loss: 0.5854\n",
      "Iteration 2600, Loss: 0.5837\n",
      "Iteration 2700, Loss: 0.5820\n",
      "Iteration 2800, Loss: 0.5805\n",
      "Iteration 2900, Loss: 0.5790\n",
      "Iteration 3000, Loss: 0.5775\n",
      "Iteration 3100, Loss: 0.5761\n",
      "Iteration 3200, Loss: 0.5748\n",
      "Iteration 3300, Loss: 0.5735\n",
      "Iteration 3400, Loss: 0.5722\n",
      "Iteration 3500, Loss: 0.5710\n",
      "Iteration 3600, Loss: 0.5698\n",
      "Iteration 3700, Loss: 0.5687\n",
      "Iteration 3800, Loss: 0.5676\n",
      "Iteration 3900, Loss: 0.5665\n",
      "Iteration 4000, Loss: 0.5655\n",
      "Iteration 4100, Loss: 0.5645\n",
      "Iteration 4200, Loss: 0.5635\n",
      "Iteration 4300, Loss: 0.5625\n",
      "Iteration 4400, Loss: 0.5616\n",
      "Iteration 4500, Loss: 0.5607\n",
      "Iteration 4600, Loss: 0.5598\n",
      "Iteration 4700, Loss: 0.5590\n",
      "Iteration 4800, Loss: 0.5581\n",
      "Iteration 4900, Loss: 0.5573\n",
      "Iteration 5000, Loss: 0.5565\n",
      "Iteration 5100, Loss: 0.5557\n",
      "Iteration 5200, Loss: 0.5549\n",
      "Iteration 5300, Loss: 0.5542\n",
      "Iteration 5400, Loss: 0.5534\n",
      "Iteration 5500, Loss: 0.5527\n",
      "Iteration 5600, Loss: 0.5520\n",
      "Iteration 5700, Loss: 0.5513\n",
      "Iteration 5800, Loss: 0.5506\n",
      "Iteration 5900, Loss: 0.5499\n",
      "Iteration 6000, Loss: 0.5493\n",
      "Iteration 6100, Loss: 0.5486\n",
      "Iteration 6200, Loss: 0.5480\n",
      "Iteration 6300, Loss: 0.5474\n",
      "Iteration 6400, Loss: 0.5468\n",
      "Iteration 6500, Loss: 0.5462\n",
      "Iteration 6600, Loss: 0.5456\n",
      "Iteration 6700, Loss: 0.5450\n",
      "Iteration 6800, Loss: 0.5444\n",
      "Iteration 6900, Loss: 0.5439\n",
      "Iteration 7000, Loss: 0.5433\n",
      "Iteration 7100, Loss: 0.5428\n",
      "Iteration 7200, Loss: 0.5422\n",
      "Iteration 7300, Loss: 0.5417\n",
      "Iteration 7400, Loss: 0.5412\n",
      "Iteration 7500, Loss: 0.5407\n",
      "Iteration 7600, Loss: 0.5401\n",
      "Iteration 7700, Loss: 0.5396\n",
      "Iteration 7800, Loss: 0.5392\n",
      "Iteration 7900, Loss: 0.5387\n",
      "Iteration 8000, Loss: 0.5382\n",
      "Iteration 8100, Loss: 0.5377\n",
      "Iteration 8200, Loss: 0.5372\n",
      "Iteration 8300, Loss: 0.5368\n",
      "Iteration 8400, Loss: 0.5363\n",
      "Iteration 8500, Loss: 0.5359\n",
      "Iteration 8600, Loss: 0.5354\n",
      "Iteration 8700, Loss: 0.5350\n",
      "Iteration 8800, Loss: 0.5345\n",
      "Iteration 8900, Loss: 0.5341\n",
      "Iteration 9000, Loss: 0.5337\n",
      "Iteration 9100, Loss: 0.5333\n",
      "Iteration 9200, Loss: 0.5329\n",
      "Iteration 9300, Loss: 0.5324\n",
      "Iteration 9400, Loss: 0.5320\n",
      "Iteration 9500, Loss: 0.5316\n",
      "Iteration 9600, Loss: 0.5312\n",
      "Iteration 9700, Loss: 0.5309\n",
      "Iteration 9800, Loss: 0.5305\n",
      "Iteration 9900, Loss: 0.5301\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8542\n",
      "Iteration 200, Loss: 0.7776\n",
      "Iteration 300, Loss: 0.7392\n",
      "Iteration 400, Loss: 0.7162\n",
      "Iteration 500, Loss: 0.7003\n",
      "Iteration 600, Loss: 0.6886\n",
      "Iteration 700, Loss: 0.6795\n",
      "Iteration 800, Loss: 0.6718\n",
      "Iteration 900, Loss: 0.6655\n",
      "Iteration 1000, Loss: 0.6599\n",
      "Iteration 1100, Loss: 0.6551\n",
      "Iteration 1200, Loss: 0.6509\n",
      "Iteration 1300, Loss: 0.6470\n",
      "Iteration 1400, Loss: 0.6435\n",
      "Iteration 1500, Loss: 0.6403\n",
      "Iteration 1600, Loss: 0.6374\n",
      "Iteration 1700, Loss: 0.6346\n",
      "Iteration 1800, Loss: 0.6321\n",
      "Iteration 1900, Loss: 0.6297\n",
      "Iteration 2000, Loss: 0.6275\n",
      "Iteration 2100, Loss: 0.6254\n",
      "Iteration 2200, Loss: 0.6234\n",
      "Iteration 2300, Loss: 0.6215\n",
      "Iteration 2400, Loss: 0.6197\n",
      "Iteration 2500, Loss: 0.6179\n",
      "Iteration 2600, Loss: 0.6163\n",
      "Iteration 2700, Loss: 0.6147\n",
      "Iteration 2800, Loss: 0.6131\n",
      "Iteration 2900, Loss: 0.6116\n",
      "Iteration 3000, Loss: 0.6102\n",
      "Iteration 3100, Loss: 0.6089\n",
      "Iteration 3200, Loss: 0.6075\n",
      "Iteration 3300, Loss: 0.6062\n",
      "Iteration 3400, Loss: 0.6050\n",
      "Iteration 3500, Loss: 0.6038\n",
      "Iteration 3600, Loss: 0.6026\n",
      "Iteration 3700, Loss: 0.6015\n",
      "Iteration 3800, Loss: 0.6004\n",
      "Iteration 3900, Loss: 0.5993\n",
      "Iteration 4000, Loss: 0.5982\n",
      "Iteration 4100, Loss: 0.5972\n",
      "Iteration 4200, Loss: 0.5962\n",
      "Iteration 4300, Loss: 0.5952\n",
      "Iteration 4400, Loss: 0.5943\n",
      "Iteration 4500, Loss: 0.5934\n",
      "Iteration 4600, Loss: 0.5924\n",
      "Iteration 4700, Loss: 0.5916\n",
      "Iteration 4800, Loss: 0.5907\n",
      "Iteration 4900, Loss: 0.5898\n",
      "Iteration 5000, Loss: 0.5890\n",
      "Iteration 5100, Loss: 0.5882\n",
      "Iteration 5200, Loss: 0.5874\n",
      "Iteration 5300, Loss: 0.5866\n",
      "Iteration 5400, Loss: 0.5858\n",
      "Iteration 5500, Loss: 0.5851\n",
      "Iteration 5600, Loss: 0.5844\n",
      "Iteration 5700, Loss: 0.5836\n",
      "Iteration 5800, Loss: 0.5829\n",
      "Iteration 5900, Loss: 0.5822\n",
      "Iteration 6000, Loss: 0.5816\n",
      "Iteration 6100, Loss: 0.5809\n",
      "Iteration 6200, Loss: 0.5802\n",
      "Iteration 6300, Loss: 0.5796\n",
      "Iteration 6400, Loss: 0.5790\n",
      "Iteration 6500, Loss: 0.5783\n",
      "Iteration 6600, Loss: 0.5777\n",
      "Iteration 6700, Loss: 0.5771\n",
      "Iteration 6800, Loss: 0.5765\n",
      "Iteration 6900, Loss: 0.5759\n",
      "Iteration 7000, Loss: 0.5754\n",
      "Iteration 7100, Loss: 0.5748\n",
      "Iteration 7200, Loss: 0.5742\n",
      "Iteration 7300, Loss: 0.5737\n",
      "Iteration 7400, Loss: 0.5731\n",
      "Iteration 7500, Loss: 0.5726\n",
      "Iteration 7600, Loss: 0.5721\n",
      "Iteration 7700, Loss: 0.5716\n",
      "Iteration 7800, Loss: 0.5711\n",
      "Iteration 7900, Loss: 0.5706\n",
      "Iteration 8000, Loss: 0.5701\n",
      "Iteration 8100, Loss: 0.5696\n",
      "Iteration 8200, Loss: 0.5691\n",
      "Iteration 8300, Loss: 0.5686\n",
      "Iteration 8400, Loss: 0.5681\n",
      "Iteration 8500, Loss: 0.5677\n",
      "Iteration 8600, Loss: 0.5672\n",
      "Iteration 8700, Loss: 0.5668\n",
      "Iteration 8800, Loss: 0.5663\n",
      "Iteration 8900, Loss: 0.5659\n",
      "Iteration 9000, Loss: 0.5655\n",
      "Iteration 9100, Loss: 0.5650\n",
      "Iteration 9200, Loss: 0.5646\n",
      "Iteration 9300, Loss: 0.5642\n",
      "Iteration 9400, Loss: 0.5638\n",
      "Iteration 9500, Loss: 0.5634\n",
      "Iteration 9600, Loss: 0.5630\n",
      "Iteration 9700, Loss: 0.5625\n",
      "Iteration 9800, Loss: 0.5622\n",
      "Iteration 9900, Loss: 0.5618\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8658\n",
      "Iteration 200, Loss: 0.7856\n",
      "Iteration 300, Loss: 0.7438\n",
      "Iteration 400, Loss: 0.7177\n",
      "Iteration 500, Loss: 0.6997\n",
      "Iteration 600, Loss: 0.6863\n",
      "Iteration 700, Loss: 0.6757\n",
      "Iteration 800, Loss: 0.6670\n",
      "Iteration 900, Loss: 0.6595\n",
      "Iteration 1000, Loss: 0.6531\n",
      "Iteration 1100, Loss: 0.6475\n",
      "Iteration 1200, Loss: 0.6425\n",
      "Iteration 1300, Loss: 0.6380\n",
      "Iteration 1400, Loss: 0.6340\n",
      "Iteration 1500, Loss: 0.6303\n",
      "Iteration 1600, Loss: 0.6268\n",
      "Iteration 1700, Loss: 0.6237\n",
      "Iteration 1800, Loss: 0.6208\n",
      "Iteration 1900, Loss: 0.6180\n",
      "Iteration 2000, Loss: 0.6155\n",
      "Iteration 2100, Loss: 0.6130\n",
      "Iteration 2200, Loss: 0.6107\n",
      "Iteration 2300, Loss: 0.6086\n",
      "Iteration 2400, Loss: 0.6065\n",
      "Iteration 2500, Loss: 0.6046\n",
      "Iteration 2600, Loss: 0.6027\n",
      "Iteration 2700, Loss: 0.6009\n",
      "Iteration 2800, Loss: 0.5992\n",
      "Iteration 2900, Loss: 0.5976\n",
      "Iteration 3000, Loss: 0.5960\n",
      "Iteration 3100, Loss: 0.5945\n",
      "Iteration 3200, Loss: 0.5930\n",
      "Iteration 3300, Loss: 0.5916\n",
      "Iteration 3400, Loss: 0.5902\n",
      "Iteration 3500, Loss: 0.5889\n",
      "Iteration 3600, Loss: 0.5876\n",
      "Iteration 3700, Loss: 0.5863\n",
      "Iteration 3800, Loss: 0.5851\n",
      "Iteration 3900, Loss: 0.5840\n",
      "Iteration 4000, Loss: 0.5828\n",
      "Iteration 4100, Loss: 0.5817\n",
      "Iteration 4200, Loss: 0.5806\n",
      "Iteration 4300, Loss: 0.5796\n",
      "Iteration 4400, Loss: 0.5786\n",
      "Iteration 4500, Loss: 0.5776\n",
      "Iteration 4600, Loss: 0.5766\n",
      "Iteration 4700, Loss: 0.5757\n",
      "Iteration 4800, Loss: 0.5747\n",
      "Iteration 4900, Loss: 0.5738\n",
      "Iteration 5000, Loss: 0.5729\n",
      "Iteration 5100, Loss: 0.5721\n",
      "Iteration 5200, Loss: 0.5712\n",
      "Iteration 5300, Loss: 0.5704\n",
      "Iteration 5400, Loss: 0.5696\n",
      "Iteration 5500, Loss: 0.5688\n",
      "Iteration 5600, Loss: 0.5680\n",
      "Iteration 5700, Loss: 0.5672\n",
      "Iteration 5800, Loss: 0.5664\n",
      "Iteration 5900, Loss: 0.5657\n",
      "Iteration 6000, Loss: 0.5650\n",
      "Iteration 6100, Loss: 0.5643\n",
      "Iteration 6200, Loss: 0.5636\n",
      "Iteration 6300, Loss: 0.5629\n",
      "Iteration 6400, Loss: 0.5622\n",
      "Iteration 6500, Loss: 0.5615\n",
      "Iteration 6600, Loss: 0.5609\n",
      "Iteration 6700, Loss: 0.5602\n",
      "Iteration 6800, Loss: 0.5596\n",
      "Iteration 6900, Loss: 0.5590\n",
      "Iteration 7000, Loss: 0.5584\n",
      "Iteration 7100, Loss: 0.5578\n",
      "Iteration 7200, Loss: 0.5572\n",
      "Iteration 7300, Loss: 0.5566\n",
      "Iteration 7400, Loss: 0.5560\n",
      "Iteration 7500, Loss: 0.5555\n",
      "Iteration 7600, Loss: 0.5549\n",
      "Iteration 7700, Loss: 0.5544\n",
      "Iteration 7800, Loss: 0.5538\n",
      "Iteration 7900, Loss: 0.5533\n",
      "Iteration 8000, Loss: 0.5528\n",
      "Iteration 8100, Loss: 0.5522\n",
      "Iteration 8200, Loss: 0.5517\n",
      "Iteration 8300, Loss: 0.5512\n",
      "Iteration 8400, Loss: 0.5507\n",
      "Iteration 8500, Loss: 0.5502\n",
      "Iteration 8600, Loss: 0.5497\n",
      "Iteration 8700, Loss: 0.5493\n",
      "Iteration 8800, Loss: 0.5488\n",
      "Iteration 8900, Loss: 0.5483\n",
      "Iteration 9000, Loss: 0.5479\n",
      "Iteration 9100, Loss: 0.5474\n",
      "Iteration 9200, Loss: 0.5470\n",
      "Iteration 9300, Loss: 0.5465\n",
      "Iteration 9400, Loss: 0.5461\n",
      "Iteration 9500, Loss: 0.5456\n",
      "Iteration 9600, Loss: 0.5452\n",
      "Iteration 9700, Loss: 0.5448\n",
      "Iteration 9800, Loss: 0.5444\n",
      "Iteration 9900, Loss: 0.5439\n",
      "Iteration 0, Loss: 1.0968\n",
      "Iteration 100, Loss: 0.9654\n",
      "Iteration 200, Loss: 0.8904\n",
      "Iteration 300, Loss: 0.8410\n",
      "Iteration 400, Loss: 0.8059\n",
      "Iteration 500, Loss: 0.7798\n",
      "Iteration 600, Loss: 0.7596\n",
      "Iteration 700, Loss: 0.7437\n",
      "Iteration 800, Loss: 0.7307\n",
      "Iteration 900, Loss: 0.7200\n",
      "Iteration 1000, Loss: 0.7109\n",
      "Iteration 1100, Loss: 0.7031\n",
      "Iteration 1200, Loss: 0.6962\n",
      "Iteration 1300, Loss: 0.6902\n",
      "Iteration 1400, Loss: 0.6848\n",
      "Iteration 1500, Loss: 0.6799\n",
      "Iteration 1600, Loss: 0.6755\n",
      "Iteration 1700, Loss: 0.6714\n",
      "Iteration 1800, Loss: 0.6677\n",
      "Iteration 1900, Loss: 0.6642\n",
      "Iteration 2000, Loss: 0.6610\n",
      "Iteration 2100, Loss: 0.6579\n",
      "Iteration 2200, Loss: 0.6551\n",
      "Iteration 2300, Loss: 0.6524\n",
      "Iteration 2400, Loss: 0.6499\n",
      "Iteration 2500, Loss: 0.6475\n",
      "Iteration 2600, Loss: 0.6453\n",
      "Iteration 2700, Loss: 0.6431\n",
      "Iteration 2800, Loss: 0.6411\n",
      "Iteration 2900, Loss: 0.6391\n",
      "Iteration 3000, Loss: 0.6373\n",
      "Iteration 3100, Loss: 0.6355\n",
      "Iteration 3200, Loss: 0.6338\n",
      "Iteration 3300, Loss: 0.6321\n",
      "Iteration 3400, Loss: 0.6305\n",
      "Iteration 3500, Loss: 0.6290\n",
      "Iteration 3600, Loss: 0.6275\n",
      "Iteration 3700, Loss: 0.6261\n",
      "Iteration 3800, Loss: 0.6247\n",
      "Iteration 3900, Loss: 0.6234\n",
      "Iteration 4000, Loss: 0.6221\n",
      "Iteration 4100, Loss: 0.6209\n",
      "Iteration 4200, Loss: 0.6197\n",
      "Iteration 4300, Loss: 0.6185\n",
      "Iteration 4400, Loss: 0.6174\n",
      "Iteration 4500, Loss: 0.6163\n",
      "Iteration 4600, Loss: 0.6152\n",
      "Iteration 4700, Loss: 0.6141\n",
      "Iteration 4800, Loss: 0.6131\n",
      "Iteration 4900, Loss: 0.6121\n",
      "Iteration 5000, Loss: 0.6111\n",
      "Iteration 5100, Loss: 0.6102\n",
      "Iteration 5200, Loss: 0.6093\n",
      "Iteration 5300, Loss: 0.6084\n",
      "Iteration 5400, Loss: 0.6075\n",
      "Iteration 5500, Loss: 0.6066\n",
      "Iteration 5600, Loss: 0.6057\n",
      "Iteration 5700, Loss: 0.6049\n",
      "Iteration 5800, Loss: 0.6041\n",
      "Iteration 5900, Loss: 0.6033\n",
      "Iteration 6000, Loss: 0.6025\n",
      "Iteration 6100, Loss: 0.6017\n",
      "Iteration 6200, Loss: 0.6010\n",
      "Iteration 6300, Loss: 0.6003\n",
      "Iteration 6400, Loss: 0.5995\n",
      "Iteration 6500, Loss: 0.5988\n",
      "Iteration 6600, Loss: 0.5981\n",
      "Iteration 6700, Loss: 0.5974\n",
      "Iteration 6800, Loss: 0.5967\n",
      "Iteration 6900, Loss: 0.5961\n",
      "Iteration 7000, Loss: 0.5954\n",
      "Iteration 7100, Loss: 0.5948\n",
      "Iteration 7200, Loss: 0.5941\n",
      "Iteration 7300, Loss: 0.5935\n",
      "Iteration 7400, Loss: 0.5929\n",
      "Iteration 7500, Loss: 0.5923\n",
      "Iteration 7600, Loss: 0.5917\n",
      "Iteration 7700, Loss: 0.5911\n",
      "Iteration 7800, Loss: 0.5905\n",
      "Iteration 7900, Loss: 0.5899\n",
      "Iteration 8000, Loss: 0.5894\n",
      "Iteration 8100, Loss: 0.5888\n",
      "Iteration 8200, Loss: 0.5883\n",
      "Iteration 8300, Loss: 0.5877\n",
      "Iteration 8400, Loss: 0.5872\n",
      "Iteration 8500, Loss: 0.5867\n",
      "Iteration 8600, Loss: 0.5861\n",
      "Iteration 8700, Loss: 0.5856\n",
      "Iteration 8800, Loss: 0.5851\n",
      "Iteration 8900, Loss: 0.5846\n",
      "Iteration 9000, Loss: 0.5841\n",
      "Iteration 9100, Loss: 0.5836\n",
      "Iteration 9200, Loss: 0.5831\n",
      "Iteration 9300, Loss: 0.5826\n",
      "Iteration 9400, Loss: 0.5822\n",
      "Iteration 9500, Loss: 0.5817\n",
      "Iteration 9600, Loss: 0.5812\n",
      "Iteration 9700, Loss: 0.5808\n",
      "Iteration 9800, Loss: 0.5803\n",
      "Iteration 9900, Loss: 0.5799\n",
      "Iteration 0, Loss: 1.0966\n",
      "Iteration 100, Loss: 0.9667\n",
      "Iteration 200, Loss: 0.8964\n",
      "Iteration 300, Loss: 0.8508\n",
      "Iteration 400, Loss: 0.8188\n",
      "Iteration 500, Loss: 0.7950\n",
      "Iteration 600, Loss: 0.7766\n",
      "Iteration 700, Loss: 0.7619\n",
      "Iteration 800, Loss: 0.7498\n",
      "Iteration 900, Loss: 0.7398\n",
      "Iteration 1000, Loss: 0.7312\n",
      "Iteration 1100, Loss: 0.7238\n",
      "Iteration 1200, Loss: 0.7173\n",
      "Iteration 1300, Loss: 0.7115\n",
      "Iteration 1400, Loss: 0.7063\n",
      "Iteration 1500, Loss: 0.7016\n",
      "Iteration 1600, Loss: 0.6973\n",
      "Iteration 1700, Loss: 0.6934\n",
      "Iteration 1800, Loss: 0.6898\n",
      "Iteration 1900, Loss: 0.6864\n",
      "Iteration 2000, Loss: 0.6832\n",
      "Iteration 2100, Loss: 0.6803\n",
      "Iteration 2200, Loss: 0.6775\n",
      "Iteration 2300, Loss: 0.6749\n",
      "Iteration 2400, Loss: 0.6724\n",
      "Iteration 2500, Loss: 0.6701\n",
      "Iteration 2600, Loss: 0.6678\n",
      "Iteration 2700, Loss: 0.6657\n",
      "Iteration 2800, Loss: 0.6637\n",
      "Iteration 2900, Loss: 0.6618\n",
      "Iteration 3000, Loss: 0.6599\n",
      "Iteration 3100, Loss: 0.6582\n",
      "Iteration 3200, Loss: 0.6565\n",
      "Iteration 3300, Loss: 0.6548\n",
      "Iteration 3400, Loss: 0.6533\n",
      "Iteration 3500, Loss: 0.6517\n",
      "Iteration 3600, Loss: 0.6503\n",
      "Iteration 3700, Loss: 0.6489\n",
      "Iteration 3800, Loss: 0.6475\n",
      "Iteration 3900, Loss: 0.6462\n",
      "Iteration 4000, Loss: 0.6449\n",
      "Iteration 4100, Loss: 0.6437\n",
      "Iteration 4200, Loss: 0.6425\n",
      "Iteration 4300, Loss: 0.6413\n",
      "Iteration 4400, Loss: 0.6402\n",
      "Iteration 4500, Loss: 0.6391\n",
      "Iteration 4600, Loss: 0.6380\n",
      "Iteration 4700, Loss: 0.6369\n",
      "Iteration 4800, Loss: 0.6359\n",
      "Iteration 4900, Loss: 0.6349\n",
      "Iteration 5000, Loss: 0.6340\n",
      "Iteration 5100, Loss: 0.6330\n",
      "Iteration 5200, Loss: 0.6321\n",
      "Iteration 5300, Loss: 0.6312\n",
      "Iteration 5400, Loss: 0.6303\n",
      "Iteration 5500, Loss: 0.6294\n",
      "Iteration 5600, Loss: 0.6286\n",
      "Iteration 5700, Loss: 0.6278\n",
      "Iteration 5800, Loss: 0.6270\n",
      "Iteration 5900, Loss: 0.6262\n",
      "Iteration 6000, Loss: 0.6254\n",
      "Iteration 6100, Loss: 0.6246\n",
      "Iteration 6200, Loss: 0.6239\n",
      "Iteration 6300, Loss: 0.6231\n",
      "Iteration 6400, Loss: 0.6224\n",
      "Iteration 6500, Loss: 0.6217\n",
      "Iteration 6600, Loss: 0.6210\n",
      "Iteration 6700, Loss: 0.6203\n",
      "Iteration 6800, Loss: 0.6196\n",
      "Iteration 6900, Loss: 0.6190\n",
      "Iteration 7000, Loss: 0.6183\n",
      "Iteration 7100, Loss: 0.6177\n",
      "Iteration 7200, Loss: 0.6171\n",
      "Iteration 7300, Loss: 0.6164\n",
      "Iteration 7400, Loss: 0.6158\n",
      "Iteration 7500, Loss: 0.6152\n",
      "Iteration 7600, Loss: 0.6146\n",
      "Iteration 7700, Loss: 0.6140\n",
      "Iteration 7800, Loss: 0.6135\n",
      "Iteration 7900, Loss: 0.6129\n",
      "Iteration 8000, Loss: 0.6123\n",
      "Iteration 8100, Loss: 0.6118\n",
      "Iteration 8200, Loss: 0.6112\n",
      "Iteration 8300, Loss: 0.6107\n",
      "Iteration 8400, Loss: 0.6102\n",
      "Iteration 8500, Loss: 0.6096\n",
      "Iteration 8600, Loss: 0.6091\n",
      "Iteration 8700, Loss: 0.6086\n",
      "Iteration 8800, Loss: 0.6081\n",
      "Iteration 8900, Loss: 0.6076\n",
      "Iteration 9000, Loss: 0.6071\n",
      "Iteration 9100, Loss: 0.6066\n",
      "Iteration 9200, Loss: 0.6061\n",
      "Iteration 9300, Loss: 0.6057\n",
      "Iteration 9400, Loss: 0.6052\n",
      "Iteration 9500, Loss: 0.6047\n",
      "Iteration 9600, Loss: 0.6043\n",
      "Iteration 9700, Loss: 0.6038\n",
      "Iteration 9800, Loss: 0.6034\n",
      "Iteration 9900, Loss: 0.6029\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0219\n",
      "Iteration 200, Loss: 0.9687\n",
      "Iteration 300, Loss: 0.9286\n",
      "Iteration 400, Loss: 0.8968\n",
      "Iteration 500, Loss: 0.8710\n",
      "Iteration 600, Loss: 0.8495\n",
      "Iteration 700, Loss: 0.8313\n",
      "Iteration 800, Loss: 0.8158\n",
      "Iteration 900, Loss: 0.8024\n",
      "Iteration 1000, Loss: 0.7907\n",
      "Iteration 1100, Loss: 0.7804\n",
      "Iteration 1200, Loss: 0.7713\n",
      "Iteration 1300, Loss: 0.7632\n",
      "Iteration 1400, Loss: 0.7559\n",
      "Iteration 1500, Loss: 0.7493\n",
      "Iteration 1600, Loss: 0.7433\n",
      "Iteration 1700, Loss: 0.7378\n",
      "Iteration 1800, Loss: 0.7328\n",
      "Iteration 1900, Loss: 0.7282\n",
      "Iteration 2000, Loss: 0.7239\n",
      "Iteration 2100, Loss: 0.7200\n",
      "Iteration 2200, Loss: 0.7163\n",
      "Iteration 2300, Loss: 0.7129\n",
      "Iteration 2400, Loss: 0.7096\n",
      "Iteration 2500, Loss: 0.7066\n",
      "Iteration 2600, Loss: 0.7037\n",
      "Iteration 2700, Loss: 0.7010\n",
      "Iteration 2800, Loss: 0.6984\n",
      "Iteration 2900, Loss: 0.6959\n",
      "Iteration 3000, Loss: 0.6936\n",
      "Iteration 3100, Loss: 0.6913\n",
      "Iteration 3200, Loss: 0.6892\n",
      "Iteration 3300, Loss: 0.6872\n",
      "Iteration 3400, Loss: 0.6852\n",
      "Iteration 3500, Loss: 0.6833\n",
      "Iteration 3600, Loss: 0.6815\n",
      "Iteration 3700, Loss: 0.6797\n",
      "Iteration 3800, Loss: 0.6781\n",
      "Iteration 3900, Loss: 0.6764\n",
      "Iteration 4000, Loss: 0.6749\n",
      "Iteration 4100, Loss: 0.6734\n",
      "Iteration 4200, Loss: 0.6719\n",
      "Iteration 4300, Loss: 0.6705\n",
      "Iteration 4400, Loss: 0.6691\n",
      "Iteration 4500, Loss: 0.6678\n",
      "Iteration 4600, Loss: 0.6665\n",
      "Iteration 4700, Loss: 0.6652\n",
      "Iteration 4800, Loss: 0.6640\n",
      "Iteration 4900, Loss: 0.6628\n",
      "Iteration 5000, Loss: 0.6616\n",
      "Iteration 5100, Loss: 0.6605\n",
      "Iteration 5200, Loss: 0.6594\n",
      "Iteration 5300, Loss: 0.6583\n",
      "Iteration 5400, Loss: 0.6573\n",
      "Iteration 5500, Loss: 0.6562\n",
      "Iteration 5600, Loss: 0.6552\n",
      "Iteration 5700, Loss: 0.6543\n",
      "Iteration 5800, Loss: 0.6533\n",
      "Iteration 5900, Loss: 0.6524\n",
      "Iteration 6000, Loss: 0.6515\n",
      "Iteration 6100, Loss: 0.6506\n",
      "Iteration 6200, Loss: 0.6497\n",
      "Iteration 6300, Loss: 0.6488\n",
      "Iteration 6400, Loss: 0.6480\n",
      "Iteration 6500, Loss: 0.6472\n",
      "Iteration 6600, Loss: 0.6464\n",
      "Iteration 6700, Loss: 0.6456\n",
      "Iteration 6800, Loss: 0.6448\n",
      "Iteration 6900, Loss: 0.6441\n",
      "Iteration 7000, Loss: 0.6433\n",
      "Iteration 7100, Loss: 0.6426\n",
      "Iteration 7200, Loss: 0.6419\n",
      "Iteration 7300, Loss: 0.6412\n",
      "Iteration 7400, Loss: 0.6405\n",
      "Iteration 7500, Loss: 0.6398\n",
      "Iteration 7600, Loss: 0.6391\n",
      "Iteration 7700, Loss: 0.6385\n",
      "Iteration 7800, Loss: 0.6378\n",
      "Iteration 7900, Loss: 0.6372\n",
      "Iteration 8000, Loss: 0.6366\n",
      "Iteration 8100, Loss: 0.6360\n",
      "Iteration 8200, Loss: 0.6354\n",
      "Iteration 8300, Loss: 0.6348\n",
      "Iteration 8400, Loss: 0.6342\n",
      "Iteration 8500, Loss: 0.6336\n",
      "Iteration 8600, Loss: 0.6330\n",
      "Iteration 8700, Loss: 0.6325\n",
      "Iteration 8800, Loss: 0.6319\n",
      "Iteration 8900, Loss: 0.6314\n",
      "Iteration 9000, Loss: 0.6308\n",
      "Iteration 9100, Loss: 0.6303\n",
      "Iteration 9200, Loss: 0.6298\n",
      "Iteration 9300, Loss: 0.6293\n",
      "Iteration 9400, Loss: 0.6288\n",
      "Iteration 9500, Loss: 0.6283\n",
      "Iteration 9600, Loss: 0.6278\n",
      "Iteration 9700, Loss: 0.6273\n",
      "Iteration 9800, Loss: 0.6268\n",
      "Iteration 9900, Loss: 0.6263\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0169\n",
      "Iteration 200, Loss: 0.9611\n",
      "Iteration 300, Loss: 0.9196\n",
      "Iteration 400, Loss: 0.8872\n",
      "Iteration 500, Loss: 0.8612\n",
      "Iteration 600, Loss: 0.8399\n",
      "Iteration 700, Loss: 0.8220\n",
      "Iteration 800, Loss: 0.8069\n",
      "Iteration 900, Loss: 0.7937\n",
      "Iteration 1000, Loss: 0.7824\n",
      "Iteration 1100, Loss: 0.7725\n",
      "Iteration 1200, Loss: 0.7636\n",
      "Iteration 1300, Loss: 0.7558\n",
      "Iteration 1400, Loss: 0.7487\n",
      "Iteration 1500, Loss: 0.7423\n",
      "Iteration 1600, Loss: 0.7366\n",
      "Iteration 1700, Loss: 0.7313\n",
      "Iteration 1800, Loss: 0.7265\n",
      "Iteration 1900, Loss: 0.7220\n",
      "Iteration 2000, Loss: 0.7179\n",
      "Iteration 2100, Loss: 0.7141\n",
      "Iteration 2200, Loss: 0.7105\n",
      "Iteration 2300, Loss: 0.7071\n",
      "Iteration 2400, Loss: 0.7040\n",
      "Iteration 2500, Loss: 0.7010\n",
      "Iteration 2600, Loss: 0.6982\n",
      "Iteration 2700, Loss: 0.6955\n",
      "Iteration 2800, Loss: 0.6930\n",
      "Iteration 2900, Loss: 0.6906\n",
      "Iteration 3000, Loss: 0.6883\n",
      "Iteration 3100, Loss: 0.6861\n",
      "Iteration 3200, Loss: 0.6840\n",
      "Iteration 3300, Loss: 0.6819\n",
      "Iteration 3400, Loss: 0.6800\n",
      "Iteration 3500, Loss: 0.6781\n",
      "Iteration 3600, Loss: 0.6763\n",
      "Iteration 3700, Loss: 0.6746\n",
      "Iteration 3800, Loss: 0.6729\n",
      "Iteration 3900, Loss: 0.6713\n",
      "Iteration 4000, Loss: 0.6697\n",
      "Iteration 4100, Loss: 0.6682\n",
      "Iteration 4200, Loss: 0.6668\n",
      "Iteration 4300, Loss: 0.6653\n",
      "Iteration 4400, Loss: 0.6639\n",
      "Iteration 4500, Loss: 0.6626\n",
      "Iteration 4600, Loss: 0.6613\n",
      "Iteration 4700, Loss: 0.6600\n",
      "Iteration 4800, Loss: 0.6588\n",
      "Iteration 4900, Loss: 0.6576\n",
      "Iteration 5000, Loss: 0.6564\n",
      "Iteration 5100, Loss: 0.6553\n",
      "Iteration 5200, Loss: 0.6541\n",
      "Iteration 5300, Loss: 0.6531\n",
      "Iteration 5400, Loss: 0.6520\n",
      "Iteration 5500, Loss: 0.6509\n",
      "Iteration 5600, Loss: 0.6499\n",
      "Iteration 5700, Loss: 0.6489\n",
      "Iteration 5800, Loss: 0.6479\n",
      "Iteration 5900, Loss: 0.6470\n",
      "Iteration 6000, Loss: 0.6461\n",
      "Iteration 6100, Loss: 0.6451\n",
      "Iteration 6200, Loss: 0.6442\n",
      "Iteration 6300, Loss: 0.6434\n",
      "Iteration 6400, Loss: 0.6425\n",
      "Iteration 6500, Loss: 0.6417\n",
      "Iteration 6600, Loss: 0.6408\n",
      "Iteration 6700, Loss: 0.6400\n",
      "Iteration 6800, Loss: 0.6392\n",
      "Iteration 6900, Loss: 0.6384\n",
      "Iteration 7000, Loss: 0.6377\n",
      "Iteration 7100, Loss: 0.6369\n",
      "Iteration 7200, Loss: 0.6362\n",
      "Iteration 7300, Loss: 0.6354\n",
      "Iteration 7400, Loss: 0.6347\n",
      "Iteration 7500, Loss: 0.6340\n",
      "Iteration 7600, Loss: 0.6333\n",
      "Iteration 7700, Loss: 0.6326\n",
      "Iteration 7800, Loss: 0.6319\n",
      "Iteration 7900, Loss: 0.6313\n",
      "Iteration 8000, Loss: 0.6306\n",
      "Iteration 8100, Loss: 0.6300\n",
      "Iteration 8200, Loss: 0.6293\n",
      "Iteration 8300, Loss: 0.6287\n",
      "Iteration 8400, Loss: 0.6281\n",
      "Iteration 8500, Loss: 0.6275\n",
      "Iteration 8600, Loss: 0.6269\n",
      "Iteration 8700, Loss: 0.6263\n",
      "Iteration 8800, Loss: 0.6257\n",
      "Iteration 8900, Loss: 0.6251\n",
      "Iteration 9000, Loss: 0.6245\n",
      "Iteration 9100, Loss: 0.6240\n",
      "Iteration 9200, Loss: 0.6234\n",
      "Iteration 9300, Loss: 0.6229\n",
      "Iteration 9400, Loss: 0.6223\n",
      "Iteration 9500, Loss: 0.6218\n",
      "Iteration 9600, Loss: 0.6213\n",
      "Iteration 9700, Loss: 0.6207\n",
      "Iteration 9800, Loss: 0.6202\n",
      "Iteration 9900, Loss: 0.6197\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0610\n",
      "Iteration 200, Loss: 1.0297\n",
      "Iteration 300, Loss: 1.0030\n",
      "Iteration 400, Loss: 0.9798\n",
      "Iteration 500, Loss: 0.9593\n",
      "Iteration 600, Loss: 0.9413\n",
      "Iteration 700, Loss: 0.9250\n",
      "Iteration 800, Loss: 0.9104\n",
      "Iteration 900, Loss: 0.8972\n",
      "Iteration 1000, Loss: 0.8853\n",
      "Iteration 1100, Loss: 0.8743\n",
      "Iteration 1200, Loss: 0.8643\n",
      "Iteration 1300, Loss: 0.8551\n",
      "Iteration 1400, Loss: 0.8465\n",
      "Iteration 1500, Loss: 0.8386\n",
      "Iteration 1600, Loss: 0.8312\n",
      "Iteration 1700, Loss: 0.8244\n",
      "Iteration 1800, Loss: 0.8180\n",
      "Iteration 1900, Loss: 0.8121\n",
      "Iteration 2000, Loss: 0.8066\n",
      "Iteration 2100, Loss: 0.8014\n",
      "Iteration 2200, Loss: 0.7965\n",
      "Iteration 2300, Loss: 0.7919\n",
      "Iteration 2400, Loss: 0.7875\n",
      "Iteration 2500, Loss: 0.7834\n",
      "Iteration 2600, Loss: 0.7796\n",
      "Iteration 2700, Loss: 0.7759\n",
      "Iteration 2800, Loss: 0.7724\n",
      "Iteration 2900, Loss: 0.7691\n",
      "Iteration 3000, Loss: 0.7659\n",
      "Iteration 3100, Loss: 0.7628\n",
      "Iteration 3200, Loss: 0.7599\n",
      "Iteration 3300, Loss: 0.7572\n",
      "Iteration 3400, Loss: 0.7545\n",
      "Iteration 3500, Loss: 0.7520\n",
      "Iteration 3600, Loss: 0.7495\n",
      "Iteration 3700, Loss: 0.7472\n",
      "Iteration 3800, Loss: 0.7449\n",
      "Iteration 3900, Loss: 0.7428\n",
      "Iteration 4000, Loss: 0.7407\n",
      "Iteration 4100, Loss: 0.7386\n",
      "Iteration 4200, Loss: 0.7367\n",
      "Iteration 4300, Loss: 0.7348\n",
      "Iteration 4400, Loss: 0.7329\n",
      "Iteration 4500, Loss: 0.7312\n",
      "Iteration 4600, Loss: 0.7294\n",
      "Iteration 4700, Loss: 0.7278\n",
      "Iteration 4800, Loss: 0.7261\n",
      "Iteration 4900, Loss: 0.7245\n",
      "Iteration 5000, Loss: 0.7230\n",
      "Iteration 5100, Loss: 0.7215\n",
      "Iteration 5200, Loss: 0.7200\n",
      "Iteration 5300, Loss: 0.7186\n",
      "Iteration 5400, Loss: 0.7172\n",
      "Iteration 5500, Loss: 0.7159\n",
      "Iteration 5600, Loss: 0.7146\n",
      "Iteration 5700, Loss: 0.7133\n",
      "Iteration 5800, Loss: 0.7120\n",
      "Iteration 5900, Loss: 0.7108\n",
      "Iteration 6000, Loss: 0.7096\n",
      "Iteration 6100, Loss: 0.7085\n",
      "Iteration 6200, Loss: 0.7073\n",
      "Iteration 6300, Loss: 0.7062\n",
      "Iteration 6400, Loss: 0.7051\n",
      "Iteration 6500, Loss: 0.7040\n",
      "Iteration 6600, Loss: 0.7030\n",
      "Iteration 6700, Loss: 0.7020\n",
      "Iteration 6800, Loss: 0.7009\n",
      "Iteration 6900, Loss: 0.7000\n",
      "Iteration 7000, Loss: 0.6990\n",
      "Iteration 7100, Loss: 0.6980\n",
      "Iteration 7200, Loss: 0.6971\n",
      "Iteration 7300, Loss: 0.6962\n",
      "Iteration 7400, Loss: 0.6953\n",
      "Iteration 7500, Loss: 0.6944\n",
      "Iteration 7600, Loss: 0.6935\n",
      "Iteration 7700, Loss: 0.6927\n",
      "Iteration 7800, Loss: 0.6918\n",
      "Iteration 7900, Loss: 0.6910\n",
      "Iteration 8000, Loss: 0.6902\n",
      "Iteration 8100, Loss: 0.6894\n",
      "Iteration 8200, Loss: 0.6886\n",
      "Iteration 8300, Loss: 0.6878\n",
      "Iteration 8400, Loss: 0.6870\n",
      "Iteration 8500, Loss: 0.6863\n",
      "Iteration 8600, Loss: 0.6856\n",
      "Iteration 8700, Loss: 0.6848\n",
      "Iteration 8800, Loss: 0.6841\n",
      "Iteration 8900, Loss: 0.6834\n",
      "Iteration 9000, Loss: 0.6827\n",
      "Iteration 9100, Loss: 0.6820\n",
      "Iteration 9200, Loss: 0.6813\n",
      "Iteration 9300, Loss: 0.6807\n",
      "Iteration 9400, Loss: 0.6800\n",
      "Iteration 9500, Loss: 0.6794\n",
      "Iteration 9600, Loss: 0.6787\n",
      "Iteration 9700, Loss: 0.6781\n",
      "Iteration 9800, Loss: 0.6775\n",
      "Iteration 9900, Loss: 0.6768\n",
      "Iteration 0, Loss: 1.0980\n",
      "Iteration 100, Loss: 1.0447\n",
      "Iteration 200, Loss: 1.0030\n",
      "Iteration 300, Loss: 0.9689\n",
      "Iteration 400, Loss: 0.9405\n",
      "Iteration 500, Loss: 0.9164\n",
      "Iteration 600, Loss: 0.8954\n",
      "Iteration 700, Loss: 0.8772\n",
      "Iteration 800, Loss: 0.8610\n",
      "Iteration 900, Loss: 0.8467\n",
      "Iteration 1000, Loss: 0.8339\n",
      "Iteration 1100, Loss: 0.8224\n",
      "Iteration 1200, Loss: 0.8119\n",
      "Iteration 1300, Loss: 0.8023\n",
      "Iteration 1400, Loss: 0.7936\n",
      "Iteration 1500, Loss: 0.7857\n",
      "Iteration 1600, Loss: 0.7783\n",
      "Iteration 1700, Loss: 0.7715\n",
      "Iteration 1800, Loss: 0.7652\n",
      "Iteration 1900, Loss: 0.7593\n",
      "Iteration 2000, Loss: 0.7538\n",
      "Iteration 2100, Loss: 0.7487\n",
      "Iteration 2200, Loss: 0.7439\n",
      "Iteration 2300, Loss: 0.7394\n",
      "Iteration 2400, Loss: 0.7352\n",
      "Iteration 2500, Loss: 0.7312\n",
      "Iteration 2600, Loss: 0.7274\n",
      "Iteration 2700, Loss: 0.7238\n",
      "Iteration 2800, Loss: 0.7204\n",
      "Iteration 2900, Loss: 0.7172\n",
      "Iteration 3000, Loss: 0.7141\n",
      "Iteration 3100, Loss: 0.7112\n",
      "Iteration 3200, Loss: 0.7084\n",
      "Iteration 3300, Loss: 0.7057\n",
      "Iteration 3400, Loss: 0.7032\n",
      "Iteration 3500, Loss: 0.7007\n",
      "Iteration 3600, Loss: 0.6984\n",
      "Iteration 3700, Loss: 0.6961\n",
      "Iteration 3800, Loss: 0.6940\n",
      "Iteration 3900, Loss: 0.6919\n",
      "Iteration 4000, Loss: 0.6899\n",
      "Iteration 4100, Loss: 0.6880\n",
      "Iteration 4200, Loss: 0.6861\n",
      "Iteration 4300, Loss: 0.6843\n",
      "Iteration 4400, Loss: 0.6826\n",
      "Iteration 4500, Loss: 0.6809\n",
      "Iteration 4600, Loss: 0.6792\n",
      "Iteration 4700, Loss: 0.6777\n",
      "Iteration 4800, Loss: 0.6761\n",
      "Iteration 4900, Loss: 0.6747\n",
      "Iteration 5000, Loss: 0.6732\n",
      "Iteration 5100, Loss: 0.6718\n",
      "Iteration 5200, Loss: 0.6705\n",
      "Iteration 5300, Loss: 0.6691\n",
      "Iteration 5400, Loss: 0.6678\n",
      "Iteration 5500, Loss: 0.6666\n",
      "Iteration 5600, Loss: 0.6654\n",
      "Iteration 5700, Loss: 0.6642\n",
      "Iteration 5800, Loss: 0.6630\n",
      "Iteration 5900, Loss: 0.6619\n",
      "Iteration 6000, Loss: 0.6608\n",
      "Iteration 6100, Loss: 0.6597\n",
      "Iteration 6200, Loss: 0.6586\n",
      "Iteration 6300, Loss: 0.6576\n",
      "Iteration 6400, Loss: 0.6566\n",
      "Iteration 6500, Loss: 0.6556\n",
      "Iteration 6600, Loss: 0.6546\n",
      "Iteration 6700, Loss: 0.6537\n",
      "Iteration 6800, Loss: 0.6528\n",
      "Iteration 6900, Loss: 0.6518\n",
      "Iteration 7000, Loss: 0.6510\n",
      "Iteration 7100, Loss: 0.6501\n",
      "Iteration 7200, Loss: 0.6492\n",
      "Iteration 7300, Loss: 0.6484\n",
      "Iteration 7400, Loss: 0.6476\n",
      "Iteration 7500, Loss: 0.6468\n",
      "Iteration 7600, Loss: 0.6459\n",
      "Iteration 7700, Loss: 0.6452\n",
      "Iteration 7800, Loss: 0.6444\n",
      "Iteration 7900, Loss: 0.6436\n",
      "Iteration 8000, Loss: 0.6429\n",
      "Iteration 8100, Loss: 0.6422\n",
      "Iteration 8200, Loss: 0.6414\n",
      "Iteration 8300, Loss: 0.6407\n",
      "Iteration 8400, Loss: 0.6400\n",
      "Iteration 8500, Loss: 0.6394\n",
      "Iteration 8600, Loss: 0.6387\n",
      "Iteration 8700, Loss: 0.6380\n",
      "Iteration 8800, Loss: 0.6374\n",
      "Iteration 8900, Loss: 0.6367\n",
      "Iteration 9000, Loss: 0.6361\n",
      "Iteration 9100, Loss: 0.6355\n",
      "Iteration 9200, Loss: 0.6349\n",
      "Iteration 9300, Loss: 0.6343\n",
      "Iteration 9400, Loss: 0.6337\n",
      "Iteration 9500, Loss: 0.6331\n",
      "Iteration 9600, Loss: 0.6325\n",
      "Iteration 9700, Loss: 0.6319\n",
      "Iteration 9800, Loss: 0.6313\n",
      "Iteration 9900, Loss: 0.6308\n",
      "{'n_iters': 7000, 'lr': 0.001, 'batch_size': 64} 0.7781007751937985\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T21:18:31.975008Z",
     "start_time": "2025-05-21T17:02:44.069459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import product\n",
    "# BASE LOGISTIC REGRESSION WITH L1 L2\n",
    "param_grid = {\n",
    "    \"n_iters\":[4000,5000,6000],\n",
    "    \"lr\":[0.001,0.0001,0.00001],\n",
    "    \"batch_size\":[64,128],\n",
    "    \"l\" :[0.001,0.0001,0.00001],\n",
    "    \"alpha\" :[0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "}\n",
    "best = -1\n",
    "best_params = None\n",
    "keys = list(param_grid.keys())\n",
    "combinations = list(product(*param_grid.values()))\n",
    "for i,combo in enumerate(combinations):\n",
    "    print(i, len(combinations))\n",
    "    params = dict(zip(keys,combo))\n",
    "    model = BaseLogisticRegressionRegCombined(**params)\n",
    "    mean = make_cross_validation_predict(model,\n",
    "                                           modified_features_preprocessor,\n",
    "                                           X_train, y_train,\n",
    "                                           k=2)[1]\n",
    "    if mean > best:\n",
    "        best = mean\n",
    "        best_params = params\n",
    "print(best_params,best)"
   ],
   "id": "1793525944fd30e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 270\n",
      "Iteration 0, Loss: 1.0218\n",
      "Iteration 100, Loss: 0.6413\n",
      "Iteration 200, Loss: 0.6107\n",
      "Iteration 300, Loss: 0.5932\n",
      "Iteration 400, Loss: 0.5812\n",
      "Iteration 500, Loss: 0.5722\n",
      "Iteration 600, Loss: 0.5651\n",
      "Iteration 700, Loss: 0.5592\n",
      "Iteration 800, Loss: 0.5543\n",
      "Iteration 900, Loss: 0.5501\n",
      "Iteration 1000, Loss: 0.5463\n",
      "Iteration 1100, Loss: 0.5431\n",
      "Iteration 1200, Loss: 0.5401\n",
      "Iteration 1300, Loss: 0.5375\n",
      "Iteration 1400, Loss: 0.5350\n",
      "Iteration 1500, Loss: 0.5328\n",
      "Iteration 1600, Loss: 0.5307\n",
      "Iteration 1700, Loss: 0.5287\n",
      "Iteration 1800, Loss: 0.5270\n",
      "Iteration 1900, Loss: 0.5252\n",
      "Iteration 2000, Loss: 0.5237\n",
      "Iteration 2100, Loss: 0.5222\n",
      "Iteration 2200, Loss: 0.5208\n",
      "Iteration 2300, Loss: 0.5194\n",
      "Iteration 2400, Loss: 0.5182\n",
      "Iteration 2500, Loss: 0.5170\n",
      "Iteration 2600, Loss: 0.5158\n",
      "Iteration 2700, Loss: 0.5147\n",
      "Iteration 2800, Loss: 0.5137\n",
      "Iteration 2900, Loss: 0.5127\n",
      "Iteration 3000, Loss: 0.5117\n",
      "Iteration 3100, Loss: 0.5108\n",
      "Iteration 3200, Loss: 0.5099\n",
      "Iteration 3300, Loss: 0.5091\n",
      "Iteration 3400, Loss: 0.5082\n",
      "Iteration 3500, Loss: 0.5074\n",
      "Iteration 3600, Loss: 0.5067\n",
      "Iteration 3700, Loss: 0.5060\n",
      "Iteration 3800, Loss: 0.5053\n",
      "Iteration 3900, Loss: 0.5046\n",
      "Iteration 0, Loss: 1.0169\n",
      "Iteration 100, Loss: 0.6043\n",
      "Iteration 200, Loss: 0.5736\n",
      "Iteration 300, Loss: 0.5575\n",
      "Iteration 400, Loss: 0.5467\n",
      "Iteration 500, Loss: 0.5387\n",
      "Iteration 600, Loss: 0.5323\n",
      "Iteration 700, Loss: 0.5272\n",
      "Iteration 800, Loss: 0.5228\n",
      "Iteration 900, Loss: 0.5189\n",
      "Iteration 1000, Loss: 0.5156\n",
      "Iteration 1100, Loss: 0.5126\n",
      "Iteration 1200, Loss: 0.5098\n",
      "Iteration 1300, Loss: 0.5074\n",
      "Iteration 1400, Loss: 0.5051\n",
      "Iteration 1500, Loss: 0.5030\n",
      "Iteration 1600, Loss: 0.5010\n",
      "Iteration 1700, Loss: 0.4992\n",
      "Iteration 1800, Loss: 0.4975\n",
      "Iteration 1900, Loss: 0.4959\n",
      "Iteration 2000, Loss: 0.4944\n",
      "Iteration 2100, Loss: 0.4930\n",
      "Iteration 2200, Loss: 0.4917\n",
      "Iteration 2300, Loss: 0.4904\n",
      "Iteration 2400, Loss: 0.4892\n",
      "Iteration 2500, Loss: 0.4881\n",
      "Iteration 2600, Loss: 0.4870\n",
      "Iteration 2700, Loss: 0.4860\n",
      "Iteration 2800, Loss: 0.4850\n",
      "Iteration 2900, Loss: 0.4840\n",
      "Iteration 3000, Loss: 0.4831\n",
      "Iteration 3100, Loss: 0.4822\n",
      "Iteration 3200, Loss: 0.4814\n",
      "Iteration 3300, Loss: 0.4806\n",
      "Iteration 3400, Loss: 0.4798\n",
      "Iteration 3500, Loss: 0.4791\n",
      "Iteration 3600, Loss: 0.4783\n",
      "Iteration 3700, Loss: 0.4776\n",
      "Iteration 3800, Loss: 0.4769\n",
      "Iteration 3900, Loss: 0.4763\n",
      "1 270\n",
      "Iteration 0, Loss: 1.0162\n",
      "Iteration 100, Loss: 0.6245\n",
      "Iteration 200, Loss: 0.5928\n",
      "Iteration 300, Loss: 0.5755\n",
      "Iteration 400, Loss: 0.5637\n",
      "Iteration 500, Loss: 0.5550\n",
      "Iteration 600, Loss: 0.5481\n",
      "Iteration 700, Loss: 0.5425\n",
      "Iteration 800, Loss: 0.5378\n",
      "Iteration 900, Loss: 0.5337\n",
      "Iteration 1000, Loss: 0.5301\n",
      "Iteration 1100, Loss: 0.5270\n",
      "Iteration 1200, Loss: 0.5241\n",
      "Iteration 1300, Loss: 0.5216\n",
      "Iteration 1400, Loss: 0.5193\n",
      "Iteration 1500, Loss: 0.5172\n",
      "Iteration 1600, Loss: 0.5152\n",
      "Iteration 1700, Loss: 0.5134\n",
      "Iteration 1800, Loss: 0.5117\n",
      "Iteration 1900, Loss: 0.5101\n",
      "Iteration 2000, Loss: 0.5087\n",
      "Iteration 2100, Loss: 0.5073\n",
      "Iteration 2200, Loss: 0.5059\n",
      "Iteration 2300, Loss: 0.5047\n",
      "Iteration 2400, Loss: 0.5035\n",
      "Iteration 2500, Loss: 0.5024\n",
      "Iteration 2600, Loss: 0.5014\n",
      "Iteration 2700, Loss: 0.5004\n",
      "Iteration 2800, Loss: 0.4994\n",
      "Iteration 2900, Loss: 0.4985\n",
      "Iteration 3000, Loss: 0.4976\n",
      "Iteration 3100, Loss: 0.4968\n",
      "Iteration 3200, Loss: 0.4960\n",
      "Iteration 3300, Loss: 0.4952\n",
      "Iteration 3400, Loss: 0.4945\n",
      "Iteration 3500, Loss: 0.4938\n",
      "Iteration 3600, Loss: 0.4931\n",
      "Iteration 3700, Loss: 0.4924\n",
      "Iteration 3800, Loss: 0.4918\n",
      "Iteration 3900, Loss: 0.4912\n",
      "Iteration 0, Loss: 1.0234\n",
      "Iteration 100, Loss: 0.6217\n",
      "Iteration 200, Loss: 0.5929\n",
      "Iteration 300, Loss: 0.5772\n",
      "Iteration 400, Loss: 0.5665\n",
      "Iteration 500, Loss: 0.5585\n",
      "Iteration 600, Loss: 0.5521\n",
      "Iteration 700, Loss: 0.5468\n",
      "Iteration 800, Loss: 0.5423\n",
      "Iteration 900, Loss: 0.5384\n",
      "Iteration 1000, Loss: 0.5351\n",
      "Iteration 1100, Loss: 0.5320\n",
      "Iteration 1200, Loss: 0.5292\n",
      "Iteration 1300, Loss: 0.5267\n",
      "Iteration 1400, Loss: 0.5244\n",
      "Iteration 1500, Loss: 0.5223\n",
      "Iteration 1600, Loss: 0.5203\n",
      "Iteration 1700, Loss: 0.5185\n",
      "Iteration 1800, Loss: 0.5167\n",
      "Iteration 1900, Loss: 0.5151\n",
      "Iteration 2000, Loss: 0.5136\n",
      "Iteration 2100, Loss: 0.5122\n",
      "Iteration 2200, Loss: 0.5108\n",
      "Iteration 2300, Loss: 0.5095\n",
      "Iteration 2400, Loss: 0.5083\n",
      "Iteration 2500, Loss: 0.5072\n",
      "Iteration 2600, Loss: 0.5060\n",
      "Iteration 2700, Loss: 0.5050\n",
      "Iteration 2800, Loss: 0.5039\n",
      "Iteration 2900, Loss: 0.5030\n",
      "Iteration 3000, Loss: 0.5020\n",
      "Iteration 3100, Loss: 0.5011\n",
      "Iteration 3200, Loss: 0.5003\n",
      "Iteration 3300, Loss: 0.4994\n",
      "Iteration 3400, Loss: 0.4986\n",
      "Iteration 3500, Loss: 0.4978\n",
      "Iteration 3600, Loss: 0.4971\n",
      "Iteration 3700, Loss: 0.4964\n",
      "Iteration 3800, Loss: 0.4957\n",
      "Iteration 3900, Loss: 0.4950\n",
      "2 270\n",
      "Iteration 0, Loss: 1.0168\n",
      "Iteration 100, Loss: 0.6154\n",
      "Iteration 200, Loss: 0.5859\n",
      "Iteration 300, Loss: 0.5701\n",
      "Iteration 400, Loss: 0.5593\n",
      "Iteration 500, Loss: 0.5512\n",
      "Iteration 600, Loss: 0.5447\n",
      "Iteration 700, Loss: 0.5393\n",
      "Iteration 800, Loss: 0.5347\n",
      "Iteration 900, Loss: 0.5307\n",
      "Iteration 1000, Loss: 0.5272\n",
      "Iteration 1100, Loss: 0.5241\n",
      "Iteration 1200, Loss: 0.5213\n",
      "Iteration 1300, Loss: 0.5188\n",
      "Iteration 1400, Loss: 0.5165\n",
      "Iteration 1500, Loss: 0.5144\n",
      "Iteration 1600, Loss: 0.5124\n",
      "Iteration 1700, Loss: 0.5106\n",
      "Iteration 1800, Loss: 0.5089\n",
      "Iteration 1900, Loss: 0.5073\n",
      "Iteration 2000, Loss: 0.5058\n",
      "Iteration 2100, Loss: 0.5044\n",
      "Iteration 2200, Loss: 0.5031\n",
      "Iteration 2300, Loss: 0.5018\n",
      "Iteration 2400, Loss: 0.5006\n",
      "Iteration 2500, Loss: 0.4994\n",
      "Iteration 2600, Loss: 0.4983\n",
      "Iteration 2700, Loss: 0.4973\n",
      "Iteration 2800, Loss: 0.4963\n",
      "Iteration 2900, Loss: 0.4954\n",
      "Iteration 3000, Loss: 0.4944\n",
      "Iteration 3100, Loss: 0.4936\n",
      "Iteration 3200, Loss: 0.4927\n",
      "Iteration 3300, Loss: 0.4919\n",
      "Iteration 3400, Loss: 0.4911\n",
      "Iteration 3500, Loss: 0.4903\n",
      "Iteration 3600, Loss: 0.4896\n",
      "Iteration 3700, Loss: 0.4889\n",
      "Iteration 3800, Loss: 0.4882\n",
      "Iteration 3900, Loss: 0.4875\n",
      "Iteration 0, Loss: 1.0236\n",
      "Iteration 100, Loss: 0.6288\n",
      "Iteration 200, Loss: 0.5984\n",
      "Iteration 300, Loss: 0.5817\n",
      "Iteration 400, Loss: 0.5704\n",
      "Iteration 500, Loss: 0.5618\n",
      "Iteration 600, Loss: 0.5551\n",
      "Iteration 700, Loss: 0.5496\n",
      "Iteration 800, Loss: 0.5449\n",
      "Iteration 900, Loss: 0.5409\n",
      "Iteration 1000, Loss: 0.5373\n",
      "Iteration 1100, Loss: 0.5341\n",
      "Iteration 1200, Loss: 0.5313\n",
      "Iteration 1300, Loss: 0.5287\n",
      "Iteration 1400, Loss: 0.5263\n",
      "Iteration 1500, Loss: 0.5241\n",
      "Iteration 1600, Loss: 0.5221\n",
      "Iteration 1700, Loss: 0.5202\n",
      "Iteration 1800, Loss: 0.5184\n",
      "Iteration 1900, Loss: 0.5168\n",
      "Iteration 2000, Loss: 0.5152\n",
      "Iteration 2100, Loss: 0.5137\n",
      "Iteration 2200, Loss: 0.5124\n",
      "Iteration 2300, Loss: 0.5110\n",
      "Iteration 2400, Loss: 0.5098\n",
      "Iteration 2500, Loss: 0.5086\n",
      "Iteration 2600, Loss: 0.5075\n",
      "Iteration 2700, Loss: 0.5064\n",
      "Iteration 2800, Loss: 0.5053\n",
      "Iteration 2900, Loss: 0.5044\n",
      "Iteration 3000, Loss: 0.5034\n",
      "Iteration 3100, Loss: 0.5026\n",
      "Iteration 3200, Loss: 0.5016\n",
      "Iteration 3300, Loss: 0.5008\n",
      "Iteration 3400, Loss: 0.5000\n",
      "Iteration 3500, Loss: 0.4992\n",
      "Iteration 3600, Loss: 0.4985\n",
      "Iteration 3700, Loss: 0.4978\n",
      "Iteration 3800, Loss: 0.4971\n",
      "Iteration 3900, Loss: 0.4964\n",
      "3 270\n",
      "Iteration 0, Loss: 1.0155\n",
      "Iteration 100, Loss: 0.6171\n",
      "Iteration 200, Loss: 0.5884\n",
      "Iteration 300, Loss: 0.5725\n",
      "Iteration 400, Loss: 0.5616\n",
      "Iteration 500, Loss: 0.5533\n",
      "Iteration 600, Loss: 0.5467\n",
      "Iteration 700, Loss: 0.5413\n",
      "Iteration 800, Loss: 0.5367\n",
      "Iteration 900, Loss: 0.5328\n",
      "Iteration 1000, Loss: 0.5293\n",
      "Iteration 1100, Loss: 0.5262\n",
      "Iteration 1200, Loss: 0.5234\n",
      "Iteration 1300, Loss: 0.5209\n",
      "Iteration 1400, Loss: 0.5185\n",
      "Iteration 1500, Loss: 0.5164\n",
      "Iteration 1600, Loss: 0.5144\n",
      "Iteration 1700, Loss: 0.5125\n",
      "Iteration 1800, Loss: 0.5108\n",
      "Iteration 1900, Loss: 0.5091\n",
      "Iteration 2000, Loss: 0.5075\n",
      "Iteration 2100, Loss: 0.5061\n",
      "Iteration 2200, Loss: 0.5047\n",
      "Iteration 2300, Loss: 0.5034\n",
      "Iteration 2400, Loss: 0.5021\n",
      "Iteration 2500, Loss: 0.5009\n",
      "Iteration 2600, Loss: 0.4998\n",
      "Iteration 2700, Loss: 0.4987\n",
      "Iteration 2800, Loss: 0.4976\n",
      "Iteration 2900, Loss: 0.4966\n",
      "Iteration 3000, Loss: 0.4957\n",
      "Iteration 3100, Loss: 0.4947\n",
      "Iteration 3200, Loss: 0.4938\n",
      "Iteration 3300, Loss: 0.4930\n",
      "Iteration 3400, Loss: 0.4921\n",
      "Iteration 3500, Loss: 0.4913\n",
      "Iteration 3600, Loss: 0.4906\n",
      "Iteration 3700, Loss: 0.4898\n",
      "Iteration 3800, Loss: 0.4891\n",
      "Iteration 3900, Loss: 0.4884\n",
      "Iteration 0, Loss: 1.0233\n",
      "Iteration 100, Loss: 0.6325\n",
      "Iteration 200, Loss: 0.6009\n",
      "Iteration 300, Loss: 0.5838\n",
      "Iteration 400, Loss: 0.5724\n",
      "Iteration 500, Loss: 0.5639\n",
      "Iteration 600, Loss: 0.5571\n",
      "Iteration 700, Loss: 0.5516\n",
      "Iteration 800, Loss: 0.5469\n",
      "Iteration 900, Loss: 0.5429\n",
      "Iteration 1000, Loss: 0.5394\n",
      "Iteration 1100, Loss: 0.5362\n",
      "Iteration 1200, Loss: 0.5333\n",
      "Iteration 1300, Loss: 0.5307\n",
      "Iteration 1400, Loss: 0.5283\n",
      "Iteration 1500, Loss: 0.5261\n",
      "Iteration 1600, Loss: 0.5240\n",
      "Iteration 1700, Loss: 0.5221\n",
      "Iteration 1800, Loss: 0.5203\n",
      "Iteration 1900, Loss: 0.5187\n",
      "Iteration 2000, Loss: 0.5171\n",
      "Iteration 2100, Loss: 0.5156\n",
      "Iteration 2200, Loss: 0.5142\n",
      "Iteration 2300, Loss: 0.5129\n",
      "Iteration 2400, Loss: 0.5116\n",
      "Iteration 2500, Loss: 0.5104\n",
      "Iteration 2600, Loss: 0.5092\n",
      "Iteration 2700, Loss: 0.5081\n",
      "Iteration 2800, Loss: 0.5071\n",
      "Iteration 2900, Loss: 0.5061\n",
      "Iteration 3000, Loss: 0.5051\n",
      "Iteration 3100, Loss: 0.5042\n",
      "Iteration 3200, Loss: 0.5033\n",
      "Iteration 3300, Loss: 0.5024\n",
      "Iteration 3400, Loss: 0.5016\n",
      "Iteration 3500, Loss: 0.5008\n",
      "Iteration 3600, Loss: 0.5001\n",
      "Iteration 3700, Loss: 0.4993\n",
      "Iteration 3800, Loss: 0.4986\n",
      "Iteration 3900, Loss: 0.4979\n",
      "4 270\n",
      "Iteration 0, Loss: 1.0196\n",
      "Iteration 100, Loss: 0.6317\n",
      "Iteration 200, Loss: 0.6010\n",
      "Iteration 300, Loss: 0.5846\n",
      "Iteration 400, Loss: 0.5734\n",
      "Iteration 500, Loss: 0.5651\n",
      "Iteration 600, Loss: 0.5585\n",
      "Iteration 700, Loss: 0.5532\n",
      "Iteration 800, Loss: 0.5487\n",
      "Iteration 900, Loss: 0.5448\n",
      "Iteration 1000, Loss: 0.5415\n",
      "Iteration 1100, Loss: 0.5385\n",
      "Iteration 1200, Loss: 0.5358\n",
      "Iteration 1300, Loss: 0.5334\n",
      "Iteration 1400, Loss: 0.5312\n",
      "Iteration 1500, Loss: 0.5292\n",
      "Iteration 1600, Loss: 0.5273\n",
      "Iteration 1700, Loss: 0.5256\n",
      "Iteration 1800, Loss: 0.5240\n",
      "Iteration 1900, Loss: 0.5225\n",
      "Iteration 2000, Loss: 0.5210\n",
      "Iteration 2100, Loss: 0.5197\n",
      "Iteration 2200, Loss: 0.5184\n",
      "Iteration 2300, Loss: 0.5172\n",
      "Iteration 2400, Loss: 0.5161\n",
      "Iteration 2500, Loss: 0.5150\n",
      "Iteration 2600, Loss: 0.5140\n",
      "Iteration 2700, Loss: 0.5130\n",
      "Iteration 2800, Loss: 0.5120\n",
      "Iteration 2900, Loss: 0.5111\n",
      "Iteration 3000, Loss: 0.5103\n",
      "Iteration 3100, Loss: 0.5095\n",
      "Iteration 3200, Loss: 0.5086\n",
      "Iteration 3300, Loss: 0.5079\n",
      "Iteration 3400, Loss: 0.5072\n",
      "Iteration 3500, Loss: 0.5065\n",
      "Iteration 3600, Loss: 0.5058\n",
      "Iteration 3700, Loss: 0.5051\n",
      "Iteration 3800, Loss: 0.5045\n",
      "Iteration 3900, Loss: 0.5039\n",
      "Iteration 0, Loss: 1.0218\n",
      "Iteration 100, Loss: 0.6168\n",
      "Iteration 200, Loss: 0.5875\n",
      "Iteration 300, Loss: 0.5719\n",
      "Iteration 400, Loss: 0.5612\n",
      "Iteration 500, Loss: 0.5533\n",
      "Iteration 600, Loss: 0.5470\n",
      "Iteration 700, Loss: 0.5419\n",
      "Iteration 800, Loss: 0.5374\n",
      "Iteration 900, Loss: 0.5337\n",
      "Iteration 1000, Loss: 0.5303\n",
      "Iteration 1100, Loss: 0.5274\n",
      "Iteration 1200, Loss: 0.5247\n",
      "Iteration 1300, Loss: 0.5223\n",
      "Iteration 1400, Loss: 0.5200\n",
      "Iteration 1500, Loss: 0.5180\n",
      "Iteration 1600, Loss: 0.5160\n",
      "Iteration 1700, Loss: 0.5142\n",
      "Iteration 1800, Loss: 0.5125\n",
      "Iteration 1900, Loss: 0.5109\n",
      "Iteration 2000, Loss: 0.5094\n",
      "Iteration 2100, Loss: 0.5080\n",
      "Iteration 2200, Loss: 0.5066\n",
      "Iteration 2300, Loss: 0.5053\n",
      "Iteration 2400, Loss: 0.5041\n",
      "Iteration 2500, Loss: 0.5030\n",
      "Iteration 2600, Loss: 0.5018\n",
      "Iteration 2700, Loss: 0.5008\n",
      "Iteration 2800, Loss: 0.4998\n",
      "Iteration 2900, Loss: 0.4988\n",
      "Iteration 3000, Loss: 0.4978\n",
      "Iteration 3100, Loss: 0.4969\n",
      "Iteration 3200, Loss: 0.4961\n",
      "Iteration 3300, Loss: 0.4952\n",
      "Iteration 3400, Loss: 0.4944\n",
      "Iteration 3500, Loss: 0.4936\n",
      "Iteration 3600, Loss: 0.4929\n",
      "Iteration 3700, Loss: 0.4921\n",
      "Iteration 3800, Loss: 0.4914\n",
      "Iteration 3900, Loss: 0.4907\n",
      "5 270\n",
      "Iteration 0, Loss: 1.0202\n",
      "Iteration 100, Loss: 0.6371\n",
      "Iteration 200, Loss: 0.6066\n",
      "Iteration 300, Loss: 0.5894\n",
      "Iteration 400, Loss: 0.5774\n",
      "Iteration 500, Loss: 0.5681\n",
      "Iteration 600, Loss: 0.5606\n",
      "Iteration 700, Loss: 0.5543\n",
      "Iteration 800, Loss: 0.5489\n",
      "Iteration 900, Loss: 0.5441\n",
      "Iteration 1000, Loss: 0.5399\n",
      "Iteration 1100, Loss: 0.5360\n",
      "Iteration 1200, Loss: 0.5325\n",
      "Iteration 1300, Loss: 0.5294\n",
      "Iteration 1400, Loss: 0.5264\n",
      "Iteration 1500, Loss: 0.5237\n",
      "Iteration 1600, Loss: 0.5211\n",
      "Iteration 1700, Loss: 0.5187\n",
      "Iteration 1800, Loss: 0.5165\n",
      "Iteration 1900, Loss: 0.5144\n",
      "Iteration 2000, Loss: 0.5124\n",
      "Iteration 2100, Loss: 0.5105\n",
      "Iteration 2200, Loss: 0.5086\n",
      "Iteration 2300, Loss: 0.5069\n",
      "Iteration 2400, Loss: 0.5053\n",
      "Iteration 2500, Loss: 0.5037\n",
      "Iteration 2600, Loss: 0.5022\n",
      "Iteration 2700, Loss: 0.5007\n",
      "Iteration 2800, Loss: 0.4993\n",
      "Iteration 2900, Loss: 0.4980\n",
      "Iteration 3000, Loss: 0.4967\n",
      "Iteration 3100, Loss: 0.4955\n",
      "Iteration 3200, Loss: 0.4943\n",
      "Iteration 3300, Loss: 0.4931\n",
      "Iteration 3400, Loss: 0.4920\n",
      "Iteration 3500, Loss: 0.4909\n",
      "Iteration 3600, Loss: 0.4899\n",
      "Iteration 3700, Loss: 0.4889\n",
      "Iteration 3800, Loss: 0.4879\n",
      "Iteration 3900, Loss: 0.4870\n",
      "Iteration 0, Loss: 1.0185\n",
      "Iteration 100, Loss: 0.6073\n",
      "Iteration 200, Loss: 0.5737\n",
      "Iteration 300, Loss: 0.5554\n",
      "Iteration 400, Loss: 0.5428\n",
      "Iteration 500, Loss: 0.5334\n",
      "Iteration 600, Loss: 0.5258\n",
      "Iteration 700, Loss: 0.5196\n",
      "Iteration 800, Loss: 0.5142\n",
      "Iteration 900, Loss: 0.5095\n",
      "Iteration 1000, Loss: 0.5054\n",
      "Iteration 1100, Loss: 0.5017\n",
      "Iteration 1200, Loss: 0.4984\n",
      "Iteration 1300, Loss: 0.4953\n",
      "Iteration 1400, Loss: 0.4926\n",
      "Iteration 1500, Loss: 0.4899\n",
      "Iteration 1600, Loss: 0.4875\n",
      "Iteration 1700, Loss: 0.4852\n",
      "Iteration 1800, Loss: 0.4832\n",
      "Iteration 1900, Loss: 0.4812\n",
      "Iteration 2000, Loss: 0.4793\n",
      "Iteration 2100, Loss: 0.4775\n",
      "Iteration 2200, Loss: 0.4758\n",
      "Iteration 2300, Loss: 0.4742\n",
      "Iteration 2400, Loss: 0.4727\n",
      "Iteration 2500, Loss: 0.4712\n",
      "Iteration 2600, Loss: 0.4698\n",
      "Iteration 2700, Loss: 0.4685\n",
      "Iteration 2800, Loss: 0.4672\n",
      "Iteration 2900, Loss: 0.4659\n",
      "Iteration 3000, Loss: 0.4647\n",
      "Iteration 3100, Loss: 0.4636\n",
      "Iteration 3200, Loss: 0.4624\n",
      "Iteration 3300, Loss: 0.4613\n",
      "Iteration 3400, Loss: 0.4603\n",
      "Iteration 3500, Loss: 0.4593\n",
      "Iteration 3600, Loss: 0.4583\n",
      "Iteration 3700, Loss: 0.4574\n",
      "Iteration 3800, Loss: 0.4564\n",
      "Iteration 3900, Loss: 0.4555\n",
      "6 270\n",
      "Iteration 0, Loss: 1.0223\n",
      "Iteration 100, Loss: 0.6268\n",
      "Iteration 200, Loss: 0.5926\n",
      "Iteration 300, Loss: 0.5745\n",
      "Iteration 400, Loss: 0.5624\n",
      "Iteration 500, Loss: 0.5534\n",
      "Iteration 600, Loss: 0.5464\n",
      "Iteration 700, Loss: 0.5406\n",
      "Iteration 800, Loss: 0.5356\n",
      "Iteration 900, Loss: 0.5314\n",
      "Iteration 1000, Loss: 0.5276\n",
      "Iteration 1100, Loss: 0.5242\n",
      "Iteration 1200, Loss: 0.5212\n",
      "Iteration 1300, Loss: 0.5184\n",
      "Iteration 1400, Loss: 0.5158\n",
      "Iteration 1500, Loss: 0.5135\n",
      "Iteration 1600, Loss: 0.5112\n",
      "Iteration 1700, Loss: 0.5091\n",
      "Iteration 1800, Loss: 0.5072\n",
      "Iteration 1900, Loss: 0.5053\n",
      "Iteration 2000, Loss: 0.5036\n",
      "Iteration 2100, Loss: 0.5019\n",
      "Iteration 2200, Loss: 0.5003\n",
      "Iteration 2300, Loss: 0.4988\n",
      "Iteration 2400, Loss: 0.4973\n",
      "Iteration 2500, Loss: 0.4959\n",
      "Iteration 2600, Loss: 0.4946\n",
      "Iteration 2700, Loss: 0.4933\n",
      "Iteration 2800, Loss: 0.4921\n",
      "Iteration 2900, Loss: 0.4909\n",
      "Iteration 3000, Loss: 0.4898\n",
      "Iteration 3100, Loss: 0.4887\n",
      "Iteration 3200, Loss: 0.4876\n",
      "Iteration 3300, Loss: 0.4866\n",
      "Iteration 3400, Loss: 0.4856\n",
      "Iteration 3500, Loss: 0.4846\n",
      "Iteration 3600, Loss: 0.4837\n",
      "Iteration 3700, Loss: 0.4828\n",
      "Iteration 3800, Loss: 0.4818\n",
      "Iteration 3900, Loss: 0.4810\n",
      "Iteration 0, Loss: 1.0153\n",
      "Iteration 100, Loss: 0.6194\n",
      "Iteration 200, Loss: 0.5911\n",
      "Iteration 300, Loss: 0.5743\n",
      "Iteration 400, Loss: 0.5622\n",
      "Iteration 500, Loss: 0.5527\n",
      "Iteration 600, Loss: 0.5449\n",
      "Iteration 700, Loss: 0.5384\n",
      "Iteration 800, Loss: 0.5328\n",
      "Iteration 900, Loss: 0.5279\n",
      "Iteration 1000, Loss: 0.5236\n",
      "Iteration 1100, Loss: 0.5197\n",
      "Iteration 1200, Loss: 0.5161\n",
      "Iteration 1300, Loss: 0.5129\n",
      "Iteration 1400, Loss: 0.5100\n",
      "Iteration 1500, Loss: 0.5073\n",
      "Iteration 1600, Loss: 0.5047\n",
      "Iteration 1700, Loss: 0.5024\n",
      "Iteration 1800, Loss: 0.5001\n",
      "Iteration 1900, Loss: 0.4980\n",
      "Iteration 2000, Loss: 0.4961\n",
      "Iteration 2100, Loss: 0.4943\n",
      "Iteration 2200, Loss: 0.4925\n",
      "Iteration 2300, Loss: 0.4908\n",
      "Iteration 2400, Loss: 0.4892\n",
      "Iteration 2500, Loss: 0.4877\n",
      "Iteration 2600, Loss: 0.4863\n",
      "Iteration 2700, Loss: 0.4849\n",
      "Iteration 2800, Loss: 0.4836\n",
      "Iteration 2900, Loss: 0.4823\n",
      "Iteration 3000, Loss: 0.4811\n",
      "Iteration 3100, Loss: 0.4799\n",
      "Iteration 3200, Loss: 0.4788\n",
      "Iteration 3300, Loss: 0.4777\n",
      "Iteration 3400, Loss: 0.4766\n",
      "Iteration 3500, Loss: 0.4756\n",
      "Iteration 3600, Loss: 0.4746\n",
      "Iteration 3700, Loss: 0.4736\n",
      "Iteration 3800, Loss: 0.4727\n",
      "Iteration 3900, Loss: 0.4718\n",
      "7 270\n",
      "Iteration 0, Loss: 1.0212\n",
      "Iteration 100, Loss: 0.6280\n",
      "Iteration 200, Loss: 0.5978\n",
      "Iteration 300, Loss: 0.5808\n",
      "Iteration 400, Loss: 0.5688\n",
      "Iteration 500, Loss: 0.5597\n",
      "Iteration 600, Loss: 0.5523\n",
      "Iteration 700, Loss: 0.5462\n",
      "Iteration 800, Loss: 0.5410\n",
      "Iteration 900, Loss: 0.5364\n",
      "Iteration 1000, Loss: 0.5324\n",
      "Iteration 1100, Loss: 0.5288\n",
      "Iteration 1200, Loss: 0.5255\n",
      "Iteration 1300, Loss: 0.5226\n",
      "Iteration 1400, Loss: 0.5198\n",
      "Iteration 1500, Loss: 0.5173\n",
      "Iteration 1600, Loss: 0.5150\n",
      "Iteration 1700, Loss: 0.5128\n",
      "Iteration 1800, Loss: 0.5107\n",
      "Iteration 1900, Loss: 0.5088\n",
      "Iteration 2000, Loss: 0.5070\n",
      "Iteration 2100, Loss: 0.5053\n",
      "Iteration 2200, Loss: 0.5037\n",
      "Iteration 2300, Loss: 0.5021\n",
      "Iteration 2400, Loss: 0.5006\n",
      "Iteration 2500, Loss: 0.4991\n",
      "Iteration 2600, Loss: 0.4978\n",
      "Iteration 2700, Loss: 0.4965\n",
      "Iteration 2800, Loss: 0.4952\n",
      "Iteration 2900, Loss: 0.4940\n",
      "Iteration 3000, Loss: 0.4929\n",
      "Iteration 3100, Loss: 0.4917\n",
      "Iteration 3200, Loss: 0.4906\n",
      "Iteration 3300, Loss: 0.4896\n",
      "Iteration 3400, Loss: 0.4886\n",
      "Iteration 3500, Loss: 0.4876\n",
      "Iteration 3600, Loss: 0.4866\n",
      "Iteration 3700, Loss: 0.4857\n",
      "Iteration 3800, Loss: 0.4848\n",
      "Iteration 3900, Loss: 0.4839\n",
      "Iteration 0, Loss: 1.0186\n",
      "Iteration 100, Loss: 0.6185\n",
      "Iteration 200, Loss: 0.5875\n",
      "Iteration 300, Loss: 0.5706\n",
      "Iteration 400, Loss: 0.5591\n",
      "Iteration 500, Loss: 0.5506\n",
      "Iteration 600, Loss: 0.5437\n",
      "Iteration 700, Loss: 0.5381\n",
      "Iteration 800, Loss: 0.5332\n",
      "Iteration 900, Loss: 0.5289\n",
      "Iteration 1000, Loss: 0.5251\n",
      "Iteration 1100, Loss: 0.5217\n",
      "Iteration 1200, Loss: 0.5186\n",
      "Iteration 1300, Loss: 0.5157\n",
      "Iteration 1400, Loss: 0.5131\n",
      "Iteration 1500, Loss: 0.5106\n",
      "Iteration 1600, Loss: 0.5083\n",
      "Iteration 1700, Loss: 0.5061\n",
      "Iteration 1800, Loss: 0.5041\n",
      "Iteration 1900, Loss: 0.5022\n",
      "Iteration 2000, Loss: 0.5003\n",
      "Iteration 2100, Loss: 0.4986\n",
      "Iteration 2200, Loss: 0.4970\n",
      "Iteration 2300, Loss: 0.4954\n",
      "Iteration 2400, Loss: 0.4939\n",
      "Iteration 2500, Loss: 0.4925\n",
      "Iteration 2600, Loss: 0.4911\n",
      "Iteration 2700, Loss: 0.4898\n",
      "Iteration 2800, Loss: 0.4885\n",
      "Iteration 2900, Loss: 0.4873\n",
      "Iteration 3000, Loss: 0.4861\n",
      "Iteration 3100, Loss: 0.4850\n",
      "Iteration 3200, Loss: 0.4839\n",
      "Iteration 3300, Loss: 0.4828\n",
      "Iteration 3400, Loss: 0.4818\n",
      "Iteration 3500, Loss: 0.4808\n",
      "Iteration 3600, Loss: 0.4798\n",
      "Iteration 3700, Loss: 0.4789\n",
      "Iteration 3800, Loss: 0.4780\n",
      "Iteration 3900, Loss: 0.4771\n",
      "8 270\n",
      "Iteration 0, Loss: 1.0201\n",
      "Iteration 100, Loss: 0.6318\n",
      "Iteration 200, Loss: 0.5997\n",
      "Iteration 300, Loss: 0.5815\n",
      "Iteration 400, Loss: 0.5688\n",
      "Iteration 500, Loss: 0.5591\n",
      "Iteration 600, Loss: 0.5514\n",
      "Iteration 700, Loss: 0.5449\n",
      "Iteration 800, Loss: 0.5393\n",
      "Iteration 900, Loss: 0.5345\n",
      "Iteration 1000, Loss: 0.5301\n",
      "Iteration 1100, Loss: 0.5262\n",
      "Iteration 1200, Loss: 0.5227\n",
      "Iteration 1300, Loss: 0.5195\n",
      "Iteration 1400, Loss: 0.5165\n",
      "Iteration 1500, Loss: 0.5138\n",
      "Iteration 1600, Loss: 0.5112\n",
      "Iteration 1700, Loss: 0.5088\n",
      "Iteration 1800, Loss: 0.5066\n",
      "Iteration 1900, Loss: 0.5045\n",
      "Iteration 2000, Loss: 0.5025\n",
      "Iteration 2100, Loss: 0.5007\n",
      "Iteration 2200, Loss: 0.4989\n",
      "Iteration 2300, Loss: 0.4972\n",
      "Iteration 2400, Loss: 0.4956\n",
      "Iteration 2500, Loss: 0.4941\n",
      "Iteration 2600, Loss: 0.4927\n",
      "Iteration 2700, Loss: 0.4913\n",
      "Iteration 2800, Loss: 0.4900\n",
      "Iteration 2900, Loss: 0.4887\n",
      "Iteration 3000, Loss: 0.4875\n",
      "Iteration 3100, Loss: 0.4863\n",
      "Iteration 3200, Loss: 0.4851\n",
      "Iteration 3300, Loss: 0.4841\n",
      "Iteration 3400, Loss: 0.4830\n",
      "Iteration 3500, Loss: 0.4820\n",
      "Iteration 3600, Loss: 0.4810\n",
      "Iteration 3700, Loss: 0.4800\n",
      "Iteration 3800, Loss: 0.4791\n",
      "Iteration 3900, Loss: 0.4782\n",
      "Iteration 0, Loss: 1.0201\n",
      "Iteration 100, Loss: 0.6149\n",
      "Iteration 200, Loss: 0.5844\n",
      "Iteration 300, Loss: 0.5676\n",
      "Iteration 400, Loss: 0.5561\n",
      "Iteration 500, Loss: 0.5473\n",
      "Iteration 600, Loss: 0.5404\n",
      "Iteration 700, Loss: 0.5345\n",
      "Iteration 800, Loss: 0.5296\n",
      "Iteration 900, Loss: 0.5253\n",
      "Iteration 1000, Loss: 0.5214\n",
      "Iteration 1100, Loss: 0.5180\n",
      "Iteration 1200, Loss: 0.5149\n",
      "Iteration 1300, Loss: 0.5121\n",
      "Iteration 1400, Loss: 0.5095\n",
      "Iteration 1500, Loss: 0.5071\n",
      "Iteration 1600, Loss: 0.5048\n",
      "Iteration 1700, Loss: 0.5027\n",
      "Iteration 1800, Loss: 0.5007\n",
      "Iteration 1900, Loss: 0.4988\n",
      "Iteration 2000, Loss: 0.4971\n",
      "Iteration 2100, Loss: 0.4954\n",
      "Iteration 2200, Loss: 0.4937\n",
      "Iteration 2300, Loss: 0.4922\n",
      "Iteration 2400, Loss: 0.4907\n",
      "Iteration 2500, Loss: 0.4893\n",
      "Iteration 2600, Loss: 0.4879\n",
      "Iteration 2700, Loss: 0.4866\n",
      "Iteration 2800, Loss: 0.4853\n",
      "Iteration 2900, Loss: 0.4841\n",
      "Iteration 3000, Loss: 0.4829\n",
      "Iteration 3100, Loss: 0.4818\n",
      "Iteration 3200, Loss: 0.4807\n",
      "Iteration 3300, Loss: 0.4796\n",
      "Iteration 3400, Loss: 0.4785\n",
      "Iteration 3500, Loss: 0.4775\n",
      "Iteration 3600, Loss: 0.4765\n",
      "Iteration 3700, Loss: 0.4755\n",
      "Iteration 3800, Loss: 0.4746\n",
      "Iteration 3900, Loss: 0.4737\n",
      "9 270\n",
      "Iteration 0, Loss: 1.0182\n",
      "Iteration 100, Loss: 0.6106\n",
      "Iteration 200, Loss: 0.5794\n",
      "Iteration 300, Loss: 0.5620\n",
      "Iteration 400, Loss: 0.5499\n",
      "Iteration 500, Loss: 0.5406\n",
      "Iteration 600, Loss: 0.5330\n",
      "Iteration 700, Loss: 0.5266\n",
      "Iteration 800, Loss: 0.5211\n",
      "Iteration 900, Loss: 0.5163\n",
      "Iteration 1000, Loss: 0.5119\n",
      "Iteration 1100, Loss: 0.5080\n",
      "Iteration 1200, Loss: 0.5045\n",
      "Iteration 1300, Loss: 0.5012\n",
      "Iteration 1400, Loss: 0.4982\n",
      "Iteration 1500, Loss: 0.4954\n",
      "Iteration 1600, Loss: 0.4928\n",
      "Iteration 1700, Loss: 0.4903\n",
      "Iteration 1800, Loss: 0.4880\n",
      "Iteration 1900, Loss: 0.4858\n",
      "Iteration 2000, Loss: 0.4838\n",
      "Iteration 2100, Loss: 0.4819\n",
      "Iteration 2200, Loss: 0.4800\n",
      "Iteration 2300, Loss: 0.4782\n",
      "Iteration 2400, Loss: 0.4766\n",
      "Iteration 2500, Loss: 0.4749\n",
      "Iteration 2600, Loss: 0.4734\n",
      "Iteration 2700, Loss: 0.4719\n",
      "Iteration 2800, Loss: 0.4705\n",
      "Iteration 2900, Loss: 0.4691\n",
      "Iteration 3000, Loss: 0.4678\n",
      "Iteration 3100, Loss: 0.4665\n",
      "Iteration 3200, Loss: 0.4653\n",
      "Iteration 3300, Loss: 0.4641\n",
      "Iteration 3400, Loss: 0.4630\n",
      "Iteration 3500, Loss: 0.4619\n",
      "Iteration 3600, Loss: 0.4608\n",
      "Iteration 3700, Loss: 0.4597\n",
      "Iteration 3800, Loss: 0.4587\n",
      "Iteration 3900, Loss: 0.4577\n",
      "Iteration 0, Loss: 1.0220\n",
      "Iteration 100, Loss: 0.6339\n",
      "Iteration 200, Loss: 0.6021\n",
      "Iteration 300, Loss: 0.5845\n",
      "Iteration 400, Loss: 0.5725\n",
      "Iteration 500, Loss: 0.5635\n",
      "Iteration 600, Loss: 0.5564\n",
      "Iteration 700, Loss: 0.5505\n",
      "Iteration 800, Loss: 0.5456\n",
      "Iteration 900, Loss: 0.5413\n",
      "Iteration 1000, Loss: 0.5376\n",
      "Iteration 1100, Loss: 0.5342\n",
      "Iteration 1200, Loss: 0.5312\n",
      "Iteration 1300, Loss: 0.5285\n",
      "Iteration 1400, Loss: 0.5260\n",
      "Iteration 1500, Loss: 0.5237\n",
      "Iteration 1600, Loss: 0.5216\n",
      "Iteration 1700, Loss: 0.5196\n",
      "Iteration 1800, Loss: 0.5178\n",
      "Iteration 1900, Loss: 0.5161\n",
      "Iteration 2000, Loss: 0.5145\n",
      "Iteration 2100, Loss: 0.5129\n",
      "Iteration 2200, Loss: 0.5115\n",
      "Iteration 2300, Loss: 0.5101\n",
      "Iteration 2400, Loss: 0.5088\n",
      "Iteration 2500, Loss: 0.5075\n",
      "Iteration 2600, Loss: 0.5063\n",
      "Iteration 2700, Loss: 0.5052\n",
      "Iteration 2800, Loss: 0.5041\n",
      "Iteration 2900, Loss: 0.5030\n",
      "Iteration 3000, Loss: 0.5020\n",
      "Iteration 3100, Loss: 0.5010\n",
      "Iteration 3200, Loss: 0.5001\n",
      "Iteration 3300, Loss: 0.4992\n",
      "Iteration 3400, Loss: 0.4983\n",
      "Iteration 3500, Loss: 0.4974\n",
      "Iteration 3600, Loss: 0.4966\n",
      "Iteration 3700, Loss: 0.4958\n",
      "Iteration 3800, Loss: 0.4950\n",
      "Iteration 3900, Loss: 0.4943\n",
      "10 270\n",
      "Iteration 0, Loss: 1.0174\n",
      "Iteration 100, Loss: 0.6278\n",
      "Iteration 200, Loss: 0.5984\n",
      "Iteration 300, Loss: 0.5819\n",
      "Iteration 400, Loss: 0.5704\n",
      "Iteration 500, Loss: 0.5617\n",
      "Iteration 600, Loss: 0.5547\n",
      "Iteration 700, Loss: 0.5488\n",
      "Iteration 800, Loss: 0.5437\n",
      "Iteration 900, Loss: 0.5391\n",
      "Iteration 1000, Loss: 0.5351\n",
      "Iteration 1100, Loss: 0.5315\n",
      "Iteration 1200, Loss: 0.5282\n",
      "Iteration 1300, Loss: 0.5252\n",
      "Iteration 1400, Loss: 0.5224\n",
      "Iteration 1500, Loss: 0.5198\n",
      "Iteration 1600, Loss: 0.5173\n",
      "Iteration 1700, Loss: 0.5150\n",
      "Iteration 1800, Loss: 0.5129\n",
      "Iteration 1900, Loss: 0.5108\n",
      "Iteration 2000, Loss: 0.5089\n",
      "Iteration 2100, Loss: 0.5070\n",
      "Iteration 2200, Loss: 0.5053\n",
      "Iteration 2300, Loss: 0.5036\n",
      "Iteration 2400, Loss: 0.5020\n",
      "Iteration 2500, Loss: 0.5005\n",
      "Iteration 2600, Loss: 0.4990\n",
      "Iteration 2700, Loss: 0.4976\n",
      "Iteration 2800, Loss: 0.4963\n",
      "Iteration 2900, Loss: 0.4950\n",
      "Iteration 3000, Loss: 0.4937\n",
      "Iteration 3100, Loss: 0.4925\n",
      "Iteration 3200, Loss: 0.4913\n",
      "Iteration 3300, Loss: 0.4902\n",
      "Iteration 3400, Loss: 0.4891\n",
      "Iteration 3500, Loss: 0.4881\n",
      "Iteration 3600, Loss: 0.4871\n",
      "Iteration 3700, Loss: 0.4861\n",
      "Iteration 3800, Loss: 0.4851\n",
      "Iteration 3900, Loss: 0.4842\n",
      "Iteration 0, Loss: 1.0238\n",
      "Iteration 100, Loss: 0.6167\n",
      "Iteration 200, Loss: 0.5836\n",
      "Iteration 300, Loss: 0.5651\n",
      "Iteration 400, Loss: 0.5524\n",
      "Iteration 500, Loss: 0.5428\n",
      "Iteration 600, Loss: 0.5352\n",
      "Iteration 700, Loss: 0.5289\n",
      "Iteration 800, Loss: 0.5235\n",
      "Iteration 900, Loss: 0.5189\n",
      "Iteration 1000, Loss: 0.5149\n",
      "Iteration 1100, Loss: 0.5113\n",
      "Iteration 1200, Loss: 0.5080\n",
      "Iteration 1300, Loss: 0.5051\n",
      "Iteration 1400, Loss: 0.5024\n",
      "Iteration 1500, Loss: 0.4999\n",
      "Iteration 1600, Loss: 0.4975\n",
      "Iteration 1700, Loss: 0.4954\n",
      "Iteration 1800, Loss: 0.4933\n",
      "Iteration 1900, Loss: 0.4915\n",
      "Iteration 2000, Loss: 0.4896\n",
      "Iteration 2100, Loss: 0.4879\n",
      "Iteration 2200, Loss: 0.4863\n",
      "Iteration 2300, Loss: 0.4848\n",
      "Iteration 2400, Loss: 0.4834\n",
      "Iteration 2500, Loss: 0.4820\n",
      "Iteration 2600, Loss: 0.4806\n",
      "Iteration 2700, Loss: 0.4793\n",
      "Iteration 2800, Loss: 0.4781\n",
      "Iteration 2900, Loss: 0.4769\n",
      "Iteration 3000, Loss: 0.4758\n",
      "Iteration 3100, Loss: 0.4747\n",
      "Iteration 3200, Loss: 0.4736\n",
      "Iteration 3300, Loss: 0.4726\n",
      "Iteration 3400, Loss: 0.4716\n",
      "Iteration 3500, Loss: 0.4706\n",
      "Iteration 3600, Loss: 0.4697\n",
      "Iteration 3700, Loss: 0.4688\n",
      "Iteration 3800, Loss: 0.4679\n",
      "Iteration 3900, Loss: 0.4670\n",
      "11 270\n",
      "Iteration 0, Loss: 1.0147\n",
      "Iteration 100, Loss: 0.6076\n",
      "Iteration 200, Loss: 0.5764\n",
      "Iteration 300, Loss: 0.5587\n",
      "Iteration 400, Loss: 0.5464\n",
      "Iteration 500, Loss: 0.5370\n",
      "Iteration 600, Loss: 0.5295\n",
      "Iteration 700, Loss: 0.5233\n",
      "Iteration 800, Loss: 0.5181\n",
      "Iteration 900, Loss: 0.5135\n",
      "Iteration 1000, Loss: 0.5095\n",
      "Iteration 1100, Loss: 0.5059\n",
      "Iteration 1200, Loss: 0.5027\n",
      "Iteration 1300, Loss: 0.4998\n",
      "Iteration 1400, Loss: 0.4971\n",
      "Iteration 1500, Loss: 0.4947\n",
      "Iteration 1600, Loss: 0.4924\n",
      "Iteration 1700, Loss: 0.4903\n",
      "Iteration 1800, Loss: 0.4883\n",
      "Iteration 1900, Loss: 0.4865\n",
      "Iteration 2000, Loss: 0.4847\n",
      "Iteration 2100, Loss: 0.4831\n",
      "Iteration 2200, Loss: 0.4815\n",
      "Iteration 2300, Loss: 0.4800\n",
      "Iteration 2400, Loss: 0.4786\n",
      "Iteration 2500, Loss: 0.4773\n",
      "Iteration 2600, Loss: 0.4760\n",
      "Iteration 2700, Loss: 0.4748\n",
      "Iteration 2800, Loss: 0.4736\n",
      "Iteration 2900, Loss: 0.4724\n",
      "Iteration 3000, Loss: 0.4714\n",
      "Iteration 3100, Loss: 0.4703\n",
      "Iteration 3200, Loss: 0.4693\n",
      "Iteration 3300, Loss: 0.4683\n",
      "Iteration 3400, Loss: 0.4674\n",
      "Iteration 3500, Loss: 0.4664\n",
      "Iteration 3600, Loss: 0.4655\n",
      "Iteration 3700, Loss: 0.4647\n",
      "Iteration 3800, Loss: 0.4638\n",
      "Iteration 3900, Loss: 0.4630\n",
      "Iteration 0, Loss: 1.0245\n",
      "Iteration 100, Loss: 0.6387\n",
      "Iteration 200, Loss: 0.6080\n",
      "Iteration 300, Loss: 0.5905\n",
      "Iteration 400, Loss: 0.5783\n",
      "Iteration 500, Loss: 0.5688\n",
      "Iteration 600, Loss: 0.5611\n",
      "Iteration 700, Loss: 0.5547\n",
      "Iteration 800, Loss: 0.5491\n",
      "Iteration 900, Loss: 0.5442\n",
      "Iteration 1000, Loss: 0.5398\n",
      "Iteration 1100, Loss: 0.5358\n",
      "Iteration 1200, Loss: 0.5322\n",
      "Iteration 1300, Loss: 0.5289\n",
      "Iteration 1400, Loss: 0.5258\n",
      "Iteration 1500, Loss: 0.5230\n",
      "Iteration 1600, Loss: 0.5203\n",
      "Iteration 1700, Loss: 0.5179\n",
      "Iteration 1800, Loss: 0.5155\n",
      "Iteration 1900, Loss: 0.5133\n",
      "Iteration 2000, Loss: 0.5113\n",
      "Iteration 2100, Loss: 0.5093\n",
      "Iteration 2200, Loss: 0.5074\n",
      "Iteration 2300, Loss: 0.5057\n",
      "Iteration 2400, Loss: 0.5040\n",
      "Iteration 2500, Loss: 0.5024\n",
      "Iteration 2600, Loss: 0.5009\n",
      "Iteration 2700, Loss: 0.4994\n",
      "Iteration 2800, Loss: 0.4980\n",
      "Iteration 2900, Loss: 0.4966\n",
      "Iteration 3000, Loss: 0.4954\n",
      "Iteration 3100, Loss: 0.4941\n",
      "Iteration 3200, Loss: 0.4929\n",
      "Iteration 3300, Loss: 0.4918\n",
      "Iteration 3400, Loss: 0.4907\n",
      "Iteration 3500, Loss: 0.4896\n",
      "Iteration 3600, Loss: 0.4885\n",
      "Iteration 3700, Loss: 0.4875\n",
      "Iteration 3800, Loss: 0.4866\n",
      "Iteration 3900, Loss: 0.4856\n",
      "12 270\n",
      "Iteration 0, Loss: 1.0224\n",
      "Iteration 100, Loss: 0.6232\n",
      "Iteration 200, Loss: 0.5932\n",
      "Iteration 300, Loss: 0.5759\n",
      "Iteration 400, Loss: 0.5637\n",
      "Iteration 500, Loss: 0.5546\n",
      "Iteration 600, Loss: 0.5472\n",
      "Iteration 700, Loss: 0.5410\n",
      "Iteration 800, Loss: 0.5358\n",
      "Iteration 900, Loss: 0.5313\n",
      "Iteration 1000, Loss: 0.5273\n",
      "Iteration 1100, Loss: 0.5237\n",
      "Iteration 1200, Loss: 0.5205\n",
      "Iteration 1300, Loss: 0.5175\n",
      "Iteration 1400, Loss: 0.5147\n",
      "Iteration 1500, Loss: 0.5122\n",
      "Iteration 1600, Loss: 0.5099\n",
      "Iteration 1700, Loss: 0.5077\n",
      "Iteration 1800, Loss: 0.5057\n",
      "Iteration 1900, Loss: 0.5037\n",
      "Iteration 2000, Loss: 0.5019\n",
      "Iteration 2100, Loss: 0.5001\n",
      "Iteration 2200, Loss: 0.4985\n",
      "Iteration 2300, Loss: 0.4969\n",
      "Iteration 2400, Loss: 0.4954\n",
      "Iteration 2500, Loss: 0.4940\n",
      "Iteration 2600, Loss: 0.4926\n",
      "Iteration 2700, Loss: 0.4912\n",
      "Iteration 2800, Loss: 0.4900\n",
      "Iteration 2900, Loss: 0.4888\n",
      "Iteration 3000, Loss: 0.4876\n",
      "Iteration 3100, Loss: 0.4864\n",
      "Iteration 3200, Loss: 0.4853\n",
      "Iteration 3300, Loss: 0.4843\n",
      "Iteration 3400, Loss: 0.4832\n",
      "Iteration 3500, Loss: 0.4822\n",
      "Iteration 3600, Loss: 0.4813\n",
      "Iteration 3700, Loss: 0.4803\n",
      "Iteration 3800, Loss: 0.4794\n",
      "Iteration 3900, Loss: 0.4786\n",
      "Iteration 0, Loss: 1.0158\n",
      "Iteration 100, Loss: 0.6180\n",
      "Iteration 200, Loss: 0.5860\n",
      "Iteration 300, Loss: 0.5683\n",
      "Iteration 400, Loss: 0.5562\n",
      "Iteration 500, Loss: 0.5470\n",
      "Iteration 600, Loss: 0.5395\n",
      "Iteration 700, Loss: 0.5333\n",
      "Iteration 800, Loss: 0.5279\n",
      "Iteration 900, Loss: 0.5233\n",
      "Iteration 1000, Loss: 0.5191\n",
      "Iteration 1100, Loss: 0.5154\n",
      "Iteration 1200, Loss: 0.5121\n",
      "Iteration 1300, Loss: 0.5090\n",
      "Iteration 1400, Loss: 0.5062\n",
      "Iteration 1500, Loss: 0.5035\n",
      "Iteration 1600, Loss: 0.5011\n",
      "Iteration 1700, Loss: 0.4988\n",
      "Iteration 1800, Loss: 0.4967\n",
      "Iteration 1900, Loss: 0.4947\n",
      "Iteration 2000, Loss: 0.4928\n",
      "Iteration 2100, Loss: 0.4910\n",
      "Iteration 2200, Loss: 0.4893\n",
      "Iteration 2300, Loss: 0.4877\n",
      "Iteration 2400, Loss: 0.4862\n",
      "Iteration 2500, Loss: 0.4847\n",
      "Iteration 2600, Loss: 0.4833\n",
      "Iteration 2700, Loss: 0.4819\n",
      "Iteration 2800, Loss: 0.4807\n",
      "Iteration 2900, Loss: 0.4794\n",
      "Iteration 3000, Loss: 0.4782\n",
      "Iteration 3100, Loss: 0.4771\n",
      "Iteration 3200, Loss: 0.4760\n",
      "Iteration 3300, Loss: 0.4749\n",
      "Iteration 3400, Loss: 0.4739\n",
      "Iteration 3500, Loss: 0.4729\n",
      "Iteration 3600, Loss: 0.4719\n",
      "Iteration 3700, Loss: 0.4710\n",
      "Iteration 3800, Loss: 0.4701\n",
      "Iteration 3900, Loss: 0.4692\n",
      "13 270\n",
      "Iteration 0, Loss: 1.0163\n",
      "Iteration 100, Loss: 0.6107\n",
      "Iteration 200, Loss: 0.5790\n",
      "Iteration 300, Loss: 0.5613\n",
      "Iteration 400, Loss: 0.5491\n",
      "Iteration 500, Loss: 0.5398\n",
      "Iteration 600, Loss: 0.5323\n",
      "Iteration 700, Loss: 0.5261\n",
      "Iteration 800, Loss: 0.5207\n",
      "Iteration 900, Loss: 0.5160\n",
      "Iteration 1000, Loss: 0.5117\n",
      "Iteration 1100, Loss: 0.5080\n",
      "Iteration 1200, Loss: 0.5046\n",
      "Iteration 1300, Loss: 0.5014\n",
      "Iteration 1400, Loss: 0.4985\n",
      "Iteration 1500, Loss: 0.4957\n",
      "Iteration 1600, Loss: 0.4932\n",
      "Iteration 1700, Loss: 0.4909\n",
      "Iteration 1800, Loss: 0.4886\n",
      "Iteration 1900, Loss: 0.4865\n",
      "Iteration 2000, Loss: 0.4845\n",
      "Iteration 2100, Loss: 0.4826\n",
      "Iteration 2200, Loss: 0.4808\n",
      "Iteration 2300, Loss: 0.4791\n",
      "Iteration 2400, Loss: 0.4775\n",
      "Iteration 2500, Loss: 0.4759\n",
      "Iteration 2600, Loss: 0.4744\n",
      "Iteration 2700, Loss: 0.4730\n",
      "Iteration 2800, Loss: 0.4716\n",
      "Iteration 2900, Loss: 0.4702\n",
      "Iteration 3000, Loss: 0.4689\n",
      "Iteration 3100, Loss: 0.4677\n",
      "Iteration 3200, Loss: 0.4665\n",
      "Iteration 3300, Loss: 0.4653\n",
      "Iteration 3400, Loss: 0.4642\n",
      "Iteration 3500, Loss: 0.4631\n",
      "Iteration 3600, Loss: 0.4620\n",
      "Iteration 3700, Loss: 0.4610\n",
      "Iteration 3800, Loss: 0.4600\n",
      "Iteration 3900, Loss: 0.4590\n",
      "Iteration 0, Loss: 1.0239\n",
      "Iteration 100, Loss: 0.6351\n",
      "Iteration 200, Loss: 0.6050\n",
      "Iteration 300, Loss: 0.5884\n",
      "Iteration 400, Loss: 0.5771\n",
      "Iteration 500, Loss: 0.5685\n",
      "Iteration 600, Loss: 0.5615\n",
      "Iteration 700, Loss: 0.5557\n",
      "Iteration 800, Loss: 0.5508\n",
      "Iteration 900, Loss: 0.5465\n",
      "Iteration 1000, Loss: 0.5426\n",
      "Iteration 1100, Loss: 0.5392\n",
      "Iteration 1200, Loss: 0.5361\n",
      "Iteration 1300, Loss: 0.5332\n",
      "Iteration 1400, Loss: 0.5306\n",
      "Iteration 1500, Loss: 0.5282\n",
      "Iteration 1600, Loss: 0.5259\n",
      "Iteration 1700, Loss: 0.5238\n",
      "Iteration 1800, Loss: 0.5218\n",
      "Iteration 1900, Loss: 0.5199\n",
      "Iteration 2000, Loss: 0.5181\n",
      "Iteration 2100, Loss: 0.5164\n",
      "Iteration 2200, Loss: 0.5148\n",
      "Iteration 2300, Loss: 0.5133\n",
      "Iteration 2400, Loss: 0.5119\n",
      "Iteration 2500, Loss: 0.5105\n",
      "Iteration 2600, Loss: 0.5091\n",
      "Iteration 2700, Loss: 0.5079\n",
      "Iteration 2800, Loss: 0.5066\n",
      "Iteration 2900, Loss: 0.5054\n",
      "Iteration 3000, Loss: 0.5043\n",
      "Iteration 3100, Loss: 0.5032\n",
      "Iteration 3200, Loss: 0.5021\n",
      "Iteration 3300, Loss: 0.5011\n",
      "Iteration 3400, Loss: 0.5001\n",
      "Iteration 3500, Loss: 0.4992\n",
      "Iteration 3600, Loss: 0.4982\n",
      "Iteration 3700, Loss: 0.4973\n",
      "Iteration 3800, Loss: 0.4964\n",
      "Iteration 3900, Loss: 0.4956\n",
      "14 270\n",
      "Iteration 0, Loss: 1.0279\n",
      "Iteration 100, Loss: 0.6390\n",
      "Iteration 200, Loss: 0.6078\n",
      "Iteration 300, Loss: 0.5906\n",
      "Iteration 400, Loss: 0.5787\n",
      "Iteration 500, Loss: 0.5697\n",
      "Iteration 600, Loss: 0.5625\n",
      "Iteration 700, Loss: 0.5566\n",
      "Iteration 800, Loss: 0.5515\n",
      "Iteration 900, Loss: 0.5470\n",
      "Iteration 1000, Loss: 0.5430\n",
      "Iteration 1100, Loss: 0.5395\n",
      "Iteration 1200, Loss: 0.5363\n",
      "Iteration 1300, Loss: 0.5334\n",
      "Iteration 1400, Loss: 0.5306\n",
      "Iteration 1500, Loss: 0.5281\n",
      "Iteration 1600, Loss: 0.5258\n",
      "Iteration 1700, Loss: 0.5236\n",
      "Iteration 1800, Loss: 0.5215\n",
      "Iteration 1900, Loss: 0.5196\n",
      "Iteration 2000, Loss: 0.5177\n",
      "Iteration 2100, Loss: 0.5160\n",
      "Iteration 2200, Loss: 0.5143\n",
      "Iteration 2300, Loss: 0.5127\n",
      "Iteration 2400, Loss: 0.5112\n",
      "Iteration 2500, Loss: 0.5097\n",
      "Iteration 2600, Loss: 0.5083\n",
      "Iteration 2700, Loss: 0.5069\n",
      "Iteration 2800, Loss: 0.5056\n",
      "Iteration 2900, Loss: 0.5043\n",
      "Iteration 3000, Loss: 0.5031\n",
      "Iteration 3100, Loss: 0.5019\n",
      "Iteration 3200, Loss: 0.5008\n",
      "Iteration 3300, Loss: 0.4997\n",
      "Iteration 3400, Loss: 0.4986\n",
      "Iteration 3500, Loss: 0.4975\n",
      "Iteration 3600, Loss: 0.4965\n",
      "Iteration 3700, Loss: 0.4955\n",
      "Iteration 3800, Loss: 0.4946\n",
      "Iteration 3900, Loss: 0.4936\n",
      "Iteration 0, Loss: 1.0121\n",
      "Iteration 100, Loss: 0.6053\n",
      "Iteration 200, Loss: 0.5737\n",
      "Iteration 300, Loss: 0.5554\n",
      "Iteration 400, Loss: 0.5426\n",
      "Iteration 500, Loss: 0.5328\n",
      "Iteration 600, Loss: 0.5248\n",
      "Iteration 700, Loss: 0.5182\n",
      "Iteration 800, Loss: 0.5125\n",
      "Iteration 900, Loss: 0.5075\n",
      "Iteration 1000, Loss: 0.5031\n",
      "Iteration 1100, Loss: 0.4992\n",
      "Iteration 1200, Loss: 0.4956\n",
      "Iteration 1300, Loss: 0.4924\n",
      "Iteration 1400, Loss: 0.4894\n",
      "Iteration 1500, Loss: 0.4867\n",
      "Iteration 1600, Loss: 0.4841\n",
      "Iteration 1700, Loss: 0.4818\n",
      "Iteration 1800, Loss: 0.4795\n",
      "Iteration 1900, Loss: 0.4774\n",
      "Iteration 2000, Loss: 0.4755\n",
      "Iteration 2100, Loss: 0.4736\n",
      "Iteration 2200, Loss: 0.4718\n",
      "Iteration 2300, Loss: 0.4701\n",
      "Iteration 2400, Loss: 0.4685\n",
      "Iteration 2500, Loss: 0.4670\n",
      "Iteration 2600, Loss: 0.4655\n",
      "Iteration 2700, Loss: 0.4641\n",
      "Iteration 2800, Loss: 0.4627\n",
      "Iteration 2900, Loss: 0.4614\n",
      "Iteration 3000, Loss: 0.4602\n",
      "Iteration 3100, Loss: 0.4590\n",
      "Iteration 3200, Loss: 0.4578\n",
      "Iteration 3300, Loss: 0.4566\n",
      "Iteration 3400, Loss: 0.4556\n",
      "Iteration 3500, Loss: 0.4545\n",
      "Iteration 3600, Loss: 0.4535\n",
      "Iteration 3700, Loss: 0.4525\n",
      "Iteration 3800, Loss: 0.4515\n",
      "Iteration 3900, Loss: 0.4505\n",
      "15 270\n",
      "Iteration 0, Loss: 1.0560\n",
      "Iteration 100, Loss: 0.6624\n",
      "Iteration 200, Loss: 0.6275\n",
      "Iteration 300, Loss: 0.6091\n",
      "Iteration 400, Loss: 0.5967\n",
      "Iteration 500, Loss: 0.5873\n",
      "Iteration 600, Loss: 0.5796\n",
      "Iteration 700, Loss: 0.5733\n",
      "Iteration 800, Loss: 0.5679\n",
      "Iteration 900, Loss: 0.5633\n",
      "Iteration 1000, Loss: 0.5592\n",
      "Iteration 1100, Loss: 0.5554\n",
      "Iteration 1200, Loss: 0.5521\n",
      "Iteration 1300, Loss: 0.5491\n",
      "Iteration 1400, Loss: 0.5464\n",
      "Iteration 1500, Loss: 0.5439\n",
      "Iteration 1600, Loss: 0.5416\n",
      "Iteration 1700, Loss: 0.5394\n",
      "Iteration 1800, Loss: 0.5374\n",
      "Iteration 1900, Loss: 0.5355\n",
      "Iteration 2000, Loss: 0.5337\n",
      "Iteration 2100, Loss: 0.5320\n",
      "Iteration 2200, Loss: 0.5305\n",
      "Iteration 2300, Loss: 0.5290\n",
      "Iteration 2400, Loss: 0.5276\n",
      "Iteration 2500, Loss: 0.5262\n",
      "Iteration 2600, Loss: 0.5249\n",
      "Iteration 2700, Loss: 0.5237\n",
      "Iteration 2800, Loss: 0.5225\n",
      "Iteration 2900, Loss: 0.5214\n",
      "Iteration 3000, Loss: 0.5203\n",
      "Iteration 3100, Loss: 0.5192\n",
      "Iteration 3200, Loss: 0.5182\n",
      "Iteration 3300, Loss: 0.5172\n",
      "Iteration 3400, Loss: 0.5163\n",
      "Iteration 3500, Loss: 0.5154\n",
      "Iteration 3600, Loss: 0.5145\n",
      "Iteration 3700, Loss: 0.5137\n",
      "Iteration 3800, Loss: 0.5129\n",
      "Iteration 3900, Loss: 0.5121\n",
      "Iteration 0, Loss: 1.0514\n",
      "Iteration 100, Loss: 0.6508\n",
      "Iteration 200, Loss: 0.6154\n",
      "Iteration 300, Loss: 0.5977\n",
      "Iteration 400, Loss: 0.5860\n",
      "Iteration 500, Loss: 0.5775\n",
      "Iteration 600, Loss: 0.5706\n",
      "Iteration 700, Loss: 0.5650\n",
      "Iteration 800, Loss: 0.5603\n",
      "Iteration 900, Loss: 0.5561\n",
      "Iteration 1000, Loss: 0.5525\n",
      "Iteration 1100, Loss: 0.5493\n",
      "Iteration 1200, Loss: 0.5463\n",
      "Iteration 1300, Loss: 0.5436\n",
      "Iteration 1400, Loss: 0.5411\n",
      "Iteration 1500, Loss: 0.5388\n",
      "Iteration 1600, Loss: 0.5367\n",
      "Iteration 1700, Loss: 0.5347\n",
      "Iteration 1800, Loss: 0.5328\n",
      "Iteration 1900, Loss: 0.5310\n",
      "Iteration 2000, Loss: 0.5293\n",
      "Iteration 2100, Loss: 0.5277\n",
      "Iteration 2200, Loss: 0.5262\n",
      "Iteration 2300, Loss: 0.5248\n",
      "Iteration 2400, Loss: 0.5234\n",
      "Iteration 2500, Loss: 0.5221\n",
      "Iteration 2600, Loss: 0.5208\n",
      "Iteration 2700, Loss: 0.5196\n",
      "Iteration 2800, Loss: 0.5184\n",
      "Iteration 2900, Loss: 0.5173\n",
      "Iteration 3000, Loss: 0.5162\n",
      "Iteration 3100, Loss: 0.5152\n",
      "Iteration 3200, Loss: 0.5141\n",
      "Iteration 3300, Loss: 0.5131\n",
      "Iteration 3400, Loss: 0.5121\n",
      "Iteration 3500, Loss: 0.5112\n",
      "Iteration 3600, Loss: 0.5103\n",
      "Iteration 3700, Loss: 0.5095\n",
      "Iteration 3800, Loss: 0.5086\n",
      "Iteration 3900, Loss: 0.5078\n",
      "16 270\n",
      "Iteration 0, Loss: 1.0564\n",
      "Iteration 100, Loss: 0.6701\n",
      "Iteration 200, Loss: 0.6349\n",
      "Iteration 300, Loss: 0.6168\n",
      "Iteration 400, Loss: 0.6048\n",
      "Iteration 500, Loss: 0.5958\n",
      "Iteration 600, Loss: 0.5887\n",
      "Iteration 700, Loss: 0.5827\n",
      "Iteration 800, Loss: 0.5776\n",
      "Iteration 900, Loss: 0.5733\n",
      "Iteration 1000, Loss: 0.5693\n",
      "Iteration 1100, Loss: 0.5659\n",
      "Iteration 1200, Loss: 0.5627\n",
      "Iteration 1300, Loss: 0.5599\n",
      "Iteration 1400, Loss: 0.5573\n",
      "Iteration 1500, Loss: 0.5549\n",
      "Iteration 1600, Loss: 0.5526\n",
      "Iteration 1700, Loss: 0.5506\n",
      "Iteration 1800, Loss: 0.5486\n",
      "Iteration 1900, Loss: 0.5468\n",
      "Iteration 2000, Loss: 0.5451\n",
      "Iteration 2100, Loss: 0.5435\n",
      "Iteration 2200, Loss: 0.5419\n",
      "Iteration 2300, Loss: 0.5405\n",
      "Iteration 2400, Loss: 0.5390\n",
      "Iteration 2500, Loss: 0.5377\n",
      "Iteration 2600, Loss: 0.5364\n",
      "Iteration 2700, Loss: 0.5353\n",
      "Iteration 2800, Loss: 0.5341\n",
      "Iteration 2900, Loss: 0.5330\n",
      "Iteration 3000, Loss: 0.5319\n",
      "Iteration 3100, Loss: 0.5309\n",
      "Iteration 3200, Loss: 0.5298\n",
      "Iteration 3300, Loss: 0.5289\n",
      "Iteration 3400, Loss: 0.5279\n",
      "Iteration 3500, Loss: 0.5270\n",
      "Iteration 3600, Loss: 0.5261\n",
      "Iteration 3700, Loss: 0.5253\n",
      "Iteration 3800, Loss: 0.5245\n",
      "Iteration 3900, Loss: 0.5237\n",
      "Iteration 0, Loss: 1.0491\n",
      "Iteration 100, Loss: 0.6469\n",
      "Iteration 200, Loss: 0.6131\n",
      "Iteration 300, Loss: 0.5950\n",
      "Iteration 400, Loss: 0.5827\n",
      "Iteration 500, Loss: 0.5735\n",
      "Iteration 600, Loss: 0.5660\n",
      "Iteration 700, Loss: 0.5601\n",
      "Iteration 800, Loss: 0.5548\n",
      "Iteration 900, Loss: 0.5504\n",
      "Iteration 1000, Loss: 0.5465\n",
      "Iteration 1100, Loss: 0.5431\n",
      "Iteration 1200, Loss: 0.5399\n",
      "Iteration 1300, Loss: 0.5371\n",
      "Iteration 1400, Loss: 0.5345\n",
      "Iteration 1500, Loss: 0.5321\n",
      "Iteration 1600, Loss: 0.5299\n",
      "Iteration 1700, Loss: 0.5280\n",
      "Iteration 1800, Loss: 0.5260\n",
      "Iteration 1900, Loss: 0.5243\n",
      "Iteration 2000, Loss: 0.5226\n",
      "Iteration 2100, Loss: 0.5210\n",
      "Iteration 2200, Loss: 0.5195\n",
      "Iteration 2300, Loss: 0.5181\n",
      "Iteration 2400, Loss: 0.5168\n",
      "Iteration 2500, Loss: 0.5155\n",
      "Iteration 2600, Loss: 0.5142\n",
      "Iteration 2700, Loss: 0.5131\n",
      "Iteration 2800, Loss: 0.5120\n",
      "Iteration 2900, Loss: 0.5109\n",
      "Iteration 3000, Loss: 0.5099\n",
      "Iteration 3100, Loss: 0.5088\n",
      "Iteration 3200, Loss: 0.5079\n",
      "Iteration 3300, Loss: 0.5069\n",
      "Iteration 3400, Loss: 0.5060\n",
      "Iteration 3500, Loss: 0.5051\n",
      "Iteration 3600, Loss: 0.5043\n",
      "Iteration 3700, Loss: 0.5035\n",
      "Iteration 3800, Loss: 0.5027\n",
      "Iteration 3900, Loss: 0.5019\n",
      "17 270\n",
      "Iteration 0, Loss: 1.0538\n",
      "Iteration 100, Loss: 0.6555\n",
      "Iteration 200, Loss: 0.6211\n",
      "Iteration 300, Loss: 0.6026\n",
      "Iteration 400, Loss: 0.5900\n",
      "Iteration 500, Loss: 0.5804\n",
      "Iteration 600, Loss: 0.5728\n",
      "Iteration 700, Loss: 0.5665\n",
      "Iteration 800, Loss: 0.5611\n",
      "Iteration 900, Loss: 0.5567\n",
      "Iteration 1000, Loss: 0.5526\n",
      "Iteration 1100, Loss: 0.5491\n",
      "Iteration 1200, Loss: 0.5460\n",
      "Iteration 1300, Loss: 0.5432\n",
      "Iteration 1400, Loss: 0.5406\n",
      "Iteration 1500, Loss: 0.5383\n",
      "Iteration 1600, Loss: 0.5361\n",
      "Iteration 1700, Loss: 0.5342\n",
      "Iteration 1800, Loss: 0.5323\n",
      "Iteration 1900, Loss: 0.5306\n",
      "Iteration 2000, Loss: 0.5290\n",
      "Iteration 2100, Loss: 0.5274\n",
      "Iteration 2200, Loss: 0.5260\n",
      "Iteration 2300, Loss: 0.5246\n",
      "Iteration 2400, Loss: 0.5233\n",
      "Iteration 2500, Loss: 0.5221\n",
      "Iteration 2600, Loss: 0.5210\n",
      "Iteration 2700, Loss: 0.5198\n",
      "Iteration 2800, Loss: 0.5188\n",
      "Iteration 2900, Loss: 0.5177\n",
      "Iteration 3000, Loss: 0.5167\n",
      "Iteration 3100, Loss: 0.5158\n",
      "Iteration 3200, Loss: 0.5149\n",
      "Iteration 3300, Loss: 0.5140\n",
      "Iteration 3400, Loss: 0.5132\n",
      "Iteration 3500, Loss: 0.5124\n",
      "Iteration 3600, Loss: 0.5116\n",
      "Iteration 3700, Loss: 0.5108\n",
      "Iteration 3800, Loss: 0.5101\n",
      "Iteration 3900, Loss: 0.5093\n",
      "Iteration 0, Loss: 1.0537\n",
      "Iteration 100, Loss: 0.6605\n",
      "Iteration 200, Loss: 0.6257\n",
      "Iteration 300, Loss: 0.6085\n",
      "Iteration 400, Loss: 0.5969\n",
      "Iteration 500, Loss: 0.5883\n",
      "Iteration 600, Loss: 0.5814\n",
      "Iteration 700, Loss: 0.5756\n",
      "Iteration 800, Loss: 0.5708\n",
      "Iteration 900, Loss: 0.5665\n",
      "Iteration 1000, Loss: 0.5627\n",
      "Iteration 1100, Loss: 0.5593\n",
      "Iteration 1200, Loss: 0.5563\n",
      "Iteration 1300, Loss: 0.5534\n",
      "Iteration 1400, Loss: 0.5509\n",
      "Iteration 1500, Loss: 0.5486\n",
      "Iteration 1600, Loss: 0.5464\n",
      "Iteration 1700, Loss: 0.5443\n",
      "Iteration 1800, Loss: 0.5424\n",
      "Iteration 1900, Loss: 0.5406\n",
      "Iteration 2000, Loss: 0.5389\n",
      "Iteration 2100, Loss: 0.5373\n",
      "Iteration 2200, Loss: 0.5358\n",
      "Iteration 2300, Loss: 0.5343\n",
      "Iteration 2400, Loss: 0.5329\n",
      "Iteration 2500, Loss: 0.5315\n",
      "Iteration 2600, Loss: 0.5303\n",
      "Iteration 2700, Loss: 0.5291\n",
      "Iteration 2800, Loss: 0.5279\n",
      "Iteration 2900, Loss: 0.5268\n",
      "Iteration 3000, Loss: 0.5257\n",
      "Iteration 3100, Loss: 0.5247\n",
      "Iteration 3200, Loss: 0.5237\n",
      "Iteration 3300, Loss: 0.5227\n",
      "Iteration 3400, Loss: 0.5217\n",
      "Iteration 3500, Loss: 0.5208\n",
      "Iteration 3600, Loss: 0.5199\n",
      "Iteration 3700, Loss: 0.5192\n",
      "Iteration 3800, Loss: 0.5183\n",
      "Iteration 3900, Loss: 0.5175\n",
      "18 270\n",
      "Iteration 0, Loss: 1.0574\n",
      "Iteration 100, Loss: 0.6645\n",
      "Iteration 200, Loss: 0.6293\n",
      "Iteration 300, Loss: 0.6111\n",
      "Iteration 400, Loss: 0.5988\n",
      "Iteration 500, Loss: 0.5894\n",
      "Iteration 600, Loss: 0.5820\n",
      "Iteration 700, Loss: 0.5758\n",
      "Iteration 800, Loss: 0.5704\n",
      "Iteration 900, Loss: 0.5658\n",
      "Iteration 1000, Loss: 0.5618\n",
      "Iteration 1100, Loss: 0.5582\n",
      "Iteration 1200, Loss: 0.5550\n",
      "Iteration 1300, Loss: 0.5521\n",
      "Iteration 1400, Loss: 0.5495\n",
      "Iteration 1500, Loss: 0.5471\n",
      "Iteration 1600, Loss: 0.5449\n",
      "Iteration 1700, Loss: 0.5427\n",
      "Iteration 1800, Loss: 0.5408\n",
      "Iteration 1900, Loss: 0.5390\n",
      "Iteration 2000, Loss: 0.5373\n",
      "Iteration 2100, Loss: 0.5356\n",
      "Iteration 2200, Loss: 0.5341\n",
      "Iteration 2300, Loss: 0.5326\n",
      "Iteration 2400, Loss: 0.5313\n",
      "Iteration 2500, Loss: 0.5301\n",
      "Iteration 2600, Loss: 0.5288\n",
      "Iteration 2700, Loss: 0.5276\n",
      "Iteration 2800, Loss: 0.5264\n",
      "Iteration 2900, Loss: 0.5253\n",
      "Iteration 3000, Loss: 0.5243\n",
      "Iteration 3100, Loss: 0.5233\n",
      "Iteration 3200, Loss: 0.5223\n",
      "Iteration 3300, Loss: 0.5214\n",
      "Iteration 3400, Loss: 0.5205\n",
      "Iteration 3500, Loss: 0.5196\n",
      "Iteration 3600, Loss: 0.5188\n",
      "Iteration 3700, Loss: 0.5179\n",
      "Iteration 3800, Loss: 0.5172\n",
      "Iteration 3900, Loss: 0.5164\n",
      "Iteration 0, Loss: 1.0527\n",
      "Iteration 100, Loss: 0.6527\n",
      "Iteration 200, Loss: 0.6181\n",
      "Iteration 300, Loss: 0.6003\n",
      "Iteration 400, Loss: 0.5883\n",
      "Iteration 500, Loss: 0.5793\n",
      "Iteration 600, Loss: 0.5721\n",
      "Iteration 700, Loss: 0.5662\n",
      "Iteration 800, Loss: 0.5612\n",
      "Iteration 900, Loss: 0.5570\n",
      "Iteration 1000, Loss: 0.5532\n",
      "Iteration 1100, Loss: 0.5499\n",
      "Iteration 1200, Loss: 0.5470\n",
      "Iteration 1300, Loss: 0.5443\n",
      "Iteration 1400, Loss: 0.5419\n",
      "Iteration 1500, Loss: 0.5395\n",
      "Iteration 1600, Loss: 0.5374\n",
      "Iteration 1700, Loss: 0.5355\n",
      "Iteration 1800, Loss: 0.5338\n",
      "Iteration 1900, Loss: 0.5320\n",
      "Iteration 2000, Loss: 0.5304\n",
      "Iteration 2100, Loss: 0.5289\n",
      "Iteration 2200, Loss: 0.5275\n",
      "Iteration 2300, Loss: 0.5262\n",
      "Iteration 2400, Loss: 0.5249\n",
      "Iteration 2500, Loss: 0.5237\n",
      "Iteration 2600, Loss: 0.5225\n",
      "Iteration 2700, Loss: 0.5214\n",
      "Iteration 2800, Loss: 0.5204\n",
      "Iteration 2900, Loss: 0.5194\n",
      "Iteration 3000, Loss: 0.5184\n",
      "Iteration 3100, Loss: 0.5175\n",
      "Iteration 3200, Loss: 0.5165\n",
      "Iteration 3300, Loss: 0.5157\n",
      "Iteration 3400, Loss: 0.5148\n",
      "Iteration 3500, Loss: 0.5140\n",
      "Iteration 3600, Loss: 0.5132\n",
      "Iteration 3700, Loss: 0.5125\n",
      "Iteration 3800, Loss: 0.5117\n",
      "Iteration 3900, Loss: 0.5110\n",
      "19 270\n",
      "Iteration 0, Loss: 1.0508\n",
      "Iteration 100, Loss: 0.6611\n",
      "Iteration 200, Loss: 0.6264\n",
      "Iteration 300, Loss: 0.6080\n",
      "Iteration 400, Loss: 0.5958\n",
      "Iteration 500, Loss: 0.5868\n",
      "Iteration 600, Loss: 0.5797\n",
      "Iteration 700, Loss: 0.5738\n",
      "Iteration 800, Loss: 0.5688\n",
      "Iteration 900, Loss: 0.5645\n",
      "Iteration 1000, Loss: 0.5609\n",
      "Iteration 1100, Loss: 0.5576\n",
      "Iteration 1200, Loss: 0.5547\n",
      "Iteration 1300, Loss: 0.5520\n",
      "Iteration 1400, Loss: 0.5496\n",
      "Iteration 1500, Loss: 0.5474\n",
      "Iteration 1600, Loss: 0.5454\n",
      "Iteration 1700, Loss: 0.5434\n",
      "Iteration 1800, Loss: 0.5416\n",
      "Iteration 1900, Loss: 0.5400\n",
      "Iteration 2000, Loss: 0.5384\n",
      "Iteration 2100, Loss: 0.5370\n",
      "Iteration 2200, Loss: 0.5356\n",
      "Iteration 2300, Loss: 0.5342\n",
      "Iteration 2400, Loss: 0.5330\n",
      "Iteration 2500, Loss: 0.5318\n",
      "Iteration 2600, Loss: 0.5307\n",
      "Iteration 2700, Loss: 0.5295\n",
      "Iteration 2800, Loss: 0.5285\n",
      "Iteration 2900, Loss: 0.5275\n",
      "Iteration 3000, Loss: 0.5265\n",
      "Iteration 3100, Loss: 0.5256\n",
      "Iteration 3200, Loss: 0.5247\n",
      "Iteration 3300, Loss: 0.5238\n",
      "Iteration 3400, Loss: 0.5230\n",
      "Iteration 3500, Loss: 0.5222\n",
      "Iteration 3600, Loss: 0.5214\n",
      "Iteration 3700, Loss: 0.5206\n",
      "Iteration 3800, Loss: 0.5200\n",
      "Iteration 3900, Loss: 0.5192\n",
      "Iteration 0, Loss: 1.0564\n",
      "Iteration 100, Loss: 0.6558\n",
      "Iteration 200, Loss: 0.6195\n",
      "Iteration 300, Loss: 0.6012\n",
      "Iteration 400, Loss: 0.5887\n",
      "Iteration 500, Loss: 0.5793\n",
      "Iteration 600, Loss: 0.5718\n",
      "Iteration 700, Loss: 0.5656\n",
      "Iteration 800, Loss: 0.5603\n",
      "Iteration 900, Loss: 0.5556\n",
      "Iteration 1000, Loss: 0.5515\n",
      "Iteration 1100, Loss: 0.5479\n",
      "Iteration 1200, Loss: 0.5446\n",
      "Iteration 1300, Loss: 0.5416\n",
      "Iteration 1400, Loss: 0.5388\n",
      "Iteration 1500, Loss: 0.5364\n",
      "Iteration 1600, Loss: 0.5340\n",
      "Iteration 1700, Loss: 0.5318\n",
      "Iteration 1800, Loss: 0.5297\n",
      "Iteration 1900, Loss: 0.5278\n",
      "Iteration 2000, Loss: 0.5261\n",
      "Iteration 2100, Loss: 0.5244\n",
      "Iteration 2200, Loss: 0.5228\n",
      "Iteration 2300, Loss: 0.5213\n",
      "Iteration 2400, Loss: 0.5198\n",
      "Iteration 2500, Loss: 0.5185\n",
      "Iteration 2600, Loss: 0.5172\n",
      "Iteration 2700, Loss: 0.5160\n",
      "Iteration 2800, Loss: 0.5148\n",
      "Iteration 2900, Loss: 0.5137\n",
      "Iteration 3000, Loss: 0.5126\n",
      "Iteration 3100, Loss: 0.5116\n",
      "Iteration 3200, Loss: 0.5105\n",
      "Iteration 3300, Loss: 0.5096\n",
      "Iteration 3400, Loss: 0.5086\n",
      "Iteration 3500, Loss: 0.5077\n",
      "Iteration 3600, Loss: 0.5068\n",
      "Iteration 3700, Loss: 0.5060\n",
      "Iteration 3800, Loss: 0.5052\n",
      "Iteration 3900, Loss: 0.5044\n",
      "20 270\n",
      "Iteration 0, Loss: 1.0553\n",
      "Iteration 100, Loss: 0.6598\n",
      "Iteration 200, Loss: 0.6255\n",
      "Iteration 300, Loss: 0.6072\n",
      "Iteration 400, Loss: 0.5946\n",
      "Iteration 500, Loss: 0.5851\n",
      "Iteration 600, Loss: 0.5776\n",
      "Iteration 700, Loss: 0.5713\n",
      "Iteration 800, Loss: 0.5660\n",
      "Iteration 900, Loss: 0.5613\n",
      "Iteration 1000, Loss: 0.5572\n",
      "Iteration 1100, Loss: 0.5536\n",
      "Iteration 1200, Loss: 0.5503\n",
      "Iteration 1300, Loss: 0.5472\n",
      "Iteration 1400, Loss: 0.5445\n",
      "Iteration 1500, Loss: 0.5419\n",
      "Iteration 1600, Loss: 0.5394\n",
      "Iteration 1700, Loss: 0.5372\n",
      "Iteration 1800, Loss: 0.5351\n",
      "Iteration 1900, Loss: 0.5331\n",
      "Iteration 2000, Loss: 0.5312\n",
      "Iteration 2100, Loss: 0.5294\n",
      "Iteration 2200, Loss: 0.5277\n",
      "Iteration 2300, Loss: 0.5261\n",
      "Iteration 2400, Loss: 0.5246\n",
      "Iteration 2500, Loss: 0.5231\n",
      "Iteration 2600, Loss: 0.5217\n",
      "Iteration 2700, Loss: 0.5203\n",
      "Iteration 2800, Loss: 0.5190\n",
      "Iteration 2900, Loss: 0.5178\n",
      "Iteration 3000, Loss: 0.5166\n",
      "Iteration 3100, Loss: 0.5154\n",
      "Iteration 3200, Loss: 0.5143\n",
      "Iteration 3300, Loss: 0.5131\n",
      "Iteration 3400, Loss: 0.5121\n",
      "Iteration 3500, Loss: 0.5111\n",
      "Iteration 3600, Loss: 0.5100\n",
      "Iteration 3700, Loss: 0.5090\n",
      "Iteration 3800, Loss: 0.5081\n",
      "Iteration 3900, Loss: 0.5072\n",
      "Iteration 0, Loss: 1.0517\n",
      "Iteration 100, Loss: 0.6527\n",
      "Iteration 200, Loss: 0.6152\n",
      "Iteration 300, Loss: 0.5956\n",
      "Iteration 400, Loss: 0.5825\n",
      "Iteration 500, Loss: 0.5725\n",
      "Iteration 600, Loss: 0.5645\n",
      "Iteration 700, Loss: 0.5578\n",
      "Iteration 800, Loss: 0.5521\n",
      "Iteration 900, Loss: 0.5471\n",
      "Iteration 1000, Loss: 0.5427\n",
      "Iteration 1100, Loss: 0.5387\n",
      "Iteration 1200, Loss: 0.5352\n",
      "Iteration 1300, Loss: 0.5319\n",
      "Iteration 1400, Loss: 0.5289\n",
      "Iteration 1500, Loss: 0.5262\n",
      "Iteration 1600, Loss: 0.5236\n",
      "Iteration 1700, Loss: 0.5212\n",
      "Iteration 1800, Loss: 0.5190\n",
      "Iteration 1900, Loss: 0.5168\n",
      "Iteration 2000, Loss: 0.5149\n",
      "Iteration 2100, Loss: 0.5130\n",
      "Iteration 2200, Loss: 0.5112\n",
      "Iteration 2300, Loss: 0.5095\n",
      "Iteration 2400, Loss: 0.5079\n",
      "Iteration 2500, Loss: 0.5064\n",
      "Iteration 2600, Loss: 0.5049\n",
      "Iteration 2700, Loss: 0.5035\n",
      "Iteration 2800, Loss: 0.5022\n",
      "Iteration 2900, Loss: 0.5009\n",
      "Iteration 3000, Loss: 0.4996\n",
      "Iteration 3100, Loss: 0.4984\n",
      "Iteration 3200, Loss: 0.4972\n",
      "Iteration 3300, Loss: 0.4961\n",
      "Iteration 3400, Loss: 0.4950\n",
      "Iteration 3500, Loss: 0.4940\n",
      "Iteration 3600, Loss: 0.4929\n",
      "Iteration 3700, Loss: 0.4920\n",
      "Iteration 3800, Loss: 0.4910\n",
      "Iteration 3900, Loss: 0.4901\n",
      "21 270\n",
      "Iteration 0, Loss: 1.0559\n",
      "Iteration 100, Loss: 0.6658\n",
      "Iteration 200, Loss: 0.6308\n",
      "Iteration 300, Loss: 0.6119\n",
      "Iteration 400, Loss: 0.5987\n",
      "Iteration 500, Loss: 0.5885\n",
      "Iteration 600, Loss: 0.5804\n",
      "Iteration 700, Loss: 0.5735\n",
      "Iteration 800, Loss: 0.5677\n",
      "Iteration 900, Loss: 0.5627\n",
      "Iteration 1000, Loss: 0.5582\n",
      "Iteration 1100, Loss: 0.5541\n",
      "Iteration 1200, Loss: 0.5505\n",
      "Iteration 1300, Loss: 0.5472\n",
      "Iteration 1400, Loss: 0.5441\n",
      "Iteration 1500, Loss: 0.5413\n",
      "Iteration 1600, Loss: 0.5387\n",
      "Iteration 1700, Loss: 0.5362\n",
      "Iteration 1800, Loss: 0.5340\n",
      "Iteration 1900, Loss: 0.5319\n",
      "Iteration 2000, Loss: 0.5298\n",
      "Iteration 2100, Loss: 0.5279\n",
      "Iteration 2200, Loss: 0.5261\n",
      "Iteration 2300, Loss: 0.5244\n",
      "Iteration 2400, Loss: 0.5227\n",
      "Iteration 2500, Loss: 0.5212\n",
      "Iteration 2600, Loss: 0.5196\n",
      "Iteration 2700, Loss: 0.5182\n",
      "Iteration 2800, Loss: 0.5168\n",
      "Iteration 2900, Loss: 0.5155\n",
      "Iteration 3000, Loss: 0.5142\n",
      "Iteration 3100, Loss: 0.5130\n",
      "Iteration 3200, Loss: 0.5117\n",
      "Iteration 3300, Loss: 0.5106\n",
      "Iteration 3400, Loss: 0.5095\n",
      "Iteration 3500, Loss: 0.5084\n",
      "Iteration 3600, Loss: 0.5073\n",
      "Iteration 3700, Loss: 0.5063\n",
      "Iteration 3800, Loss: 0.5054\n",
      "Iteration 3900, Loss: 0.5044\n",
      "Iteration 0, Loss: 1.0517\n",
      "Iteration 100, Loss: 0.6492\n",
      "Iteration 200, Loss: 0.6139\n",
      "Iteration 300, Loss: 0.5958\n",
      "Iteration 400, Loss: 0.5836\n",
      "Iteration 500, Loss: 0.5743\n",
      "Iteration 600, Loss: 0.5668\n",
      "Iteration 700, Loss: 0.5605\n",
      "Iteration 800, Loss: 0.5551\n",
      "Iteration 900, Loss: 0.5504\n",
      "Iteration 1000, Loss: 0.5462\n",
      "Iteration 1100, Loss: 0.5425\n",
      "Iteration 1200, Loss: 0.5391\n",
      "Iteration 1300, Loss: 0.5360\n",
      "Iteration 1400, Loss: 0.5331\n",
      "Iteration 1500, Loss: 0.5304\n",
      "Iteration 1600, Loss: 0.5279\n",
      "Iteration 1700, Loss: 0.5256\n",
      "Iteration 1800, Loss: 0.5235\n",
      "Iteration 1900, Loss: 0.5214\n",
      "Iteration 2000, Loss: 0.5194\n",
      "Iteration 2100, Loss: 0.5176\n",
      "Iteration 2200, Loss: 0.5158\n",
      "Iteration 2300, Loss: 0.5141\n",
      "Iteration 2400, Loss: 0.5126\n",
      "Iteration 2500, Loss: 0.5111\n",
      "Iteration 2600, Loss: 0.5097\n",
      "Iteration 2700, Loss: 0.5082\n",
      "Iteration 2800, Loss: 0.5069\n",
      "Iteration 2900, Loss: 0.5056\n",
      "Iteration 3000, Loss: 0.5044\n",
      "Iteration 3100, Loss: 0.5032\n",
      "Iteration 3200, Loss: 0.5020\n",
      "Iteration 3300, Loss: 0.5009\n",
      "Iteration 3400, Loss: 0.4998\n",
      "Iteration 3500, Loss: 0.4988\n",
      "Iteration 3600, Loss: 0.4978\n",
      "Iteration 3700, Loss: 0.4968\n",
      "Iteration 3800, Loss: 0.4958\n",
      "Iteration 3900, Loss: 0.4949\n",
      "22 270\n",
      "Iteration 0, Loss: 1.0583\n",
      "Iteration 100, Loss: 0.6681\n",
      "Iteration 200, Loss: 0.6310\n",
      "Iteration 300, Loss: 0.6113\n",
      "Iteration 400, Loss: 0.5981\n",
      "Iteration 500, Loss: 0.5882\n",
      "Iteration 600, Loss: 0.5803\n",
      "Iteration 700, Loss: 0.5737\n",
      "Iteration 800, Loss: 0.5683\n",
      "Iteration 900, Loss: 0.5634\n",
      "Iteration 1000, Loss: 0.5592\n",
      "Iteration 1100, Loss: 0.5555\n",
      "Iteration 1200, Loss: 0.5521\n",
      "Iteration 1300, Loss: 0.5490\n",
      "Iteration 1400, Loss: 0.5462\n",
      "Iteration 1500, Loss: 0.5435\n",
      "Iteration 1600, Loss: 0.5410\n",
      "Iteration 1700, Loss: 0.5387\n",
      "Iteration 1800, Loss: 0.5366\n",
      "Iteration 1900, Loss: 0.5345\n",
      "Iteration 2000, Loss: 0.5327\n",
      "Iteration 2100, Loss: 0.5308\n",
      "Iteration 2200, Loss: 0.5291\n",
      "Iteration 2300, Loss: 0.5274\n",
      "Iteration 2400, Loss: 0.5259\n",
      "Iteration 2500, Loss: 0.5244\n",
      "Iteration 2600, Loss: 0.5229\n",
      "Iteration 2700, Loss: 0.5215\n",
      "Iteration 2800, Loss: 0.5202\n",
      "Iteration 2900, Loss: 0.5190\n",
      "Iteration 3000, Loss: 0.5177\n",
      "Iteration 3100, Loss: 0.5165\n",
      "Iteration 3200, Loss: 0.5153\n",
      "Iteration 3300, Loss: 0.5143\n",
      "Iteration 3400, Loss: 0.5131\n",
      "Iteration 3500, Loss: 0.5120\n",
      "Iteration 3600, Loss: 0.5110\n",
      "Iteration 3700, Loss: 0.5100\n",
      "Iteration 3800, Loss: 0.5090\n",
      "Iteration 3900, Loss: 0.5081\n",
      "Iteration 0, Loss: 1.0514\n",
      "Iteration 100, Loss: 0.6459\n",
      "Iteration 200, Loss: 0.6126\n",
      "Iteration 300, Loss: 0.5953\n",
      "Iteration 400, Loss: 0.5832\n",
      "Iteration 500, Loss: 0.5740\n",
      "Iteration 600, Loss: 0.5665\n",
      "Iteration 700, Loss: 0.5602\n",
      "Iteration 800, Loss: 0.5547\n",
      "Iteration 900, Loss: 0.5499\n",
      "Iteration 1000, Loss: 0.5457\n",
      "Iteration 1100, Loss: 0.5419\n",
      "Iteration 1200, Loss: 0.5385\n",
      "Iteration 1300, Loss: 0.5353\n",
      "Iteration 1400, Loss: 0.5324\n",
      "Iteration 1500, Loss: 0.5297\n",
      "Iteration 1600, Loss: 0.5271\n",
      "Iteration 1700, Loss: 0.5248\n",
      "Iteration 1800, Loss: 0.5225\n",
      "Iteration 1900, Loss: 0.5205\n",
      "Iteration 2000, Loss: 0.5185\n",
      "Iteration 2100, Loss: 0.5166\n",
      "Iteration 2200, Loss: 0.5148\n",
      "Iteration 2300, Loss: 0.5131\n",
      "Iteration 2400, Loss: 0.5114\n",
      "Iteration 2500, Loss: 0.5098\n",
      "Iteration 2600, Loss: 0.5083\n",
      "Iteration 2700, Loss: 0.5069\n",
      "Iteration 2800, Loss: 0.5055\n",
      "Iteration 2900, Loss: 0.5041\n",
      "Iteration 3000, Loss: 0.5029\n",
      "Iteration 3100, Loss: 0.5016\n",
      "Iteration 3200, Loss: 0.5004\n",
      "Iteration 3300, Loss: 0.4993\n",
      "Iteration 3400, Loss: 0.4981\n",
      "Iteration 3500, Loss: 0.4970\n",
      "Iteration 3600, Loss: 0.4960\n",
      "Iteration 3700, Loss: 0.4949\n",
      "Iteration 3800, Loss: 0.4939\n",
      "Iteration 3900, Loss: 0.4930\n",
      "23 270\n",
      "Iteration 0, Loss: 1.0529\n",
      "Iteration 100, Loss: 0.6607\n",
      "Iteration 200, Loss: 0.6241\n",
      "Iteration 300, Loss: 0.6052\n",
      "Iteration 400, Loss: 0.5923\n",
      "Iteration 500, Loss: 0.5826\n",
      "Iteration 600, Loss: 0.5747\n",
      "Iteration 700, Loss: 0.5680\n",
      "Iteration 800, Loss: 0.5624\n",
      "Iteration 900, Loss: 0.5574\n",
      "Iteration 1000, Loss: 0.5530\n",
      "Iteration 1100, Loss: 0.5491\n",
      "Iteration 1200, Loss: 0.5456\n",
      "Iteration 1300, Loss: 0.5423\n",
      "Iteration 1400, Loss: 0.5394\n",
      "Iteration 1500, Loss: 0.5366\n",
      "Iteration 1600, Loss: 0.5341\n",
      "Iteration 1700, Loss: 0.5317\n",
      "Iteration 1800, Loss: 0.5295\n",
      "Iteration 1900, Loss: 0.5274\n",
      "Iteration 2000, Loss: 0.5254\n",
      "Iteration 2100, Loss: 0.5235\n",
      "Iteration 2200, Loss: 0.5217\n",
      "Iteration 2300, Loss: 0.5200\n",
      "Iteration 2400, Loss: 0.5184\n",
      "Iteration 2500, Loss: 0.5169\n",
      "Iteration 2600, Loss: 0.5154\n",
      "Iteration 2700, Loss: 0.5140\n",
      "Iteration 2800, Loss: 0.5126\n",
      "Iteration 2900, Loss: 0.5113\n",
      "Iteration 3000, Loss: 0.5100\n",
      "Iteration 3100, Loss: 0.5088\n",
      "Iteration 3200, Loss: 0.5076\n",
      "Iteration 3300, Loss: 0.5065\n",
      "Iteration 3400, Loss: 0.5054\n",
      "Iteration 3500, Loss: 0.5043\n",
      "Iteration 3600, Loss: 0.5033\n",
      "Iteration 3700, Loss: 0.5023\n",
      "Iteration 3800, Loss: 0.5013\n",
      "Iteration 3900, Loss: 0.5003\n",
      "Iteration 0, Loss: 1.0547\n",
      "Iteration 100, Loss: 0.6535\n",
      "Iteration 200, Loss: 0.6194\n",
      "Iteration 300, Loss: 0.6014\n",
      "Iteration 400, Loss: 0.5892\n",
      "Iteration 500, Loss: 0.5799\n",
      "Iteration 600, Loss: 0.5722\n",
      "Iteration 700, Loss: 0.5660\n",
      "Iteration 800, Loss: 0.5607\n",
      "Iteration 900, Loss: 0.5560\n",
      "Iteration 1000, Loss: 0.5518\n",
      "Iteration 1100, Loss: 0.5481\n",
      "Iteration 1200, Loss: 0.5448\n",
      "Iteration 1300, Loss: 0.5417\n",
      "Iteration 1400, Loss: 0.5389\n",
      "Iteration 1500, Loss: 0.5363\n",
      "Iteration 1600, Loss: 0.5339\n",
      "Iteration 1700, Loss: 0.5316\n",
      "Iteration 1800, Loss: 0.5295\n",
      "Iteration 1900, Loss: 0.5275\n",
      "Iteration 2000, Loss: 0.5256\n",
      "Iteration 2100, Loss: 0.5239\n",
      "Iteration 2200, Loss: 0.5222\n",
      "Iteration 2300, Loss: 0.5206\n",
      "Iteration 2400, Loss: 0.5190\n",
      "Iteration 2500, Loss: 0.5175\n",
      "Iteration 2600, Loss: 0.5161\n",
      "Iteration 2700, Loss: 0.5148\n",
      "Iteration 2800, Loss: 0.5135\n",
      "Iteration 2900, Loss: 0.5123\n",
      "Iteration 3000, Loss: 0.5110\n",
      "Iteration 3100, Loss: 0.5099\n",
      "Iteration 3200, Loss: 0.5088\n",
      "Iteration 3300, Loss: 0.5077\n",
      "Iteration 3400, Loss: 0.5066\n",
      "Iteration 3500, Loss: 0.5056\n",
      "Iteration 3600, Loss: 0.5046\n",
      "Iteration 3700, Loss: 0.5036\n",
      "Iteration 3800, Loss: 0.5027\n",
      "Iteration 3900, Loss: 0.5018\n",
      "24 270\n",
      "Iteration 0, Loss: 1.0538\n",
      "Iteration 100, Loss: 0.6648\n",
      "Iteration 200, Loss: 0.6306\n",
      "Iteration 300, Loss: 0.6128\n",
      "Iteration 400, Loss: 0.6005\n",
      "Iteration 500, Loss: 0.5911\n",
      "Iteration 600, Loss: 0.5835\n",
      "Iteration 700, Loss: 0.5771\n",
      "Iteration 800, Loss: 0.5716\n",
      "Iteration 900, Loss: 0.5667\n",
      "Iteration 1000, Loss: 0.5624\n",
      "Iteration 1100, Loss: 0.5585\n",
      "Iteration 1200, Loss: 0.5551\n",
      "Iteration 1300, Loss: 0.5519\n",
      "Iteration 1400, Loss: 0.5488\n",
      "Iteration 1500, Loss: 0.5461\n",
      "Iteration 1600, Loss: 0.5436\n",
      "Iteration 1700, Loss: 0.5411\n",
      "Iteration 1800, Loss: 0.5388\n",
      "Iteration 1900, Loss: 0.5367\n",
      "Iteration 2000, Loss: 0.5346\n",
      "Iteration 2100, Loss: 0.5328\n",
      "Iteration 2200, Loss: 0.5309\n",
      "Iteration 2300, Loss: 0.5292\n",
      "Iteration 2400, Loss: 0.5275\n",
      "Iteration 2500, Loss: 0.5259\n",
      "Iteration 2600, Loss: 0.5244\n",
      "Iteration 2700, Loss: 0.5230\n",
      "Iteration 2800, Loss: 0.5215\n",
      "Iteration 2900, Loss: 0.5203\n",
      "Iteration 3000, Loss: 0.5189\n",
      "Iteration 3100, Loss: 0.5176\n",
      "Iteration 3200, Loss: 0.5164\n",
      "Iteration 3300, Loss: 0.5153\n",
      "Iteration 3400, Loss: 0.5142\n",
      "Iteration 3500, Loss: 0.5131\n",
      "Iteration 3600, Loss: 0.5120\n",
      "Iteration 3700, Loss: 0.5109\n",
      "Iteration 3800, Loss: 0.5100\n",
      "Iteration 3900, Loss: 0.5090\n",
      "Iteration 0, Loss: 1.0527\n",
      "Iteration 100, Loss: 0.6468\n",
      "Iteration 200, Loss: 0.6096\n",
      "Iteration 300, Loss: 0.5904\n",
      "Iteration 400, Loss: 0.5775\n",
      "Iteration 500, Loss: 0.5677\n",
      "Iteration 600, Loss: 0.5598\n",
      "Iteration 700, Loss: 0.5533\n",
      "Iteration 800, Loss: 0.5477\n",
      "Iteration 900, Loss: 0.5430\n",
      "Iteration 1000, Loss: 0.5387\n",
      "Iteration 1100, Loss: 0.5349\n",
      "Iteration 1200, Loss: 0.5316\n",
      "Iteration 1300, Loss: 0.5285\n",
      "Iteration 1400, Loss: 0.5257\n",
      "Iteration 1500, Loss: 0.5231\n",
      "Iteration 1600, Loss: 0.5207\n",
      "Iteration 1700, Loss: 0.5184\n",
      "Iteration 1800, Loss: 0.5163\n",
      "Iteration 1900, Loss: 0.5143\n",
      "Iteration 2000, Loss: 0.5125\n",
      "Iteration 2100, Loss: 0.5107\n",
      "Iteration 2200, Loss: 0.5090\n",
      "Iteration 2300, Loss: 0.5075\n",
      "Iteration 2400, Loss: 0.5060\n",
      "Iteration 2500, Loss: 0.5045\n",
      "Iteration 2600, Loss: 0.5031\n",
      "Iteration 2700, Loss: 0.5018\n",
      "Iteration 2800, Loss: 0.5005\n",
      "Iteration 2900, Loss: 0.4993\n",
      "Iteration 3000, Loss: 0.4981\n",
      "Iteration 3100, Loss: 0.4970\n",
      "Iteration 3200, Loss: 0.4959\n",
      "Iteration 3300, Loss: 0.4948\n",
      "Iteration 3400, Loss: 0.4938\n",
      "Iteration 3500, Loss: 0.4928\n",
      "Iteration 3600, Loss: 0.4918\n",
      "Iteration 3700, Loss: 0.4909\n",
      "Iteration 3800, Loss: 0.4899\n",
      "Iteration 3900, Loss: 0.4891\n",
      "25 270\n",
      "Iteration 0, Loss: 1.0544\n",
      "Iteration 100, Loss: 0.6453\n",
      "Iteration 200, Loss: 0.6100\n",
      "Iteration 300, Loss: 0.5917\n",
      "Iteration 400, Loss: 0.5795\n",
      "Iteration 500, Loss: 0.5706\n",
      "Iteration 600, Loss: 0.5634\n",
      "Iteration 700, Loss: 0.5575\n",
      "Iteration 800, Loss: 0.5525\n",
      "Iteration 900, Loss: 0.5483\n",
      "Iteration 1000, Loss: 0.5445\n",
      "Iteration 1100, Loss: 0.5412\n",
      "Iteration 1200, Loss: 0.5382\n",
      "Iteration 1300, Loss: 0.5355\n",
      "Iteration 1400, Loss: 0.5330\n",
      "Iteration 1500, Loss: 0.5307\n",
      "Iteration 1600, Loss: 0.5286\n",
      "Iteration 1700, Loss: 0.5267\n",
      "Iteration 1800, Loss: 0.5248\n",
      "Iteration 1900, Loss: 0.5231\n",
      "Iteration 2000, Loss: 0.5214\n",
      "Iteration 2100, Loss: 0.5199\n",
      "Iteration 2200, Loss: 0.5184\n",
      "Iteration 2300, Loss: 0.5170\n",
      "Iteration 2400, Loss: 0.5158\n",
      "Iteration 2500, Loss: 0.5144\n",
      "Iteration 2600, Loss: 0.5132\n",
      "Iteration 2700, Loss: 0.5120\n",
      "Iteration 2800, Loss: 0.5110\n",
      "Iteration 2900, Loss: 0.5098\n",
      "Iteration 3000, Loss: 0.5087\n",
      "Iteration 3100, Loss: 0.5077\n",
      "Iteration 3200, Loss: 0.5067\n",
      "Iteration 3300, Loss: 0.5057\n",
      "Iteration 3400, Loss: 0.5048\n",
      "Iteration 3500, Loss: 0.5039\n",
      "Iteration 3600, Loss: 0.5031\n",
      "Iteration 3700, Loss: 0.5022\n",
      "Iteration 3800, Loss: 0.5014\n",
      "Iteration 3900, Loss: 0.5006\n",
      "Iteration 0, Loss: 1.0559\n",
      "Iteration 100, Loss: 0.6681\n",
      "Iteration 200, Loss: 0.6337\n",
      "Iteration 300, Loss: 0.6144\n",
      "Iteration 400, Loss: 0.6009\n",
      "Iteration 500, Loss: 0.5904\n",
      "Iteration 600, Loss: 0.5817\n",
      "Iteration 700, Loss: 0.5742\n",
      "Iteration 800, Loss: 0.5677\n",
      "Iteration 900, Loss: 0.5621\n",
      "Iteration 1000, Loss: 0.5571\n",
      "Iteration 1100, Loss: 0.5526\n",
      "Iteration 1200, Loss: 0.5483\n",
      "Iteration 1300, Loss: 0.5445\n",
      "Iteration 1400, Loss: 0.5410\n",
      "Iteration 1500, Loss: 0.5377\n",
      "Iteration 1600, Loss: 0.5347\n",
      "Iteration 1700, Loss: 0.5318\n",
      "Iteration 1800, Loss: 0.5292\n",
      "Iteration 1900, Loss: 0.5267\n",
      "Iteration 2000, Loss: 0.5243\n",
      "Iteration 2100, Loss: 0.5221\n",
      "Iteration 2200, Loss: 0.5200\n",
      "Iteration 2300, Loss: 0.5180\n",
      "Iteration 2400, Loss: 0.5161\n",
      "Iteration 2500, Loss: 0.5142\n",
      "Iteration 2600, Loss: 0.5125\n",
      "Iteration 2700, Loss: 0.5108\n",
      "Iteration 2800, Loss: 0.5091\n",
      "Iteration 2900, Loss: 0.5076\n",
      "Iteration 3000, Loss: 0.5061\n",
      "Iteration 3100, Loss: 0.5046\n",
      "Iteration 3200, Loss: 0.5033\n",
      "Iteration 3300, Loss: 0.5019\n",
      "Iteration 3400, Loss: 0.5006\n",
      "Iteration 3500, Loss: 0.4994\n",
      "Iteration 3600, Loss: 0.4982\n",
      "Iteration 3700, Loss: 0.4970\n",
      "Iteration 3800, Loss: 0.4959\n",
      "Iteration 3900, Loss: 0.4948\n",
      "26 270\n",
      "Iteration 0, Loss: 1.0548\n",
      "Iteration 100, Loss: 0.6663\n",
      "Iteration 200, Loss: 0.6326\n",
      "Iteration 300, Loss: 0.6152\n",
      "Iteration 400, Loss: 0.6033\n",
      "Iteration 500, Loss: 0.5942\n",
      "Iteration 600, Loss: 0.5866\n",
      "Iteration 700, Loss: 0.5803\n",
      "Iteration 800, Loss: 0.5749\n",
      "Iteration 900, Loss: 0.5701\n",
      "Iteration 1000, Loss: 0.5658\n",
      "Iteration 1100, Loss: 0.5621\n",
      "Iteration 1200, Loss: 0.5586\n",
      "Iteration 1300, Loss: 0.5554\n",
      "Iteration 1400, Loss: 0.5525\n",
      "Iteration 1500, Loss: 0.5497\n",
      "Iteration 1600, Loss: 0.5472\n",
      "Iteration 1700, Loss: 0.5449\n",
      "Iteration 1800, Loss: 0.5427\n",
      "Iteration 1900, Loss: 0.5406\n",
      "Iteration 2000, Loss: 0.5386\n",
      "Iteration 2100, Loss: 0.5367\n",
      "Iteration 2200, Loss: 0.5350\n",
      "Iteration 2300, Loss: 0.5333\n",
      "Iteration 2400, Loss: 0.5317\n",
      "Iteration 2500, Loss: 0.5301\n",
      "Iteration 2600, Loss: 0.5286\n",
      "Iteration 2700, Loss: 0.5272\n",
      "Iteration 2800, Loss: 0.5258\n",
      "Iteration 2900, Loss: 0.5245\n",
      "Iteration 3000, Loss: 0.5233\n",
      "Iteration 3100, Loss: 0.5220\n",
      "Iteration 3200, Loss: 0.5208\n",
      "Iteration 3300, Loss: 0.5197\n",
      "Iteration 3400, Loss: 0.5186\n",
      "Iteration 3500, Loss: 0.5175\n",
      "Iteration 3600, Loss: 0.5165\n",
      "Iteration 3700, Loss: 0.5155\n",
      "Iteration 3800, Loss: 0.5145\n",
      "Iteration 3900, Loss: 0.5135\n",
      "Iteration 0, Loss: 1.0528\n",
      "Iteration 100, Loss: 0.6480\n",
      "Iteration 200, Loss: 0.6118\n",
      "Iteration 300, Loss: 0.5923\n",
      "Iteration 400, Loss: 0.5790\n",
      "Iteration 500, Loss: 0.5691\n",
      "Iteration 600, Loss: 0.5611\n",
      "Iteration 700, Loss: 0.5546\n",
      "Iteration 800, Loss: 0.5491\n",
      "Iteration 900, Loss: 0.5444\n",
      "Iteration 1000, Loss: 0.5401\n",
      "Iteration 1100, Loss: 0.5363\n",
      "Iteration 1200, Loss: 0.5330\n",
      "Iteration 1300, Loss: 0.5299\n",
      "Iteration 1400, Loss: 0.5271\n",
      "Iteration 1500, Loss: 0.5245\n",
      "Iteration 1600, Loss: 0.5221\n",
      "Iteration 1700, Loss: 0.5200\n",
      "Iteration 1800, Loss: 0.5178\n",
      "Iteration 1900, Loss: 0.5159\n",
      "Iteration 2000, Loss: 0.5141\n",
      "Iteration 2100, Loss: 0.5123\n",
      "Iteration 2200, Loss: 0.5106\n",
      "Iteration 2300, Loss: 0.5090\n",
      "Iteration 2400, Loss: 0.5075\n",
      "Iteration 2500, Loss: 0.5061\n",
      "Iteration 2600, Loss: 0.5047\n",
      "Iteration 2700, Loss: 0.5034\n",
      "Iteration 2800, Loss: 0.5021\n",
      "Iteration 2900, Loss: 0.5009\n",
      "Iteration 3000, Loss: 0.4997\n",
      "Iteration 3100, Loss: 0.4986\n",
      "Iteration 3200, Loss: 0.4975\n",
      "Iteration 3300, Loss: 0.4964\n",
      "Iteration 3400, Loss: 0.4954\n",
      "Iteration 3500, Loss: 0.4944\n",
      "Iteration 3600, Loss: 0.4934\n",
      "Iteration 3700, Loss: 0.4925\n",
      "Iteration 3800, Loss: 0.4916\n",
      "Iteration 3900, Loss: 0.4907\n",
      "27 270\n",
      "Iteration 0, Loss: 1.0561\n",
      "Iteration 100, Loss: 0.6429\n",
      "Iteration 200, Loss: 0.6042\n",
      "Iteration 300, Loss: 0.5842\n",
      "Iteration 400, Loss: 0.5703\n",
      "Iteration 500, Loss: 0.5597\n",
      "Iteration 600, Loss: 0.5511\n",
      "Iteration 700, Loss: 0.5439\n",
      "Iteration 800, Loss: 0.5378\n",
      "Iteration 900, Loss: 0.5324\n",
      "Iteration 1000, Loss: 0.5276\n",
      "Iteration 1100, Loss: 0.5234\n",
      "Iteration 1200, Loss: 0.5195\n",
      "Iteration 1300, Loss: 0.5161\n",
      "Iteration 1400, Loss: 0.5129\n",
      "Iteration 1500, Loss: 0.5099\n",
      "Iteration 1600, Loss: 0.5072\n",
      "Iteration 1700, Loss: 0.5047\n",
      "Iteration 1800, Loss: 0.5023\n",
      "Iteration 1900, Loss: 0.5001\n",
      "Iteration 2000, Loss: 0.4980\n",
      "Iteration 2100, Loss: 0.4960\n",
      "Iteration 2200, Loss: 0.4942\n",
      "Iteration 2300, Loss: 0.4924\n",
      "Iteration 2400, Loss: 0.4907\n",
      "Iteration 2500, Loss: 0.4891\n",
      "Iteration 2600, Loss: 0.4876\n",
      "Iteration 2700, Loss: 0.4861\n",
      "Iteration 2800, Loss: 0.4847\n",
      "Iteration 2900, Loss: 0.4833\n",
      "Iteration 3000, Loss: 0.4820\n",
      "Iteration 3100, Loss: 0.4807\n",
      "Iteration 3200, Loss: 0.4795\n",
      "Iteration 3300, Loss: 0.4784\n",
      "Iteration 3400, Loss: 0.4772\n",
      "Iteration 3500, Loss: 0.4761\n",
      "Iteration 3600, Loss: 0.4750\n",
      "Iteration 3700, Loss: 0.4739\n",
      "Iteration 3800, Loss: 0.4730\n",
      "Iteration 3900, Loss: 0.4720\n",
      "Iteration 0, Loss: 1.0540\n",
      "Iteration 100, Loss: 0.6679\n",
      "Iteration 200, Loss: 0.6347\n",
      "Iteration 300, Loss: 0.6170\n",
      "Iteration 400, Loss: 0.6049\n",
      "Iteration 500, Loss: 0.5958\n",
      "Iteration 600, Loss: 0.5886\n",
      "Iteration 700, Loss: 0.5826\n",
      "Iteration 800, Loss: 0.5774\n",
      "Iteration 900, Loss: 0.5729\n",
      "Iteration 1000, Loss: 0.5689\n",
      "Iteration 1100, Loss: 0.5654\n",
      "Iteration 1200, Loss: 0.5622\n",
      "Iteration 1300, Loss: 0.5592\n",
      "Iteration 1400, Loss: 0.5565\n",
      "Iteration 1500, Loss: 0.5539\n",
      "Iteration 1600, Loss: 0.5516\n",
      "Iteration 1700, Loss: 0.5494\n",
      "Iteration 1800, Loss: 0.5472\n",
      "Iteration 1900, Loss: 0.5453\n",
      "Iteration 2000, Loss: 0.5434\n",
      "Iteration 2100, Loss: 0.5416\n",
      "Iteration 2200, Loss: 0.5399\n",
      "Iteration 2300, Loss: 0.5382\n",
      "Iteration 2400, Loss: 0.5367\n",
      "Iteration 2500, Loss: 0.5351\n",
      "Iteration 2600, Loss: 0.5337\n",
      "Iteration 2700, Loss: 0.5323\n",
      "Iteration 2800, Loss: 0.5310\n",
      "Iteration 2900, Loss: 0.5297\n",
      "Iteration 3000, Loss: 0.5285\n",
      "Iteration 3100, Loss: 0.5273\n",
      "Iteration 3200, Loss: 0.5261\n",
      "Iteration 3300, Loss: 0.5250\n",
      "Iteration 3400, Loss: 0.5239\n",
      "Iteration 3500, Loss: 0.5228\n",
      "Iteration 3600, Loss: 0.5218\n",
      "Iteration 3700, Loss: 0.5208\n",
      "Iteration 3800, Loss: 0.5198\n",
      "Iteration 3900, Loss: 0.5188\n",
      "28 270\n",
      "Iteration 0, Loss: 1.0565\n",
      "Iteration 100, Loss: 0.6562\n",
      "Iteration 200, Loss: 0.6185\n",
      "Iteration 300, Loss: 0.5988\n",
      "Iteration 400, Loss: 0.5854\n",
      "Iteration 500, Loss: 0.5751\n",
      "Iteration 600, Loss: 0.5669\n",
      "Iteration 700, Loss: 0.5599\n",
      "Iteration 800, Loss: 0.5540\n",
      "Iteration 900, Loss: 0.5488\n",
      "Iteration 1000, Loss: 0.5442\n",
      "Iteration 1100, Loss: 0.5401\n",
      "Iteration 1200, Loss: 0.5364\n",
      "Iteration 1300, Loss: 0.5330\n",
      "Iteration 1400, Loss: 0.5299\n",
      "Iteration 1500, Loss: 0.5271\n",
      "Iteration 1600, Loss: 0.5244\n",
      "Iteration 1700, Loss: 0.5220\n",
      "Iteration 1800, Loss: 0.5197\n",
      "Iteration 1900, Loss: 0.5175\n",
      "Iteration 2000, Loss: 0.5155\n",
      "Iteration 2100, Loss: 0.5136\n",
      "Iteration 2200, Loss: 0.5118\n",
      "Iteration 2300, Loss: 0.5101\n",
      "Iteration 2400, Loss: 0.5084\n",
      "Iteration 2500, Loss: 0.5069\n",
      "Iteration 2600, Loss: 0.5054\n",
      "Iteration 2700, Loss: 0.5040\n",
      "Iteration 2800, Loss: 0.5026\n",
      "Iteration 2900, Loss: 0.5013\n",
      "Iteration 3000, Loss: 0.5000\n",
      "Iteration 3100, Loss: 0.4988\n",
      "Iteration 3200, Loss: 0.4976\n",
      "Iteration 3300, Loss: 0.4966\n",
      "Iteration 3400, Loss: 0.4954\n",
      "Iteration 3500, Loss: 0.4943\n",
      "Iteration 3600, Loss: 0.4933\n",
      "Iteration 3700, Loss: 0.4923\n",
      "Iteration 3800, Loss: 0.4913\n",
      "Iteration 3900, Loss: 0.4904\n",
      "Iteration 0, Loss: 1.0510\n",
      "Iteration 100, Loss: 0.6544\n",
      "Iteration 200, Loss: 0.6204\n",
      "Iteration 300, Loss: 0.6025\n",
      "Iteration 400, Loss: 0.5901\n",
      "Iteration 500, Loss: 0.5807\n",
      "Iteration 600, Loss: 0.5731\n",
      "Iteration 700, Loss: 0.5669\n",
      "Iteration 800, Loss: 0.5615\n",
      "Iteration 900, Loss: 0.5568\n",
      "Iteration 1000, Loss: 0.5527\n",
      "Iteration 1100, Loss: 0.5490\n",
      "Iteration 1200, Loss: 0.5456\n",
      "Iteration 1300, Loss: 0.5426\n",
      "Iteration 1400, Loss: 0.5397\n",
      "Iteration 1500, Loss: 0.5372\n",
      "Iteration 1600, Loss: 0.5347\n",
      "Iteration 1700, Loss: 0.5324\n",
      "Iteration 1800, Loss: 0.5303\n",
      "Iteration 1900, Loss: 0.5283\n",
      "Iteration 2000, Loss: 0.5265\n",
      "Iteration 2100, Loss: 0.5247\n",
      "Iteration 2200, Loss: 0.5230\n",
      "Iteration 2300, Loss: 0.5213\n",
      "Iteration 2400, Loss: 0.5198\n",
      "Iteration 2500, Loss: 0.5183\n",
      "Iteration 2600, Loss: 0.5169\n",
      "Iteration 2700, Loss: 0.5156\n",
      "Iteration 2800, Loss: 0.5142\n",
      "Iteration 2900, Loss: 0.5130\n",
      "Iteration 3000, Loss: 0.5118\n",
      "Iteration 3100, Loss: 0.5106\n",
      "Iteration 3200, Loss: 0.5095\n",
      "Iteration 3300, Loss: 0.5084\n",
      "Iteration 3400, Loss: 0.5073\n",
      "Iteration 3500, Loss: 0.5063\n",
      "Iteration 3600, Loss: 0.5054\n",
      "Iteration 3700, Loss: 0.5044\n",
      "Iteration 3800, Loss: 0.5035\n",
      "Iteration 3900, Loss: 0.5026\n",
      "29 270\n",
      "Iteration 0, Loss: 1.0554\n",
      "Iteration 100, Loss: 0.6571\n",
      "Iteration 200, Loss: 0.6217\n",
      "Iteration 300, Loss: 0.6031\n",
      "Iteration 400, Loss: 0.5902\n",
      "Iteration 500, Loss: 0.5803\n",
      "Iteration 600, Loss: 0.5723\n",
      "Iteration 700, Loss: 0.5655\n",
      "Iteration 800, Loss: 0.5597\n",
      "Iteration 900, Loss: 0.5546\n",
      "Iteration 1000, Loss: 0.5501\n",
      "Iteration 1100, Loss: 0.5460\n",
      "Iteration 1200, Loss: 0.5424\n",
      "Iteration 1300, Loss: 0.5390\n",
      "Iteration 1400, Loss: 0.5359\n",
      "Iteration 1500, Loss: 0.5330\n",
      "Iteration 1600, Loss: 0.5304\n",
      "Iteration 1700, Loss: 0.5279\n",
      "Iteration 1800, Loss: 0.5256\n",
      "Iteration 1900, Loss: 0.5235\n",
      "Iteration 2000, Loss: 0.5213\n",
      "Iteration 2100, Loss: 0.5193\n",
      "Iteration 2200, Loss: 0.5175\n",
      "Iteration 2300, Loss: 0.5158\n",
      "Iteration 2400, Loss: 0.5141\n",
      "Iteration 2500, Loss: 0.5125\n",
      "Iteration 2600, Loss: 0.5110\n",
      "Iteration 2700, Loss: 0.5095\n",
      "Iteration 2800, Loss: 0.5081\n",
      "Iteration 2900, Loss: 0.5067\n",
      "Iteration 3000, Loss: 0.5054\n",
      "Iteration 3100, Loss: 0.5042\n",
      "Iteration 3200, Loss: 0.5030\n",
      "Iteration 3300, Loss: 0.5018\n",
      "Iteration 3400, Loss: 0.5007\n",
      "Iteration 3500, Loss: 0.4996\n",
      "Iteration 3600, Loss: 0.4985\n",
      "Iteration 3700, Loss: 0.4975\n",
      "Iteration 3800, Loss: 0.4965\n",
      "Iteration 3900, Loss: 0.4955\n",
      "Iteration 0, Loss: 1.0527\n",
      "Iteration 100, Loss: 0.6570\n",
      "Iteration 200, Loss: 0.6219\n",
      "Iteration 300, Loss: 0.6028\n",
      "Iteration 400, Loss: 0.5897\n",
      "Iteration 500, Loss: 0.5797\n",
      "Iteration 600, Loss: 0.5715\n",
      "Iteration 700, Loss: 0.5648\n",
      "Iteration 800, Loss: 0.5590\n",
      "Iteration 900, Loss: 0.5538\n",
      "Iteration 1000, Loss: 0.5493\n",
      "Iteration 1100, Loss: 0.5452\n",
      "Iteration 1200, Loss: 0.5415\n",
      "Iteration 1300, Loss: 0.5380\n",
      "Iteration 1400, Loss: 0.5349\n",
      "Iteration 1500, Loss: 0.5320\n",
      "Iteration 1600, Loss: 0.5292\n",
      "Iteration 1700, Loss: 0.5266\n",
      "Iteration 1800, Loss: 0.5242\n",
      "Iteration 1900, Loss: 0.5219\n",
      "Iteration 2000, Loss: 0.5197\n",
      "Iteration 2100, Loss: 0.5177\n",
      "Iteration 2200, Loss: 0.5157\n",
      "Iteration 2300, Loss: 0.5138\n",
      "Iteration 2400, Loss: 0.5120\n",
      "Iteration 2500, Loss: 0.5103\n",
      "Iteration 2600, Loss: 0.5087\n",
      "Iteration 2700, Loss: 0.5071\n",
      "Iteration 2800, Loss: 0.5055\n",
      "Iteration 2900, Loss: 0.5040\n",
      "Iteration 3000, Loss: 0.5026\n",
      "Iteration 3100, Loss: 0.5012\n",
      "Iteration 3200, Loss: 0.4999\n",
      "Iteration 3300, Loss: 0.4987\n",
      "Iteration 3400, Loss: 0.4973\n",
      "Iteration 3500, Loss: 0.4962\n",
      "Iteration 3600, Loss: 0.4949\n",
      "Iteration 3700, Loss: 0.4938\n",
      "Iteration 3800, Loss: 0.4927\n",
      "Iteration 3900, Loss: 0.4916\n",
      "30 270\n",
      "Iteration 0, Loss: 1.0887\n",
      "Iteration 100, Loss: 0.7744\n",
      "Iteration 200, Loss: 0.7072\n",
      "Iteration 300, Loss: 0.6752\n",
      "Iteration 400, Loss: 0.6553\n",
      "Iteration 500, Loss: 0.6413\n",
      "Iteration 600, Loss: 0.6306\n",
      "Iteration 700, Loss: 0.6221\n",
      "Iteration 800, Loss: 0.6150\n",
      "Iteration 900, Loss: 0.6091\n",
      "Iteration 1000, Loss: 0.6039\n",
      "Iteration 1100, Loss: 0.5994\n",
      "Iteration 1200, Loss: 0.5953\n",
      "Iteration 1300, Loss: 0.5917\n",
      "Iteration 1400, Loss: 0.5884\n",
      "Iteration 1500, Loss: 0.5853\n",
      "Iteration 1600, Loss: 0.5825\n",
      "Iteration 1700, Loss: 0.5799\n",
      "Iteration 1800, Loss: 0.5775\n",
      "Iteration 1900, Loss: 0.5753\n",
      "Iteration 2000, Loss: 0.5731\n",
      "Iteration 2100, Loss: 0.5711\n",
      "Iteration 2200, Loss: 0.5692\n",
      "Iteration 2300, Loss: 0.5674\n",
      "Iteration 2400, Loss: 0.5657\n",
      "Iteration 2500, Loss: 0.5641\n",
      "Iteration 2600, Loss: 0.5625\n",
      "Iteration 2700, Loss: 0.5611\n",
      "Iteration 2800, Loss: 0.5596\n",
      "Iteration 2900, Loss: 0.5583\n",
      "Iteration 3000, Loss: 0.5570\n",
      "Iteration 3100, Loss: 0.5557\n",
      "Iteration 3200, Loss: 0.5545\n",
      "Iteration 3300, Loss: 0.5533\n",
      "Iteration 3400, Loss: 0.5522\n",
      "Iteration 3500, Loss: 0.5510\n",
      "Iteration 3600, Loss: 0.5500\n",
      "Iteration 3700, Loss: 0.5490\n",
      "Iteration 3800, Loss: 0.5480\n",
      "Iteration 3900, Loss: 0.5470\n",
      "Iteration 0, Loss: 1.0898\n",
      "Iteration 100, Loss: 0.7978\n",
      "Iteration 200, Loss: 0.7350\n",
      "Iteration 300, Loss: 0.7071\n",
      "Iteration 400, Loss: 0.6900\n",
      "Iteration 500, Loss: 0.6778\n",
      "Iteration 600, Loss: 0.6683\n",
      "Iteration 700, Loss: 0.6607\n",
      "Iteration 800, Loss: 0.6543\n",
      "Iteration 900, Loss: 0.6488\n",
      "Iteration 1000, Loss: 0.6440\n",
      "Iteration 1100, Loss: 0.6397\n",
      "Iteration 1200, Loss: 0.6359\n",
      "Iteration 1300, Loss: 0.6324\n",
      "Iteration 1400, Loss: 0.6292\n",
      "Iteration 1500, Loss: 0.6262\n",
      "Iteration 1600, Loss: 0.6235\n",
      "Iteration 1700, Loss: 0.6209\n",
      "Iteration 1800, Loss: 0.6184\n",
      "Iteration 1900, Loss: 0.6161\n",
      "Iteration 2000, Loss: 0.6140\n",
      "Iteration 2100, Loss: 0.6119\n",
      "Iteration 2200, Loss: 0.6100\n",
      "Iteration 2300, Loss: 0.6082\n",
      "Iteration 2400, Loss: 0.6064\n",
      "Iteration 2500, Loss: 0.6047\n",
      "Iteration 2600, Loss: 0.6031\n",
      "Iteration 2700, Loss: 0.6015\n",
      "Iteration 2800, Loss: 0.6000\n",
      "Iteration 2900, Loss: 0.5986\n",
      "Iteration 3000, Loss: 0.5972\n",
      "Iteration 3100, Loss: 0.5959\n",
      "Iteration 3200, Loss: 0.5946\n",
      "Iteration 3300, Loss: 0.5934\n",
      "Iteration 3400, Loss: 0.5922\n",
      "Iteration 3500, Loss: 0.5910\n",
      "Iteration 3600, Loss: 0.5899\n",
      "Iteration 3700, Loss: 0.5888\n",
      "Iteration 3800, Loss: 0.5877\n",
      "Iteration 3900, Loss: 0.5867\n",
      "31 270\n",
      "Iteration 0, Loss: 1.0886\n",
      "Iteration 100, Loss: 0.7728\n",
      "Iteration 200, Loss: 0.7072\n",
      "Iteration 300, Loss: 0.6777\n",
      "Iteration 400, Loss: 0.6599\n",
      "Iteration 500, Loss: 0.6475\n",
      "Iteration 600, Loss: 0.6380\n",
      "Iteration 700, Loss: 0.6303\n",
      "Iteration 800, Loss: 0.6240\n",
      "Iteration 900, Loss: 0.6186\n",
      "Iteration 1000, Loss: 0.6138\n",
      "Iteration 1100, Loss: 0.6096\n",
      "Iteration 1200, Loss: 0.6058\n",
      "Iteration 1300, Loss: 0.6024\n",
      "Iteration 1400, Loss: 0.5992\n",
      "Iteration 1500, Loss: 0.5963\n",
      "Iteration 1600, Loss: 0.5936\n",
      "Iteration 1700, Loss: 0.5911\n",
      "Iteration 1800, Loss: 0.5887\n",
      "Iteration 1900, Loss: 0.5865\n",
      "Iteration 2000, Loss: 0.5844\n",
      "Iteration 2100, Loss: 0.5824\n",
      "Iteration 2200, Loss: 0.5805\n",
      "Iteration 2300, Loss: 0.5787\n",
      "Iteration 2400, Loss: 0.5770\n",
      "Iteration 2500, Loss: 0.5754\n",
      "Iteration 2600, Loss: 0.5738\n",
      "Iteration 2700, Loss: 0.5724\n",
      "Iteration 2800, Loss: 0.5709\n",
      "Iteration 2900, Loss: 0.5696\n",
      "Iteration 3000, Loss: 0.5683\n",
      "Iteration 3100, Loss: 0.5670\n",
      "Iteration 3200, Loss: 0.5658\n",
      "Iteration 3300, Loss: 0.5646\n",
      "Iteration 3400, Loss: 0.5635\n",
      "Iteration 3500, Loss: 0.5624\n",
      "Iteration 3600, Loss: 0.5613\n",
      "Iteration 3700, Loss: 0.5603\n",
      "Iteration 3800, Loss: 0.5593\n",
      "Iteration 3900, Loss: 0.5583\n",
      "Iteration 0, Loss: 1.0898\n",
      "Iteration 100, Loss: 0.8001\n",
      "Iteration 200, Loss: 0.7352\n",
      "Iteration 300, Loss: 0.7049\n",
      "Iteration 400, Loss: 0.6861\n",
      "Iteration 500, Loss: 0.6726\n",
      "Iteration 600, Loss: 0.6622\n",
      "Iteration 700, Loss: 0.6537\n",
      "Iteration 800, Loss: 0.6467\n",
      "Iteration 900, Loss: 0.6407\n",
      "Iteration 1000, Loss: 0.6354\n",
      "Iteration 1100, Loss: 0.6308\n",
      "Iteration 1200, Loss: 0.6267\n",
      "Iteration 1300, Loss: 0.6229\n",
      "Iteration 1400, Loss: 0.6194\n",
      "Iteration 1500, Loss: 0.6163\n",
      "Iteration 1600, Loss: 0.6133\n",
      "Iteration 1700, Loss: 0.6106\n",
      "Iteration 1800, Loss: 0.6080\n",
      "Iteration 1900, Loss: 0.6056\n",
      "Iteration 2000, Loss: 0.6033\n",
      "Iteration 2100, Loss: 0.6012\n",
      "Iteration 2200, Loss: 0.5991\n",
      "Iteration 2300, Loss: 0.5972\n",
      "Iteration 2400, Loss: 0.5953\n",
      "Iteration 2500, Loss: 0.5935\n",
      "Iteration 2600, Loss: 0.5918\n",
      "Iteration 2700, Loss: 0.5902\n",
      "Iteration 2800, Loss: 0.5886\n",
      "Iteration 2900, Loss: 0.5871\n",
      "Iteration 3000, Loss: 0.5857\n",
      "Iteration 3100, Loss: 0.5843\n",
      "Iteration 3200, Loss: 0.5830\n",
      "Iteration 3300, Loss: 0.5817\n",
      "Iteration 3400, Loss: 0.5804\n",
      "Iteration 3500, Loss: 0.5792\n",
      "Iteration 3600, Loss: 0.5780\n",
      "Iteration 3700, Loss: 0.5769\n",
      "Iteration 3800, Loss: 0.5758\n",
      "Iteration 3900, Loss: 0.5747\n",
      "32 270\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7884\n",
      "Iteration 200, Loss: 0.7239\n",
      "Iteration 300, Loss: 0.6940\n",
      "Iteration 400, Loss: 0.6755\n",
      "Iteration 500, Loss: 0.6623\n",
      "Iteration 600, Loss: 0.6523\n",
      "Iteration 700, Loss: 0.6442\n",
      "Iteration 800, Loss: 0.6376\n",
      "Iteration 900, Loss: 0.6319\n",
      "Iteration 1000, Loss: 0.6271\n",
      "Iteration 1100, Loss: 0.6228\n",
      "Iteration 1200, Loss: 0.6189\n",
      "Iteration 1300, Loss: 0.6155\n",
      "Iteration 1400, Loss: 0.6124\n",
      "Iteration 1500, Loss: 0.6095\n",
      "Iteration 1600, Loss: 0.6068\n",
      "Iteration 1700, Loss: 0.6044\n",
      "Iteration 1800, Loss: 0.6021\n",
      "Iteration 1900, Loss: 0.5999\n",
      "Iteration 2000, Loss: 0.5979\n",
      "Iteration 2100, Loss: 0.5960\n",
      "Iteration 2200, Loss: 0.5942\n",
      "Iteration 2300, Loss: 0.5924\n",
      "Iteration 2400, Loss: 0.5908\n",
      "Iteration 2500, Loss: 0.5892\n",
      "Iteration 2600, Loss: 0.5877\n",
      "Iteration 2700, Loss: 0.5863\n",
      "Iteration 2800, Loss: 0.5849\n",
      "Iteration 2900, Loss: 0.5836\n",
      "Iteration 3000, Loss: 0.5824\n",
      "Iteration 3100, Loss: 0.5811\n",
      "Iteration 3200, Loss: 0.5800\n",
      "Iteration 3300, Loss: 0.5788\n",
      "Iteration 3400, Loss: 0.5777\n",
      "Iteration 3500, Loss: 0.5766\n",
      "Iteration 3600, Loss: 0.5756\n",
      "Iteration 3700, Loss: 0.5746\n",
      "Iteration 3800, Loss: 0.5736\n",
      "Iteration 3900, Loss: 0.5727\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7847\n",
      "Iteration 200, Loss: 0.7180\n",
      "Iteration 300, Loss: 0.6876\n",
      "Iteration 400, Loss: 0.6692\n",
      "Iteration 500, Loss: 0.6560\n",
      "Iteration 600, Loss: 0.6459\n",
      "Iteration 700, Loss: 0.6376\n",
      "Iteration 800, Loss: 0.6307\n",
      "Iteration 900, Loss: 0.6248\n",
      "Iteration 1000, Loss: 0.6195\n",
      "Iteration 1100, Loss: 0.6149\n",
      "Iteration 1200, Loss: 0.6107\n",
      "Iteration 1300, Loss: 0.6068\n",
      "Iteration 1400, Loss: 0.6033\n",
      "Iteration 1500, Loss: 0.6000\n",
      "Iteration 1600, Loss: 0.5970\n",
      "Iteration 1700, Loss: 0.5941\n",
      "Iteration 1800, Loss: 0.5915\n",
      "Iteration 1900, Loss: 0.5889\n",
      "Iteration 2000, Loss: 0.5866\n",
      "Iteration 2100, Loss: 0.5843\n",
      "Iteration 2200, Loss: 0.5822\n",
      "Iteration 2300, Loss: 0.5802\n",
      "Iteration 2400, Loss: 0.5782\n",
      "Iteration 2500, Loss: 0.5764\n",
      "Iteration 2600, Loss: 0.5746\n",
      "Iteration 2700, Loss: 0.5729\n",
      "Iteration 2800, Loss: 0.5713\n",
      "Iteration 2900, Loss: 0.5698\n",
      "Iteration 3000, Loss: 0.5682\n",
      "Iteration 3100, Loss: 0.5668\n",
      "Iteration 3200, Loss: 0.5654\n",
      "Iteration 3300, Loss: 0.5641\n",
      "Iteration 3400, Loss: 0.5628\n",
      "Iteration 3500, Loss: 0.5615\n",
      "Iteration 3600, Loss: 0.5603\n",
      "Iteration 3700, Loss: 0.5591\n",
      "Iteration 3800, Loss: 0.5579\n",
      "Iteration 3900, Loss: 0.5568\n",
      "33 270\n",
      "Iteration 0, Loss: 1.0897\n",
      "Iteration 100, Loss: 0.7870\n",
      "Iteration 200, Loss: 0.7209\n",
      "Iteration 300, Loss: 0.6910\n",
      "Iteration 400, Loss: 0.6729\n",
      "Iteration 500, Loss: 0.6601\n",
      "Iteration 600, Loss: 0.6503\n",
      "Iteration 700, Loss: 0.6425\n",
      "Iteration 800, Loss: 0.6359\n",
      "Iteration 900, Loss: 0.6303\n",
      "Iteration 1000, Loss: 0.6254\n",
      "Iteration 1100, Loss: 0.6210\n",
      "Iteration 1200, Loss: 0.6171\n",
      "Iteration 1300, Loss: 0.6135\n",
      "Iteration 1400, Loss: 0.6102\n",
      "Iteration 1500, Loss: 0.6072\n",
      "Iteration 1600, Loss: 0.6044\n",
      "Iteration 1700, Loss: 0.6018\n",
      "Iteration 1800, Loss: 0.5994\n",
      "Iteration 1900, Loss: 0.5971\n",
      "Iteration 2000, Loss: 0.5949\n",
      "Iteration 2100, Loss: 0.5928\n",
      "Iteration 2200, Loss: 0.5909\n",
      "Iteration 2300, Loss: 0.5890\n",
      "Iteration 2400, Loss: 0.5872\n",
      "Iteration 2500, Loss: 0.5855\n",
      "Iteration 2600, Loss: 0.5839\n",
      "Iteration 2700, Loss: 0.5824\n",
      "Iteration 2800, Loss: 0.5809\n",
      "Iteration 2900, Loss: 0.5794\n",
      "Iteration 3000, Loss: 0.5781\n",
      "Iteration 3100, Loss: 0.5767\n",
      "Iteration 3200, Loss: 0.5754\n",
      "Iteration 3300, Loss: 0.5742\n",
      "Iteration 3400, Loss: 0.5730\n",
      "Iteration 3500, Loss: 0.5718\n",
      "Iteration 3600, Loss: 0.5707\n",
      "Iteration 3700, Loss: 0.5696\n",
      "Iteration 3800, Loss: 0.5686\n",
      "Iteration 3900, Loss: 0.5675\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7885\n",
      "Iteration 200, Loss: 0.7234\n",
      "Iteration 300, Loss: 0.6932\n",
      "Iteration 400, Loss: 0.6744\n",
      "Iteration 500, Loss: 0.6610\n",
      "Iteration 600, Loss: 0.6508\n",
      "Iteration 700, Loss: 0.6425\n",
      "Iteration 800, Loss: 0.6356\n",
      "Iteration 900, Loss: 0.6298\n",
      "Iteration 1000, Loss: 0.6246\n",
      "Iteration 1100, Loss: 0.6201\n",
      "Iteration 1200, Loss: 0.6160\n",
      "Iteration 1300, Loss: 0.6124\n",
      "Iteration 1400, Loss: 0.6090\n",
      "Iteration 1500, Loss: 0.6059\n",
      "Iteration 1600, Loss: 0.6030\n",
      "Iteration 1700, Loss: 0.6004\n",
      "Iteration 1800, Loss: 0.5979\n",
      "Iteration 1900, Loss: 0.5955\n",
      "Iteration 2000, Loss: 0.5933\n",
      "Iteration 2100, Loss: 0.5913\n",
      "Iteration 2200, Loss: 0.5893\n",
      "Iteration 2300, Loss: 0.5874\n",
      "Iteration 2400, Loss: 0.5857\n",
      "Iteration 2500, Loss: 0.5840\n",
      "Iteration 2600, Loss: 0.5823\n",
      "Iteration 2700, Loss: 0.5808\n",
      "Iteration 2800, Loss: 0.5793\n",
      "Iteration 2900, Loss: 0.5779\n",
      "Iteration 3000, Loss: 0.5765\n",
      "Iteration 3100, Loss: 0.5752\n",
      "Iteration 3200, Loss: 0.5739\n",
      "Iteration 3300, Loss: 0.5727\n",
      "Iteration 3400, Loss: 0.5716\n",
      "Iteration 3500, Loss: 0.5704\n",
      "Iteration 3600, Loss: 0.5693\n",
      "Iteration 3700, Loss: 0.5683\n",
      "Iteration 3800, Loss: 0.5672\n",
      "Iteration 3900, Loss: 0.5662\n",
      "34 270\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7888\n",
      "Iteration 200, Loss: 0.7246\n",
      "Iteration 300, Loss: 0.6951\n",
      "Iteration 400, Loss: 0.6770\n",
      "Iteration 500, Loss: 0.6641\n",
      "Iteration 600, Loss: 0.6542\n",
      "Iteration 700, Loss: 0.6463\n",
      "Iteration 800, Loss: 0.6398\n",
      "Iteration 900, Loss: 0.6342\n",
      "Iteration 1000, Loss: 0.6294\n",
      "Iteration 1100, Loss: 0.6252\n",
      "Iteration 1200, Loss: 0.6215\n",
      "Iteration 1300, Loss: 0.6181\n",
      "Iteration 1400, Loss: 0.6150\n",
      "Iteration 1500, Loss: 0.6122\n",
      "Iteration 1600, Loss: 0.6096\n",
      "Iteration 1700, Loss: 0.6071\n",
      "Iteration 1800, Loss: 0.6049\n",
      "Iteration 1900, Loss: 0.6027\n",
      "Iteration 2000, Loss: 0.6007\n",
      "Iteration 2100, Loss: 0.5989\n",
      "Iteration 2200, Loss: 0.5971\n",
      "Iteration 2300, Loss: 0.5954\n",
      "Iteration 2400, Loss: 0.5938\n",
      "Iteration 2500, Loss: 0.5922\n",
      "Iteration 2600, Loss: 0.5908\n",
      "Iteration 2700, Loss: 0.5894\n",
      "Iteration 2800, Loss: 0.5880\n",
      "Iteration 2900, Loss: 0.5868\n",
      "Iteration 3000, Loss: 0.5855\n",
      "Iteration 3100, Loss: 0.5843\n",
      "Iteration 3200, Loss: 0.5832\n",
      "Iteration 3300, Loss: 0.5820\n",
      "Iteration 3400, Loss: 0.5810\n",
      "Iteration 3500, Loss: 0.5799\n",
      "Iteration 3600, Loss: 0.5789\n",
      "Iteration 3700, Loss: 0.5779\n",
      "Iteration 3800, Loss: 0.5770\n",
      "Iteration 3900, Loss: 0.5761\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7856\n",
      "Iteration 200, Loss: 0.7190\n",
      "Iteration 300, Loss: 0.6882\n",
      "Iteration 400, Loss: 0.6692\n",
      "Iteration 500, Loss: 0.6556\n",
      "Iteration 600, Loss: 0.6453\n",
      "Iteration 700, Loss: 0.6369\n",
      "Iteration 800, Loss: 0.6300\n",
      "Iteration 900, Loss: 0.6240\n",
      "Iteration 1000, Loss: 0.6188\n",
      "Iteration 1100, Loss: 0.6142\n",
      "Iteration 1200, Loss: 0.6100\n",
      "Iteration 1300, Loss: 0.6063\n",
      "Iteration 1400, Loss: 0.6028\n",
      "Iteration 1500, Loss: 0.5997\n",
      "Iteration 1600, Loss: 0.5967\n",
      "Iteration 1700, Loss: 0.5940\n",
      "Iteration 1800, Loss: 0.5914\n",
      "Iteration 1900, Loss: 0.5890\n",
      "Iteration 2000, Loss: 0.5867\n",
      "Iteration 2100, Loss: 0.5845\n",
      "Iteration 2200, Loss: 0.5825\n",
      "Iteration 2300, Loss: 0.5805\n",
      "Iteration 2400, Loss: 0.5787\n",
      "Iteration 2500, Loss: 0.5769\n",
      "Iteration 2600, Loss: 0.5752\n",
      "Iteration 2700, Loss: 0.5736\n",
      "Iteration 2800, Loss: 0.5720\n",
      "Iteration 2900, Loss: 0.5705\n",
      "Iteration 3000, Loss: 0.5691\n",
      "Iteration 3100, Loss: 0.5677\n",
      "Iteration 3200, Loss: 0.5664\n",
      "Iteration 3300, Loss: 0.5652\n",
      "Iteration 3400, Loss: 0.5639\n",
      "Iteration 3500, Loss: 0.5627\n",
      "Iteration 3600, Loss: 0.5616\n",
      "Iteration 3700, Loss: 0.5605\n",
      "Iteration 3800, Loss: 0.5594\n",
      "Iteration 3900, Loss: 0.5584\n",
      "35 270\n",
      "Iteration 0, Loss: 1.0896\n",
      "Iteration 100, Loss: 0.7958\n",
      "Iteration 200, Loss: 0.7316\n",
      "Iteration 300, Loss: 0.7018\n",
      "Iteration 400, Loss: 0.6833\n",
      "Iteration 500, Loss: 0.6702\n",
      "Iteration 600, Loss: 0.6600\n",
      "Iteration 700, Loss: 0.6519\n",
      "Iteration 800, Loss: 0.6452\n",
      "Iteration 900, Loss: 0.6393\n",
      "Iteration 1000, Loss: 0.6342\n",
      "Iteration 1100, Loss: 0.6297\n",
      "Iteration 1200, Loss: 0.6256\n",
      "Iteration 1300, Loss: 0.6219\n",
      "Iteration 1400, Loss: 0.6185\n",
      "Iteration 1500, Loss: 0.6154\n",
      "Iteration 1600, Loss: 0.6125\n",
      "Iteration 1700, Loss: 0.6097\n",
      "Iteration 1800, Loss: 0.6071\n",
      "Iteration 1900, Loss: 0.6047\n",
      "Iteration 2000, Loss: 0.6024\n",
      "Iteration 2100, Loss: 0.6002\n",
      "Iteration 2200, Loss: 0.5982\n",
      "Iteration 2300, Loss: 0.5962\n",
      "Iteration 2400, Loss: 0.5943\n",
      "Iteration 2500, Loss: 0.5925\n",
      "Iteration 2600, Loss: 0.5908\n",
      "Iteration 2700, Loss: 0.5891\n",
      "Iteration 2800, Loss: 0.5875\n",
      "Iteration 2900, Loss: 0.5860\n",
      "Iteration 3000, Loss: 0.5845\n",
      "Iteration 3100, Loss: 0.5831\n",
      "Iteration 3200, Loss: 0.5817\n",
      "Iteration 3300, Loss: 0.5804\n",
      "Iteration 3400, Loss: 0.5791\n",
      "Iteration 3500, Loss: 0.5778\n",
      "Iteration 3600, Loss: 0.5766\n",
      "Iteration 3700, Loss: 0.5754\n",
      "Iteration 3800, Loss: 0.5743\n",
      "Iteration 3900, Loss: 0.5732\n",
      "Iteration 0, Loss: 1.0889\n",
      "Iteration 100, Loss: 0.7754\n",
      "Iteration 200, Loss: 0.7079\n",
      "Iteration 300, Loss: 0.6774\n",
      "Iteration 400, Loss: 0.6586\n",
      "Iteration 500, Loss: 0.6451\n",
      "Iteration 600, Loss: 0.6347\n",
      "Iteration 700, Loss: 0.6263\n",
      "Iteration 800, Loss: 0.6194\n",
      "Iteration 900, Loss: 0.6135\n",
      "Iteration 1000, Loss: 0.6083\n",
      "Iteration 1100, Loss: 0.6037\n",
      "Iteration 1200, Loss: 0.5996\n",
      "Iteration 1300, Loss: 0.5959\n",
      "Iteration 1400, Loss: 0.5925\n",
      "Iteration 1500, Loss: 0.5894\n",
      "Iteration 1600, Loss: 0.5865\n",
      "Iteration 1700, Loss: 0.5839\n",
      "Iteration 1800, Loss: 0.5814\n",
      "Iteration 1900, Loss: 0.5790\n",
      "Iteration 2000, Loss: 0.5768\n",
      "Iteration 2100, Loss: 0.5747\n",
      "Iteration 2200, Loss: 0.5727\n",
      "Iteration 2300, Loss: 0.5707\n",
      "Iteration 2400, Loss: 0.5689\n",
      "Iteration 2500, Loss: 0.5672\n",
      "Iteration 2600, Loss: 0.5655\n",
      "Iteration 2700, Loss: 0.5639\n",
      "Iteration 2800, Loss: 0.5624\n",
      "Iteration 2900, Loss: 0.5609\n",
      "Iteration 3000, Loss: 0.5594\n",
      "Iteration 3100, Loss: 0.5581\n",
      "Iteration 3200, Loss: 0.5567\n",
      "Iteration 3300, Loss: 0.5554\n",
      "Iteration 3400, Loss: 0.5542\n",
      "Iteration 3500, Loss: 0.5530\n",
      "Iteration 3600, Loss: 0.5518\n",
      "Iteration 3700, Loss: 0.5507\n",
      "Iteration 3800, Loss: 0.5496\n",
      "Iteration 3900, Loss: 0.5485\n",
      "36 270\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7929\n",
      "Iteration 200, Loss: 0.7294\n",
      "Iteration 300, Loss: 0.6996\n",
      "Iteration 400, Loss: 0.6809\n",
      "Iteration 500, Loss: 0.6675\n",
      "Iteration 600, Loss: 0.6571\n",
      "Iteration 700, Loss: 0.6488\n",
      "Iteration 800, Loss: 0.6419\n",
      "Iteration 900, Loss: 0.6360\n",
      "Iteration 1000, Loss: 0.6309\n",
      "Iteration 1100, Loss: 0.6264\n",
      "Iteration 1200, Loss: 0.6224\n",
      "Iteration 1300, Loss: 0.6188\n",
      "Iteration 1400, Loss: 0.6154\n",
      "Iteration 1500, Loss: 0.6124\n",
      "Iteration 1600, Loss: 0.6095\n",
      "Iteration 1700, Loss: 0.6069\n",
      "Iteration 1800, Loss: 0.6044\n",
      "Iteration 1900, Loss: 0.6021\n",
      "Iteration 2000, Loss: 0.5999\n",
      "Iteration 2100, Loss: 0.5979\n",
      "Iteration 2200, Loss: 0.5959\n",
      "Iteration 2300, Loss: 0.5940\n",
      "Iteration 2400, Loss: 0.5922\n",
      "Iteration 2500, Loss: 0.5905\n",
      "Iteration 2600, Loss: 0.5889\n",
      "Iteration 2700, Loss: 0.5873\n",
      "Iteration 2800, Loss: 0.5858\n",
      "Iteration 2900, Loss: 0.5843\n",
      "Iteration 3000, Loss: 0.5830\n",
      "Iteration 3100, Loss: 0.5816\n",
      "Iteration 3200, Loss: 0.5803\n",
      "Iteration 3300, Loss: 0.5790\n",
      "Iteration 3400, Loss: 0.5778\n",
      "Iteration 3500, Loss: 0.5766\n",
      "Iteration 3600, Loss: 0.5755\n",
      "Iteration 3700, Loss: 0.5744\n",
      "Iteration 3800, Loss: 0.5733\n",
      "Iteration 3900, Loss: 0.5722\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7804\n",
      "Iteration 200, Loss: 0.7133\n",
      "Iteration 300, Loss: 0.6829\n",
      "Iteration 400, Loss: 0.6645\n",
      "Iteration 500, Loss: 0.6516\n",
      "Iteration 600, Loss: 0.6417\n",
      "Iteration 700, Loss: 0.6337\n",
      "Iteration 800, Loss: 0.6270\n",
      "Iteration 900, Loss: 0.6213\n",
      "Iteration 1000, Loss: 0.6162\n",
      "Iteration 1100, Loss: 0.6118\n",
      "Iteration 1200, Loss: 0.6078\n",
      "Iteration 1300, Loss: 0.6041\n",
      "Iteration 1400, Loss: 0.6008\n",
      "Iteration 1500, Loss: 0.5976\n",
      "Iteration 1600, Loss: 0.5947\n",
      "Iteration 1700, Loss: 0.5920\n",
      "Iteration 1800, Loss: 0.5895\n",
      "Iteration 1900, Loss: 0.5870\n",
      "Iteration 2000, Loss: 0.5848\n",
      "Iteration 2100, Loss: 0.5826\n",
      "Iteration 2200, Loss: 0.5806\n",
      "Iteration 2300, Loss: 0.5786\n",
      "Iteration 2400, Loss: 0.5767\n",
      "Iteration 2500, Loss: 0.5749\n",
      "Iteration 2600, Loss: 0.5732\n",
      "Iteration 2700, Loss: 0.5716\n",
      "Iteration 2800, Loss: 0.5700\n",
      "Iteration 2900, Loss: 0.5684\n",
      "Iteration 3000, Loss: 0.5670\n",
      "Iteration 3100, Loss: 0.5655\n",
      "Iteration 3200, Loss: 0.5642\n",
      "Iteration 3300, Loss: 0.5628\n",
      "Iteration 3400, Loss: 0.5616\n",
      "Iteration 3500, Loss: 0.5603\n",
      "Iteration 3600, Loss: 0.5591\n",
      "Iteration 3700, Loss: 0.5579\n",
      "Iteration 3800, Loss: 0.5568\n",
      "Iteration 3900, Loss: 0.5557\n",
      "37 270\n",
      "Iteration 0, Loss: 1.0896\n",
      "Iteration 100, Loss: 0.7862\n",
      "Iteration 200, Loss: 0.7192\n",
      "Iteration 300, Loss: 0.6896\n",
      "Iteration 400, Loss: 0.6717\n",
      "Iteration 500, Loss: 0.6591\n",
      "Iteration 600, Loss: 0.6494\n",
      "Iteration 700, Loss: 0.6416\n",
      "Iteration 800, Loss: 0.6350\n",
      "Iteration 900, Loss: 0.6294\n",
      "Iteration 1000, Loss: 0.6245\n",
      "Iteration 1100, Loss: 0.6202\n",
      "Iteration 1200, Loss: 0.6163\n",
      "Iteration 1300, Loss: 0.6127\n",
      "Iteration 1400, Loss: 0.6094\n",
      "Iteration 1500, Loss: 0.6064\n",
      "Iteration 1600, Loss: 0.6035\n",
      "Iteration 1700, Loss: 0.6009\n",
      "Iteration 1800, Loss: 0.5984\n",
      "Iteration 1900, Loss: 0.5960\n",
      "Iteration 2000, Loss: 0.5938\n",
      "Iteration 2100, Loss: 0.5917\n",
      "Iteration 2200, Loss: 0.5897\n",
      "Iteration 2300, Loss: 0.5877\n",
      "Iteration 2400, Loss: 0.5859\n",
      "Iteration 2500, Loss: 0.5841\n",
      "Iteration 2600, Loss: 0.5825\n",
      "Iteration 2700, Loss: 0.5808\n",
      "Iteration 2800, Loss: 0.5793\n",
      "Iteration 2900, Loss: 0.5778\n",
      "Iteration 3000, Loss: 0.5763\n",
      "Iteration 3100, Loss: 0.5749\n",
      "Iteration 3200, Loss: 0.5736\n",
      "Iteration 3300, Loss: 0.5723\n",
      "Iteration 3400, Loss: 0.5710\n",
      "Iteration 3500, Loss: 0.5698\n",
      "Iteration 3600, Loss: 0.5687\n",
      "Iteration 3700, Loss: 0.5675\n",
      "Iteration 3800, Loss: 0.5664\n",
      "Iteration 3900, Loss: 0.5653\n",
      "Iteration 0, Loss: 1.0888\n",
      "Iteration 100, Loss: 0.7830\n",
      "Iteration 200, Loss: 0.7180\n",
      "Iteration 300, Loss: 0.6868\n",
      "Iteration 400, Loss: 0.6672\n",
      "Iteration 500, Loss: 0.6534\n",
      "Iteration 600, Loss: 0.6427\n",
      "Iteration 700, Loss: 0.6343\n",
      "Iteration 800, Loss: 0.6273\n",
      "Iteration 900, Loss: 0.6214\n",
      "Iteration 1000, Loss: 0.6162\n",
      "Iteration 1100, Loss: 0.6117\n",
      "Iteration 1200, Loss: 0.6077\n",
      "Iteration 1300, Loss: 0.6040\n",
      "Iteration 1400, Loss: 0.6007\n",
      "Iteration 1500, Loss: 0.5976\n",
      "Iteration 1600, Loss: 0.5948\n",
      "Iteration 1700, Loss: 0.5922\n",
      "Iteration 1800, Loss: 0.5897\n",
      "Iteration 1900, Loss: 0.5874\n",
      "Iteration 2000, Loss: 0.5852\n",
      "Iteration 2100, Loss: 0.5831\n",
      "Iteration 2200, Loss: 0.5812\n",
      "Iteration 2300, Loss: 0.5793\n",
      "Iteration 2400, Loss: 0.5775\n",
      "Iteration 2500, Loss: 0.5758\n",
      "Iteration 2600, Loss: 0.5742\n",
      "Iteration 2700, Loss: 0.5726\n",
      "Iteration 2800, Loss: 0.5711\n",
      "Iteration 2900, Loss: 0.5697\n",
      "Iteration 3000, Loss: 0.5683\n",
      "Iteration 3100, Loss: 0.5669\n",
      "Iteration 3200, Loss: 0.5656\n",
      "Iteration 3300, Loss: 0.5644\n",
      "Iteration 3400, Loss: 0.5631\n",
      "Iteration 3500, Loss: 0.5620\n",
      "Iteration 3600, Loss: 0.5608\n",
      "Iteration 3700, Loss: 0.5597\n",
      "Iteration 3800, Loss: 0.5586\n",
      "Iteration 3900, Loss: 0.5576\n",
      "38 270\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7811\n",
      "Iteration 200, Loss: 0.7147\n",
      "Iteration 300, Loss: 0.6840\n",
      "Iteration 400, Loss: 0.6650\n",
      "Iteration 500, Loss: 0.6514\n",
      "Iteration 600, Loss: 0.6410\n",
      "Iteration 700, Loss: 0.6326\n",
      "Iteration 800, Loss: 0.6255\n",
      "Iteration 900, Loss: 0.6195\n",
      "Iteration 1000, Loss: 0.6142\n",
      "Iteration 1100, Loss: 0.6096\n",
      "Iteration 1200, Loss: 0.6054\n",
      "Iteration 1300, Loss: 0.6016\n",
      "Iteration 1400, Loss: 0.5981\n",
      "Iteration 1500, Loss: 0.5949\n",
      "Iteration 1600, Loss: 0.5919\n",
      "Iteration 1700, Loss: 0.5892\n",
      "Iteration 1800, Loss: 0.5866\n",
      "Iteration 1900, Loss: 0.5841\n",
      "Iteration 2000, Loss: 0.5818\n",
      "Iteration 2100, Loss: 0.5796\n",
      "Iteration 2200, Loss: 0.5776\n",
      "Iteration 2300, Loss: 0.5756\n",
      "Iteration 2400, Loss: 0.5737\n",
      "Iteration 2500, Loss: 0.5719\n",
      "Iteration 2600, Loss: 0.5702\n",
      "Iteration 2700, Loss: 0.5686\n",
      "Iteration 2800, Loss: 0.5670\n",
      "Iteration 2900, Loss: 0.5655\n",
      "Iteration 3000, Loss: 0.5640\n",
      "Iteration 3100, Loss: 0.5626\n",
      "Iteration 3200, Loss: 0.5613\n",
      "Iteration 3300, Loss: 0.5600\n",
      "Iteration 3400, Loss: 0.5587\n",
      "Iteration 3500, Loss: 0.5575\n",
      "Iteration 3600, Loss: 0.5563\n",
      "Iteration 3700, Loss: 0.5551\n",
      "Iteration 3800, Loss: 0.5540\n",
      "Iteration 3900, Loss: 0.5530\n",
      "Iteration 0, Loss: 1.0895\n",
      "Iteration 100, Loss: 0.7912\n",
      "Iteration 200, Loss: 0.7262\n",
      "Iteration 300, Loss: 0.6966\n",
      "Iteration 400, Loss: 0.6786\n",
      "Iteration 500, Loss: 0.6658\n",
      "Iteration 600, Loss: 0.6561\n",
      "Iteration 700, Loss: 0.6482\n",
      "Iteration 800, Loss: 0.6416\n",
      "Iteration 900, Loss: 0.6360\n",
      "Iteration 1000, Loss: 0.6310\n",
      "Iteration 1100, Loss: 0.6267\n",
      "Iteration 1200, Loss: 0.6227\n",
      "Iteration 1300, Loss: 0.6191\n",
      "Iteration 1400, Loss: 0.6158\n",
      "Iteration 1500, Loss: 0.6127\n",
      "Iteration 1600, Loss: 0.6099\n",
      "Iteration 1700, Loss: 0.6072\n",
      "Iteration 1800, Loss: 0.6047\n",
      "Iteration 1900, Loss: 0.6024\n",
      "Iteration 2000, Loss: 0.6001\n",
      "Iteration 2100, Loss: 0.5980\n",
      "Iteration 2200, Loss: 0.5960\n",
      "Iteration 2300, Loss: 0.5941\n",
      "Iteration 2400, Loss: 0.5922\n",
      "Iteration 2500, Loss: 0.5904\n",
      "Iteration 2600, Loss: 0.5887\n",
      "Iteration 2700, Loss: 0.5871\n",
      "Iteration 2800, Loss: 0.5855\n",
      "Iteration 2900, Loss: 0.5840\n",
      "Iteration 3000, Loss: 0.5826\n",
      "Iteration 3100, Loss: 0.5812\n",
      "Iteration 3200, Loss: 0.5798\n",
      "Iteration 3300, Loss: 0.5785\n",
      "Iteration 3400, Loss: 0.5772\n",
      "Iteration 3500, Loss: 0.5759\n",
      "Iteration 3600, Loss: 0.5747\n",
      "Iteration 3700, Loss: 0.5736\n",
      "Iteration 3800, Loss: 0.5724\n",
      "Iteration 3900, Loss: 0.5713\n",
      "39 270\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7871\n",
      "Iteration 200, Loss: 0.7210\n",
      "Iteration 300, Loss: 0.6904\n",
      "Iteration 400, Loss: 0.6714\n",
      "Iteration 500, Loss: 0.6581\n",
      "Iteration 600, Loss: 0.6478\n",
      "Iteration 700, Loss: 0.6395\n",
      "Iteration 800, Loss: 0.6326\n",
      "Iteration 900, Loss: 0.6267\n",
      "Iteration 1000, Loss: 0.6216\n",
      "Iteration 1100, Loss: 0.6170\n",
      "Iteration 1200, Loss: 0.6129\n",
      "Iteration 1300, Loss: 0.6092\n",
      "Iteration 1400, Loss: 0.6058\n",
      "Iteration 1500, Loss: 0.6027\n",
      "Iteration 1600, Loss: 0.5998\n",
      "Iteration 1700, Loss: 0.5971\n",
      "Iteration 1800, Loss: 0.5946\n",
      "Iteration 1900, Loss: 0.5922\n",
      "Iteration 2000, Loss: 0.5899\n",
      "Iteration 2100, Loss: 0.5878\n",
      "Iteration 2200, Loss: 0.5858\n",
      "Iteration 2300, Loss: 0.5839\n",
      "Iteration 2400, Loss: 0.5820\n",
      "Iteration 2500, Loss: 0.5803\n",
      "Iteration 2600, Loss: 0.5786\n",
      "Iteration 2700, Loss: 0.5770\n",
      "Iteration 2800, Loss: 0.5754\n",
      "Iteration 2900, Loss: 0.5739\n",
      "Iteration 3000, Loss: 0.5725\n",
      "Iteration 3100, Loss: 0.5711\n",
      "Iteration 3200, Loss: 0.5698\n",
      "Iteration 3300, Loss: 0.5685\n",
      "Iteration 3400, Loss: 0.5672\n",
      "Iteration 3500, Loss: 0.5660\n",
      "Iteration 3600, Loss: 0.5648\n",
      "Iteration 3700, Loss: 0.5637\n",
      "Iteration 3800, Loss: 0.5626\n",
      "Iteration 3900, Loss: 0.5615\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7848\n",
      "Iteration 200, Loss: 0.7200\n",
      "Iteration 300, Loss: 0.6907\n",
      "Iteration 400, Loss: 0.6726\n",
      "Iteration 500, Loss: 0.6596\n",
      "Iteration 600, Loss: 0.6496\n",
      "Iteration 700, Loss: 0.6416\n",
      "Iteration 800, Loss: 0.6349\n",
      "Iteration 900, Loss: 0.6291\n",
      "Iteration 1000, Loss: 0.6241\n",
      "Iteration 1100, Loss: 0.6196\n",
      "Iteration 1200, Loss: 0.6156\n",
      "Iteration 1300, Loss: 0.6119\n",
      "Iteration 1400, Loss: 0.6086\n",
      "Iteration 1500, Loss: 0.6055\n",
      "Iteration 1600, Loss: 0.6026\n",
      "Iteration 1700, Loss: 0.6000\n",
      "Iteration 1800, Loss: 0.5974\n",
      "Iteration 1900, Loss: 0.5951\n",
      "Iteration 2000, Loss: 0.5928\n",
      "Iteration 2100, Loss: 0.5907\n",
      "Iteration 2200, Loss: 0.5887\n",
      "Iteration 2300, Loss: 0.5868\n",
      "Iteration 2400, Loss: 0.5850\n",
      "Iteration 2500, Loss: 0.5832\n",
      "Iteration 2600, Loss: 0.5816\n",
      "Iteration 2700, Loss: 0.5800\n",
      "Iteration 2800, Loss: 0.5784\n",
      "Iteration 2900, Loss: 0.5769\n",
      "Iteration 3000, Loss: 0.5755\n",
      "Iteration 3100, Loss: 0.5741\n",
      "Iteration 3200, Loss: 0.5728\n",
      "Iteration 3300, Loss: 0.5715\n",
      "Iteration 3400, Loss: 0.5703\n",
      "Iteration 3500, Loss: 0.5691\n",
      "Iteration 3600, Loss: 0.5680\n",
      "Iteration 3700, Loss: 0.5668\n",
      "Iteration 3800, Loss: 0.5658\n",
      "Iteration 3900, Loss: 0.5647\n",
      "40 270\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7895\n",
      "Iteration 200, Loss: 0.7257\n",
      "Iteration 300, Loss: 0.6962\n",
      "Iteration 400, Loss: 0.6778\n",
      "Iteration 500, Loss: 0.6648\n",
      "Iteration 600, Loss: 0.6547\n",
      "Iteration 700, Loss: 0.6465\n",
      "Iteration 800, Loss: 0.6397\n",
      "Iteration 900, Loss: 0.6338\n",
      "Iteration 1000, Loss: 0.6287\n",
      "Iteration 1100, Loss: 0.6242\n",
      "Iteration 1200, Loss: 0.6201\n",
      "Iteration 1300, Loss: 0.6164\n",
      "Iteration 1400, Loss: 0.6130\n",
      "Iteration 1500, Loss: 0.6099\n",
      "Iteration 1600, Loss: 0.6070\n",
      "Iteration 1700, Loss: 0.6043\n",
      "Iteration 1800, Loss: 0.6017\n",
      "Iteration 1900, Loss: 0.5993\n",
      "Iteration 2000, Loss: 0.5970\n",
      "Iteration 2100, Loss: 0.5949\n",
      "Iteration 2200, Loss: 0.5928\n",
      "Iteration 2300, Loss: 0.5909\n",
      "Iteration 2400, Loss: 0.5890\n",
      "Iteration 2500, Loss: 0.5872\n",
      "Iteration 2600, Loss: 0.5855\n",
      "Iteration 2700, Loss: 0.5838\n",
      "Iteration 2800, Loss: 0.5823\n",
      "Iteration 2900, Loss: 0.5807\n",
      "Iteration 3000, Loss: 0.5793\n",
      "Iteration 3100, Loss: 0.5778\n",
      "Iteration 3200, Loss: 0.5765\n",
      "Iteration 3300, Loss: 0.5751\n",
      "Iteration 3400, Loss: 0.5738\n",
      "Iteration 3500, Loss: 0.5726\n",
      "Iteration 3600, Loss: 0.5714\n",
      "Iteration 3700, Loss: 0.5702\n",
      "Iteration 3800, Loss: 0.5690\n",
      "Iteration 3900, Loss: 0.5679\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7835\n",
      "Iteration 200, Loss: 0.7159\n",
      "Iteration 300, Loss: 0.6849\n",
      "Iteration 400, Loss: 0.6659\n",
      "Iteration 500, Loss: 0.6523\n",
      "Iteration 600, Loss: 0.6418\n",
      "Iteration 700, Loss: 0.6334\n",
      "Iteration 800, Loss: 0.6264\n",
      "Iteration 900, Loss: 0.6204\n",
      "Iteration 1000, Loss: 0.6152\n",
      "Iteration 1100, Loss: 0.6106\n",
      "Iteration 1200, Loss: 0.6065\n",
      "Iteration 1300, Loss: 0.6027\n",
      "Iteration 1400, Loss: 0.5993\n",
      "Iteration 1500, Loss: 0.5961\n",
      "Iteration 1600, Loss: 0.5932\n",
      "Iteration 1700, Loss: 0.5905\n",
      "Iteration 1800, Loss: 0.5879\n",
      "Iteration 1900, Loss: 0.5855\n",
      "Iteration 2000, Loss: 0.5832\n",
      "Iteration 2100, Loss: 0.5811\n",
      "Iteration 2200, Loss: 0.5790\n",
      "Iteration 2300, Loss: 0.5771\n",
      "Iteration 2400, Loss: 0.5752\n",
      "Iteration 2500, Loss: 0.5734\n",
      "Iteration 2600, Loss: 0.5717\n",
      "Iteration 2700, Loss: 0.5701\n",
      "Iteration 2800, Loss: 0.5685\n",
      "Iteration 2900, Loss: 0.5670\n",
      "Iteration 3000, Loss: 0.5655\n",
      "Iteration 3100, Loss: 0.5641\n",
      "Iteration 3200, Loss: 0.5628\n",
      "Iteration 3300, Loss: 0.5614\n",
      "Iteration 3400, Loss: 0.5602\n",
      "Iteration 3500, Loss: 0.5589\n",
      "Iteration 3600, Loss: 0.5578\n",
      "Iteration 3700, Loss: 0.5566\n",
      "Iteration 3800, Loss: 0.5555\n",
      "Iteration 3900, Loss: 0.5544\n",
      "41 270\n",
      "Iteration 0, Loss: 1.0894\n",
      "Iteration 100, Loss: 0.7848\n",
      "Iteration 200, Loss: 0.7185\n",
      "Iteration 300, Loss: 0.6884\n",
      "Iteration 400, Loss: 0.6700\n",
      "Iteration 500, Loss: 0.6569\n",
      "Iteration 600, Loss: 0.6469\n",
      "Iteration 700, Loss: 0.6389\n",
      "Iteration 800, Loss: 0.6323\n",
      "Iteration 900, Loss: 0.6268\n",
      "Iteration 1000, Loss: 0.6219\n",
      "Iteration 1100, Loss: 0.6177\n",
      "Iteration 1200, Loss: 0.6139\n",
      "Iteration 1300, Loss: 0.6105\n",
      "Iteration 1400, Loss: 0.6074\n",
      "Iteration 1500, Loss: 0.6045\n",
      "Iteration 1600, Loss: 0.6019\n",
      "Iteration 1700, Loss: 0.5994\n",
      "Iteration 1800, Loss: 0.5971\n",
      "Iteration 1900, Loss: 0.5950\n",
      "Iteration 2000, Loss: 0.5930\n",
      "Iteration 2100, Loss: 0.5911\n",
      "Iteration 2200, Loss: 0.5892\n",
      "Iteration 2300, Loss: 0.5875\n",
      "Iteration 2400, Loss: 0.5859\n",
      "Iteration 2500, Loss: 0.5843\n",
      "Iteration 2600, Loss: 0.5829\n",
      "Iteration 2700, Loss: 0.5814\n",
      "Iteration 2800, Loss: 0.5801\n",
      "Iteration 2900, Loss: 0.5788\n",
      "Iteration 3000, Loss: 0.5775\n",
      "Iteration 3100, Loss: 0.5763\n",
      "Iteration 3200, Loss: 0.5751\n",
      "Iteration 3300, Loss: 0.5740\n",
      "Iteration 3400, Loss: 0.5729\n",
      "Iteration 3500, Loss: 0.5718\n",
      "Iteration 3600, Loss: 0.5708\n",
      "Iteration 3700, Loss: 0.5698\n",
      "Iteration 3800, Loss: 0.5688\n",
      "Iteration 3900, Loss: 0.5679\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7878\n",
      "Iteration 200, Loss: 0.7245\n",
      "Iteration 300, Loss: 0.6950\n",
      "Iteration 400, Loss: 0.6763\n",
      "Iteration 500, Loss: 0.6627\n",
      "Iteration 600, Loss: 0.6523\n",
      "Iteration 700, Loss: 0.6438\n",
      "Iteration 800, Loss: 0.6367\n",
      "Iteration 900, Loss: 0.6306\n",
      "Iteration 1000, Loss: 0.6253\n",
      "Iteration 1100, Loss: 0.6206\n",
      "Iteration 1200, Loss: 0.6164\n",
      "Iteration 1300, Loss: 0.6125\n",
      "Iteration 1400, Loss: 0.6090\n",
      "Iteration 1500, Loss: 0.6058\n",
      "Iteration 1600, Loss: 0.6027\n",
      "Iteration 1700, Loss: 0.5999\n",
      "Iteration 1800, Loss: 0.5972\n",
      "Iteration 1900, Loss: 0.5947\n",
      "Iteration 2000, Loss: 0.5923\n",
      "Iteration 2100, Loss: 0.5901\n",
      "Iteration 2200, Loss: 0.5880\n",
      "Iteration 2300, Loss: 0.5859\n",
      "Iteration 2400, Loss: 0.5840\n",
      "Iteration 2500, Loss: 0.5821\n",
      "Iteration 2600, Loss: 0.5803\n",
      "Iteration 2700, Loss: 0.5786\n",
      "Iteration 2800, Loss: 0.5769\n",
      "Iteration 2900, Loss: 0.5753\n",
      "Iteration 3000, Loss: 0.5738\n",
      "Iteration 3100, Loss: 0.5723\n",
      "Iteration 3200, Loss: 0.5709\n",
      "Iteration 3300, Loss: 0.5695\n",
      "Iteration 3400, Loss: 0.5682\n",
      "Iteration 3500, Loss: 0.5669\n",
      "Iteration 3600, Loss: 0.5656\n",
      "Iteration 3700, Loss: 0.5644\n",
      "Iteration 3800, Loss: 0.5632\n",
      "Iteration 3900, Loss: 0.5620\n",
      "42 270\n",
      "Iteration 0, Loss: 1.0895\n",
      "Iteration 100, Loss: 0.7975\n",
      "Iteration 200, Loss: 0.7341\n",
      "Iteration 300, Loss: 0.7039\n",
      "Iteration 400, Loss: 0.6850\n",
      "Iteration 500, Loss: 0.6714\n",
      "Iteration 600, Loss: 0.6609\n",
      "Iteration 700, Loss: 0.6523\n",
      "Iteration 800, Loss: 0.6451\n",
      "Iteration 900, Loss: 0.6389\n",
      "Iteration 1000, Loss: 0.6335\n",
      "Iteration 1100, Loss: 0.6287\n",
      "Iteration 1200, Loss: 0.6244\n",
      "Iteration 1300, Loss: 0.6205\n",
      "Iteration 1400, Loss: 0.6169\n",
      "Iteration 1500, Loss: 0.6136\n",
      "Iteration 1600, Loss: 0.6105\n",
      "Iteration 1700, Loss: 0.6077\n",
      "Iteration 1800, Loss: 0.6050\n",
      "Iteration 1900, Loss: 0.6025\n",
      "Iteration 2000, Loss: 0.6001\n",
      "Iteration 2100, Loss: 0.5979\n",
      "Iteration 2200, Loss: 0.5957\n",
      "Iteration 2300, Loss: 0.5937\n",
      "Iteration 2400, Loss: 0.5918\n",
      "Iteration 2500, Loss: 0.5900\n",
      "Iteration 2600, Loss: 0.5882\n",
      "Iteration 2700, Loss: 0.5866\n",
      "Iteration 2800, Loss: 0.5850\n",
      "Iteration 2900, Loss: 0.5834\n",
      "Iteration 3000, Loss: 0.5819\n",
      "Iteration 3100, Loss: 0.5805\n",
      "Iteration 3200, Loss: 0.5791\n",
      "Iteration 3300, Loss: 0.5778\n",
      "Iteration 3400, Loss: 0.5765\n",
      "Iteration 3500, Loss: 0.5753\n",
      "Iteration 3600, Loss: 0.5741\n",
      "Iteration 3700, Loss: 0.5729\n",
      "Iteration 3800, Loss: 0.5718\n",
      "Iteration 3900, Loss: 0.5707\n",
      "Iteration 0, Loss: 1.0887\n",
      "Iteration 100, Loss: 0.7726\n",
      "Iteration 200, Loss: 0.7053\n",
      "Iteration 300, Loss: 0.6752\n",
      "Iteration 400, Loss: 0.6569\n",
      "Iteration 500, Loss: 0.6441\n",
      "Iteration 600, Loss: 0.6344\n",
      "Iteration 700, Loss: 0.6267\n",
      "Iteration 800, Loss: 0.6203\n",
      "Iteration 900, Loss: 0.6149\n",
      "Iteration 1000, Loss: 0.6102\n",
      "Iteration 1100, Loss: 0.6060\n",
      "Iteration 1200, Loss: 0.6023\n",
      "Iteration 1300, Loss: 0.5990\n",
      "Iteration 1400, Loss: 0.5959\n",
      "Iteration 1500, Loss: 0.5930\n",
      "Iteration 1600, Loss: 0.5904\n",
      "Iteration 1700, Loss: 0.5879\n",
      "Iteration 1800, Loss: 0.5856\n",
      "Iteration 1900, Loss: 0.5834\n",
      "Iteration 2000, Loss: 0.5813\n",
      "Iteration 2100, Loss: 0.5793\n",
      "Iteration 2200, Loss: 0.5774\n",
      "Iteration 2300, Loss: 0.5756\n",
      "Iteration 2400, Loss: 0.5739\n",
      "Iteration 2500, Loss: 0.5723\n",
      "Iteration 2600, Loss: 0.5707\n",
      "Iteration 2700, Loss: 0.5691\n",
      "Iteration 2800, Loss: 0.5677\n",
      "Iteration 2900, Loss: 0.5663\n",
      "Iteration 3000, Loss: 0.5649\n",
      "Iteration 3100, Loss: 0.5636\n",
      "Iteration 3200, Loss: 0.5623\n",
      "Iteration 3300, Loss: 0.5610\n",
      "Iteration 3400, Loss: 0.5598\n",
      "Iteration 3500, Loss: 0.5587\n",
      "Iteration 3600, Loss: 0.5575\n",
      "Iteration 3700, Loss: 0.5564\n",
      "Iteration 3800, Loss: 0.5554\n",
      "Iteration 3900, Loss: 0.5543\n",
      "43 270\n",
      "Iteration 0, Loss: 1.0884\n",
      "Iteration 100, Loss: 0.7687\n",
      "Iteration 200, Loss: 0.7021\n",
      "Iteration 300, Loss: 0.6721\n",
      "Iteration 400, Loss: 0.6536\n",
      "Iteration 500, Loss: 0.6407\n",
      "Iteration 600, Loss: 0.6307\n",
      "Iteration 700, Loss: 0.6227\n",
      "Iteration 800, Loss: 0.6161\n",
      "Iteration 900, Loss: 0.6104\n",
      "Iteration 1000, Loss: 0.6054\n",
      "Iteration 1100, Loss: 0.6010\n",
      "Iteration 1200, Loss: 0.5970\n",
      "Iteration 1300, Loss: 0.5934\n",
      "Iteration 1400, Loss: 0.5901\n",
      "Iteration 1500, Loss: 0.5871\n",
      "Iteration 1600, Loss: 0.5842\n",
      "Iteration 1700, Loss: 0.5816\n",
      "Iteration 1800, Loss: 0.5791\n",
      "Iteration 1900, Loss: 0.5768\n",
      "Iteration 2000, Loss: 0.5745\n",
      "Iteration 2100, Loss: 0.5724\n",
      "Iteration 2200, Loss: 0.5704\n",
      "Iteration 2300, Loss: 0.5685\n",
      "Iteration 2400, Loss: 0.5667\n",
      "Iteration 2500, Loss: 0.5650\n",
      "Iteration 2600, Loss: 0.5633\n",
      "Iteration 2700, Loss: 0.5617\n",
      "Iteration 2800, Loss: 0.5602\n",
      "Iteration 2900, Loss: 0.5587\n",
      "Iteration 3000, Loss: 0.5573\n",
      "Iteration 3100, Loss: 0.5559\n",
      "Iteration 3200, Loss: 0.5546\n",
      "Iteration 3300, Loss: 0.5533\n",
      "Iteration 3400, Loss: 0.5521\n",
      "Iteration 3500, Loss: 0.5509\n",
      "Iteration 3600, Loss: 0.5497\n",
      "Iteration 3700, Loss: 0.5486\n",
      "Iteration 3800, Loss: 0.5475\n",
      "Iteration 3900, Loss: 0.5465\n",
      "Iteration 0, Loss: 1.0898\n",
      "Iteration 100, Loss: 0.8024\n",
      "Iteration 200, Loss: 0.7384\n",
      "Iteration 300, Loss: 0.7084\n",
      "Iteration 400, Loss: 0.6895\n",
      "Iteration 500, Loss: 0.6761\n",
      "Iteration 600, Loss: 0.6657\n",
      "Iteration 700, Loss: 0.6574\n",
      "Iteration 800, Loss: 0.6505\n",
      "Iteration 900, Loss: 0.6446\n",
      "Iteration 1000, Loss: 0.6394\n",
      "Iteration 1100, Loss: 0.6348\n",
      "Iteration 1200, Loss: 0.6307\n",
      "Iteration 1300, Loss: 0.6270\n",
      "Iteration 1400, Loss: 0.6235\n",
      "Iteration 1500, Loss: 0.6203\n",
      "Iteration 1600, Loss: 0.6174\n",
      "Iteration 1700, Loss: 0.6146\n",
      "Iteration 1800, Loss: 0.6121\n",
      "Iteration 1900, Loss: 0.6096\n",
      "Iteration 2000, Loss: 0.6073\n",
      "Iteration 2100, Loss: 0.6052\n",
      "Iteration 2200, Loss: 0.6031\n",
      "Iteration 2300, Loss: 0.6011\n",
      "Iteration 2400, Loss: 0.5992\n",
      "Iteration 2500, Loss: 0.5974\n",
      "Iteration 2600, Loss: 0.5957\n",
      "Iteration 2700, Loss: 0.5941\n",
      "Iteration 2800, Loss: 0.5925\n",
      "Iteration 2900, Loss: 0.5909\n",
      "Iteration 3000, Loss: 0.5894\n",
      "Iteration 3100, Loss: 0.5880\n",
      "Iteration 3200, Loss: 0.5866\n",
      "Iteration 3300, Loss: 0.5853\n",
      "Iteration 3400, Loss: 0.5840\n",
      "Iteration 3500, Loss: 0.5828\n",
      "Iteration 3600, Loss: 0.5816\n",
      "Iteration 3700, Loss: 0.5804\n",
      "Iteration 3800, Loss: 0.5793\n",
      "Iteration 3900, Loss: 0.5781\n",
      "44 270\n",
      "Iteration 0, Loss: 1.0899\n",
      "Iteration 100, Loss: 0.7937\n",
      "Iteration 200, Loss: 0.7255\n",
      "Iteration 300, Loss: 0.6930\n",
      "Iteration 400, Loss: 0.6728\n",
      "Iteration 500, Loss: 0.6584\n",
      "Iteration 600, Loss: 0.6474\n",
      "Iteration 700, Loss: 0.6387\n",
      "Iteration 800, Loss: 0.6314\n",
      "Iteration 900, Loss: 0.6254\n",
      "Iteration 1000, Loss: 0.6201\n",
      "Iteration 1100, Loss: 0.6155\n",
      "Iteration 1200, Loss: 0.6114\n",
      "Iteration 1300, Loss: 0.6077\n",
      "Iteration 1400, Loss: 0.6043\n",
      "Iteration 1500, Loss: 0.6012\n",
      "Iteration 1600, Loss: 0.5984\n",
      "Iteration 1700, Loss: 0.5957\n",
      "Iteration 1800, Loss: 0.5932\n",
      "Iteration 1900, Loss: 0.5909\n",
      "Iteration 2000, Loss: 0.5887\n",
      "Iteration 2100, Loss: 0.5866\n",
      "Iteration 2200, Loss: 0.5846\n",
      "Iteration 2300, Loss: 0.5828\n",
      "Iteration 2400, Loss: 0.5810\n",
      "Iteration 2500, Loss: 0.5793\n",
      "Iteration 2600, Loss: 0.5776\n",
      "Iteration 2700, Loss: 0.5761\n",
      "Iteration 2800, Loss: 0.5746\n",
      "Iteration 2900, Loss: 0.5731\n",
      "Iteration 3000, Loss: 0.5718\n",
      "Iteration 3100, Loss: 0.5704\n",
      "Iteration 3200, Loss: 0.5691\n",
      "Iteration 3300, Loss: 0.5679\n",
      "Iteration 3400, Loss: 0.5667\n",
      "Iteration 3500, Loss: 0.5655\n",
      "Iteration 3600, Loss: 0.5644\n",
      "Iteration 3700, Loss: 0.5633\n",
      "Iteration 3800, Loss: 0.5622\n",
      "Iteration 3900, Loss: 0.5612\n",
      "Iteration 0, Loss: 1.0884\n",
      "Iteration 100, Loss: 0.7778\n",
      "Iteration 200, Loss: 0.7145\n",
      "Iteration 300, Loss: 0.6865\n",
      "Iteration 400, Loss: 0.6696\n",
      "Iteration 500, Loss: 0.6574\n",
      "Iteration 600, Loss: 0.6480\n",
      "Iteration 700, Loss: 0.6404\n",
      "Iteration 800, Loss: 0.6339\n",
      "Iteration 900, Loss: 0.6284\n",
      "Iteration 1000, Loss: 0.6235\n",
      "Iteration 1100, Loss: 0.6191\n",
      "Iteration 1200, Loss: 0.6152\n",
      "Iteration 1300, Loss: 0.6115\n",
      "Iteration 1400, Loss: 0.6082\n",
      "Iteration 1500, Loss: 0.6051\n",
      "Iteration 1600, Loss: 0.6022\n",
      "Iteration 1700, Loss: 0.5995\n",
      "Iteration 1800, Loss: 0.5969\n",
      "Iteration 1900, Loss: 0.5945\n",
      "Iteration 2000, Loss: 0.5922\n",
      "Iteration 2100, Loss: 0.5900\n",
      "Iteration 2200, Loss: 0.5879\n",
      "Iteration 2300, Loss: 0.5859\n",
      "Iteration 2400, Loss: 0.5840\n",
      "Iteration 2500, Loss: 0.5822\n",
      "Iteration 2600, Loss: 0.5804\n",
      "Iteration 2700, Loss: 0.5787\n",
      "Iteration 2800, Loss: 0.5771\n",
      "Iteration 2900, Loss: 0.5755\n",
      "Iteration 3000, Loss: 0.5740\n",
      "Iteration 3100, Loss: 0.5725\n",
      "Iteration 3200, Loss: 0.5711\n",
      "Iteration 3300, Loss: 0.5698\n",
      "Iteration 3400, Loss: 0.5684\n",
      "Iteration 3500, Loss: 0.5671\n",
      "Iteration 3600, Loss: 0.5659\n",
      "Iteration 3700, Loss: 0.5647\n",
      "Iteration 3800, Loss: 0.5635\n",
      "Iteration 3900, Loss: 0.5623\n",
      "45 270\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8644\n",
      "Iteration 200, Loss: 0.7875\n",
      "Iteration 300, Loss: 0.7483\n",
      "Iteration 400, Loss: 0.7242\n",
      "Iteration 500, Loss: 0.7074\n",
      "Iteration 600, Loss: 0.6949\n",
      "Iteration 700, Loss: 0.6850\n",
      "Iteration 800, Loss: 0.6770\n",
      "Iteration 900, Loss: 0.6702\n",
      "Iteration 1000, Loss: 0.6644\n",
      "Iteration 1100, Loss: 0.6592\n",
      "Iteration 1200, Loss: 0.6547\n",
      "Iteration 1300, Loss: 0.6506\n",
      "Iteration 1400, Loss: 0.6470\n",
      "Iteration 1500, Loss: 0.6436\n",
      "Iteration 1600, Loss: 0.6406\n",
      "Iteration 1700, Loss: 0.6377\n",
      "Iteration 1800, Loss: 0.6351\n",
      "Iteration 1900, Loss: 0.6326\n",
      "Iteration 2000, Loss: 0.6303\n",
      "Iteration 2100, Loss: 0.6281\n",
      "Iteration 2200, Loss: 0.6261\n",
      "Iteration 2300, Loss: 0.6241\n",
      "Iteration 2400, Loss: 0.6223\n",
      "Iteration 2500, Loss: 0.6205\n",
      "Iteration 2600, Loss: 0.6188\n",
      "Iteration 2700, Loss: 0.6172\n",
      "Iteration 2800, Loss: 0.6157\n",
      "Iteration 2900, Loss: 0.6142\n",
      "Iteration 3000, Loss: 0.6128\n",
      "Iteration 3100, Loss: 0.6114\n",
      "Iteration 3200, Loss: 0.6100\n",
      "Iteration 3300, Loss: 0.6088\n",
      "Iteration 3400, Loss: 0.6075\n",
      "Iteration 3500, Loss: 0.6063\n",
      "Iteration 3600, Loss: 0.6051\n",
      "Iteration 3700, Loss: 0.6040\n",
      "Iteration 3800, Loss: 0.6029\n",
      "Iteration 3900, Loss: 0.6018\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8582\n",
      "Iteration 200, Loss: 0.7774\n",
      "Iteration 300, Loss: 0.7365\n",
      "Iteration 400, Loss: 0.7117\n",
      "Iteration 500, Loss: 0.6947\n",
      "Iteration 600, Loss: 0.6820\n",
      "Iteration 700, Loss: 0.6719\n",
      "Iteration 800, Loss: 0.6636\n",
      "Iteration 900, Loss: 0.6565\n",
      "Iteration 1000, Loss: 0.6504\n",
      "Iteration 1100, Loss: 0.6451\n",
      "Iteration 1200, Loss: 0.6403\n",
      "Iteration 1300, Loss: 0.6360\n",
      "Iteration 1400, Loss: 0.6321\n",
      "Iteration 1500, Loss: 0.6286\n",
      "Iteration 1600, Loss: 0.6253\n",
      "Iteration 1700, Loss: 0.6223\n",
      "Iteration 1800, Loss: 0.6194\n",
      "Iteration 1900, Loss: 0.6168\n",
      "Iteration 2000, Loss: 0.6143\n",
      "Iteration 2100, Loss: 0.6120\n",
      "Iteration 2200, Loss: 0.6098\n",
      "Iteration 2300, Loss: 0.6077\n",
      "Iteration 2400, Loss: 0.6057\n",
      "Iteration 2500, Loss: 0.6038\n",
      "Iteration 2600, Loss: 0.6020\n",
      "Iteration 2700, Loss: 0.6003\n",
      "Iteration 2800, Loss: 0.5986\n",
      "Iteration 2900, Loss: 0.5970\n",
      "Iteration 3000, Loss: 0.5955\n",
      "Iteration 3100, Loss: 0.5940\n",
      "Iteration 3200, Loss: 0.5926\n",
      "Iteration 3300, Loss: 0.5912\n",
      "Iteration 3400, Loss: 0.5899\n",
      "Iteration 3500, Loss: 0.5886\n",
      "Iteration 3600, Loss: 0.5873\n",
      "Iteration 3700, Loss: 0.5861\n",
      "Iteration 3800, Loss: 0.5849\n",
      "Iteration 3900, Loss: 0.5838\n",
      "46 270\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8525\n",
      "Iteration 200, Loss: 0.7739\n",
      "Iteration 300, Loss: 0.7342\n",
      "Iteration 400, Loss: 0.7105\n",
      "Iteration 500, Loss: 0.6941\n",
      "Iteration 600, Loss: 0.6819\n",
      "Iteration 700, Loss: 0.6722\n",
      "Iteration 800, Loss: 0.6644\n",
      "Iteration 900, Loss: 0.6578\n",
      "Iteration 1000, Loss: 0.6520\n",
      "Iteration 1100, Loss: 0.6470\n",
      "Iteration 1200, Loss: 0.6425\n",
      "Iteration 1300, Loss: 0.6385\n",
      "Iteration 1400, Loss: 0.6349\n",
      "Iteration 1500, Loss: 0.6315\n",
      "Iteration 1600, Loss: 0.6284\n",
      "Iteration 1700, Loss: 0.6255\n",
      "Iteration 1800, Loss: 0.6229\n",
      "Iteration 1900, Loss: 0.6204\n",
      "Iteration 2000, Loss: 0.6181\n",
      "Iteration 2100, Loss: 0.6159\n",
      "Iteration 2200, Loss: 0.6138\n",
      "Iteration 2300, Loss: 0.6118\n",
      "Iteration 2400, Loss: 0.6100\n",
      "Iteration 2500, Loss: 0.6082\n",
      "Iteration 2600, Loss: 0.6065\n",
      "Iteration 2700, Loss: 0.6049\n",
      "Iteration 2800, Loss: 0.6033\n",
      "Iteration 2900, Loss: 0.6018\n",
      "Iteration 3000, Loss: 0.6004\n",
      "Iteration 3100, Loss: 0.5990\n",
      "Iteration 3200, Loss: 0.5977\n",
      "Iteration 3300, Loss: 0.5964\n",
      "Iteration 3400, Loss: 0.5952\n",
      "Iteration 3500, Loss: 0.5940\n",
      "Iteration 3600, Loss: 0.5928\n",
      "Iteration 3700, Loss: 0.5917\n",
      "Iteration 3800, Loss: 0.5906\n",
      "Iteration 3900, Loss: 0.5895\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8704\n",
      "Iteration 200, Loss: 0.7927\n",
      "Iteration 300, Loss: 0.7525\n",
      "Iteration 400, Loss: 0.7276\n",
      "Iteration 500, Loss: 0.7103\n",
      "Iteration 600, Loss: 0.6972\n",
      "Iteration 700, Loss: 0.6868\n",
      "Iteration 800, Loss: 0.6783\n",
      "Iteration 900, Loss: 0.6712\n",
      "Iteration 1000, Loss: 0.6649\n",
      "Iteration 1100, Loss: 0.6595\n",
      "Iteration 1200, Loss: 0.6546\n",
      "Iteration 1300, Loss: 0.6503\n",
      "Iteration 1400, Loss: 0.6463\n",
      "Iteration 1500, Loss: 0.6427\n",
      "Iteration 1600, Loss: 0.6394\n",
      "Iteration 1700, Loss: 0.6363\n",
      "Iteration 1800, Loss: 0.6335\n",
      "Iteration 1900, Loss: 0.6309\n",
      "Iteration 2000, Loss: 0.6284\n",
      "Iteration 2100, Loss: 0.6261\n",
      "Iteration 2200, Loss: 0.6239\n",
      "Iteration 2300, Loss: 0.6218\n",
      "Iteration 2400, Loss: 0.6198\n",
      "Iteration 2500, Loss: 0.6179\n",
      "Iteration 2600, Loss: 0.6161\n",
      "Iteration 2700, Loss: 0.6144\n",
      "Iteration 2800, Loss: 0.6128\n",
      "Iteration 2900, Loss: 0.6112\n",
      "Iteration 3000, Loss: 0.6097\n",
      "Iteration 3100, Loss: 0.6082\n",
      "Iteration 3200, Loss: 0.6068\n",
      "Iteration 3300, Loss: 0.6055\n",
      "Iteration 3400, Loss: 0.6042\n",
      "Iteration 3500, Loss: 0.6029\n",
      "Iteration 3600, Loss: 0.6017\n",
      "Iteration 3700, Loss: 0.6005\n",
      "Iteration 3800, Loss: 0.5994\n",
      "Iteration 3900, Loss: 0.5983\n",
      "47 270\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8674\n",
      "Iteration 200, Loss: 0.7914\n",
      "Iteration 300, Loss: 0.7532\n",
      "Iteration 400, Loss: 0.7300\n",
      "Iteration 500, Loss: 0.7141\n",
      "Iteration 600, Loss: 0.7022\n",
      "Iteration 700, Loss: 0.6928\n",
      "Iteration 800, Loss: 0.6851\n",
      "Iteration 900, Loss: 0.6786\n",
      "Iteration 1000, Loss: 0.6728\n",
      "Iteration 1100, Loss: 0.6678\n",
      "Iteration 1200, Loss: 0.6633\n",
      "Iteration 1300, Loss: 0.6593\n",
      "Iteration 1400, Loss: 0.6557\n",
      "Iteration 1500, Loss: 0.6523\n",
      "Iteration 1600, Loss: 0.6492\n",
      "Iteration 1700, Loss: 0.6463\n",
      "Iteration 1800, Loss: 0.6437\n",
      "Iteration 1900, Loss: 0.6412\n",
      "Iteration 2000, Loss: 0.6388\n",
      "Iteration 2100, Loss: 0.6366\n",
      "Iteration 2200, Loss: 0.6345\n",
      "Iteration 2300, Loss: 0.6325\n",
      "Iteration 2400, Loss: 0.6307\n",
      "Iteration 2500, Loss: 0.6289\n",
      "Iteration 2600, Loss: 0.6271\n",
      "Iteration 2700, Loss: 0.6255\n",
      "Iteration 2800, Loss: 0.6239\n",
      "Iteration 2900, Loss: 0.6224\n",
      "Iteration 3000, Loss: 0.6210\n",
      "Iteration 3100, Loss: 0.6195\n",
      "Iteration 3200, Loss: 0.6182\n",
      "Iteration 3300, Loss: 0.6169\n",
      "Iteration 3400, Loss: 0.6156\n",
      "Iteration 3500, Loss: 0.6144\n",
      "Iteration 3600, Loss: 0.6132\n",
      "Iteration 3700, Loss: 0.6120\n",
      "Iteration 3800, Loss: 0.6109\n",
      "Iteration 3900, Loss: 0.6098\n",
      "Iteration 0, Loss: 1.0941\n",
      "Iteration 100, Loss: 0.8568\n",
      "Iteration 200, Loss: 0.7757\n",
      "Iteration 300, Loss: 0.7340\n",
      "Iteration 400, Loss: 0.7083\n",
      "Iteration 500, Loss: 0.6905\n",
      "Iteration 600, Loss: 0.6772\n",
      "Iteration 700, Loss: 0.6667\n",
      "Iteration 800, Loss: 0.6581\n",
      "Iteration 900, Loss: 0.6508\n",
      "Iteration 1000, Loss: 0.6445\n",
      "Iteration 1100, Loss: 0.6391\n",
      "Iteration 1200, Loss: 0.6343\n",
      "Iteration 1300, Loss: 0.6299\n",
      "Iteration 1400, Loss: 0.6260\n",
      "Iteration 1500, Loss: 0.6225\n",
      "Iteration 1600, Loss: 0.6192\n",
      "Iteration 1700, Loss: 0.6162\n",
      "Iteration 1800, Loss: 0.6134\n",
      "Iteration 1900, Loss: 0.6108\n",
      "Iteration 2000, Loss: 0.6084\n",
      "Iteration 2100, Loss: 0.6061\n",
      "Iteration 2200, Loss: 0.6040\n",
      "Iteration 2300, Loss: 0.6019\n",
      "Iteration 2400, Loss: 0.6000\n",
      "Iteration 2500, Loss: 0.5982\n",
      "Iteration 2600, Loss: 0.5964\n",
      "Iteration 2700, Loss: 0.5947\n",
      "Iteration 2800, Loss: 0.5931\n",
      "Iteration 2900, Loss: 0.5916\n",
      "Iteration 3000, Loss: 0.5901\n",
      "Iteration 3100, Loss: 0.5887\n",
      "Iteration 3200, Loss: 0.5873\n",
      "Iteration 3300, Loss: 0.5860\n",
      "Iteration 3400, Loss: 0.5847\n",
      "Iteration 3500, Loss: 0.5835\n",
      "Iteration 3600, Loss: 0.5823\n",
      "Iteration 3700, Loss: 0.5811\n",
      "Iteration 3800, Loss: 0.5800\n",
      "Iteration 3900, Loss: 0.5789\n",
      "48 270\n",
      "Iteration 0, Loss: 1.0935\n",
      "Iteration 100, Loss: 0.8566\n",
      "Iteration 200, Loss: 0.7813\n",
      "Iteration 300, Loss: 0.7431\n",
      "Iteration 400, Loss: 0.7193\n",
      "Iteration 500, Loss: 0.7030\n",
      "Iteration 600, Loss: 0.6909\n",
      "Iteration 700, Loss: 0.6813\n",
      "Iteration 800, Loss: 0.6734\n",
      "Iteration 900, Loss: 0.6666\n",
      "Iteration 1000, Loss: 0.6609\n",
      "Iteration 1100, Loss: 0.6558\n",
      "Iteration 1200, Loss: 0.6513\n",
      "Iteration 1300, Loss: 0.6472\n",
      "Iteration 1400, Loss: 0.6434\n",
      "Iteration 1500, Loss: 0.6400\n",
      "Iteration 1600, Loss: 0.6369\n",
      "Iteration 1700, Loss: 0.6340\n",
      "Iteration 1800, Loss: 0.6313\n",
      "Iteration 1900, Loss: 0.6287\n",
      "Iteration 2000, Loss: 0.6263\n",
      "Iteration 2100, Loss: 0.6240\n",
      "Iteration 2200, Loss: 0.6218\n",
      "Iteration 2300, Loss: 0.6198\n",
      "Iteration 2400, Loss: 0.6178\n",
      "Iteration 2500, Loss: 0.6159\n",
      "Iteration 2600, Loss: 0.6141\n",
      "Iteration 2700, Loss: 0.6124\n",
      "Iteration 2800, Loss: 0.6107\n",
      "Iteration 2900, Loss: 0.6091\n",
      "Iteration 3000, Loss: 0.6075\n",
      "Iteration 3100, Loss: 0.6061\n",
      "Iteration 3200, Loss: 0.6046\n",
      "Iteration 3300, Loss: 0.6032\n",
      "Iteration 3400, Loss: 0.6019\n",
      "Iteration 3500, Loss: 0.6006\n",
      "Iteration 3600, Loss: 0.5993\n",
      "Iteration 3700, Loss: 0.5980\n",
      "Iteration 3800, Loss: 0.5968\n",
      "Iteration 3900, Loss: 0.5957\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8663\n",
      "Iteration 200, Loss: 0.7850\n",
      "Iteration 300, Loss: 0.7432\n",
      "Iteration 400, Loss: 0.7177\n",
      "Iteration 500, Loss: 0.7000\n",
      "Iteration 600, Loss: 0.6869\n",
      "Iteration 700, Loss: 0.6765\n",
      "Iteration 800, Loss: 0.6681\n",
      "Iteration 900, Loss: 0.6609\n",
      "Iteration 1000, Loss: 0.6547\n",
      "Iteration 1100, Loss: 0.6493\n",
      "Iteration 1200, Loss: 0.6445\n",
      "Iteration 1300, Loss: 0.6402\n",
      "Iteration 1400, Loss: 0.6364\n",
      "Iteration 1500, Loss: 0.6329\n",
      "Iteration 1600, Loss: 0.6297\n",
      "Iteration 1700, Loss: 0.6267\n",
      "Iteration 1800, Loss: 0.6240\n",
      "Iteration 1900, Loss: 0.6215\n",
      "Iteration 2000, Loss: 0.6191\n",
      "Iteration 2100, Loss: 0.6169\n",
      "Iteration 2200, Loss: 0.6148\n",
      "Iteration 2300, Loss: 0.6128\n",
      "Iteration 2400, Loss: 0.6110\n",
      "Iteration 2500, Loss: 0.6092\n",
      "Iteration 2600, Loss: 0.6075\n",
      "Iteration 2700, Loss: 0.6059\n",
      "Iteration 2800, Loss: 0.6043\n",
      "Iteration 2900, Loss: 0.6029\n",
      "Iteration 3000, Loss: 0.6015\n",
      "Iteration 3100, Loss: 0.6001\n",
      "Iteration 3200, Loss: 0.5988\n",
      "Iteration 3300, Loss: 0.5976\n",
      "Iteration 3400, Loss: 0.5963\n",
      "Iteration 3500, Loss: 0.5952\n",
      "Iteration 3600, Loss: 0.5940\n",
      "Iteration 3700, Loss: 0.5929\n",
      "Iteration 3800, Loss: 0.5919\n",
      "Iteration 3900, Loss: 0.5908\n",
      "49 270\n",
      "Iteration 0, Loss: 1.0941\n",
      "Iteration 100, Loss: 0.8605\n",
      "Iteration 200, Loss: 0.7847\n",
      "Iteration 300, Loss: 0.7467\n",
      "Iteration 400, Loss: 0.7234\n",
      "Iteration 500, Loss: 0.7073\n",
      "Iteration 600, Loss: 0.6952\n",
      "Iteration 700, Loss: 0.6856\n",
      "Iteration 800, Loss: 0.6777\n",
      "Iteration 900, Loss: 0.6708\n",
      "Iteration 1000, Loss: 0.6649\n",
      "Iteration 1100, Loss: 0.6596\n",
      "Iteration 1200, Loss: 0.6549\n",
      "Iteration 1300, Loss: 0.6507\n",
      "Iteration 1400, Loss: 0.6468\n",
      "Iteration 1500, Loss: 0.6433\n",
      "Iteration 1600, Loss: 0.6401\n",
      "Iteration 1700, Loss: 0.6371\n",
      "Iteration 1800, Loss: 0.6342\n",
      "Iteration 1900, Loss: 0.6316\n",
      "Iteration 2000, Loss: 0.6291\n",
      "Iteration 2100, Loss: 0.6269\n",
      "Iteration 2200, Loss: 0.6247\n",
      "Iteration 2300, Loss: 0.6226\n",
      "Iteration 2400, Loss: 0.6206\n",
      "Iteration 2500, Loss: 0.6188\n",
      "Iteration 2600, Loss: 0.6170\n",
      "Iteration 2700, Loss: 0.6153\n",
      "Iteration 2800, Loss: 0.6136\n",
      "Iteration 2900, Loss: 0.6121\n",
      "Iteration 3000, Loss: 0.6106\n",
      "Iteration 3100, Loss: 0.6092\n",
      "Iteration 3200, Loss: 0.6078\n",
      "Iteration 3300, Loss: 0.6065\n",
      "Iteration 3400, Loss: 0.6052\n",
      "Iteration 3500, Loss: 0.6039\n",
      "Iteration 3600, Loss: 0.6027\n",
      "Iteration 3700, Loss: 0.6016\n",
      "Iteration 3800, Loss: 0.6004\n",
      "Iteration 3900, Loss: 0.5993\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8650\n",
      "Iteration 200, Loss: 0.7841\n",
      "Iteration 300, Loss: 0.7422\n",
      "Iteration 400, Loss: 0.7164\n",
      "Iteration 500, Loss: 0.6988\n",
      "Iteration 600, Loss: 0.6858\n",
      "Iteration 700, Loss: 0.6754\n",
      "Iteration 800, Loss: 0.6670\n",
      "Iteration 900, Loss: 0.6600\n",
      "Iteration 1000, Loss: 0.6539\n",
      "Iteration 1100, Loss: 0.6486\n",
      "Iteration 1200, Loss: 0.6440\n",
      "Iteration 1300, Loss: 0.6398\n",
      "Iteration 1400, Loss: 0.6360\n",
      "Iteration 1500, Loss: 0.6326\n",
      "Iteration 1600, Loss: 0.6295\n",
      "Iteration 1700, Loss: 0.6266\n",
      "Iteration 1800, Loss: 0.6239\n",
      "Iteration 1900, Loss: 0.6214\n",
      "Iteration 2000, Loss: 0.6190\n",
      "Iteration 2100, Loss: 0.6168\n",
      "Iteration 2200, Loss: 0.6147\n",
      "Iteration 2300, Loss: 0.6128\n",
      "Iteration 2400, Loss: 0.6109\n",
      "Iteration 2500, Loss: 0.6091\n",
      "Iteration 2600, Loss: 0.6074\n",
      "Iteration 2700, Loss: 0.6058\n",
      "Iteration 2800, Loss: 0.6043\n",
      "Iteration 2900, Loss: 0.6028\n",
      "Iteration 3000, Loss: 0.6014\n",
      "Iteration 3100, Loss: 0.6000\n",
      "Iteration 3200, Loss: 0.5986\n",
      "Iteration 3300, Loss: 0.5974\n",
      "Iteration 3400, Loss: 0.5961\n",
      "Iteration 3500, Loss: 0.5949\n",
      "Iteration 3600, Loss: 0.5937\n",
      "Iteration 3700, Loss: 0.5926\n",
      "Iteration 3800, Loss: 0.5915\n",
      "Iteration 3900, Loss: 0.5904\n",
      "50 270\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8594\n",
      "Iteration 200, Loss: 0.7790\n",
      "Iteration 300, Loss: 0.7377\n",
      "Iteration 400, Loss: 0.7123\n",
      "Iteration 500, Loss: 0.6946\n",
      "Iteration 600, Loss: 0.6813\n",
      "Iteration 700, Loss: 0.6707\n",
      "Iteration 800, Loss: 0.6619\n",
      "Iteration 900, Loss: 0.6544\n",
      "Iteration 1000, Loss: 0.6478\n",
      "Iteration 1100, Loss: 0.6422\n",
      "Iteration 1200, Loss: 0.6370\n",
      "Iteration 1300, Loss: 0.6323\n",
      "Iteration 1400, Loss: 0.6281\n",
      "Iteration 1500, Loss: 0.6243\n",
      "Iteration 1600, Loss: 0.6207\n",
      "Iteration 1700, Loss: 0.6175\n",
      "Iteration 1800, Loss: 0.6144\n",
      "Iteration 1900, Loss: 0.6115\n",
      "Iteration 2000, Loss: 0.6089\n",
      "Iteration 2100, Loss: 0.6064\n",
      "Iteration 2200, Loss: 0.6039\n",
      "Iteration 2300, Loss: 0.6017\n",
      "Iteration 2400, Loss: 0.5995\n",
      "Iteration 2500, Loss: 0.5974\n",
      "Iteration 2600, Loss: 0.5955\n",
      "Iteration 2700, Loss: 0.5936\n",
      "Iteration 2800, Loss: 0.5918\n",
      "Iteration 2900, Loss: 0.5901\n",
      "Iteration 3000, Loss: 0.5884\n",
      "Iteration 3100, Loss: 0.5868\n",
      "Iteration 3200, Loss: 0.5852\n",
      "Iteration 3300, Loss: 0.5838\n",
      "Iteration 3400, Loss: 0.5823\n",
      "Iteration 3500, Loss: 0.5809\n",
      "Iteration 3600, Loss: 0.5796\n",
      "Iteration 3700, Loss: 0.5783\n",
      "Iteration 3800, Loss: 0.5770\n",
      "Iteration 3900, Loss: 0.5757\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8625\n",
      "Iteration 200, Loss: 0.7855\n",
      "Iteration 300, Loss: 0.7467\n",
      "Iteration 400, Loss: 0.7230\n",
      "Iteration 500, Loss: 0.7065\n",
      "Iteration 600, Loss: 0.6943\n",
      "Iteration 700, Loss: 0.6848\n",
      "Iteration 800, Loss: 0.6770\n",
      "Iteration 900, Loss: 0.6704\n",
      "Iteration 1000, Loss: 0.6648\n",
      "Iteration 1100, Loss: 0.6599\n",
      "Iteration 1200, Loss: 0.6555\n",
      "Iteration 1300, Loss: 0.6517\n",
      "Iteration 1400, Loss: 0.6482\n",
      "Iteration 1500, Loss: 0.6450\n",
      "Iteration 1600, Loss: 0.6420\n",
      "Iteration 1700, Loss: 0.6393\n",
      "Iteration 1800, Loss: 0.6368\n",
      "Iteration 1900, Loss: 0.6344\n",
      "Iteration 2000, Loss: 0.6322\n",
      "Iteration 2100, Loss: 0.6301\n",
      "Iteration 2200, Loss: 0.6282\n",
      "Iteration 2300, Loss: 0.6263\n",
      "Iteration 2400, Loss: 0.6246\n",
      "Iteration 2500, Loss: 0.6229\n",
      "Iteration 2600, Loss: 0.6213\n",
      "Iteration 2700, Loss: 0.6197\n",
      "Iteration 2800, Loss: 0.6183\n",
      "Iteration 2900, Loss: 0.6168\n",
      "Iteration 3000, Loss: 0.6155\n",
      "Iteration 3100, Loss: 0.6141\n",
      "Iteration 3200, Loss: 0.6128\n",
      "Iteration 3300, Loss: 0.6116\n",
      "Iteration 3400, Loss: 0.6104\n",
      "Iteration 3500, Loss: 0.6092\n",
      "Iteration 3600, Loss: 0.6081\n",
      "Iteration 3700, Loss: 0.6070\n",
      "Iteration 3800, Loss: 0.6059\n",
      "Iteration 3900, Loss: 0.6049\n",
      "51 270\n",
      "Iteration 0, Loss: 1.0932\n",
      "Iteration 100, Loss: 0.8472\n",
      "Iteration 200, Loss: 0.7682\n",
      "Iteration 300, Loss: 0.7286\n",
      "Iteration 400, Loss: 0.7049\n",
      "Iteration 500, Loss: 0.6887\n",
      "Iteration 600, Loss: 0.6769\n",
      "Iteration 700, Loss: 0.6677\n",
      "Iteration 800, Loss: 0.6601\n",
      "Iteration 900, Loss: 0.6538\n",
      "Iteration 1000, Loss: 0.6484\n",
      "Iteration 1100, Loss: 0.6437\n",
      "Iteration 1200, Loss: 0.6395\n",
      "Iteration 1300, Loss: 0.6358\n",
      "Iteration 1400, Loss: 0.6325\n",
      "Iteration 1500, Loss: 0.6295\n",
      "Iteration 1600, Loss: 0.6267\n",
      "Iteration 1700, Loss: 0.6242\n",
      "Iteration 1800, Loss: 0.6218\n",
      "Iteration 1900, Loss: 0.6196\n",
      "Iteration 2000, Loss: 0.6175\n",
      "Iteration 2100, Loss: 0.6156\n",
      "Iteration 2200, Loss: 0.6137\n",
      "Iteration 2300, Loss: 0.6120\n",
      "Iteration 2400, Loss: 0.6103\n",
      "Iteration 2500, Loss: 0.6087\n",
      "Iteration 2600, Loss: 0.6072\n",
      "Iteration 2700, Loss: 0.6058\n",
      "Iteration 2800, Loss: 0.6044\n",
      "Iteration 2900, Loss: 0.6030\n",
      "Iteration 3000, Loss: 0.6017\n",
      "Iteration 3100, Loss: 0.6005\n",
      "Iteration 3200, Loss: 0.5993\n",
      "Iteration 3300, Loss: 0.5981\n",
      "Iteration 3400, Loss: 0.5970\n",
      "Iteration 3500, Loss: 0.5959\n",
      "Iteration 3600, Loss: 0.5948\n",
      "Iteration 3700, Loss: 0.5938\n",
      "Iteration 3800, Loss: 0.5928\n",
      "Iteration 3900, Loss: 0.5918\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8727\n",
      "Iteration 200, Loss: 0.7956\n",
      "Iteration 300, Loss: 0.7553\n",
      "Iteration 400, Loss: 0.7301\n",
      "Iteration 500, Loss: 0.7123\n",
      "Iteration 600, Loss: 0.6989\n",
      "Iteration 700, Loss: 0.6880\n",
      "Iteration 800, Loss: 0.6791\n",
      "Iteration 900, Loss: 0.6714\n",
      "Iteration 1000, Loss: 0.6648\n",
      "Iteration 1100, Loss: 0.6589\n",
      "Iteration 1200, Loss: 0.6537\n",
      "Iteration 1300, Loss: 0.6489\n",
      "Iteration 1400, Loss: 0.6446\n",
      "Iteration 1500, Loss: 0.6407\n",
      "Iteration 1600, Loss: 0.6371\n",
      "Iteration 1700, Loss: 0.6338\n",
      "Iteration 1800, Loss: 0.6307\n",
      "Iteration 1900, Loss: 0.6278\n",
      "Iteration 2000, Loss: 0.6251\n",
      "Iteration 2100, Loss: 0.6225\n",
      "Iteration 2200, Loss: 0.6201\n",
      "Iteration 2300, Loss: 0.6177\n",
      "Iteration 2400, Loss: 0.6155\n",
      "Iteration 2500, Loss: 0.6134\n",
      "Iteration 2600, Loss: 0.6114\n",
      "Iteration 2700, Loss: 0.6095\n",
      "Iteration 2800, Loss: 0.6077\n",
      "Iteration 2900, Loss: 0.6059\n",
      "Iteration 3000, Loss: 0.6042\n",
      "Iteration 3100, Loss: 0.6026\n",
      "Iteration 3200, Loss: 0.6010\n",
      "Iteration 3300, Loss: 0.5995\n",
      "Iteration 3400, Loss: 0.5980\n",
      "Iteration 3500, Loss: 0.5966\n",
      "Iteration 3600, Loss: 0.5952\n",
      "Iteration 3700, Loss: 0.5938\n",
      "Iteration 3800, Loss: 0.5925\n",
      "Iteration 3900, Loss: 0.5913\n",
      "52 270\n",
      "Iteration 0, Loss: 1.0935\n",
      "Iteration 100, Loss: 0.8608\n",
      "Iteration 200, Loss: 0.7816\n",
      "Iteration 300, Loss: 0.7412\n",
      "Iteration 400, Loss: 0.7165\n",
      "Iteration 500, Loss: 0.6994\n",
      "Iteration 600, Loss: 0.6866\n",
      "Iteration 700, Loss: 0.6764\n",
      "Iteration 800, Loss: 0.6681\n",
      "Iteration 900, Loss: 0.6609\n",
      "Iteration 1000, Loss: 0.6548\n",
      "Iteration 1100, Loss: 0.6495\n",
      "Iteration 1200, Loss: 0.6448\n",
      "Iteration 1300, Loss: 0.6406\n",
      "Iteration 1400, Loss: 0.6368\n",
      "Iteration 1500, Loss: 0.6333\n",
      "Iteration 1600, Loss: 0.6301\n",
      "Iteration 1700, Loss: 0.6271\n",
      "Iteration 1800, Loss: 0.6244\n",
      "Iteration 1900, Loss: 0.6219\n",
      "Iteration 2000, Loss: 0.6195\n",
      "Iteration 2100, Loss: 0.6173\n",
      "Iteration 2200, Loss: 0.6152\n",
      "Iteration 2300, Loss: 0.6132\n",
      "Iteration 2400, Loss: 0.6113\n",
      "Iteration 2500, Loss: 0.6095\n",
      "Iteration 2600, Loss: 0.6078\n",
      "Iteration 2700, Loss: 0.6062\n",
      "Iteration 2800, Loss: 0.6046\n",
      "Iteration 2900, Loss: 0.6031\n",
      "Iteration 3000, Loss: 0.6017\n",
      "Iteration 3100, Loss: 0.6003\n",
      "Iteration 3200, Loss: 0.5990\n",
      "Iteration 3300, Loss: 0.5977\n",
      "Iteration 3400, Loss: 0.5964\n",
      "Iteration 3500, Loss: 0.5952\n",
      "Iteration 3600, Loss: 0.5940\n",
      "Iteration 3700, Loss: 0.5929\n",
      "Iteration 3800, Loss: 0.5918\n",
      "Iteration 3900, Loss: 0.5907\n",
      "Iteration 0, Loss: 1.0935\n",
      "Iteration 100, Loss: 0.8611\n",
      "Iteration 200, Loss: 0.7826\n",
      "Iteration 300, Loss: 0.7426\n",
      "Iteration 400, Loss: 0.7181\n",
      "Iteration 500, Loss: 0.7013\n",
      "Iteration 600, Loss: 0.6886\n",
      "Iteration 700, Loss: 0.6784\n",
      "Iteration 800, Loss: 0.6701\n",
      "Iteration 900, Loss: 0.6629\n",
      "Iteration 1000, Loss: 0.6567\n",
      "Iteration 1100, Loss: 0.6512\n",
      "Iteration 1200, Loss: 0.6463\n",
      "Iteration 1300, Loss: 0.6418\n",
      "Iteration 1400, Loss: 0.6378\n",
      "Iteration 1500, Loss: 0.6341\n",
      "Iteration 1600, Loss: 0.6307\n",
      "Iteration 1700, Loss: 0.6275\n",
      "Iteration 1800, Loss: 0.6245\n",
      "Iteration 1900, Loss: 0.6218\n",
      "Iteration 2000, Loss: 0.6192\n",
      "Iteration 2100, Loss: 0.6167\n",
      "Iteration 2200, Loss: 0.6144\n",
      "Iteration 2300, Loss: 0.6122\n",
      "Iteration 2400, Loss: 0.6101\n",
      "Iteration 2500, Loss: 0.6081\n",
      "Iteration 2600, Loss: 0.6062\n",
      "Iteration 2700, Loss: 0.6044\n",
      "Iteration 2800, Loss: 0.6027\n",
      "Iteration 2900, Loss: 0.6010\n",
      "Iteration 3000, Loss: 0.5993\n",
      "Iteration 3100, Loss: 0.5978\n",
      "Iteration 3200, Loss: 0.5963\n",
      "Iteration 3300, Loss: 0.5948\n",
      "Iteration 3400, Loss: 0.5934\n",
      "Iteration 3500, Loss: 0.5920\n",
      "Iteration 3600, Loss: 0.5907\n",
      "Iteration 3700, Loss: 0.5894\n",
      "Iteration 3800, Loss: 0.5882\n",
      "Iteration 3900, Loss: 0.5870\n",
      "53 270\n",
      "Iteration 0, Loss: 1.0931\n",
      "Iteration 100, Loss: 0.8497\n",
      "Iteration 200, Loss: 0.7720\n",
      "Iteration 300, Loss: 0.7322\n",
      "Iteration 400, Loss: 0.7079\n",
      "Iteration 500, Loss: 0.6911\n",
      "Iteration 600, Loss: 0.6785\n",
      "Iteration 700, Loss: 0.6685\n",
      "Iteration 800, Loss: 0.6604\n",
      "Iteration 900, Loss: 0.6535\n",
      "Iteration 1000, Loss: 0.6476\n",
      "Iteration 1100, Loss: 0.6425\n",
      "Iteration 1200, Loss: 0.6379\n",
      "Iteration 1300, Loss: 0.6338\n",
      "Iteration 1400, Loss: 0.6300\n",
      "Iteration 1500, Loss: 0.6267\n",
      "Iteration 1600, Loss: 0.6236\n",
      "Iteration 1700, Loss: 0.6207\n",
      "Iteration 1800, Loss: 0.6180\n",
      "Iteration 1900, Loss: 0.6154\n",
      "Iteration 2000, Loss: 0.6131\n",
      "Iteration 2100, Loss: 0.6109\n",
      "Iteration 2200, Loss: 0.6088\n",
      "Iteration 2300, Loss: 0.6069\n",
      "Iteration 2400, Loss: 0.6050\n",
      "Iteration 2500, Loss: 0.6032\n",
      "Iteration 2600, Loss: 0.6015\n",
      "Iteration 2700, Loss: 0.5998\n",
      "Iteration 2800, Loss: 0.5982\n",
      "Iteration 2900, Loss: 0.5967\n",
      "Iteration 3000, Loss: 0.5953\n",
      "Iteration 3100, Loss: 0.5939\n",
      "Iteration 3200, Loss: 0.5925\n",
      "Iteration 3300, Loss: 0.5912\n",
      "Iteration 3400, Loss: 0.5899\n",
      "Iteration 3500, Loss: 0.5887\n",
      "Iteration 3600, Loss: 0.5875\n",
      "Iteration 3700, Loss: 0.5863\n",
      "Iteration 3800, Loss: 0.5852\n",
      "Iteration 3900, Loss: 0.5841\n",
      "Iteration 0, Loss: 1.0941\n",
      "Iteration 100, Loss: 0.8728\n",
      "Iteration 200, Loss: 0.7943\n",
      "Iteration 300, Loss: 0.7542\n",
      "Iteration 400, Loss: 0.7296\n",
      "Iteration 500, Loss: 0.7127\n",
      "Iteration 600, Loss: 0.6999\n",
      "Iteration 700, Loss: 0.6898\n",
      "Iteration 800, Loss: 0.6815\n",
      "Iteration 900, Loss: 0.6745\n",
      "Iteration 1000, Loss: 0.6683\n",
      "Iteration 1100, Loss: 0.6629\n",
      "Iteration 1200, Loss: 0.6580\n",
      "Iteration 1300, Loss: 0.6537\n",
      "Iteration 1400, Loss: 0.6498\n",
      "Iteration 1500, Loss: 0.6461\n",
      "Iteration 1600, Loss: 0.6428\n",
      "Iteration 1700, Loss: 0.6397\n",
      "Iteration 1800, Loss: 0.6368\n",
      "Iteration 1900, Loss: 0.6341\n",
      "Iteration 2000, Loss: 0.6316\n",
      "Iteration 2100, Loss: 0.6291\n",
      "Iteration 2200, Loss: 0.6269\n",
      "Iteration 2300, Loss: 0.6247\n",
      "Iteration 2400, Loss: 0.6227\n",
      "Iteration 2500, Loss: 0.6207\n",
      "Iteration 2600, Loss: 0.6189\n",
      "Iteration 2700, Loss: 0.6171\n",
      "Iteration 2800, Loss: 0.6154\n",
      "Iteration 2900, Loss: 0.6137\n",
      "Iteration 3000, Loss: 0.6121\n",
      "Iteration 3100, Loss: 0.6106\n",
      "Iteration 3200, Loss: 0.6091\n",
      "Iteration 3300, Loss: 0.6077\n",
      "Iteration 3400, Loss: 0.6063\n",
      "Iteration 3500, Loss: 0.6049\n",
      "Iteration 3600, Loss: 0.6036\n",
      "Iteration 3700, Loss: 0.6024\n",
      "Iteration 3800, Loss: 0.6011\n",
      "Iteration 3900, Loss: 0.5999\n",
      "54 270\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8705\n",
      "Iteration 200, Loss: 0.7940\n",
      "Iteration 300, Loss: 0.7536\n",
      "Iteration 400, Loss: 0.7281\n",
      "Iteration 500, Loss: 0.7102\n",
      "Iteration 600, Loss: 0.6965\n",
      "Iteration 700, Loss: 0.6857\n",
      "Iteration 800, Loss: 0.6768\n",
      "Iteration 900, Loss: 0.6692\n",
      "Iteration 1000, Loss: 0.6627\n",
      "Iteration 1100, Loss: 0.6570\n",
      "Iteration 1200, Loss: 0.6520\n",
      "Iteration 1300, Loss: 0.6474\n",
      "Iteration 1400, Loss: 0.6433\n",
      "Iteration 1500, Loss: 0.6396\n",
      "Iteration 1600, Loss: 0.6363\n",
      "Iteration 1700, Loss: 0.6332\n",
      "Iteration 1800, Loss: 0.6302\n",
      "Iteration 1900, Loss: 0.6276\n",
      "Iteration 2000, Loss: 0.6250\n",
      "Iteration 2100, Loss: 0.6226\n",
      "Iteration 2200, Loss: 0.6204\n",
      "Iteration 2300, Loss: 0.6183\n",
      "Iteration 2400, Loss: 0.6163\n",
      "Iteration 2500, Loss: 0.6144\n",
      "Iteration 2600, Loss: 0.6126\n",
      "Iteration 2700, Loss: 0.6109\n",
      "Iteration 2800, Loss: 0.6093\n",
      "Iteration 2900, Loss: 0.6077\n",
      "Iteration 3000, Loss: 0.6062\n",
      "Iteration 3100, Loss: 0.6047\n",
      "Iteration 3200, Loss: 0.6033\n",
      "Iteration 3300, Loss: 0.6019\n",
      "Iteration 3400, Loss: 0.6006\n",
      "Iteration 3500, Loss: 0.5994\n",
      "Iteration 3600, Loss: 0.5981\n",
      "Iteration 3700, Loss: 0.5969\n",
      "Iteration 3800, Loss: 0.5958\n",
      "Iteration 3900, Loss: 0.5946\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8510\n",
      "Iteration 200, Loss: 0.7697\n",
      "Iteration 300, Loss: 0.7291\n",
      "Iteration 400, Loss: 0.7048\n",
      "Iteration 500, Loss: 0.6882\n",
      "Iteration 600, Loss: 0.6761\n",
      "Iteration 700, Loss: 0.6664\n",
      "Iteration 800, Loss: 0.6585\n",
      "Iteration 900, Loss: 0.6518\n",
      "Iteration 1000, Loss: 0.6460\n",
      "Iteration 1100, Loss: 0.6409\n",
      "Iteration 1200, Loss: 0.6365\n",
      "Iteration 1300, Loss: 0.6324\n",
      "Iteration 1400, Loss: 0.6287\n",
      "Iteration 1500, Loss: 0.6253\n",
      "Iteration 1600, Loss: 0.6222\n",
      "Iteration 1700, Loss: 0.6194\n",
      "Iteration 1800, Loss: 0.6167\n",
      "Iteration 1900, Loss: 0.6142\n",
      "Iteration 2000, Loss: 0.6118\n",
      "Iteration 2100, Loss: 0.6096\n",
      "Iteration 2200, Loss: 0.6075\n",
      "Iteration 2300, Loss: 0.6055\n",
      "Iteration 2400, Loss: 0.6036\n",
      "Iteration 2500, Loss: 0.6018\n",
      "Iteration 2600, Loss: 0.6001\n",
      "Iteration 2700, Loss: 0.5984\n",
      "Iteration 2800, Loss: 0.5968\n",
      "Iteration 2900, Loss: 0.5953\n",
      "Iteration 3000, Loss: 0.5938\n",
      "Iteration 3100, Loss: 0.5924\n",
      "Iteration 3200, Loss: 0.5910\n",
      "Iteration 3300, Loss: 0.5896\n",
      "Iteration 3400, Loss: 0.5883\n",
      "Iteration 3500, Loss: 0.5871\n",
      "Iteration 3600, Loss: 0.5859\n",
      "Iteration 3700, Loss: 0.5847\n",
      "Iteration 3800, Loss: 0.5835\n",
      "Iteration 3900, Loss: 0.5824\n",
      "55 270\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8653\n",
      "Iteration 200, Loss: 0.7870\n",
      "Iteration 300, Loss: 0.7472\n",
      "Iteration 400, Loss: 0.7227\n",
      "Iteration 500, Loss: 0.7058\n",
      "Iteration 600, Loss: 0.6931\n",
      "Iteration 700, Loss: 0.6830\n",
      "Iteration 800, Loss: 0.6748\n",
      "Iteration 900, Loss: 0.6678\n",
      "Iteration 1000, Loss: 0.6617\n",
      "Iteration 1100, Loss: 0.6564\n",
      "Iteration 1200, Loss: 0.6517\n",
      "Iteration 1300, Loss: 0.6475\n",
      "Iteration 1400, Loss: 0.6437\n",
      "Iteration 1500, Loss: 0.6402\n",
      "Iteration 1600, Loss: 0.6370\n",
      "Iteration 1700, Loss: 0.6340\n",
      "Iteration 1800, Loss: 0.6312\n",
      "Iteration 1900, Loss: 0.6286\n",
      "Iteration 2000, Loss: 0.6262\n",
      "Iteration 2100, Loss: 0.6239\n",
      "Iteration 2200, Loss: 0.6217\n",
      "Iteration 2300, Loss: 0.6197\n",
      "Iteration 2400, Loss: 0.6177\n",
      "Iteration 2500, Loss: 0.6158\n",
      "Iteration 2600, Loss: 0.6141\n",
      "Iteration 2700, Loss: 0.6124\n",
      "Iteration 2800, Loss: 0.6107\n",
      "Iteration 2900, Loss: 0.6091\n",
      "Iteration 3000, Loss: 0.6076\n",
      "Iteration 3100, Loss: 0.6062\n",
      "Iteration 3200, Loss: 0.6048\n",
      "Iteration 3300, Loss: 0.6034\n",
      "Iteration 3400, Loss: 0.6021\n",
      "Iteration 3500, Loss: 0.6008\n",
      "Iteration 3600, Loss: 0.5995\n",
      "Iteration 3700, Loss: 0.5983\n",
      "Iteration 3800, Loss: 0.5971\n",
      "Iteration 3900, Loss: 0.5960\n",
      "Iteration 0, Loss: 1.0935\n",
      "Iteration 100, Loss: 0.8576\n",
      "Iteration 200, Loss: 0.7779\n",
      "Iteration 300, Loss: 0.7373\n",
      "Iteration 400, Loss: 0.7124\n",
      "Iteration 500, Loss: 0.6952\n",
      "Iteration 600, Loss: 0.6823\n",
      "Iteration 700, Loss: 0.6723\n",
      "Iteration 800, Loss: 0.6640\n",
      "Iteration 900, Loss: 0.6569\n",
      "Iteration 1000, Loss: 0.6508\n",
      "Iteration 1100, Loss: 0.6455\n",
      "Iteration 1200, Loss: 0.6408\n",
      "Iteration 1300, Loss: 0.6365\n",
      "Iteration 1400, Loss: 0.6326\n",
      "Iteration 1500, Loss: 0.6291\n",
      "Iteration 1600, Loss: 0.6258\n",
      "Iteration 1700, Loss: 0.6227\n",
      "Iteration 1800, Loss: 0.6199\n",
      "Iteration 1900, Loss: 0.6172\n",
      "Iteration 2000, Loss: 0.6147\n",
      "Iteration 2100, Loss: 0.6123\n",
      "Iteration 2200, Loss: 0.6100\n",
      "Iteration 2300, Loss: 0.6079\n",
      "Iteration 2400, Loss: 0.6059\n",
      "Iteration 2500, Loss: 0.6039\n",
      "Iteration 2600, Loss: 0.6020\n",
      "Iteration 2700, Loss: 0.6002\n",
      "Iteration 2800, Loss: 0.5985\n",
      "Iteration 2900, Loss: 0.5969\n",
      "Iteration 3000, Loss: 0.5953\n",
      "Iteration 3100, Loss: 0.5937\n",
      "Iteration 3200, Loss: 0.5922\n",
      "Iteration 3300, Loss: 0.5908\n",
      "Iteration 3400, Loss: 0.5894\n",
      "Iteration 3500, Loss: 0.5880\n",
      "Iteration 3600, Loss: 0.5867\n",
      "Iteration 3700, Loss: 0.5854\n",
      "Iteration 3800, Loss: 0.5842\n",
      "Iteration 3900, Loss: 0.5830\n",
      "56 270\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8556\n",
      "Iteration 200, Loss: 0.7776\n",
      "Iteration 300, Loss: 0.7376\n",
      "Iteration 400, Loss: 0.7130\n",
      "Iteration 500, Loss: 0.6961\n",
      "Iteration 600, Loss: 0.6834\n",
      "Iteration 700, Loss: 0.6733\n",
      "Iteration 800, Loss: 0.6650\n",
      "Iteration 900, Loss: 0.6581\n",
      "Iteration 1000, Loss: 0.6522\n",
      "Iteration 1100, Loss: 0.6470\n",
      "Iteration 1200, Loss: 0.6424\n",
      "Iteration 1300, Loss: 0.6382\n",
      "Iteration 1400, Loss: 0.6344\n",
      "Iteration 1500, Loss: 0.6310\n",
      "Iteration 1600, Loss: 0.6279\n",
      "Iteration 1700, Loss: 0.6250\n",
      "Iteration 1800, Loss: 0.6223\n",
      "Iteration 1900, Loss: 0.6199\n",
      "Iteration 2000, Loss: 0.6175\n",
      "Iteration 2100, Loss: 0.6154\n",
      "Iteration 2200, Loss: 0.6133\n",
      "Iteration 2300, Loss: 0.6113\n",
      "Iteration 2400, Loss: 0.6095\n",
      "Iteration 2500, Loss: 0.6077\n",
      "Iteration 2600, Loss: 0.6061\n",
      "Iteration 2700, Loss: 0.6045\n",
      "Iteration 2800, Loss: 0.6029\n",
      "Iteration 2900, Loss: 0.6015\n",
      "Iteration 3000, Loss: 0.6001\n",
      "Iteration 3100, Loss: 0.5987\n",
      "Iteration 3200, Loss: 0.5974\n",
      "Iteration 3300, Loss: 0.5961\n",
      "Iteration 3400, Loss: 0.5949\n",
      "Iteration 3500, Loss: 0.5937\n",
      "Iteration 3600, Loss: 0.5925\n",
      "Iteration 3700, Loss: 0.5914\n",
      "Iteration 3800, Loss: 0.5903\n",
      "Iteration 3900, Loss: 0.5893\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8672\n",
      "Iteration 200, Loss: 0.7879\n",
      "Iteration 300, Loss: 0.7476\n",
      "Iteration 400, Loss: 0.7231\n",
      "Iteration 500, Loss: 0.7065\n",
      "Iteration 600, Loss: 0.6940\n",
      "Iteration 700, Loss: 0.6841\n",
      "Iteration 800, Loss: 0.6759\n",
      "Iteration 900, Loss: 0.6690\n",
      "Iteration 1000, Loss: 0.6629\n",
      "Iteration 1100, Loss: 0.6575\n",
      "Iteration 1200, Loss: 0.6527\n",
      "Iteration 1300, Loss: 0.6483\n",
      "Iteration 1400, Loss: 0.6443\n",
      "Iteration 1500, Loss: 0.6407\n",
      "Iteration 1600, Loss: 0.6373\n",
      "Iteration 1700, Loss: 0.6341\n",
      "Iteration 1800, Loss: 0.6311\n",
      "Iteration 1900, Loss: 0.6283\n",
      "Iteration 2000, Loss: 0.6258\n",
      "Iteration 2100, Loss: 0.6233\n",
      "Iteration 2200, Loss: 0.6209\n",
      "Iteration 2300, Loss: 0.6187\n",
      "Iteration 2400, Loss: 0.6166\n",
      "Iteration 2500, Loss: 0.6146\n",
      "Iteration 2600, Loss: 0.6126\n",
      "Iteration 2700, Loss: 0.6108\n",
      "Iteration 2800, Loss: 0.6090\n",
      "Iteration 2900, Loss: 0.6073\n",
      "Iteration 3000, Loss: 0.6056\n",
      "Iteration 3100, Loss: 0.6040\n",
      "Iteration 3200, Loss: 0.6025\n",
      "Iteration 3300, Loss: 0.6010\n",
      "Iteration 3400, Loss: 0.5996\n",
      "Iteration 3500, Loss: 0.5982\n",
      "Iteration 3600, Loss: 0.5968\n",
      "Iteration 3700, Loss: 0.5955\n",
      "Iteration 3800, Loss: 0.5942\n",
      "Iteration 3900, Loss: 0.5930\n",
      "57 270\n",
      "Iteration 0, Loss: 1.0926\n",
      "Iteration 100, Loss: 0.8368\n",
      "Iteration 200, Loss: 0.7583\n",
      "Iteration 300, Loss: 0.7196\n",
      "Iteration 400, Loss: 0.6959\n",
      "Iteration 500, Loss: 0.6796\n",
      "Iteration 600, Loss: 0.6673\n",
      "Iteration 700, Loss: 0.6577\n",
      "Iteration 800, Loss: 0.6498\n",
      "Iteration 900, Loss: 0.6432\n",
      "Iteration 1000, Loss: 0.6374\n",
      "Iteration 1100, Loss: 0.6324\n",
      "Iteration 1200, Loss: 0.6280\n",
      "Iteration 1300, Loss: 0.6241\n",
      "Iteration 1400, Loss: 0.6205\n",
      "Iteration 1500, Loss: 0.6172\n",
      "Iteration 1600, Loss: 0.6142\n",
      "Iteration 1700, Loss: 0.6114\n",
      "Iteration 1800, Loss: 0.6088\n",
      "Iteration 1900, Loss: 0.6064\n",
      "Iteration 2000, Loss: 0.6041\n",
      "Iteration 2100, Loss: 0.6020\n",
      "Iteration 2200, Loss: 0.6000\n",
      "Iteration 2300, Loss: 0.5981\n",
      "Iteration 2400, Loss: 0.5963\n",
      "Iteration 2500, Loss: 0.5945\n",
      "Iteration 2600, Loss: 0.5929\n",
      "Iteration 2700, Loss: 0.5913\n",
      "Iteration 2800, Loss: 0.5897\n",
      "Iteration 2900, Loss: 0.5883\n",
      "Iteration 3000, Loss: 0.5869\n",
      "Iteration 3100, Loss: 0.5855\n",
      "Iteration 3200, Loss: 0.5842\n",
      "Iteration 3300, Loss: 0.5829\n",
      "Iteration 3400, Loss: 0.5817\n",
      "Iteration 3500, Loss: 0.5805\n",
      "Iteration 3600, Loss: 0.5793\n",
      "Iteration 3700, Loss: 0.5782\n",
      "Iteration 3800, Loss: 0.5771\n",
      "Iteration 3900, Loss: 0.5760\n",
      "Iteration 0, Loss: 1.0943\n",
      "Iteration 100, Loss: 0.8825\n",
      "Iteration 200, Loss: 0.8042\n",
      "Iteration 300, Loss: 0.7634\n",
      "Iteration 400, Loss: 0.7384\n",
      "Iteration 500, Loss: 0.7210\n",
      "Iteration 600, Loss: 0.7079\n",
      "Iteration 700, Loss: 0.6975\n",
      "Iteration 800, Loss: 0.6889\n",
      "Iteration 900, Loss: 0.6815\n",
      "Iteration 1000, Loss: 0.6751\n",
      "Iteration 1100, Loss: 0.6695\n",
      "Iteration 1200, Loss: 0.6646\n",
      "Iteration 1300, Loss: 0.6601\n",
      "Iteration 1400, Loss: 0.6560\n",
      "Iteration 1500, Loss: 0.6523\n",
      "Iteration 1600, Loss: 0.6489\n",
      "Iteration 1700, Loss: 0.6458\n",
      "Iteration 1800, Loss: 0.6428\n",
      "Iteration 1900, Loss: 0.6401\n",
      "Iteration 2000, Loss: 0.6375\n",
      "Iteration 2100, Loss: 0.6351\n",
      "Iteration 2200, Loss: 0.6328\n",
      "Iteration 2300, Loss: 0.6306\n",
      "Iteration 2400, Loss: 0.6285\n",
      "Iteration 2500, Loss: 0.6265\n",
      "Iteration 2600, Loss: 0.6247\n",
      "Iteration 2700, Loss: 0.6228\n",
      "Iteration 2800, Loss: 0.6211\n",
      "Iteration 2900, Loss: 0.6195\n",
      "Iteration 3000, Loss: 0.6178\n",
      "Iteration 3100, Loss: 0.6163\n",
      "Iteration 3200, Loss: 0.6148\n",
      "Iteration 3300, Loss: 0.6133\n",
      "Iteration 3400, Loss: 0.6119\n",
      "Iteration 3500, Loss: 0.6106\n",
      "Iteration 3600, Loss: 0.6093\n",
      "Iteration 3700, Loss: 0.6080\n",
      "Iteration 3800, Loss: 0.6068\n",
      "Iteration 3900, Loss: 0.6056\n",
      "58 270\n",
      "Iteration 0, Loss: 1.0940\n",
      "Iteration 100, Loss: 0.8712\n",
      "Iteration 200, Loss: 0.7933\n",
      "Iteration 300, Loss: 0.7533\n",
      "Iteration 400, Loss: 0.7284\n",
      "Iteration 500, Loss: 0.7110\n",
      "Iteration 600, Loss: 0.6977\n",
      "Iteration 700, Loss: 0.6870\n",
      "Iteration 800, Loss: 0.6782\n",
      "Iteration 900, Loss: 0.6707\n",
      "Iteration 1000, Loss: 0.6641\n",
      "Iteration 1100, Loss: 0.6584\n",
      "Iteration 1200, Loss: 0.6534\n",
      "Iteration 1300, Loss: 0.6488\n",
      "Iteration 1400, Loss: 0.6447\n",
      "Iteration 1500, Loss: 0.6410\n",
      "Iteration 1600, Loss: 0.6375\n",
      "Iteration 1700, Loss: 0.6344\n",
      "Iteration 1800, Loss: 0.6314\n",
      "Iteration 1900, Loss: 0.6288\n",
      "Iteration 2000, Loss: 0.6262\n",
      "Iteration 2100, Loss: 0.6238\n",
      "Iteration 2200, Loss: 0.6215\n",
      "Iteration 2300, Loss: 0.6194\n",
      "Iteration 2400, Loss: 0.6174\n",
      "Iteration 2500, Loss: 0.6154\n",
      "Iteration 2600, Loss: 0.6136\n",
      "Iteration 2700, Loss: 0.6119\n",
      "Iteration 2800, Loss: 0.6102\n",
      "Iteration 2900, Loss: 0.6086\n",
      "Iteration 3000, Loss: 0.6070\n",
      "Iteration 3100, Loss: 0.6056\n",
      "Iteration 3200, Loss: 0.6042\n",
      "Iteration 3300, Loss: 0.6028\n",
      "Iteration 3400, Loss: 0.6014\n",
      "Iteration 3500, Loss: 0.6001\n",
      "Iteration 3600, Loss: 0.5989\n",
      "Iteration 3700, Loss: 0.5977\n",
      "Iteration 3800, Loss: 0.5965\n",
      "Iteration 3900, Loss: 0.5954\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8507\n",
      "Iteration 200, Loss: 0.7714\n",
      "Iteration 300, Loss: 0.7319\n",
      "Iteration 400, Loss: 0.7078\n",
      "Iteration 500, Loss: 0.6914\n",
      "Iteration 600, Loss: 0.6793\n",
      "Iteration 700, Loss: 0.6697\n",
      "Iteration 800, Loss: 0.6618\n",
      "Iteration 900, Loss: 0.6552\n",
      "Iteration 1000, Loss: 0.6494\n",
      "Iteration 1100, Loss: 0.6444\n",
      "Iteration 1200, Loss: 0.6398\n",
      "Iteration 1300, Loss: 0.6357\n",
      "Iteration 1400, Loss: 0.6320\n",
      "Iteration 1500, Loss: 0.6286\n",
      "Iteration 1600, Loss: 0.6254\n",
      "Iteration 1700, Loss: 0.6225\n",
      "Iteration 1800, Loss: 0.6198\n",
      "Iteration 1900, Loss: 0.6172\n",
      "Iteration 2000, Loss: 0.6148\n",
      "Iteration 2100, Loss: 0.6125\n",
      "Iteration 2200, Loss: 0.6104\n",
      "Iteration 2300, Loss: 0.6083\n",
      "Iteration 2400, Loss: 0.6064\n",
      "Iteration 2500, Loss: 0.6045\n",
      "Iteration 2600, Loss: 0.6027\n",
      "Iteration 2700, Loss: 0.6010\n",
      "Iteration 2800, Loss: 0.5993\n",
      "Iteration 2900, Loss: 0.5977\n",
      "Iteration 3000, Loss: 0.5962\n",
      "Iteration 3100, Loss: 0.5947\n",
      "Iteration 3200, Loss: 0.5932\n",
      "Iteration 3300, Loss: 0.5918\n",
      "Iteration 3400, Loss: 0.5905\n",
      "Iteration 3500, Loss: 0.5892\n",
      "Iteration 3600, Loss: 0.5879\n",
      "Iteration 3700, Loss: 0.5867\n",
      "Iteration 3800, Loss: 0.5855\n",
      "Iteration 3900, Loss: 0.5843\n",
      "59 270\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8566\n",
      "Iteration 200, Loss: 0.7761\n",
      "Iteration 300, Loss: 0.7351\n",
      "Iteration 400, Loss: 0.7100\n",
      "Iteration 500, Loss: 0.6925\n",
      "Iteration 600, Loss: 0.6793\n",
      "Iteration 700, Loss: 0.6690\n",
      "Iteration 800, Loss: 0.6604\n",
      "Iteration 900, Loss: 0.6532\n",
      "Iteration 1000, Loss: 0.6468\n",
      "Iteration 1100, Loss: 0.6413\n",
      "Iteration 1200, Loss: 0.6364\n",
      "Iteration 1300, Loss: 0.6319\n",
      "Iteration 1400, Loss: 0.6279\n",
      "Iteration 1500, Loss: 0.6243\n",
      "Iteration 1600, Loss: 0.6209\n",
      "Iteration 1700, Loss: 0.6178\n",
      "Iteration 1800, Loss: 0.6149\n",
      "Iteration 1900, Loss: 0.6122\n",
      "Iteration 2000, Loss: 0.6097\n",
      "Iteration 2100, Loss: 0.6073\n",
      "Iteration 2200, Loss: 0.6051\n",
      "Iteration 2300, Loss: 0.6030\n",
      "Iteration 2400, Loss: 0.6010\n",
      "Iteration 2500, Loss: 0.5991\n",
      "Iteration 2600, Loss: 0.5972\n",
      "Iteration 2700, Loss: 0.5955\n",
      "Iteration 2800, Loss: 0.5938\n",
      "Iteration 2900, Loss: 0.5922\n",
      "Iteration 3000, Loss: 0.5907\n",
      "Iteration 3100, Loss: 0.5892\n",
      "Iteration 3200, Loss: 0.5878\n",
      "Iteration 3300, Loss: 0.5864\n",
      "Iteration 3400, Loss: 0.5851\n",
      "Iteration 3500, Loss: 0.5838\n",
      "Iteration 3600, Loss: 0.5826\n",
      "Iteration 3700, Loss: 0.5814\n",
      "Iteration 3800, Loss: 0.5802\n",
      "Iteration 3900, Loss: 0.5791\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8663\n",
      "Iteration 200, Loss: 0.7889\n",
      "Iteration 300, Loss: 0.7494\n",
      "Iteration 400, Loss: 0.7250\n",
      "Iteration 500, Loss: 0.7082\n",
      "Iteration 600, Loss: 0.6956\n",
      "Iteration 700, Loss: 0.6857\n",
      "Iteration 800, Loss: 0.6776\n",
      "Iteration 900, Loss: 0.6707\n",
      "Iteration 1000, Loss: 0.6647\n",
      "Iteration 1100, Loss: 0.6595\n",
      "Iteration 1200, Loss: 0.6549\n",
      "Iteration 1300, Loss: 0.6508\n",
      "Iteration 1400, Loss: 0.6470\n",
      "Iteration 1500, Loss: 0.6436\n",
      "Iteration 1600, Loss: 0.6404\n",
      "Iteration 1700, Loss: 0.6376\n",
      "Iteration 1800, Loss: 0.6349\n",
      "Iteration 1900, Loss: 0.6323\n",
      "Iteration 2000, Loss: 0.6300\n",
      "Iteration 2100, Loss: 0.6277\n",
      "Iteration 2200, Loss: 0.6256\n",
      "Iteration 2300, Loss: 0.6236\n",
      "Iteration 2400, Loss: 0.6217\n",
      "Iteration 2500, Loss: 0.6199\n",
      "Iteration 2600, Loss: 0.6182\n",
      "Iteration 2700, Loss: 0.6165\n",
      "Iteration 2800, Loss: 0.6149\n",
      "Iteration 2900, Loss: 0.6133\n",
      "Iteration 3000, Loss: 0.6119\n",
      "Iteration 3100, Loss: 0.6104\n",
      "Iteration 3200, Loss: 0.6090\n",
      "Iteration 3300, Loss: 0.6077\n",
      "Iteration 3400, Loss: 0.6064\n",
      "Iteration 3500, Loss: 0.6051\n",
      "Iteration 3600, Loss: 0.6039\n",
      "Iteration 3700, Loss: 0.6027\n",
      "Iteration 3800, Loss: 0.6016\n",
      "Iteration 3900, Loss: 0.6004\n",
      "60 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0188\n",
      "Iteration 200, Loss: 0.9633\n",
      "Iteration 300, Loss: 0.9214\n",
      "Iteration 400, Loss: 0.8884\n",
      "Iteration 500, Loss: 0.8616\n",
      "Iteration 600, Loss: 0.8394\n",
      "Iteration 700, Loss: 0.8207\n",
      "Iteration 800, Loss: 0.8047\n",
      "Iteration 900, Loss: 0.7908\n",
      "Iteration 1000, Loss: 0.7788\n",
      "Iteration 1100, Loss: 0.7681\n",
      "Iteration 1200, Loss: 0.7587\n",
      "Iteration 1300, Loss: 0.7503\n",
      "Iteration 1400, Loss: 0.7428\n",
      "Iteration 1500, Loss: 0.7359\n",
      "Iteration 1600, Loss: 0.7297\n",
      "Iteration 1700, Loss: 0.7241\n",
      "Iteration 1800, Loss: 0.7189\n",
      "Iteration 1900, Loss: 0.7141\n",
      "Iteration 2000, Loss: 0.7096\n",
      "Iteration 2100, Loss: 0.7055\n",
      "Iteration 2200, Loss: 0.7016\n",
      "Iteration 2300, Loss: 0.6980\n",
      "Iteration 2400, Loss: 0.6946\n",
      "Iteration 2500, Loss: 0.6914\n",
      "Iteration 2600, Loss: 0.6884\n",
      "Iteration 2700, Loss: 0.6855\n",
      "Iteration 2800, Loss: 0.6828\n",
      "Iteration 2900, Loss: 0.6802\n",
      "Iteration 3000, Loss: 0.6777\n",
      "Iteration 3100, Loss: 0.6754\n",
      "Iteration 3200, Loss: 0.6731\n",
      "Iteration 3300, Loss: 0.6710\n",
      "Iteration 3400, Loss: 0.6689\n",
      "Iteration 3500, Loss: 0.6669\n",
      "Iteration 3600, Loss: 0.6650\n",
      "Iteration 3700, Loss: 0.6631\n",
      "Iteration 3800, Loss: 0.6614\n",
      "Iteration 3900, Loss: 0.6596\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0213\n",
      "Iteration 200, Loss: 0.9685\n",
      "Iteration 300, Loss: 0.9292\n",
      "Iteration 400, Loss: 0.8982\n",
      "Iteration 500, Loss: 0.8733\n",
      "Iteration 600, Loss: 0.8527\n",
      "Iteration 700, Loss: 0.8355\n",
      "Iteration 800, Loss: 0.8208\n",
      "Iteration 900, Loss: 0.8081\n",
      "Iteration 1000, Loss: 0.7970\n",
      "Iteration 1100, Loss: 0.7873\n",
      "Iteration 1200, Loss: 0.7787\n",
      "Iteration 1300, Loss: 0.7710\n",
      "Iteration 1400, Loss: 0.7642\n",
      "Iteration 1500, Loss: 0.7579\n",
      "Iteration 1600, Loss: 0.7522\n",
      "Iteration 1700, Loss: 0.7471\n",
      "Iteration 1800, Loss: 0.7423\n",
      "Iteration 1900, Loss: 0.7379\n",
      "Iteration 2000, Loss: 0.7339\n",
      "Iteration 2100, Loss: 0.7301\n",
      "Iteration 2200, Loss: 0.7266\n",
      "Iteration 2300, Loss: 0.7233\n",
      "Iteration 2400, Loss: 0.7202\n",
      "Iteration 2500, Loss: 0.7173\n",
      "Iteration 2600, Loss: 0.7145\n",
      "Iteration 2700, Loss: 0.7119\n",
      "Iteration 2800, Loss: 0.7095\n",
      "Iteration 2900, Loss: 0.7071\n",
      "Iteration 3000, Loss: 0.7049\n",
      "Iteration 3100, Loss: 0.7027\n",
      "Iteration 3200, Loss: 0.7007\n",
      "Iteration 3300, Loss: 0.6987\n",
      "Iteration 3400, Loss: 0.6969\n",
      "Iteration 3500, Loss: 0.6951\n",
      "Iteration 3600, Loss: 0.6933\n",
      "Iteration 3700, Loss: 0.6917\n",
      "Iteration 3800, Loss: 0.6901\n",
      "Iteration 3900, Loss: 0.6885\n",
      "61 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0257\n",
      "Iteration 200, Loss: 0.9752\n",
      "Iteration 300, Loss: 0.9370\n",
      "Iteration 400, Loss: 0.9068\n",
      "Iteration 500, Loss: 0.8823\n",
      "Iteration 600, Loss: 0.8620\n",
      "Iteration 700, Loss: 0.8449\n",
      "Iteration 800, Loss: 0.8302\n",
      "Iteration 900, Loss: 0.8175\n",
      "Iteration 1000, Loss: 0.8064\n",
      "Iteration 1100, Loss: 0.7967\n",
      "Iteration 1200, Loss: 0.7880\n",
      "Iteration 1300, Loss: 0.7803\n",
      "Iteration 1400, Loss: 0.7734\n",
      "Iteration 1500, Loss: 0.7671\n",
      "Iteration 1600, Loss: 0.7613\n",
      "Iteration 1700, Loss: 0.7561\n",
      "Iteration 1800, Loss: 0.7513\n",
      "Iteration 1900, Loss: 0.7468\n",
      "Iteration 2000, Loss: 0.7427\n",
      "Iteration 2100, Loss: 0.7389\n",
      "Iteration 2200, Loss: 0.7353\n",
      "Iteration 2300, Loss: 0.7319\n",
      "Iteration 2400, Loss: 0.7287\n",
      "Iteration 2500, Loss: 0.7257\n",
      "Iteration 2600, Loss: 0.7229\n",
      "Iteration 2700, Loss: 0.7203\n",
      "Iteration 2800, Loss: 0.7177\n",
      "Iteration 2900, Loss: 0.7153\n",
      "Iteration 3000, Loss: 0.7130\n",
      "Iteration 3100, Loss: 0.7108\n",
      "Iteration 3200, Loss: 0.7087\n",
      "Iteration 3300, Loss: 0.7067\n",
      "Iteration 3400, Loss: 0.7047\n",
      "Iteration 3500, Loss: 0.7028\n",
      "Iteration 3600, Loss: 0.7010\n",
      "Iteration 3700, Loss: 0.6993\n",
      "Iteration 3800, Loss: 0.6976\n",
      "Iteration 3900, Loss: 0.6960\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0136\n",
      "Iteration 200, Loss: 0.9554\n",
      "Iteration 300, Loss: 0.9120\n",
      "Iteration 400, Loss: 0.8782\n",
      "Iteration 500, Loss: 0.8509\n",
      "Iteration 600, Loss: 0.8284\n",
      "Iteration 700, Loss: 0.8096\n",
      "Iteration 800, Loss: 0.7935\n",
      "Iteration 900, Loss: 0.7797\n",
      "Iteration 1000, Loss: 0.7677\n",
      "Iteration 1100, Loss: 0.7571\n",
      "Iteration 1200, Loss: 0.7478\n",
      "Iteration 1300, Loss: 0.7395\n",
      "Iteration 1400, Loss: 0.7321\n",
      "Iteration 1500, Loss: 0.7254\n",
      "Iteration 1600, Loss: 0.7193\n",
      "Iteration 1700, Loss: 0.7138\n",
      "Iteration 1800, Loss: 0.7088\n",
      "Iteration 1900, Loss: 0.7041\n",
      "Iteration 2000, Loss: 0.6998\n",
      "Iteration 2100, Loss: 0.6958\n",
      "Iteration 2200, Loss: 0.6921\n",
      "Iteration 2300, Loss: 0.6886\n",
      "Iteration 2400, Loss: 0.6854\n",
      "Iteration 2500, Loss: 0.6823\n",
      "Iteration 2600, Loss: 0.6794\n",
      "Iteration 2700, Loss: 0.6767\n",
      "Iteration 2800, Loss: 0.6741\n",
      "Iteration 2900, Loss: 0.6717\n",
      "Iteration 3000, Loss: 0.6693\n",
      "Iteration 3100, Loss: 0.6671\n",
      "Iteration 3200, Loss: 0.6650\n",
      "Iteration 3300, Loss: 0.6630\n",
      "Iteration 3400, Loss: 0.6610\n",
      "Iteration 3500, Loss: 0.6592\n",
      "Iteration 3600, Loss: 0.6574\n",
      "Iteration 3700, Loss: 0.6556\n",
      "Iteration 3800, Loss: 0.6540\n",
      "Iteration 3900, Loss: 0.6524\n",
      "62 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0195\n",
      "Iteration 200, Loss: 0.9646\n",
      "Iteration 300, Loss: 0.9230\n",
      "Iteration 400, Loss: 0.8902\n",
      "Iteration 500, Loss: 0.8636\n",
      "Iteration 600, Loss: 0.8416\n",
      "Iteration 700, Loss: 0.8230\n",
      "Iteration 800, Loss: 0.8070\n",
      "Iteration 900, Loss: 0.7932\n",
      "Iteration 1000, Loss: 0.7812\n",
      "Iteration 1100, Loss: 0.7707\n",
      "Iteration 1200, Loss: 0.7613\n",
      "Iteration 1300, Loss: 0.7529\n",
      "Iteration 1400, Loss: 0.7453\n",
      "Iteration 1500, Loss: 0.7385\n",
      "Iteration 1600, Loss: 0.7323\n",
      "Iteration 1700, Loss: 0.7266\n",
      "Iteration 1800, Loss: 0.7214\n",
      "Iteration 1900, Loss: 0.7166\n",
      "Iteration 2000, Loss: 0.7121\n",
      "Iteration 2100, Loss: 0.7080\n",
      "Iteration 2200, Loss: 0.7041\n",
      "Iteration 2300, Loss: 0.7005\n",
      "Iteration 2400, Loss: 0.6971\n",
      "Iteration 2500, Loss: 0.6939\n",
      "Iteration 2600, Loss: 0.6909\n",
      "Iteration 2700, Loss: 0.6880\n",
      "Iteration 2800, Loss: 0.6853\n",
      "Iteration 2900, Loss: 0.6827\n",
      "Iteration 3000, Loss: 0.6803\n",
      "Iteration 3100, Loss: 0.6779\n",
      "Iteration 3200, Loss: 0.6756\n",
      "Iteration 3300, Loss: 0.6735\n",
      "Iteration 3400, Loss: 0.6714\n",
      "Iteration 3500, Loss: 0.6694\n",
      "Iteration 3600, Loss: 0.6675\n",
      "Iteration 3700, Loss: 0.6657\n",
      "Iteration 3800, Loss: 0.6639\n",
      "Iteration 3900, Loss: 0.6622\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0207\n",
      "Iteration 200, Loss: 0.9674\n",
      "Iteration 300, Loss: 0.9276\n",
      "Iteration 400, Loss: 0.8964\n",
      "Iteration 500, Loss: 0.8713\n",
      "Iteration 600, Loss: 0.8505\n",
      "Iteration 700, Loss: 0.8332\n",
      "Iteration 800, Loss: 0.8184\n",
      "Iteration 900, Loss: 0.8057\n",
      "Iteration 1000, Loss: 0.7947\n",
      "Iteration 1100, Loss: 0.7849\n",
      "Iteration 1200, Loss: 0.7763\n",
      "Iteration 1300, Loss: 0.7687\n",
      "Iteration 1400, Loss: 0.7618\n",
      "Iteration 1500, Loss: 0.7556\n",
      "Iteration 1600, Loss: 0.7499\n",
      "Iteration 1700, Loss: 0.7448\n",
      "Iteration 1800, Loss: 0.7401\n",
      "Iteration 1900, Loss: 0.7358\n",
      "Iteration 2000, Loss: 0.7318\n",
      "Iteration 2100, Loss: 0.7281\n",
      "Iteration 2200, Loss: 0.7246\n",
      "Iteration 2300, Loss: 0.7214\n",
      "Iteration 2400, Loss: 0.7184\n",
      "Iteration 2500, Loss: 0.7155\n",
      "Iteration 2600, Loss: 0.7129\n",
      "Iteration 2700, Loss: 0.7103\n",
      "Iteration 2800, Loss: 0.7080\n",
      "Iteration 2900, Loss: 0.7057\n",
      "Iteration 3000, Loss: 0.7035\n",
      "Iteration 3100, Loss: 0.7015\n",
      "Iteration 3200, Loss: 0.6995\n",
      "Iteration 3300, Loss: 0.6977\n",
      "Iteration 3400, Loss: 0.6959\n",
      "Iteration 3500, Loss: 0.6942\n",
      "Iteration 3600, Loss: 0.6925\n",
      "Iteration 3700, Loss: 0.6909\n",
      "Iteration 3800, Loss: 0.6894\n",
      "Iteration 3900, Loss: 0.6879\n",
      "63 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0136\n",
      "Iteration 200, Loss: 0.9566\n",
      "Iteration 300, Loss: 0.9146\n",
      "Iteration 400, Loss: 0.8819\n",
      "Iteration 500, Loss: 0.8557\n",
      "Iteration 600, Loss: 0.8342\n",
      "Iteration 700, Loss: 0.8163\n",
      "Iteration 800, Loss: 0.8010\n",
      "Iteration 900, Loss: 0.7879\n",
      "Iteration 1000, Loss: 0.7765\n",
      "Iteration 1100, Loss: 0.7664\n",
      "Iteration 1200, Loss: 0.7575\n",
      "Iteration 1300, Loss: 0.7496\n",
      "Iteration 1400, Loss: 0.7424\n",
      "Iteration 1500, Loss: 0.7360\n",
      "Iteration 1600, Loss: 0.7301\n",
      "Iteration 1700, Loss: 0.7248\n",
      "Iteration 1800, Loss: 0.7198\n",
      "Iteration 1900, Loss: 0.7153\n",
      "Iteration 2000, Loss: 0.7111\n",
      "Iteration 2100, Loss: 0.7072\n",
      "Iteration 2200, Loss: 0.7036\n",
      "Iteration 2300, Loss: 0.7002\n",
      "Iteration 2400, Loss: 0.6970\n",
      "Iteration 2500, Loss: 0.6940\n",
      "Iteration 2600, Loss: 0.6912\n",
      "Iteration 2700, Loss: 0.6885\n",
      "Iteration 2800, Loss: 0.6860\n",
      "Iteration 2900, Loss: 0.6836\n",
      "Iteration 3000, Loss: 0.6813\n",
      "Iteration 3100, Loss: 0.6792\n",
      "Iteration 3200, Loss: 0.6771\n",
      "Iteration 3300, Loss: 0.6751\n",
      "Iteration 3400, Loss: 0.6732\n",
      "Iteration 3500, Loss: 0.6714\n",
      "Iteration 3600, Loss: 0.6697\n",
      "Iteration 3700, Loss: 0.6680\n",
      "Iteration 3800, Loss: 0.6664\n",
      "Iteration 3900, Loss: 0.6649\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0260\n",
      "Iteration 200, Loss: 0.9742\n",
      "Iteration 300, Loss: 0.9347\n",
      "Iteration 400, Loss: 0.9032\n",
      "Iteration 500, Loss: 0.8775\n",
      "Iteration 600, Loss: 0.8561\n",
      "Iteration 700, Loss: 0.8381\n",
      "Iteration 800, Loss: 0.8227\n",
      "Iteration 900, Loss: 0.8093\n",
      "Iteration 1000, Loss: 0.7977\n",
      "Iteration 1100, Loss: 0.7875\n",
      "Iteration 1200, Loss: 0.7784\n",
      "Iteration 1300, Loss: 0.7704\n",
      "Iteration 1400, Loss: 0.7631\n",
      "Iteration 1500, Loss: 0.7566\n",
      "Iteration 1600, Loss: 0.7507\n",
      "Iteration 1700, Loss: 0.7453\n",
      "Iteration 1800, Loss: 0.7403\n",
      "Iteration 1900, Loss: 0.7357\n",
      "Iteration 2000, Loss: 0.7315\n",
      "Iteration 2100, Loss: 0.7276\n",
      "Iteration 2200, Loss: 0.7239\n",
      "Iteration 2300, Loss: 0.7205\n",
      "Iteration 2400, Loss: 0.7172\n",
      "Iteration 2500, Loss: 0.7142\n",
      "Iteration 2600, Loss: 0.7113\n",
      "Iteration 2700, Loss: 0.7086\n",
      "Iteration 2800, Loss: 0.7060\n",
      "Iteration 2900, Loss: 0.7035\n",
      "Iteration 3000, Loss: 0.7012\n",
      "Iteration 3100, Loss: 0.6989\n",
      "Iteration 3200, Loss: 0.6968\n",
      "Iteration 3300, Loss: 0.6947\n",
      "Iteration 3400, Loss: 0.6927\n",
      "Iteration 3500, Loss: 0.6908\n",
      "Iteration 3600, Loss: 0.6889\n",
      "Iteration 3700, Loss: 0.6872\n",
      "Iteration 3800, Loss: 0.6855\n",
      "Iteration 3900, Loss: 0.6838\n",
      "64 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0206\n",
      "Iteration 200, Loss: 0.9668\n",
      "Iteration 300, Loss: 0.9262\n",
      "Iteration 400, Loss: 0.8941\n",
      "Iteration 500, Loss: 0.8680\n",
      "Iteration 600, Loss: 0.8464\n",
      "Iteration 700, Loss: 0.8281\n",
      "Iteration 800, Loss: 0.8125\n",
      "Iteration 900, Loss: 0.7990\n",
      "Iteration 1000, Loss: 0.7872\n",
      "Iteration 1100, Loss: 0.7768\n",
      "Iteration 1200, Loss: 0.7675\n",
      "Iteration 1300, Loss: 0.7592\n",
      "Iteration 1400, Loss: 0.7517\n",
      "Iteration 1500, Loss: 0.7450\n",
      "Iteration 1600, Loss: 0.7388\n",
      "Iteration 1700, Loss: 0.7332\n",
      "Iteration 1800, Loss: 0.7281\n",
      "Iteration 1900, Loss: 0.7233\n",
      "Iteration 2000, Loss: 0.7189\n",
      "Iteration 2100, Loss: 0.7148\n",
      "Iteration 2200, Loss: 0.7110\n",
      "Iteration 2300, Loss: 0.7075\n",
      "Iteration 2400, Loss: 0.7041\n",
      "Iteration 2500, Loss: 0.7010\n",
      "Iteration 2600, Loss: 0.6980\n",
      "Iteration 2700, Loss: 0.6952\n",
      "Iteration 2800, Loss: 0.6926\n",
      "Iteration 2900, Loss: 0.6901\n",
      "Iteration 3000, Loss: 0.6877\n",
      "Iteration 3100, Loss: 0.6854\n",
      "Iteration 3200, Loss: 0.6832\n",
      "Iteration 3300, Loss: 0.6811\n",
      "Iteration 3400, Loss: 0.6791\n",
      "Iteration 3500, Loss: 0.6772\n",
      "Iteration 3600, Loss: 0.6753\n",
      "Iteration 3700, Loss: 0.6736\n",
      "Iteration 3800, Loss: 0.6719\n",
      "Iteration 3900, Loss: 0.6702\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0199\n",
      "Iteration 200, Loss: 0.9656\n",
      "Iteration 300, Loss: 0.9252\n",
      "Iteration 400, Loss: 0.8935\n",
      "Iteration 500, Loss: 0.8680\n",
      "Iteration 600, Loss: 0.8470\n",
      "Iteration 700, Loss: 0.8295\n",
      "Iteration 800, Loss: 0.8146\n",
      "Iteration 900, Loss: 0.8017\n",
      "Iteration 1000, Loss: 0.7905\n",
      "Iteration 1100, Loss: 0.7807\n",
      "Iteration 1200, Loss: 0.7720\n",
      "Iteration 1300, Loss: 0.7643\n",
      "Iteration 1400, Loss: 0.7574\n",
      "Iteration 1500, Loss: 0.7511\n",
      "Iteration 1600, Loss: 0.7455\n",
      "Iteration 1700, Loss: 0.7403\n",
      "Iteration 1800, Loss: 0.7355\n",
      "Iteration 1900, Loss: 0.7311\n",
      "Iteration 2000, Loss: 0.7271\n",
      "Iteration 2100, Loss: 0.7233\n",
      "Iteration 2200, Loss: 0.7198\n",
      "Iteration 2300, Loss: 0.7165\n",
      "Iteration 2400, Loss: 0.7134\n",
      "Iteration 2500, Loss: 0.7104\n",
      "Iteration 2600, Loss: 0.7077\n",
      "Iteration 2700, Loss: 0.7051\n",
      "Iteration 2800, Loss: 0.7026\n",
      "Iteration 2900, Loss: 0.7002\n",
      "Iteration 3000, Loss: 0.6980\n",
      "Iteration 3100, Loss: 0.6958\n",
      "Iteration 3200, Loss: 0.6938\n",
      "Iteration 3300, Loss: 0.6918\n",
      "Iteration 3400, Loss: 0.6899\n",
      "Iteration 3500, Loss: 0.6881\n",
      "Iteration 3600, Loss: 0.6863\n",
      "Iteration 3700, Loss: 0.6846\n",
      "Iteration 3800, Loss: 0.6830\n",
      "Iteration 3900, Loss: 0.6814\n",
      "65 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0197\n",
      "Iteration 200, Loss: 0.9656\n",
      "Iteration 300, Loss: 0.9250\n",
      "Iteration 400, Loss: 0.8930\n",
      "Iteration 500, Loss: 0.8673\n",
      "Iteration 600, Loss: 0.8460\n",
      "Iteration 700, Loss: 0.8282\n",
      "Iteration 800, Loss: 0.8130\n",
      "Iteration 900, Loss: 0.8000\n",
      "Iteration 1000, Loss: 0.7886\n",
      "Iteration 1100, Loss: 0.7785\n",
      "Iteration 1200, Loss: 0.7696\n",
      "Iteration 1300, Loss: 0.7617\n",
      "Iteration 1400, Loss: 0.7545\n",
      "Iteration 1500, Loss: 0.7480\n",
      "Iteration 1600, Loss: 0.7422\n",
      "Iteration 1700, Loss: 0.7368\n",
      "Iteration 1800, Loss: 0.7319\n",
      "Iteration 1900, Loss: 0.7273\n",
      "Iteration 2000, Loss: 0.7231\n",
      "Iteration 2100, Loss: 0.7192\n",
      "Iteration 2200, Loss: 0.7155\n",
      "Iteration 2300, Loss: 0.7121\n",
      "Iteration 2400, Loss: 0.7089\n",
      "Iteration 2500, Loss: 0.7058\n",
      "Iteration 2600, Loss: 0.7029\n",
      "Iteration 2700, Loss: 0.7002\n",
      "Iteration 2800, Loss: 0.6977\n",
      "Iteration 2900, Loss: 0.6952\n",
      "Iteration 3000, Loss: 0.6929\n",
      "Iteration 3100, Loss: 0.6906\n",
      "Iteration 3200, Loss: 0.6885\n",
      "Iteration 3300, Loss: 0.6864\n",
      "Iteration 3400, Loss: 0.6845\n",
      "Iteration 3500, Loss: 0.6826\n",
      "Iteration 3600, Loss: 0.6807\n",
      "Iteration 3700, Loss: 0.6790\n",
      "Iteration 3800, Loss: 0.6773\n",
      "Iteration 3900, Loss: 0.6756\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0199\n",
      "Iteration 200, Loss: 0.9654\n",
      "Iteration 300, Loss: 0.9244\n",
      "Iteration 400, Loss: 0.8922\n",
      "Iteration 500, Loss: 0.8660\n",
      "Iteration 600, Loss: 0.8444\n",
      "Iteration 700, Loss: 0.8262\n",
      "Iteration 800, Loss: 0.8107\n",
      "Iteration 900, Loss: 0.7973\n",
      "Iteration 1000, Loss: 0.7856\n",
      "Iteration 1100, Loss: 0.7754\n",
      "Iteration 1200, Loss: 0.7663\n",
      "Iteration 1300, Loss: 0.7583\n",
      "Iteration 1400, Loss: 0.7510\n",
      "Iteration 1500, Loss: 0.7445\n",
      "Iteration 1600, Loss: 0.7386\n",
      "Iteration 1700, Loss: 0.7331\n",
      "Iteration 1800, Loss: 0.7282\n",
      "Iteration 1900, Loss: 0.7236\n",
      "Iteration 2000, Loss: 0.7194\n",
      "Iteration 2100, Loss: 0.7155\n",
      "Iteration 2200, Loss: 0.7118\n",
      "Iteration 2300, Loss: 0.7084\n",
      "Iteration 2400, Loss: 0.7052\n",
      "Iteration 2500, Loss: 0.7021\n",
      "Iteration 2600, Loss: 0.6993\n",
      "Iteration 2700, Loss: 0.6966\n",
      "Iteration 2800, Loss: 0.6940\n",
      "Iteration 2900, Loss: 0.6916\n",
      "Iteration 3000, Loss: 0.6893\n",
      "Iteration 3100, Loss: 0.6871\n",
      "Iteration 3200, Loss: 0.6850\n",
      "Iteration 3300, Loss: 0.6829\n",
      "Iteration 3400, Loss: 0.6810\n",
      "Iteration 3500, Loss: 0.6791\n",
      "Iteration 3600, Loss: 0.6774\n",
      "Iteration 3700, Loss: 0.6756\n",
      "Iteration 3800, Loss: 0.6740\n",
      "Iteration 3900, Loss: 0.6724\n",
      "66 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0212\n",
      "Iteration 200, Loss: 0.9666\n",
      "Iteration 300, Loss: 0.9251\n",
      "Iteration 400, Loss: 0.8923\n",
      "Iteration 500, Loss: 0.8656\n",
      "Iteration 600, Loss: 0.8436\n",
      "Iteration 700, Loss: 0.8251\n",
      "Iteration 800, Loss: 0.8093\n",
      "Iteration 900, Loss: 0.7958\n",
      "Iteration 1000, Loss: 0.7840\n",
      "Iteration 1100, Loss: 0.7737\n",
      "Iteration 1200, Loss: 0.7646\n",
      "Iteration 1300, Loss: 0.7565\n",
      "Iteration 1400, Loss: 0.7493\n",
      "Iteration 1500, Loss: 0.7428\n",
      "Iteration 1600, Loss: 0.7370\n",
      "Iteration 1700, Loss: 0.7316\n",
      "Iteration 1800, Loss: 0.7267\n",
      "Iteration 1900, Loss: 0.7223\n",
      "Iteration 2000, Loss: 0.7182\n",
      "Iteration 2100, Loss: 0.7143\n",
      "Iteration 2200, Loss: 0.7108\n",
      "Iteration 2300, Loss: 0.7074\n",
      "Iteration 2400, Loss: 0.7043\n",
      "Iteration 2500, Loss: 0.7014\n",
      "Iteration 2600, Loss: 0.6987\n",
      "Iteration 2700, Loss: 0.6961\n",
      "Iteration 2800, Loss: 0.6936\n",
      "Iteration 2900, Loss: 0.6913\n",
      "Iteration 3000, Loss: 0.6890\n",
      "Iteration 3100, Loss: 0.6869\n",
      "Iteration 3200, Loss: 0.6849\n",
      "Iteration 3300, Loss: 0.6829\n",
      "Iteration 3400, Loss: 0.6811\n",
      "Iteration 3500, Loss: 0.6793\n",
      "Iteration 3600, Loss: 0.6775\n",
      "Iteration 3700, Loss: 0.6759\n",
      "Iteration 3800, Loss: 0.6743\n",
      "Iteration 3900, Loss: 0.6727\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0174\n",
      "Iteration 200, Loss: 0.9629\n",
      "Iteration 300, Loss: 0.9225\n",
      "Iteration 400, Loss: 0.8911\n",
      "Iteration 500, Loss: 0.8657\n",
      "Iteration 600, Loss: 0.8447\n",
      "Iteration 700, Loss: 0.8271\n",
      "Iteration 800, Loss: 0.8119\n",
      "Iteration 900, Loss: 0.7989\n",
      "Iteration 1000, Loss: 0.7875\n",
      "Iteration 1100, Loss: 0.7774\n",
      "Iteration 1200, Loss: 0.7685\n",
      "Iteration 1300, Loss: 0.7605\n",
      "Iteration 1400, Loss: 0.7532\n",
      "Iteration 1500, Loss: 0.7466\n",
      "Iteration 1600, Loss: 0.7407\n",
      "Iteration 1700, Loss: 0.7352\n",
      "Iteration 1800, Loss: 0.7301\n",
      "Iteration 1900, Loss: 0.7255\n",
      "Iteration 2000, Loss: 0.7211\n",
      "Iteration 2100, Loss: 0.7171\n",
      "Iteration 2200, Loss: 0.7133\n",
      "Iteration 2300, Loss: 0.7098\n",
      "Iteration 2400, Loss: 0.7065\n",
      "Iteration 2500, Loss: 0.7034\n",
      "Iteration 2600, Loss: 0.7004\n",
      "Iteration 2700, Loss: 0.6976\n",
      "Iteration 2800, Loss: 0.6950\n",
      "Iteration 2900, Loss: 0.6925\n",
      "Iteration 3000, Loss: 0.6901\n",
      "Iteration 3100, Loss: 0.6878\n",
      "Iteration 3200, Loss: 0.6856\n",
      "Iteration 3300, Loss: 0.6835\n",
      "Iteration 3400, Loss: 0.6815\n",
      "Iteration 3500, Loss: 0.6796\n",
      "Iteration 3600, Loss: 0.6777\n",
      "Iteration 3700, Loss: 0.6759\n",
      "Iteration 3800, Loss: 0.6742\n",
      "Iteration 3900, Loss: 0.6725\n",
      "67 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0210\n",
      "Iteration 200, Loss: 0.9681\n",
      "Iteration 300, Loss: 0.9288\n",
      "Iteration 400, Loss: 0.8980\n",
      "Iteration 500, Loss: 0.8732\n",
      "Iteration 600, Loss: 0.8527\n",
      "Iteration 700, Loss: 0.8354\n",
      "Iteration 800, Loss: 0.8205\n",
      "Iteration 900, Loss: 0.8077\n",
      "Iteration 1000, Loss: 0.7965\n",
      "Iteration 1100, Loss: 0.7866\n",
      "Iteration 1200, Loss: 0.7778\n",
      "Iteration 1300, Loss: 0.7700\n",
      "Iteration 1400, Loss: 0.7629\n",
      "Iteration 1500, Loss: 0.7565\n",
      "Iteration 1600, Loss: 0.7506\n",
      "Iteration 1700, Loss: 0.7453\n",
      "Iteration 1800, Loss: 0.7404\n",
      "Iteration 1900, Loss: 0.7358\n",
      "Iteration 2000, Loss: 0.7316\n",
      "Iteration 2100, Loss: 0.7276\n",
      "Iteration 2200, Loss: 0.7240\n",
      "Iteration 2300, Loss: 0.7205\n",
      "Iteration 2400, Loss: 0.7173\n",
      "Iteration 2500, Loss: 0.7142\n",
      "Iteration 2600, Loss: 0.7113\n",
      "Iteration 2700, Loss: 0.7086\n",
      "Iteration 2800, Loss: 0.7060\n",
      "Iteration 2900, Loss: 0.7035\n",
      "Iteration 3000, Loss: 0.7011\n",
      "Iteration 3100, Loss: 0.6988\n",
      "Iteration 3200, Loss: 0.6966\n",
      "Iteration 3300, Loss: 0.6945\n",
      "Iteration 3400, Loss: 0.6925\n",
      "Iteration 3500, Loss: 0.6906\n",
      "Iteration 3600, Loss: 0.6887\n",
      "Iteration 3700, Loss: 0.6869\n",
      "Iteration 3800, Loss: 0.6852\n",
      "Iteration 3900, Loss: 0.6835\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0185\n",
      "Iteration 200, Loss: 0.9626\n",
      "Iteration 300, Loss: 0.9204\n",
      "Iteration 400, Loss: 0.8870\n",
      "Iteration 500, Loss: 0.8601\n",
      "Iteration 600, Loss: 0.8378\n",
      "Iteration 700, Loss: 0.8190\n",
      "Iteration 800, Loss: 0.8031\n",
      "Iteration 900, Loss: 0.7894\n",
      "Iteration 1000, Loss: 0.7775\n",
      "Iteration 1100, Loss: 0.7671\n",
      "Iteration 1200, Loss: 0.7579\n",
      "Iteration 1300, Loss: 0.7497\n",
      "Iteration 1400, Loss: 0.7424\n",
      "Iteration 1500, Loss: 0.7358\n",
      "Iteration 1600, Loss: 0.7299\n",
      "Iteration 1700, Loss: 0.7244\n",
      "Iteration 1800, Loss: 0.7194\n",
      "Iteration 1900, Loss: 0.7149\n",
      "Iteration 2000, Loss: 0.7106\n",
      "Iteration 2100, Loss: 0.7067\n",
      "Iteration 2200, Loss: 0.7031\n",
      "Iteration 2300, Loss: 0.6997\n",
      "Iteration 2400, Loss: 0.6965\n",
      "Iteration 2500, Loss: 0.6935\n",
      "Iteration 2600, Loss: 0.6906\n",
      "Iteration 2700, Loss: 0.6880\n",
      "Iteration 2800, Loss: 0.6854\n",
      "Iteration 2900, Loss: 0.6830\n",
      "Iteration 3000, Loss: 0.6807\n",
      "Iteration 3100, Loss: 0.6786\n",
      "Iteration 3200, Loss: 0.6765\n",
      "Iteration 3300, Loss: 0.6745\n",
      "Iteration 3400, Loss: 0.6726\n",
      "Iteration 3500, Loss: 0.6707\n",
      "Iteration 3600, Loss: 0.6690\n",
      "Iteration 3700, Loss: 0.6673\n",
      "Iteration 3800, Loss: 0.6657\n",
      "Iteration 3900, Loss: 0.6641\n",
      "68 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0183\n",
      "Iteration 200, Loss: 0.9622\n",
      "Iteration 300, Loss: 0.9197\n",
      "Iteration 400, Loss: 0.8866\n",
      "Iteration 500, Loss: 0.8597\n",
      "Iteration 600, Loss: 0.8376\n",
      "Iteration 700, Loss: 0.8190\n",
      "Iteration 800, Loss: 0.8033\n",
      "Iteration 900, Loss: 0.7897\n",
      "Iteration 1000, Loss: 0.7779\n",
      "Iteration 1100, Loss: 0.7676\n",
      "Iteration 1200, Loss: 0.7585\n",
      "Iteration 1300, Loss: 0.7504\n",
      "Iteration 1400, Loss: 0.7432\n",
      "Iteration 1500, Loss: 0.7367\n",
      "Iteration 1600, Loss: 0.7308\n",
      "Iteration 1700, Loss: 0.7254\n",
      "Iteration 1800, Loss: 0.7205\n",
      "Iteration 1900, Loss: 0.7160\n",
      "Iteration 2000, Loss: 0.7119\n",
      "Iteration 2100, Loss: 0.7080\n",
      "Iteration 2200, Loss: 0.7044\n",
      "Iteration 2300, Loss: 0.7010\n",
      "Iteration 2400, Loss: 0.6979\n",
      "Iteration 2500, Loss: 0.6949\n",
      "Iteration 2600, Loss: 0.6921\n",
      "Iteration 2700, Loss: 0.6895\n",
      "Iteration 2800, Loss: 0.6870\n",
      "Iteration 2900, Loss: 0.6846\n",
      "Iteration 3000, Loss: 0.6823\n",
      "Iteration 3100, Loss: 0.6802\n",
      "Iteration 3200, Loss: 0.6781\n",
      "Iteration 3300, Loss: 0.6761\n",
      "Iteration 3400, Loss: 0.6742\n",
      "Iteration 3500, Loss: 0.6724\n",
      "Iteration 3600, Loss: 0.6706\n",
      "Iteration 3700, Loss: 0.6690\n",
      "Iteration 3800, Loss: 0.6673\n",
      "Iteration 3900, Loss: 0.6657\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0213\n",
      "Iteration 200, Loss: 0.9687\n",
      "Iteration 300, Loss: 0.9293\n",
      "Iteration 400, Loss: 0.8985\n",
      "Iteration 500, Loss: 0.8735\n",
      "Iteration 600, Loss: 0.8528\n",
      "Iteration 700, Loss: 0.8354\n",
      "Iteration 800, Loss: 0.8204\n",
      "Iteration 900, Loss: 0.8074\n",
      "Iteration 1000, Loss: 0.7961\n",
      "Iteration 1100, Loss: 0.7860\n",
      "Iteration 1200, Loss: 0.7771\n",
      "Iteration 1300, Loss: 0.7691\n",
      "Iteration 1400, Loss: 0.7619\n",
      "Iteration 1500, Loss: 0.7554\n",
      "Iteration 1600, Loss: 0.7495\n",
      "Iteration 1700, Loss: 0.7440\n",
      "Iteration 1800, Loss: 0.7390\n",
      "Iteration 1900, Loss: 0.7343\n",
      "Iteration 2000, Loss: 0.7300\n",
      "Iteration 2100, Loss: 0.7260\n",
      "Iteration 2200, Loss: 0.7223\n",
      "Iteration 2300, Loss: 0.7188\n",
      "Iteration 2400, Loss: 0.7155\n",
      "Iteration 2500, Loss: 0.7124\n",
      "Iteration 2600, Loss: 0.7094\n",
      "Iteration 2700, Loss: 0.7066\n",
      "Iteration 2800, Loss: 0.7040\n",
      "Iteration 2900, Loss: 0.7015\n",
      "Iteration 3000, Loss: 0.6991\n",
      "Iteration 3100, Loss: 0.6968\n",
      "Iteration 3200, Loss: 0.6946\n",
      "Iteration 3300, Loss: 0.6925\n",
      "Iteration 3400, Loss: 0.6905\n",
      "Iteration 3500, Loss: 0.6886\n",
      "Iteration 3600, Loss: 0.6867\n",
      "Iteration 3700, Loss: 0.6849\n",
      "Iteration 3800, Loss: 0.6832\n",
      "Iteration 3900, Loss: 0.6816\n",
      "69 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0237\n",
      "Iteration 200, Loss: 0.9714\n",
      "Iteration 300, Loss: 0.9320\n",
      "Iteration 400, Loss: 0.9008\n",
      "Iteration 500, Loss: 0.8754\n",
      "Iteration 600, Loss: 0.8544\n",
      "Iteration 700, Loss: 0.8368\n",
      "Iteration 800, Loss: 0.8216\n",
      "Iteration 900, Loss: 0.8086\n",
      "Iteration 1000, Loss: 0.7971\n",
      "Iteration 1100, Loss: 0.7871\n",
      "Iteration 1200, Loss: 0.7781\n",
      "Iteration 1300, Loss: 0.7702\n",
      "Iteration 1400, Loss: 0.7630\n",
      "Iteration 1500, Loss: 0.7565\n",
      "Iteration 1600, Loss: 0.7506\n",
      "Iteration 1700, Loss: 0.7452\n",
      "Iteration 1800, Loss: 0.7403\n",
      "Iteration 1900, Loss: 0.7357\n",
      "Iteration 2000, Loss: 0.7315\n",
      "Iteration 2100, Loss: 0.7276\n",
      "Iteration 2200, Loss: 0.7239\n",
      "Iteration 2300, Loss: 0.7204\n",
      "Iteration 2400, Loss: 0.7172\n",
      "Iteration 2500, Loss: 0.7141\n",
      "Iteration 2600, Loss: 0.7113\n",
      "Iteration 2700, Loss: 0.7085\n",
      "Iteration 2800, Loss: 0.7059\n",
      "Iteration 2900, Loss: 0.7035\n",
      "Iteration 3000, Loss: 0.7011\n",
      "Iteration 3100, Loss: 0.6988\n",
      "Iteration 3200, Loss: 0.6967\n",
      "Iteration 3300, Loss: 0.6946\n",
      "Iteration 3400, Loss: 0.6926\n",
      "Iteration 3500, Loss: 0.6907\n",
      "Iteration 3600, Loss: 0.6889\n",
      "Iteration 3700, Loss: 0.6871\n",
      "Iteration 3800, Loss: 0.6854\n",
      "Iteration 3900, Loss: 0.6837\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0161\n",
      "Iteration 200, Loss: 0.9598\n",
      "Iteration 300, Loss: 0.9181\n",
      "Iteration 400, Loss: 0.8854\n",
      "Iteration 500, Loss: 0.8591\n",
      "Iteration 600, Loss: 0.8374\n",
      "Iteration 700, Loss: 0.8192\n",
      "Iteration 800, Loss: 0.8037\n",
      "Iteration 900, Loss: 0.7903\n",
      "Iteration 1000, Loss: 0.7788\n",
      "Iteration 1100, Loss: 0.7685\n",
      "Iteration 1200, Loss: 0.7595\n",
      "Iteration 1300, Loss: 0.7515\n",
      "Iteration 1400, Loss: 0.7443\n",
      "Iteration 1500, Loss: 0.7378\n",
      "Iteration 1600, Loss: 0.7318\n",
      "Iteration 1700, Loss: 0.7264\n",
      "Iteration 1800, Loss: 0.7215\n",
      "Iteration 1900, Loss: 0.7169\n",
      "Iteration 2000, Loss: 0.7127\n",
      "Iteration 2100, Loss: 0.7088\n",
      "Iteration 2200, Loss: 0.7052\n",
      "Iteration 2300, Loss: 0.7018\n",
      "Iteration 2400, Loss: 0.6986\n",
      "Iteration 2500, Loss: 0.6956\n",
      "Iteration 2600, Loss: 0.6927\n",
      "Iteration 2700, Loss: 0.6901\n",
      "Iteration 2800, Loss: 0.6875\n",
      "Iteration 2900, Loss: 0.6851\n",
      "Iteration 3000, Loss: 0.6829\n",
      "Iteration 3100, Loss: 0.6807\n",
      "Iteration 3200, Loss: 0.6786\n",
      "Iteration 3300, Loss: 0.6766\n",
      "Iteration 3400, Loss: 0.6747\n",
      "Iteration 3500, Loss: 0.6729\n",
      "Iteration 3600, Loss: 0.6712\n",
      "Iteration 3700, Loss: 0.6695\n",
      "Iteration 3800, Loss: 0.6679\n",
      "Iteration 3900, Loss: 0.6663\n",
      "70 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0212\n",
      "Iteration 200, Loss: 0.9680\n",
      "Iteration 300, Loss: 0.9279\n",
      "Iteration 400, Loss: 0.8964\n",
      "Iteration 500, Loss: 0.8710\n",
      "Iteration 600, Loss: 0.8499\n",
      "Iteration 700, Loss: 0.8322\n",
      "Iteration 800, Loss: 0.8171\n",
      "Iteration 900, Loss: 0.8041\n",
      "Iteration 1000, Loss: 0.7927\n",
      "Iteration 1100, Loss: 0.7828\n",
      "Iteration 1200, Loss: 0.7739\n",
      "Iteration 1300, Loss: 0.7660\n",
      "Iteration 1400, Loss: 0.7589\n",
      "Iteration 1500, Loss: 0.7525\n",
      "Iteration 1600, Loss: 0.7467\n",
      "Iteration 1700, Loss: 0.7413\n",
      "Iteration 1800, Loss: 0.7364\n",
      "Iteration 1900, Loss: 0.7319\n",
      "Iteration 2000, Loss: 0.7277\n",
      "Iteration 2100, Loss: 0.7238\n",
      "Iteration 2200, Loss: 0.7202\n",
      "Iteration 2300, Loss: 0.7168\n",
      "Iteration 2400, Loss: 0.7136\n",
      "Iteration 2500, Loss: 0.7105\n",
      "Iteration 2600, Loss: 0.7077\n",
      "Iteration 2700, Loss: 0.7050\n",
      "Iteration 2800, Loss: 0.7024\n",
      "Iteration 2900, Loss: 0.7000\n",
      "Iteration 3000, Loss: 0.6976\n",
      "Iteration 3100, Loss: 0.6954\n",
      "Iteration 3200, Loss: 0.6933\n",
      "Iteration 3300, Loss: 0.6912\n",
      "Iteration 3400, Loss: 0.6892\n",
      "Iteration 3500, Loss: 0.6873\n",
      "Iteration 3600, Loss: 0.6855\n",
      "Iteration 3700, Loss: 0.6838\n",
      "Iteration 3800, Loss: 0.6821\n",
      "Iteration 3900, Loss: 0.6804\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0183\n",
      "Iteration 200, Loss: 0.9627\n",
      "Iteration 300, Loss: 0.9212\n",
      "Iteration 400, Loss: 0.8886\n",
      "Iteration 500, Loss: 0.8623\n",
      "Iteration 600, Loss: 0.8404\n",
      "Iteration 700, Loss: 0.8221\n",
      "Iteration 800, Loss: 0.8065\n",
      "Iteration 900, Loss: 0.7930\n",
      "Iteration 1000, Loss: 0.7812\n",
      "Iteration 1100, Loss: 0.7708\n",
      "Iteration 1200, Loss: 0.7617\n",
      "Iteration 1300, Loss: 0.7536\n",
      "Iteration 1400, Loss: 0.7463\n",
      "Iteration 1500, Loss: 0.7397\n",
      "Iteration 1600, Loss: 0.7337\n",
      "Iteration 1700, Loss: 0.7283\n",
      "Iteration 1800, Loss: 0.7233\n",
      "Iteration 1900, Loss: 0.7187\n",
      "Iteration 2000, Loss: 0.7144\n",
      "Iteration 2100, Loss: 0.7104\n",
      "Iteration 2200, Loss: 0.7068\n",
      "Iteration 2300, Loss: 0.7033\n",
      "Iteration 2400, Loss: 0.7001\n",
      "Iteration 2500, Loss: 0.6971\n",
      "Iteration 2600, Loss: 0.6942\n",
      "Iteration 2700, Loss: 0.6915\n",
      "Iteration 2800, Loss: 0.6889\n",
      "Iteration 2900, Loss: 0.6864\n",
      "Iteration 3000, Loss: 0.6841\n",
      "Iteration 3100, Loss: 0.6819\n",
      "Iteration 3200, Loss: 0.6798\n",
      "Iteration 3300, Loss: 0.6777\n",
      "Iteration 3400, Loss: 0.6758\n",
      "Iteration 3500, Loss: 0.6739\n",
      "Iteration 3600, Loss: 0.6721\n",
      "Iteration 3700, Loss: 0.6704\n",
      "Iteration 3800, Loss: 0.6687\n",
      "Iteration 3900, Loss: 0.6671\n",
      "71 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0245\n",
      "Iteration 200, Loss: 0.9723\n",
      "Iteration 300, Loss: 0.9327\n",
      "Iteration 400, Loss: 0.9012\n",
      "Iteration 500, Loss: 0.8754\n",
      "Iteration 600, Loss: 0.8540\n",
      "Iteration 700, Loss: 0.8359\n",
      "Iteration 800, Loss: 0.8204\n",
      "Iteration 900, Loss: 0.8071\n",
      "Iteration 1000, Loss: 0.7954\n",
      "Iteration 1100, Loss: 0.7851\n",
      "Iteration 1200, Loss: 0.7760\n",
      "Iteration 1300, Loss: 0.7678\n",
      "Iteration 1400, Loss: 0.7605\n",
      "Iteration 1500, Loss: 0.7539\n",
      "Iteration 1600, Loss: 0.7479\n",
      "Iteration 1700, Loss: 0.7425\n",
      "Iteration 1800, Loss: 0.7374\n",
      "Iteration 1900, Loss: 0.7328\n",
      "Iteration 2000, Loss: 0.7285\n",
      "Iteration 2100, Loss: 0.7246\n",
      "Iteration 2200, Loss: 0.7209\n",
      "Iteration 2300, Loss: 0.7174\n",
      "Iteration 2400, Loss: 0.7141\n",
      "Iteration 2500, Loss: 0.7111\n",
      "Iteration 2600, Loss: 0.7082\n",
      "Iteration 2700, Loss: 0.7054\n",
      "Iteration 2800, Loss: 0.7028\n",
      "Iteration 2900, Loss: 0.7004\n",
      "Iteration 3000, Loss: 0.6980\n",
      "Iteration 3100, Loss: 0.6958\n",
      "Iteration 3200, Loss: 0.6936\n",
      "Iteration 3300, Loss: 0.6916\n",
      "Iteration 3400, Loss: 0.6896\n",
      "Iteration 3500, Loss: 0.6877\n",
      "Iteration 3600, Loss: 0.6859\n",
      "Iteration 3700, Loss: 0.6842\n",
      "Iteration 3800, Loss: 0.6825\n",
      "Iteration 3900, Loss: 0.6808\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0151\n",
      "Iteration 200, Loss: 0.9585\n",
      "Iteration 300, Loss: 0.9167\n",
      "Iteration 400, Loss: 0.8842\n",
      "Iteration 500, Loss: 0.8582\n",
      "Iteration 600, Loss: 0.8368\n",
      "Iteration 700, Loss: 0.8189\n",
      "Iteration 800, Loss: 0.8038\n",
      "Iteration 900, Loss: 0.7907\n",
      "Iteration 1000, Loss: 0.7794\n",
      "Iteration 1100, Loss: 0.7694\n",
      "Iteration 1200, Loss: 0.7605\n",
      "Iteration 1300, Loss: 0.7526\n",
      "Iteration 1400, Loss: 0.7455\n",
      "Iteration 1500, Loss: 0.7391\n",
      "Iteration 1600, Loss: 0.7333\n",
      "Iteration 1700, Loss: 0.7280\n",
      "Iteration 1800, Loss: 0.7231\n",
      "Iteration 1900, Loss: 0.7187\n",
      "Iteration 2000, Loss: 0.7145\n",
      "Iteration 2100, Loss: 0.7107\n",
      "Iteration 2200, Loss: 0.7071\n",
      "Iteration 2300, Loss: 0.7037\n",
      "Iteration 2400, Loss: 0.7005\n",
      "Iteration 2500, Loss: 0.6976\n",
      "Iteration 2600, Loss: 0.6948\n",
      "Iteration 2700, Loss: 0.6921\n",
      "Iteration 2800, Loss: 0.6896\n",
      "Iteration 2900, Loss: 0.6872\n",
      "Iteration 3000, Loss: 0.6849\n",
      "Iteration 3100, Loss: 0.6828\n",
      "Iteration 3200, Loss: 0.6807\n",
      "Iteration 3300, Loss: 0.6787\n",
      "Iteration 3400, Loss: 0.6768\n",
      "Iteration 3500, Loss: 0.6750\n",
      "Iteration 3600, Loss: 0.6732\n",
      "Iteration 3700, Loss: 0.6715\n",
      "Iteration 3800, Loss: 0.6699\n",
      "Iteration 3900, Loss: 0.6683\n",
      "72 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0204\n",
      "Iteration 200, Loss: 0.9667\n",
      "Iteration 300, Loss: 0.9265\n",
      "Iteration 400, Loss: 0.8951\n",
      "Iteration 500, Loss: 0.8698\n",
      "Iteration 600, Loss: 0.8490\n",
      "Iteration 700, Loss: 0.8315\n",
      "Iteration 800, Loss: 0.8167\n",
      "Iteration 900, Loss: 0.8039\n",
      "Iteration 1000, Loss: 0.7929\n",
      "Iteration 1100, Loss: 0.7832\n",
      "Iteration 1200, Loss: 0.7747\n",
      "Iteration 1300, Loss: 0.7671\n",
      "Iteration 1400, Loss: 0.7603\n",
      "Iteration 1500, Loss: 0.7542\n",
      "Iteration 1600, Loss: 0.7486\n",
      "Iteration 1700, Loss: 0.7436\n",
      "Iteration 1800, Loss: 0.7390\n",
      "Iteration 1900, Loss: 0.7347\n",
      "Iteration 2000, Loss: 0.7308\n",
      "Iteration 2100, Loss: 0.7271\n",
      "Iteration 2200, Loss: 0.7237\n",
      "Iteration 2300, Loss: 0.7206\n",
      "Iteration 2400, Loss: 0.7176\n",
      "Iteration 2500, Loss: 0.7147\n",
      "Iteration 2600, Loss: 0.7121\n",
      "Iteration 2700, Loss: 0.7096\n",
      "Iteration 2800, Loss: 0.7072\n",
      "Iteration 2900, Loss: 0.7050\n",
      "Iteration 3000, Loss: 0.7028\n",
      "Iteration 3100, Loss: 0.7008\n",
      "Iteration 3200, Loss: 0.6988\n",
      "Iteration 3300, Loss: 0.6969\n",
      "Iteration 3400, Loss: 0.6951\n",
      "Iteration 3500, Loss: 0.6934\n",
      "Iteration 3600, Loss: 0.6917\n",
      "Iteration 3700, Loss: 0.6901\n",
      "Iteration 3800, Loss: 0.6886\n",
      "Iteration 3900, Loss: 0.6871\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0189\n",
      "Iteration 200, Loss: 0.9638\n",
      "Iteration 300, Loss: 0.9221\n",
      "Iteration 400, Loss: 0.8892\n",
      "Iteration 500, Loss: 0.8625\n",
      "Iteration 600, Loss: 0.8404\n",
      "Iteration 700, Loss: 0.8217\n",
      "Iteration 800, Loss: 0.8057\n",
      "Iteration 900, Loss: 0.7918\n",
      "Iteration 1000, Loss: 0.7796\n",
      "Iteration 1100, Loss: 0.7689\n",
      "Iteration 1200, Loss: 0.7593\n",
      "Iteration 1300, Loss: 0.7508\n",
      "Iteration 1400, Loss: 0.7430\n",
      "Iteration 1500, Loss: 0.7361\n",
      "Iteration 1600, Loss: 0.7297\n",
      "Iteration 1700, Loss: 0.7239\n",
      "Iteration 1800, Loss: 0.7186\n",
      "Iteration 1900, Loss: 0.7136\n",
      "Iteration 2000, Loss: 0.7090\n",
      "Iteration 2100, Loss: 0.7048\n",
      "Iteration 2200, Loss: 0.7008\n",
      "Iteration 2300, Loss: 0.6971\n",
      "Iteration 2400, Loss: 0.6936\n",
      "Iteration 2500, Loss: 0.6903\n",
      "Iteration 2600, Loss: 0.6872\n",
      "Iteration 2700, Loss: 0.6843\n",
      "Iteration 2800, Loss: 0.6815\n",
      "Iteration 2900, Loss: 0.6788\n",
      "Iteration 3000, Loss: 0.6763\n",
      "Iteration 3100, Loss: 0.6739\n",
      "Iteration 3200, Loss: 0.6716\n",
      "Iteration 3300, Loss: 0.6694\n",
      "Iteration 3400, Loss: 0.6673\n",
      "Iteration 3500, Loss: 0.6653\n",
      "Iteration 3600, Loss: 0.6633\n",
      "Iteration 3700, Loss: 0.6614\n",
      "Iteration 3800, Loss: 0.6596\n",
      "Iteration 3900, Loss: 0.6579\n",
      "73 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0208\n",
      "Iteration 200, Loss: 0.9665\n",
      "Iteration 300, Loss: 0.9258\n",
      "Iteration 400, Loss: 0.8938\n",
      "Iteration 500, Loss: 0.8679\n",
      "Iteration 600, Loss: 0.8465\n",
      "Iteration 700, Loss: 0.8285\n",
      "Iteration 800, Loss: 0.8132\n",
      "Iteration 900, Loss: 0.8001\n",
      "Iteration 1000, Loss: 0.7887\n",
      "Iteration 1100, Loss: 0.7787\n",
      "Iteration 1200, Loss: 0.7699\n",
      "Iteration 1300, Loss: 0.7621\n",
      "Iteration 1400, Loss: 0.7551\n",
      "Iteration 1500, Loss: 0.7488\n",
      "Iteration 1600, Loss: 0.7431\n",
      "Iteration 1700, Loss: 0.7379\n",
      "Iteration 1800, Loss: 0.7331\n",
      "Iteration 1900, Loss: 0.7287\n",
      "Iteration 2000, Loss: 0.7247\n",
      "Iteration 2100, Loss: 0.7209\n",
      "Iteration 2200, Loss: 0.7174\n",
      "Iteration 2300, Loss: 0.7142\n",
      "Iteration 2400, Loss: 0.7111\n",
      "Iteration 2500, Loss: 0.7082\n",
      "Iteration 2600, Loss: 0.7055\n",
      "Iteration 2700, Loss: 0.7030\n",
      "Iteration 2800, Loss: 0.7005\n",
      "Iteration 2900, Loss: 0.6982\n",
      "Iteration 3000, Loss: 0.6960\n",
      "Iteration 3100, Loss: 0.6939\n",
      "Iteration 3200, Loss: 0.6919\n",
      "Iteration 3300, Loss: 0.6900\n",
      "Iteration 3400, Loss: 0.6881\n",
      "Iteration 3500, Loss: 0.6863\n",
      "Iteration 3600, Loss: 0.6846\n",
      "Iteration 3700, Loss: 0.6830\n",
      "Iteration 3800, Loss: 0.6814\n",
      "Iteration 3900, Loss: 0.6799\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0186\n",
      "Iteration 200, Loss: 0.9639\n",
      "Iteration 300, Loss: 0.9229\n",
      "Iteration 400, Loss: 0.8907\n",
      "Iteration 500, Loss: 0.8647\n",
      "Iteration 600, Loss: 0.8433\n",
      "Iteration 700, Loss: 0.8252\n",
      "Iteration 800, Loss: 0.8098\n",
      "Iteration 900, Loss: 0.7966\n",
      "Iteration 1000, Loss: 0.7850\n",
      "Iteration 1100, Loss: 0.7748\n",
      "Iteration 1200, Loss: 0.7657\n",
      "Iteration 1300, Loss: 0.7576\n",
      "Iteration 1400, Loss: 0.7503\n",
      "Iteration 1500, Loss: 0.7437\n",
      "Iteration 1600, Loss: 0.7377\n",
      "Iteration 1700, Loss: 0.7321\n",
      "Iteration 1800, Loss: 0.7271\n",
      "Iteration 1900, Loss: 0.7224\n",
      "Iteration 2000, Loss: 0.7180\n",
      "Iteration 2100, Loss: 0.7139\n",
      "Iteration 2200, Loss: 0.7102\n",
      "Iteration 2300, Loss: 0.7066\n",
      "Iteration 2400, Loss: 0.7032\n",
      "Iteration 2500, Loss: 0.7001\n",
      "Iteration 2600, Loss: 0.6971\n",
      "Iteration 2700, Loss: 0.6943\n",
      "Iteration 2800, Loss: 0.6916\n",
      "Iteration 2900, Loss: 0.6890\n",
      "Iteration 3000, Loss: 0.6866\n",
      "Iteration 3100, Loss: 0.6842\n",
      "Iteration 3200, Loss: 0.6820\n",
      "Iteration 3300, Loss: 0.6798\n",
      "Iteration 3400, Loss: 0.6778\n",
      "Iteration 3500, Loss: 0.6758\n",
      "Iteration 3600, Loss: 0.6739\n",
      "Iteration 3700, Loss: 0.6721\n",
      "Iteration 3800, Loss: 0.6703\n",
      "Iteration 3900, Loss: 0.6686\n",
      "74 270\n",
      "Iteration 0, Loss: 1.0978\n",
      "Iteration 100, Loss: 1.0283\n",
      "Iteration 200, Loss: 0.9785\n",
      "Iteration 300, Loss: 0.9405\n",
      "Iteration 400, Loss: 0.9103\n",
      "Iteration 500, Loss: 0.8857\n",
      "Iteration 600, Loss: 0.8653\n",
      "Iteration 700, Loss: 0.8480\n",
      "Iteration 800, Loss: 0.8333\n",
      "Iteration 900, Loss: 0.8205\n",
      "Iteration 1000, Loss: 0.8094\n",
      "Iteration 1100, Loss: 0.7996\n",
      "Iteration 1200, Loss: 0.7909\n",
      "Iteration 1300, Loss: 0.7832\n",
      "Iteration 1400, Loss: 0.7762\n",
      "Iteration 1500, Loss: 0.7699\n",
      "Iteration 1600, Loss: 0.7641\n",
      "Iteration 1700, Loss: 0.7589\n",
      "Iteration 1800, Loss: 0.7541\n",
      "Iteration 1900, Loss: 0.7496\n",
      "Iteration 2000, Loss: 0.7455\n",
      "Iteration 2100, Loss: 0.7417\n",
      "Iteration 2200, Loss: 0.7381\n",
      "Iteration 2300, Loss: 0.7347\n",
      "Iteration 2400, Loss: 0.7316\n",
      "Iteration 2500, Loss: 0.7286\n",
      "Iteration 2600, Loss: 0.7258\n",
      "Iteration 2700, Loss: 0.7231\n",
      "Iteration 2800, Loss: 0.7206\n",
      "Iteration 2900, Loss: 0.7182\n",
      "Iteration 3000, Loss: 0.7159\n",
      "Iteration 3100, Loss: 0.7137\n",
      "Iteration 3200, Loss: 0.7116\n",
      "Iteration 3300, Loss: 0.7096\n",
      "Iteration 3400, Loss: 0.7077\n",
      "Iteration 3500, Loss: 0.7058\n",
      "Iteration 3600, Loss: 0.7040\n",
      "Iteration 3700, Loss: 0.7023\n",
      "Iteration 3800, Loss: 0.7007\n",
      "Iteration 3900, Loss: 0.6991\n",
      "Iteration 0, Loss: 1.0975\n",
      "Iteration 100, Loss: 1.0109\n",
      "Iteration 200, Loss: 0.9520\n",
      "Iteration 300, Loss: 0.9086\n",
      "Iteration 400, Loss: 0.8748\n",
      "Iteration 500, Loss: 0.8477\n",
      "Iteration 600, Loss: 0.8254\n",
      "Iteration 700, Loss: 0.8066\n",
      "Iteration 800, Loss: 0.7907\n",
      "Iteration 900, Loss: 0.7770\n",
      "Iteration 1000, Loss: 0.7650\n",
      "Iteration 1100, Loss: 0.7544\n",
      "Iteration 1200, Loss: 0.7451\n",
      "Iteration 1300, Loss: 0.7368\n",
      "Iteration 1400, Loss: 0.7293\n",
      "Iteration 1500, Loss: 0.7225\n",
      "Iteration 1600, Loss: 0.7163\n",
      "Iteration 1700, Loss: 0.7107\n",
      "Iteration 1800, Loss: 0.7056\n",
      "Iteration 1900, Loss: 0.7008\n",
      "Iteration 2000, Loss: 0.6965\n",
      "Iteration 2100, Loss: 0.6924\n",
      "Iteration 2200, Loss: 0.6886\n",
      "Iteration 2300, Loss: 0.6851\n",
      "Iteration 2400, Loss: 0.6818\n",
      "Iteration 2500, Loss: 0.6787\n",
      "Iteration 2600, Loss: 0.6757\n",
      "Iteration 2700, Loss: 0.6729\n",
      "Iteration 2800, Loss: 0.6703\n",
      "Iteration 2900, Loss: 0.6678\n",
      "Iteration 3000, Loss: 0.6654\n",
      "Iteration 3100, Loss: 0.6632\n",
      "Iteration 3200, Loss: 0.6610\n",
      "Iteration 3300, Loss: 0.6589\n",
      "Iteration 3400, Loss: 0.6569\n",
      "Iteration 3500, Loss: 0.6550\n",
      "Iteration 3600, Loss: 0.6532\n",
      "Iteration 3700, Loss: 0.6514\n",
      "Iteration 3800, Loss: 0.6497\n",
      "Iteration 3900, Loss: 0.6481\n",
      "75 270\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0571\n",
      "Iteration 200, Loss: 1.0237\n",
      "Iteration 300, Loss: 0.9955\n",
      "Iteration 400, Loss: 0.9715\n",
      "Iteration 500, Loss: 0.9503\n",
      "Iteration 600, Loss: 0.9317\n",
      "Iteration 700, Loss: 0.9151\n",
      "Iteration 800, Loss: 0.9002\n",
      "Iteration 900, Loss: 0.8868\n",
      "Iteration 1000, Loss: 0.8745\n",
      "Iteration 1100, Loss: 0.8635\n",
      "Iteration 1200, Loss: 0.8533\n",
      "Iteration 1300, Loss: 0.8440\n",
      "Iteration 1400, Loss: 0.8353\n",
      "Iteration 1500, Loss: 0.8273\n",
      "Iteration 1600, Loss: 0.8199\n",
      "Iteration 1700, Loss: 0.8130\n",
      "Iteration 1800, Loss: 0.8066\n",
      "Iteration 1900, Loss: 0.8006\n",
      "Iteration 2000, Loss: 0.7950\n",
      "Iteration 2100, Loss: 0.7897\n",
      "Iteration 2200, Loss: 0.7848\n",
      "Iteration 2300, Loss: 0.7801\n",
      "Iteration 2400, Loss: 0.7758\n",
      "Iteration 2500, Loss: 0.7716\n",
      "Iteration 2600, Loss: 0.7677\n",
      "Iteration 2700, Loss: 0.7640\n",
      "Iteration 2800, Loss: 0.7604\n",
      "Iteration 2900, Loss: 0.7571\n",
      "Iteration 3000, Loss: 0.7539\n",
      "Iteration 3100, Loss: 0.7508\n",
      "Iteration 3200, Loss: 0.7479\n",
      "Iteration 3300, Loss: 0.7451\n",
      "Iteration 3400, Loss: 0.7425\n",
      "Iteration 3500, Loss: 0.7399\n",
      "Iteration 3600, Loss: 0.7375\n",
      "Iteration 3700, Loss: 0.7351\n",
      "Iteration 3800, Loss: 0.7328\n",
      "Iteration 3900, Loss: 0.7306\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0497\n",
      "Iteration 200, Loss: 1.0111\n",
      "Iteration 300, Loss: 0.9792\n",
      "Iteration 400, Loss: 0.9522\n",
      "Iteration 500, Loss: 0.9293\n",
      "Iteration 600, Loss: 0.9094\n",
      "Iteration 700, Loss: 0.8919\n",
      "Iteration 800, Loss: 0.8764\n",
      "Iteration 900, Loss: 0.8626\n",
      "Iteration 1000, Loss: 0.8502\n",
      "Iteration 1100, Loss: 0.8390\n",
      "Iteration 1200, Loss: 0.8289\n",
      "Iteration 1300, Loss: 0.8196\n",
      "Iteration 1400, Loss: 0.8111\n",
      "Iteration 1500, Loss: 0.8033\n",
      "Iteration 1600, Loss: 0.7961\n",
      "Iteration 1700, Loss: 0.7894\n",
      "Iteration 1800, Loss: 0.7832\n",
      "Iteration 1900, Loss: 0.7774\n",
      "Iteration 2000, Loss: 0.7720\n",
      "Iteration 2100, Loss: 0.7670\n",
      "Iteration 2200, Loss: 0.7622\n",
      "Iteration 2300, Loss: 0.7578\n",
      "Iteration 2400, Loss: 0.7536\n",
      "Iteration 2500, Loss: 0.7497\n",
      "Iteration 2600, Loss: 0.7459\n",
      "Iteration 2700, Loss: 0.7424\n",
      "Iteration 2800, Loss: 0.7390\n",
      "Iteration 2900, Loss: 0.7358\n",
      "Iteration 3000, Loss: 0.7328\n",
      "Iteration 3100, Loss: 0.7300\n",
      "Iteration 3200, Loss: 0.7272\n",
      "Iteration 3300, Loss: 0.7246\n",
      "Iteration 3400, Loss: 0.7221\n",
      "Iteration 3500, Loss: 0.7197\n",
      "Iteration 3600, Loss: 0.7174\n",
      "Iteration 3700, Loss: 0.7152\n",
      "Iteration 3800, Loss: 0.7131\n",
      "Iteration 3900, Loss: 0.7111\n",
      "76 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0541\n",
      "Iteration 200, Loss: 1.0185\n",
      "Iteration 300, Loss: 0.9890\n",
      "Iteration 400, Loss: 0.9640\n",
      "Iteration 500, Loss: 0.9424\n",
      "Iteration 600, Loss: 0.9234\n",
      "Iteration 700, Loss: 0.9066\n",
      "Iteration 800, Loss: 0.8916\n",
      "Iteration 900, Loss: 0.8782\n",
      "Iteration 1000, Loss: 0.8661\n",
      "Iteration 1100, Loss: 0.8551\n",
      "Iteration 1200, Loss: 0.8452\n",
      "Iteration 1300, Loss: 0.8361\n",
      "Iteration 1400, Loss: 0.8277\n",
      "Iteration 1500, Loss: 0.8199\n",
      "Iteration 1600, Loss: 0.8128\n",
      "Iteration 1700, Loss: 0.8062\n",
      "Iteration 1800, Loss: 0.8001\n",
      "Iteration 1900, Loss: 0.7944\n",
      "Iteration 2000, Loss: 0.7890\n",
      "Iteration 2100, Loss: 0.7840\n",
      "Iteration 2200, Loss: 0.7793\n",
      "Iteration 2300, Loss: 0.7749\n",
      "Iteration 2400, Loss: 0.7708\n",
      "Iteration 2500, Loss: 0.7668\n",
      "Iteration 2600, Loss: 0.7631\n",
      "Iteration 2700, Loss: 0.7596\n",
      "Iteration 2800, Loss: 0.7563\n",
      "Iteration 2900, Loss: 0.7531\n",
      "Iteration 3000, Loss: 0.7501\n",
      "Iteration 3100, Loss: 0.7473\n",
      "Iteration 3200, Loss: 0.7445\n",
      "Iteration 3300, Loss: 0.7419\n",
      "Iteration 3400, Loss: 0.7394\n",
      "Iteration 3500, Loss: 0.7370\n",
      "Iteration 3600, Loss: 0.7347\n",
      "Iteration 3700, Loss: 0.7325\n",
      "Iteration 3800, Loss: 0.7304\n",
      "Iteration 3900, Loss: 0.7284\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0532\n",
      "Iteration 200, Loss: 1.0168\n",
      "Iteration 300, Loss: 0.9864\n",
      "Iteration 400, Loss: 0.9606\n",
      "Iteration 500, Loss: 0.9382\n",
      "Iteration 600, Loss: 0.9186\n",
      "Iteration 700, Loss: 0.9012\n",
      "Iteration 800, Loss: 0.8856\n",
      "Iteration 900, Loss: 0.8717\n",
      "Iteration 1000, Loss: 0.8591\n",
      "Iteration 1100, Loss: 0.8476\n",
      "Iteration 1200, Loss: 0.8371\n",
      "Iteration 1300, Loss: 0.8276\n",
      "Iteration 1400, Loss: 0.8187\n",
      "Iteration 1500, Loss: 0.8106\n",
      "Iteration 1600, Loss: 0.8030\n",
      "Iteration 1700, Loss: 0.7960\n",
      "Iteration 1800, Loss: 0.7895\n",
      "Iteration 1900, Loss: 0.7834\n",
      "Iteration 2000, Loss: 0.7776\n",
      "Iteration 2100, Loss: 0.7722\n",
      "Iteration 2200, Loss: 0.7672\n",
      "Iteration 2300, Loss: 0.7625\n",
      "Iteration 2400, Loss: 0.7580\n",
      "Iteration 2500, Loss: 0.7538\n",
      "Iteration 2600, Loss: 0.7498\n",
      "Iteration 2700, Loss: 0.7459\n",
      "Iteration 2800, Loss: 0.7423\n",
      "Iteration 2900, Loss: 0.7388\n",
      "Iteration 3000, Loss: 0.7355\n",
      "Iteration 3100, Loss: 0.7324\n",
      "Iteration 3200, Loss: 0.7294\n",
      "Iteration 3300, Loss: 0.7266\n",
      "Iteration 3400, Loss: 0.7238\n",
      "Iteration 3500, Loss: 0.7212\n",
      "Iteration 3600, Loss: 0.7187\n",
      "Iteration 3700, Loss: 0.7163\n",
      "Iteration 3800, Loss: 0.7140\n",
      "Iteration 3900, Loss: 0.7117\n",
      "77 270\n",
      "Iteration 0, Loss: 1.0980\n",
      "Iteration 100, Loss: 1.0494\n",
      "Iteration 200, Loss: 1.0107\n",
      "Iteration 300, Loss: 0.9787\n",
      "Iteration 400, Loss: 0.9517\n",
      "Iteration 500, Loss: 0.9285\n",
      "Iteration 600, Loss: 0.9084\n",
      "Iteration 700, Loss: 0.8906\n",
      "Iteration 800, Loss: 0.8749\n",
      "Iteration 900, Loss: 0.8607\n",
      "Iteration 1000, Loss: 0.8480\n",
      "Iteration 1100, Loss: 0.8364\n",
      "Iteration 1200, Loss: 0.8259\n",
      "Iteration 1300, Loss: 0.8164\n",
      "Iteration 1400, Loss: 0.8076\n",
      "Iteration 1500, Loss: 0.7995\n",
      "Iteration 1600, Loss: 0.7920\n",
      "Iteration 1700, Loss: 0.7851\n",
      "Iteration 1800, Loss: 0.7786\n",
      "Iteration 1900, Loss: 0.7726\n",
      "Iteration 2000, Loss: 0.7670\n",
      "Iteration 2100, Loss: 0.7617\n",
      "Iteration 2200, Loss: 0.7568\n",
      "Iteration 2300, Loss: 0.7521\n",
      "Iteration 2400, Loss: 0.7477\n",
      "Iteration 2500, Loss: 0.7436\n",
      "Iteration 2600, Loss: 0.7398\n",
      "Iteration 2700, Loss: 0.7361\n",
      "Iteration 2800, Loss: 0.7326\n",
      "Iteration 2900, Loss: 0.7292\n",
      "Iteration 3000, Loss: 0.7261\n",
      "Iteration 3100, Loss: 0.7230\n",
      "Iteration 3200, Loss: 0.7202\n",
      "Iteration 3300, Loss: 0.7174\n",
      "Iteration 3400, Loss: 0.7148\n",
      "Iteration 3500, Loss: 0.7123\n",
      "Iteration 3600, Loss: 0.7099\n",
      "Iteration 3700, Loss: 0.7076\n",
      "Iteration 3800, Loss: 0.7053\n",
      "Iteration 3900, Loss: 0.7032\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0579\n",
      "Iteration 200, Loss: 1.0246\n",
      "Iteration 300, Loss: 0.9964\n",
      "Iteration 400, Loss: 0.9721\n",
      "Iteration 500, Loss: 0.9511\n",
      "Iteration 600, Loss: 0.9325\n",
      "Iteration 700, Loss: 0.9162\n",
      "Iteration 800, Loss: 0.9014\n",
      "Iteration 900, Loss: 0.8881\n",
      "Iteration 1000, Loss: 0.8761\n",
      "Iteration 1100, Loss: 0.8652\n",
      "Iteration 1200, Loss: 0.8552\n",
      "Iteration 1300, Loss: 0.8460\n",
      "Iteration 1400, Loss: 0.8376\n",
      "Iteration 1500, Loss: 0.8298\n",
      "Iteration 1600, Loss: 0.8226\n",
      "Iteration 1700, Loss: 0.8159\n",
      "Iteration 1800, Loss: 0.8097\n",
      "Iteration 1900, Loss: 0.8038\n",
      "Iteration 2000, Loss: 0.7984\n",
      "Iteration 2100, Loss: 0.7933\n",
      "Iteration 2200, Loss: 0.7885\n",
      "Iteration 2300, Loss: 0.7839\n",
      "Iteration 2400, Loss: 0.7797\n",
      "Iteration 2500, Loss: 0.7756\n",
      "Iteration 2600, Loss: 0.7718\n",
      "Iteration 2700, Loss: 0.7682\n",
      "Iteration 2800, Loss: 0.7648\n",
      "Iteration 2900, Loss: 0.7615\n",
      "Iteration 3000, Loss: 0.7583\n",
      "Iteration 3100, Loss: 0.7554\n",
      "Iteration 3200, Loss: 0.7526\n",
      "Iteration 3300, Loss: 0.7499\n",
      "Iteration 3400, Loss: 0.7473\n",
      "Iteration 3500, Loss: 0.7448\n",
      "Iteration 3600, Loss: 0.7424\n",
      "Iteration 3700, Loss: 0.7401\n",
      "Iteration 3800, Loss: 0.7379\n",
      "Iteration 3900, Loss: 0.7358\n",
      "78 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0535\n",
      "Iteration 200, Loss: 1.0171\n",
      "Iteration 300, Loss: 0.9867\n",
      "Iteration 400, Loss: 0.9608\n",
      "Iteration 500, Loss: 0.9384\n",
      "Iteration 600, Loss: 0.9189\n",
      "Iteration 700, Loss: 0.9016\n",
      "Iteration 800, Loss: 0.8864\n",
      "Iteration 900, Loss: 0.8727\n",
      "Iteration 1000, Loss: 0.8603\n",
      "Iteration 1100, Loss: 0.8491\n",
      "Iteration 1200, Loss: 0.8389\n",
      "Iteration 1300, Loss: 0.8295\n",
      "Iteration 1400, Loss: 0.8209\n",
      "Iteration 1500, Loss: 0.8129\n",
      "Iteration 1600, Loss: 0.8055\n",
      "Iteration 1700, Loss: 0.7987\n",
      "Iteration 1800, Loss: 0.7924\n",
      "Iteration 1900, Loss: 0.7864\n",
      "Iteration 2000, Loss: 0.7809\n",
      "Iteration 2100, Loss: 0.7757\n",
      "Iteration 2200, Loss: 0.7708\n",
      "Iteration 2300, Loss: 0.7662\n",
      "Iteration 2400, Loss: 0.7619\n",
      "Iteration 2500, Loss: 0.7578\n",
      "Iteration 2600, Loss: 0.7540\n",
      "Iteration 2700, Loss: 0.7503\n",
      "Iteration 2800, Loss: 0.7469\n",
      "Iteration 2900, Loss: 0.7436\n",
      "Iteration 3000, Loss: 0.7404\n",
      "Iteration 3100, Loss: 0.7374\n",
      "Iteration 3200, Loss: 0.7345\n",
      "Iteration 3300, Loss: 0.7318\n",
      "Iteration 3400, Loss: 0.7292\n",
      "Iteration 3500, Loss: 0.7267\n",
      "Iteration 3600, Loss: 0.7243\n",
      "Iteration 3700, Loss: 0.7220\n",
      "Iteration 3800, Loss: 0.7198\n",
      "Iteration 3900, Loss: 0.7176\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0538\n",
      "Iteration 200, Loss: 1.0181\n",
      "Iteration 300, Loss: 0.9886\n",
      "Iteration 400, Loss: 0.9635\n",
      "Iteration 500, Loss: 0.9418\n",
      "Iteration 600, Loss: 0.9227\n",
      "Iteration 700, Loss: 0.9058\n",
      "Iteration 800, Loss: 0.8907\n",
      "Iteration 900, Loss: 0.8770\n",
      "Iteration 1000, Loss: 0.8648\n",
      "Iteration 1100, Loss: 0.8536\n",
      "Iteration 1200, Loss: 0.8434\n",
      "Iteration 1300, Loss: 0.8341\n",
      "Iteration 1400, Loss: 0.8255\n",
      "Iteration 1500, Loss: 0.8177\n",
      "Iteration 1600, Loss: 0.8103\n",
      "Iteration 1700, Loss: 0.8036\n",
      "Iteration 1800, Loss: 0.7972\n",
      "Iteration 1900, Loss: 0.7913\n",
      "Iteration 2000, Loss: 0.7858\n",
      "Iteration 2100, Loss: 0.7806\n",
      "Iteration 2200, Loss: 0.7758\n",
      "Iteration 2300, Loss: 0.7712\n",
      "Iteration 2400, Loss: 0.7669\n",
      "Iteration 2500, Loss: 0.7629\n",
      "Iteration 2600, Loss: 0.7591\n",
      "Iteration 2700, Loss: 0.7554\n",
      "Iteration 2800, Loss: 0.7520\n",
      "Iteration 2900, Loss: 0.7488\n",
      "Iteration 3000, Loss: 0.7457\n",
      "Iteration 3100, Loss: 0.7427\n",
      "Iteration 3200, Loss: 0.7399\n",
      "Iteration 3300, Loss: 0.7372\n",
      "Iteration 3400, Loss: 0.7346\n",
      "Iteration 3500, Loss: 0.7322\n",
      "Iteration 3600, Loss: 0.7298\n",
      "Iteration 3700, Loss: 0.7275\n",
      "Iteration 3800, Loss: 0.7254\n",
      "Iteration 3900, Loss: 0.7233\n",
      "79 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0524\n",
      "Iteration 200, Loss: 1.0157\n",
      "Iteration 300, Loss: 0.9855\n",
      "Iteration 400, Loss: 0.9600\n",
      "Iteration 500, Loss: 0.9379\n",
      "Iteration 600, Loss: 0.9187\n",
      "Iteration 700, Loss: 0.9019\n",
      "Iteration 800, Loss: 0.8869\n",
      "Iteration 900, Loss: 0.8735\n",
      "Iteration 1000, Loss: 0.8615\n",
      "Iteration 1100, Loss: 0.8506\n",
      "Iteration 1200, Loss: 0.8408\n",
      "Iteration 1300, Loss: 0.8317\n",
      "Iteration 1400, Loss: 0.8233\n",
      "Iteration 1500, Loss: 0.8156\n",
      "Iteration 1600, Loss: 0.8086\n",
      "Iteration 1700, Loss: 0.8021\n",
      "Iteration 1800, Loss: 0.7960\n",
      "Iteration 1900, Loss: 0.7904\n",
      "Iteration 2000, Loss: 0.7851\n",
      "Iteration 2100, Loss: 0.7801\n",
      "Iteration 2200, Loss: 0.7755\n",
      "Iteration 2300, Loss: 0.7711\n",
      "Iteration 2400, Loss: 0.7670\n",
      "Iteration 2500, Loss: 0.7630\n",
      "Iteration 2600, Loss: 0.7594\n",
      "Iteration 2700, Loss: 0.7559\n",
      "Iteration 2800, Loss: 0.7526\n",
      "Iteration 2900, Loss: 0.7495\n",
      "Iteration 3000, Loss: 0.7465\n",
      "Iteration 3100, Loss: 0.7436\n",
      "Iteration 3200, Loss: 0.7409\n",
      "Iteration 3300, Loss: 0.7383\n",
      "Iteration 3400, Loss: 0.7359\n",
      "Iteration 3500, Loss: 0.7335\n",
      "Iteration 3600, Loss: 0.7312\n",
      "Iteration 3700, Loss: 0.7290\n",
      "Iteration 3800, Loss: 0.7269\n",
      "Iteration 3900, Loss: 0.7249\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0556\n",
      "Iteration 200, Loss: 1.0203\n",
      "Iteration 300, Loss: 0.9906\n",
      "Iteration 400, Loss: 0.9652\n",
      "Iteration 500, Loss: 0.9431\n",
      "Iteration 600, Loss: 0.9236\n",
      "Iteration 700, Loss: 0.9063\n",
      "Iteration 800, Loss: 0.8908\n",
      "Iteration 900, Loss: 0.8768\n",
      "Iteration 1000, Loss: 0.8641\n",
      "Iteration 1100, Loss: 0.8526\n",
      "Iteration 1200, Loss: 0.8420\n",
      "Iteration 1300, Loss: 0.8323\n",
      "Iteration 1400, Loss: 0.8234\n",
      "Iteration 1500, Loss: 0.8152\n",
      "Iteration 1600, Loss: 0.8076\n",
      "Iteration 1700, Loss: 0.8004\n",
      "Iteration 1800, Loss: 0.7938\n",
      "Iteration 1900, Loss: 0.7877\n",
      "Iteration 2000, Loss: 0.7819\n",
      "Iteration 2100, Loss: 0.7765\n",
      "Iteration 2200, Loss: 0.7715\n",
      "Iteration 2300, Loss: 0.7667\n",
      "Iteration 2400, Loss: 0.7621\n",
      "Iteration 2500, Loss: 0.7579\n",
      "Iteration 2600, Loss: 0.7538\n",
      "Iteration 2700, Loss: 0.7500\n",
      "Iteration 2800, Loss: 0.7464\n",
      "Iteration 2900, Loss: 0.7430\n",
      "Iteration 3000, Loss: 0.7397\n",
      "Iteration 3100, Loss: 0.7366\n",
      "Iteration 3200, Loss: 0.7335\n",
      "Iteration 3300, Loss: 0.7307\n",
      "Iteration 3400, Loss: 0.7280\n",
      "Iteration 3500, Loss: 0.7254\n",
      "Iteration 3600, Loss: 0.7229\n",
      "Iteration 3700, Loss: 0.7205\n",
      "Iteration 3800, Loss: 0.7182\n",
      "Iteration 3900, Loss: 0.7159\n",
      "80 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0532\n",
      "Iteration 200, Loss: 1.0165\n",
      "Iteration 300, Loss: 0.9857\n",
      "Iteration 400, Loss: 0.9595\n",
      "Iteration 500, Loss: 0.9367\n",
      "Iteration 600, Loss: 0.9168\n",
      "Iteration 700, Loss: 0.8992\n",
      "Iteration 800, Loss: 0.8834\n",
      "Iteration 900, Loss: 0.8694\n",
      "Iteration 1000, Loss: 0.8568\n",
      "Iteration 1100, Loss: 0.8452\n",
      "Iteration 1200, Loss: 0.8347\n",
      "Iteration 1300, Loss: 0.8251\n",
      "Iteration 1400, Loss: 0.8163\n",
      "Iteration 1500, Loss: 0.8082\n",
      "Iteration 1600, Loss: 0.8007\n",
      "Iteration 1700, Loss: 0.7938\n",
      "Iteration 1800, Loss: 0.7873\n",
      "Iteration 1900, Loss: 0.7812\n",
      "Iteration 2000, Loss: 0.7756\n",
      "Iteration 2100, Loss: 0.7703\n",
      "Iteration 2200, Loss: 0.7653\n",
      "Iteration 2300, Loss: 0.7606\n",
      "Iteration 2400, Loss: 0.7561\n",
      "Iteration 2500, Loss: 0.7520\n",
      "Iteration 2600, Loss: 0.7480\n",
      "Iteration 2700, Loss: 0.7443\n",
      "Iteration 2800, Loss: 0.7407\n",
      "Iteration 2900, Loss: 0.7374\n",
      "Iteration 3000, Loss: 0.7342\n",
      "Iteration 3100, Loss: 0.7312\n",
      "Iteration 3200, Loss: 0.7283\n",
      "Iteration 3300, Loss: 0.7255\n",
      "Iteration 3400, Loss: 0.7228\n",
      "Iteration 3500, Loss: 0.7203\n",
      "Iteration 3600, Loss: 0.7178\n",
      "Iteration 3700, Loss: 0.7155\n",
      "Iteration 3800, Loss: 0.7133\n",
      "Iteration 3900, Loss: 0.7111\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0532\n",
      "Iteration 200, Loss: 1.0173\n",
      "Iteration 300, Loss: 0.9876\n",
      "Iteration 400, Loss: 0.9623\n",
      "Iteration 500, Loss: 0.9405\n",
      "Iteration 600, Loss: 0.9215\n",
      "Iteration 700, Loss: 0.9048\n",
      "Iteration 800, Loss: 0.8898\n",
      "Iteration 900, Loss: 0.8763\n",
      "Iteration 1000, Loss: 0.8641\n",
      "Iteration 1100, Loss: 0.8531\n",
      "Iteration 1200, Loss: 0.8430\n",
      "Iteration 1300, Loss: 0.8338\n",
      "Iteration 1400, Loss: 0.8254\n",
      "Iteration 1500, Loss: 0.8176\n",
      "Iteration 1600, Loss: 0.8104\n",
      "Iteration 1700, Loss: 0.8037\n",
      "Iteration 1800, Loss: 0.7975\n",
      "Iteration 1900, Loss: 0.7917\n",
      "Iteration 2000, Loss: 0.7863\n",
      "Iteration 2100, Loss: 0.7812\n",
      "Iteration 2200, Loss: 0.7765\n",
      "Iteration 2300, Loss: 0.7720\n",
      "Iteration 2400, Loss: 0.7678\n",
      "Iteration 2500, Loss: 0.7639\n",
      "Iteration 2600, Loss: 0.7601\n",
      "Iteration 2700, Loss: 0.7566\n",
      "Iteration 2800, Loss: 0.7532\n",
      "Iteration 2900, Loss: 0.7501\n",
      "Iteration 3000, Loss: 0.7471\n",
      "Iteration 3100, Loss: 0.7442\n",
      "Iteration 3200, Loss: 0.7414\n",
      "Iteration 3300, Loss: 0.7388\n",
      "Iteration 3400, Loss: 0.7363\n",
      "Iteration 3500, Loss: 0.7339\n",
      "Iteration 3600, Loss: 0.7316\n",
      "Iteration 3700, Loss: 0.7294\n",
      "Iteration 3800, Loss: 0.7273\n",
      "Iteration 3900, Loss: 0.7252\n",
      "81 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0541\n",
      "Iteration 200, Loss: 1.0183\n",
      "Iteration 300, Loss: 0.9884\n",
      "Iteration 400, Loss: 0.9630\n",
      "Iteration 500, Loss: 0.9409\n",
      "Iteration 600, Loss: 0.9216\n",
      "Iteration 700, Loss: 0.9045\n",
      "Iteration 800, Loss: 0.8893\n",
      "Iteration 900, Loss: 0.8757\n",
      "Iteration 1000, Loss: 0.8633\n",
      "Iteration 1100, Loss: 0.8521\n",
      "Iteration 1200, Loss: 0.8419\n",
      "Iteration 1300, Loss: 0.8325\n",
      "Iteration 1400, Loss: 0.8239\n",
      "Iteration 1500, Loss: 0.8159\n",
      "Iteration 1600, Loss: 0.8086\n",
      "Iteration 1700, Loss: 0.8018\n",
      "Iteration 1800, Loss: 0.7955\n",
      "Iteration 1900, Loss: 0.7896\n",
      "Iteration 2000, Loss: 0.7841\n",
      "Iteration 2100, Loss: 0.7789\n",
      "Iteration 2200, Loss: 0.7741\n",
      "Iteration 2300, Loss: 0.7695\n",
      "Iteration 2400, Loss: 0.7653\n",
      "Iteration 2500, Loss: 0.7612\n",
      "Iteration 2600, Loss: 0.7574\n",
      "Iteration 2700, Loss: 0.7538\n",
      "Iteration 2800, Loss: 0.7504\n",
      "Iteration 2900, Loss: 0.7472\n",
      "Iteration 3000, Loss: 0.7441\n",
      "Iteration 3100, Loss: 0.7412\n",
      "Iteration 3200, Loss: 0.7384\n",
      "Iteration 3300, Loss: 0.7357\n",
      "Iteration 3400, Loss: 0.7332\n",
      "Iteration 3500, Loss: 0.7307\n",
      "Iteration 3600, Loss: 0.7284\n",
      "Iteration 3700, Loss: 0.7262\n",
      "Iteration 3800, Loss: 0.7240\n",
      "Iteration 3900, Loss: 0.7219\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0528\n",
      "Iteration 200, Loss: 1.0166\n",
      "Iteration 300, Loss: 0.9865\n",
      "Iteration 400, Loss: 0.9608\n",
      "Iteration 500, Loss: 0.9386\n",
      "Iteration 600, Loss: 0.9191\n",
      "Iteration 700, Loss: 0.9020\n",
      "Iteration 800, Loss: 0.8868\n",
      "Iteration 900, Loss: 0.8731\n",
      "Iteration 1000, Loss: 0.8607\n",
      "Iteration 1100, Loss: 0.8495\n",
      "Iteration 1200, Loss: 0.8393\n",
      "Iteration 1300, Loss: 0.8299\n",
      "Iteration 1400, Loss: 0.8213\n",
      "Iteration 1500, Loss: 0.8134\n",
      "Iteration 1600, Loss: 0.8061\n",
      "Iteration 1700, Loss: 0.7993\n",
      "Iteration 1800, Loss: 0.7929\n",
      "Iteration 1900, Loss: 0.7870\n",
      "Iteration 2000, Loss: 0.7815\n",
      "Iteration 2100, Loss: 0.7763\n",
      "Iteration 2200, Loss: 0.7714\n",
      "Iteration 2300, Loss: 0.7668\n",
      "Iteration 2400, Loss: 0.7624\n",
      "Iteration 2500, Loss: 0.7583\n",
      "Iteration 2600, Loss: 0.7544\n",
      "Iteration 2700, Loss: 0.7508\n",
      "Iteration 2800, Loss: 0.7472\n",
      "Iteration 2900, Loss: 0.7439\n",
      "Iteration 3000, Loss: 0.7408\n",
      "Iteration 3100, Loss: 0.7378\n",
      "Iteration 3200, Loss: 0.7349\n",
      "Iteration 3300, Loss: 0.7321\n",
      "Iteration 3400, Loss: 0.7295\n",
      "Iteration 3500, Loss: 0.7269\n",
      "Iteration 3600, Loss: 0.7245\n",
      "Iteration 3700, Loss: 0.7222\n",
      "Iteration 3800, Loss: 0.7199\n",
      "Iteration 3900, Loss: 0.7177\n",
      "82 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0528\n",
      "Iteration 200, Loss: 1.0158\n",
      "Iteration 300, Loss: 0.9852\n",
      "Iteration 400, Loss: 0.9592\n",
      "Iteration 500, Loss: 0.9367\n",
      "Iteration 600, Loss: 0.9170\n",
      "Iteration 700, Loss: 0.8997\n",
      "Iteration 800, Loss: 0.8844\n",
      "Iteration 900, Loss: 0.8706\n",
      "Iteration 1000, Loss: 0.8582\n",
      "Iteration 1100, Loss: 0.8469\n",
      "Iteration 1200, Loss: 0.8368\n",
      "Iteration 1300, Loss: 0.8274\n",
      "Iteration 1400, Loss: 0.8189\n",
      "Iteration 1500, Loss: 0.8109\n",
      "Iteration 1600, Loss: 0.8036\n",
      "Iteration 1700, Loss: 0.7968\n",
      "Iteration 1800, Loss: 0.7905\n",
      "Iteration 1900, Loss: 0.7846\n",
      "Iteration 2000, Loss: 0.7790\n",
      "Iteration 2100, Loss: 0.7739\n",
      "Iteration 2200, Loss: 0.7690\n",
      "Iteration 2300, Loss: 0.7644\n",
      "Iteration 2400, Loss: 0.7601\n",
      "Iteration 2500, Loss: 0.7560\n",
      "Iteration 2600, Loss: 0.7522\n",
      "Iteration 2700, Loss: 0.7485\n",
      "Iteration 2800, Loss: 0.7450\n",
      "Iteration 2900, Loss: 0.7417\n",
      "Iteration 3000, Loss: 0.7386\n",
      "Iteration 3100, Loss: 0.7356\n",
      "Iteration 3200, Loss: 0.7327\n",
      "Iteration 3300, Loss: 0.7300\n",
      "Iteration 3400, Loss: 0.7274\n",
      "Iteration 3500, Loss: 0.7249\n",
      "Iteration 3600, Loss: 0.7224\n",
      "Iteration 3700, Loss: 0.7201\n",
      "Iteration 3800, Loss: 0.7179\n",
      "Iteration 3900, Loss: 0.7157\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0548\n",
      "Iteration 200, Loss: 1.0197\n",
      "Iteration 300, Loss: 0.9905\n",
      "Iteration 400, Loss: 0.9654\n",
      "Iteration 500, Loss: 0.9437\n",
      "Iteration 600, Loss: 0.9246\n",
      "Iteration 700, Loss: 0.9076\n",
      "Iteration 800, Loss: 0.8925\n",
      "Iteration 900, Loss: 0.8790\n",
      "Iteration 1000, Loss: 0.8667\n",
      "Iteration 1100, Loss: 0.8555\n",
      "Iteration 1200, Loss: 0.8453\n",
      "Iteration 1300, Loss: 0.8359\n",
      "Iteration 1400, Loss: 0.8273\n",
      "Iteration 1500, Loss: 0.8194\n",
      "Iteration 1600, Loss: 0.8120\n",
      "Iteration 1700, Loss: 0.8052\n",
      "Iteration 1800, Loss: 0.7989\n",
      "Iteration 1900, Loss: 0.7930\n",
      "Iteration 2000, Loss: 0.7874\n",
      "Iteration 2100, Loss: 0.7823\n",
      "Iteration 2200, Loss: 0.7774\n",
      "Iteration 2300, Loss: 0.7728\n",
      "Iteration 2400, Loss: 0.7685\n",
      "Iteration 2500, Loss: 0.7645\n",
      "Iteration 2600, Loss: 0.7606\n",
      "Iteration 2700, Loss: 0.7570\n",
      "Iteration 2800, Loss: 0.7536\n",
      "Iteration 2900, Loss: 0.7503\n",
      "Iteration 3000, Loss: 0.7472\n",
      "Iteration 3100, Loss: 0.7443\n",
      "Iteration 3200, Loss: 0.7415\n",
      "Iteration 3300, Loss: 0.7387\n",
      "Iteration 3400, Loss: 0.7362\n",
      "Iteration 3500, Loss: 0.7337\n",
      "Iteration 3600, Loss: 0.7314\n",
      "Iteration 3700, Loss: 0.7291\n",
      "Iteration 3800, Loss: 0.7270\n",
      "Iteration 3900, Loss: 0.7249\n",
      "83 270\n",
      "Iteration 0, Loss: 1.0980\n",
      "Iteration 100, Loss: 1.0514\n",
      "Iteration 200, Loss: 1.0139\n",
      "Iteration 300, Loss: 0.9829\n",
      "Iteration 400, Loss: 0.9565\n",
      "Iteration 500, Loss: 0.9337\n",
      "Iteration 600, Loss: 0.9138\n",
      "Iteration 700, Loss: 0.8963\n",
      "Iteration 800, Loss: 0.8807\n",
      "Iteration 900, Loss: 0.8668\n",
      "Iteration 1000, Loss: 0.8542\n",
      "Iteration 1100, Loss: 0.8428\n",
      "Iteration 1200, Loss: 0.8324\n",
      "Iteration 1300, Loss: 0.8230\n",
      "Iteration 1400, Loss: 0.8143\n",
      "Iteration 1500, Loss: 0.8063\n",
      "Iteration 1600, Loss: 0.7989\n",
      "Iteration 1700, Loss: 0.7921\n",
      "Iteration 1800, Loss: 0.7857\n",
      "Iteration 1900, Loss: 0.7798\n",
      "Iteration 2000, Loss: 0.7743\n",
      "Iteration 2100, Loss: 0.7691\n",
      "Iteration 2200, Loss: 0.7643\n",
      "Iteration 2300, Loss: 0.7598\n",
      "Iteration 2400, Loss: 0.7555\n",
      "Iteration 2500, Loss: 0.7515\n",
      "Iteration 2600, Loss: 0.7477\n",
      "Iteration 2700, Loss: 0.7441\n",
      "Iteration 2800, Loss: 0.7407\n",
      "Iteration 2900, Loss: 0.7375\n",
      "Iteration 3000, Loss: 0.7344\n",
      "Iteration 3100, Loss: 0.7315\n",
      "Iteration 3200, Loss: 0.7287\n",
      "Iteration 3300, Loss: 0.7261\n",
      "Iteration 3400, Loss: 0.7236\n",
      "Iteration 3500, Loss: 0.7211\n",
      "Iteration 3600, Loss: 0.7188\n",
      "Iteration 3700, Loss: 0.7166\n",
      "Iteration 3800, Loss: 0.7145\n",
      "Iteration 3900, Loss: 0.7124\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0555\n",
      "Iteration 200, Loss: 1.0210\n",
      "Iteration 300, Loss: 0.9922\n",
      "Iteration 400, Loss: 0.9674\n",
      "Iteration 500, Loss: 0.9460\n",
      "Iteration 600, Loss: 0.9273\n",
      "Iteration 700, Loss: 0.9106\n",
      "Iteration 800, Loss: 0.8957\n",
      "Iteration 900, Loss: 0.8823\n",
      "Iteration 1000, Loss: 0.8702\n",
      "Iteration 1100, Loss: 0.8592\n",
      "Iteration 1200, Loss: 0.8490\n",
      "Iteration 1300, Loss: 0.8397\n",
      "Iteration 1400, Loss: 0.8312\n",
      "Iteration 1500, Loss: 0.8232\n",
      "Iteration 1600, Loss: 0.8159\n",
      "Iteration 1700, Loss: 0.8091\n",
      "Iteration 1800, Loss: 0.8027\n",
      "Iteration 1900, Loss: 0.7968\n",
      "Iteration 2000, Loss: 0.7913\n",
      "Iteration 2100, Loss: 0.7861\n",
      "Iteration 2200, Loss: 0.7812\n",
      "Iteration 2300, Loss: 0.7766\n",
      "Iteration 2400, Loss: 0.7722\n",
      "Iteration 2500, Loss: 0.7681\n",
      "Iteration 2600, Loss: 0.7642\n",
      "Iteration 2700, Loss: 0.7605\n",
      "Iteration 2800, Loss: 0.7570\n",
      "Iteration 2900, Loss: 0.7537\n",
      "Iteration 3000, Loss: 0.7506\n",
      "Iteration 3100, Loss: 0.7475\n",
      "Iteration 3200, Loss: 0.7446\n",
      "Iteration 3300, Loss: 0.7419\n",
      "Iteration 3400, Loss: 0.7393\n",
      "Iteration 3500, Loss: 0.7367\n",
      "Iteration 3600, Loss: 0.7343\n",
      "Iteration 3700, Loss: 0.7320\n",
      "Iteration 3800, Loss: 0.7297\n",
      "Iteration 3900, Loss: 0.7276\n",
      "84 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0549\n",
      "Iteration 200, Loss: 1.0200\n",
      "Iteration 300, Loss: 0.9908\n",
      "Iteration 400, Loss: 0.9661\n",
      "Iteration 500, Loss: 0.9446\n",
      "Iteration 600, Loss: 0.9256\n",
      "Iteration 700, Loss: 0.9089\n",
      "Iteration 800, Loss: 0.8939\n",
      "Iteration 900, Loss: 0.8805\n",
      "Iteration 1000, Loss: 0.8684\n",
      "Iteration 1100, Loss: 0.8573\n",
      "Iteration 1200, Loss: 0.8473\n",
      "Iteration 1300, Loss: 0.8381\n",
      "Iteration 1400, Loss: 0.8297\n",
      "Iteration 1500, Loss: 0.8219\n",
      "Iteration 1600, Loss: 0.8147\n",
      "Iteration 1700, Loss: 0.8080\n",
      "Iteration 1800, Loss: 0.8017\n",
      "Iteration 1900, Loss: 0.7959\n",
      "Iteration 2000, Loss: 0.7905\n",
      "Iteration 2100, Loss: 0.7854\n",
      "Iteration 2200, Loss: 0.7807\n",
      "Iteration 2300, Loss: 0.7762\n",
      "Iteration 2400, Loss: 0.7719\n",
      "Iteration 2500, Loss: 0.7680\n",
      "Iteration 2600, Loss: 0.7643\n",
      "Iteration 2700, Loss: 0.7607\n",
      "Iteration 2800, Loss: 0.7573\n",
      "Iteration 2900, Loss: 0.7541\n",
      "Iteration 3000, Loss: 0.7510\n",
      "Iteration 3100, Loss: 0.7482\n",
      "Iteration 3200, Loss: 0.7454\n",
      "Iteration 3300, Loss: 0.7428\n",
      "Iteration 3400, Loss: 0.7402\n",
      "Iteration 3500, Loss: 0.7378\n",
      "Iteration 3600, Loss: 0.7355\n",
      "Iteration 3700, Loss: 0.7333\n",
      "Iteration 3800, Loss: 0.7312\n",
      "Iteration 3900, Loss: 0.7291\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0520\n",
      "Iteration 200, Loss: 1.0148\n",
      "Iteration 300, Loss: 0.9840\n",
      "Iteration 400, Loss: 0.9578\n",
      "Iteration 500, Loss: 0.9352\n",
      "Iteration 600, Loss: 0.9155\n",
      "Iteration 700, Loss: 0.8981\n",
      "Iteration 800, Loss: 0.8826\n",
      "Iteration 900, Loss: 0.8687\n",
      "Iteration 1000, Loss: 0.8561\n",
      "Iteration 1100, Loss: 0.8448\n",
      "Iteration 1200, Loss: 0.8344\n",
      "Iteration 1300, Loss: 0.8249\n",
      "Iteration 1400, Loss: 0.8162\n",
      "Iteration 1500, Loss: 0.8082\n",
      "Iteration 1600, Loss: 0.8008\n",
      "Iteration 1700, Loss: 0.7939\n",
      "Iteration 1800, Loss: 0.7875\n",
      "Iteration 1900, Loss: 0.7815\n",
      "Iteration 2000, Loss: 0.7759\n",
      "Iteration 2100, Loss: 0.7706\n",
      "Iteration 2200, Loss: 0.7656\n",
      "Iteration 2300, Loss: 0.7609\n",
      "Iteration 2400, Loss: 0.7565\n",
      "Iteration 2500, Loss: 0.7524\n",
      "Iteration 2600, Loss: 0.7484\n",
      "Iteration 2700, Loss: 0.7447\n",
      "Iteration 2800, Loss: 0.7412\n",
      "Iteration 2900, Loss: 0.7378\n",
      "Iteration 3000, Loss: 0.7346\n",
      "Iteration 3100, Loss: 0.7315\n",
      "Iteration 3200, Loss: 0.7286\n",
      "Iteration 3300, Loss: 0.7258\n",
      "Iteration 3400, Loss: 0.7231\n",
      "Iteration 3500, Loss: 0.7206\n",
      "Iteration 3600, Loss: 0.7181\n",
      "Iteration 3700, Loss: 0.7157\n",
      "Iteration 3800, Loss: 0.7135\n",
      "Iteration 3900, Loss: 0.7113\n",
      "85 270\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0588\n",
      "Iteration 200, Loss: 1.0269\n",
      "Iteration 300, Loss: 1.0002\n",
      "Iteration 400, Loss: 0.9771\n",
      "Iteration 500, Loss: 0.9570\n",
      "Iteration 600, Loss: 0.9392\n",
      "Iteration 700, Loss: 0.9233\n",
      "Iteration 800, Loss: 0.9091\n",
      "Iteration 900, Loss: 0.8962\n",
      "Iteration 1000, Loss: 0.8845\n",
      "Iteration 1100, Loss: 0.8739\n",
      "Iteration 1200, Loss: 0.8641\n",
      "Iteration 1300, Loss: 0.8551\n",
      "Iteration 1400, Loss: 0.8468\n",
      "Iteration 1500, Loss: 0.8392\n",
      "Iteration 1600, Loss: 0.8321\n",
      "Iteration 1700, Loss: 0.8255\n",
      "Iteration 1800, Loss: 0.8193\n",
      "Iteration 1900, Loss: 0.8135\n",
      "Iteration 2000, Loss: 0.8081\n",
      "Iteration 2100, Loss: 0.8031\n",
      "Iteration 2200, Loss: 0.7984\n",
      "Iteration 2300, Loss: 0.7939\n",
      "Iteration 2400, Loss: 0.7896\n",
      "Iteration 2500, Loss: 0.7857\n",
      "Iteration 2600, Loss: 0.7819\n",
      "Iteration 2700, Loss: 0.7783\n",
      "Iteration 2800, Loss: 0.7750\n",
      "Iteration 2900, Loss: 0.7717\n",
      "Iteration 3000, Loss: 0.7687\n",
      "Iteration 3100, Loss: 0.7657\n",
      "Iteration 3200, Loss: 0.7629\n",
      "Iteration 3300, Loss: 0.7603\n",
      "Iteration 3400, Loss: 0.7577\n",
      "Iteration 3500, Loss: 0.7552\n",
      "Iteration 3600, Loss: 0.7529\n",
      "Iteration 3700, Loss: 0.7506\n",
      "Iteration 3800, Loss: 0.7484\n",
      "Iteration 3900, Loss: 0.7464\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0476\n",
      "Iteration 200, Loss: 1.0071\n",
      "Iteration 300, Loss: 0.9737\n",
      "Iteration 400, Loss: 0.9455\n",
      "Iteration 500, Loss: 0.9214\n",
      "Iteration 600, Loss: 0.9005\n",
      "Iteration 700, Loss: 0.8821\n",
      "Iteration 800, Loss: 0.8658\n",
      "Iteration 900, Loss: 0.8513\n",
      "Iteration 1000, Loss: 0.8384\n",
      "Iteration 1100, Loss: 0.8267\n",
      "Iteration 1200, Loss: 0.8160\n",
      "Iteration 1300, Loss: 0.8063\n",
      "Iteration 1400, Loss: 0.7974\n",
      "Iteration 1500, Loss: 0.7892\n",
      "Iteration 1600, Loss: 0.7816\n",
      "Iteration 1700, Loss: 0.7746\n",
      "Iteration 1800, Loss: 0.7682\n",
      "Iteration 1900, Loss: 0.7621\n",
      "Iteration 2000, Loss: 0.7565\n",
      "Iteration 2100, Loss: 0.7512\n",
      "Iteration 2200, Loss: 0.7462\n",
      "Iteration 2300, Loss: 0.7415\n",
      "Iteration 2400, Loss: 0.7372\n",
      "Iteration 2500, Loss: 0.7330\n",
      "Iteration 2600, Loss: 0.7291\n",
      "Iteration 2700, Loss: 0.7254\n",
      "Iteration 2800, Loss: 0.7218\n",
      "Iteration 2900, Loss: 0.7185\n",
      "Iteration 3000, Loss: 0.7153\n",
      "Iteration 3100, Loss: 0.7123\n",
      "Iteration 3200, Loss: 0.7095\n",
      "Iteration 3300, Loss: 0.7067\n",
      "Iteration 3400, Loss: 0.7041\n",
      "Iteration 3500, Loss: 0.7016\n",
      "Iteration 3600, Loss: 0.6992\n",
      "Iteration 3700, Loss: 0.6968\n",
      "Iteration 3800, Loss: 0.6946\n",
      "Iteration 3900, Loss: 0.6925\n",
      "86 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0506\n",
      "Iteration 200, Loss: 1.0119\n",
      "Iteration 300, Loss: 0.9798\n",
      "Iteration 400, Loss: 0.9524\n",
      "Iteration 500, Loss: 0.9289\n",
      "Iteration 600, Loss: 0.9085\n",
      "Iteration 700, Loss: 0.8904\n",
      "Iteration 800, Loss: 0.8743\n",
      "Iteration 900, Loss: 0.8600\n",
      "Iteration 1000, Loss: 0.8470\n",
      "Iteration 1100, Loss: 0.8353\n",
      "Iteration 1200, Loss: 0.8247\n",
      "Iteration 1300, Loss: 0.8150\n",
      "Iteration 1400, Loss: 0.8061\n",
      "Iteration 1500, Loss: 0.7979\n",
      "Iteration 1600, Loss: 0.7904\n",
      "Iteration 1700, Loss: 0.7833\n",
      "Iteration 1800, Loss: 0.7769\n",
      "Iteration 1900, Loss: 0.7708\n",
      "Iteration 2000, Loss: 0.7651\n",
      "Iteration 2100, Loss: 0.7598\n",
      "Iteration 2200, Loss: 0.7549\n",
      "Iteration 2300, Loss: 0.7502\n",
      "Iteration 2400, Loss: 0.7459\n",
      "Iteration 2500, Loss: 0.7418\n",
      "Iteration 2600, Loss: 0.7380\n",
      "Iteration 2700, Loss: 0.7343\n",
      "Iteration 2800, Loss: 0.7308\n",
      "Iteration 2900, Loss: 0.7275\n",
      "Iteration 3000, Loss: 0.7244\n",
      "Iteration 3100, Loss: 0.7214\n",
      "Iteration 3200, Loss: 0.7186\n",
      "Iteration 3300, Loss: 0.7159\n",
      "Iteration 3400, Loss: 0.7134\n",
      "Iteration 3500, Loss: 0.7109\n",
      "Iteration 3600, Loss: 0.7085\n",
      "Iteration 3700, Loss: 0.7062\n",
      "Iteration 3800, Loss: 0.7041\n",
      "Iteration 3900, Loss: 0.7020\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0564\n",
      "Iteration 200, Loss: 1.0228\n",
      "Iteration 300, Loss: 0.9948\n",
      "Iteration 400, Loss: 0.9708\n",
      "Iteration 500, Loss: 0.9501\n",
      "Iteration 600, Loss: 0.9318\n",
      "Iteration 700, Loss: 0.9157\n",
      "Iteration 800, Loss: 0.9013\n",
      "Iteration 900, Loss: 0.8883\n",
      "Iteration 1000, Loss: 0.8765\n",
      "Iteration 1100, Loss: 0.8658\n",
      "Iteration 1200, Loss: 0.8560\n",
      "Iteration 1300, Loss: 0.8470\n",
      "Iteration 1400, Loss: 0.8387\n",
      "Iteration 1500, Loss: 0.8311\n",
      "Iteration 1600, Loss: 0.8240\n",
      "Iteration 1700, Loss: 0.8173\n",
      "Iteration 1800, Loss: 0.8112\n",
      "Iteration 1900, Loss: 0.8054\n",
      "Iteration 2000, Loss: 0.8000\n",
      "Iteration 2100, Loss: 0.7949\n",
      "Iteration 2200, Loss: 0.7901\n",
      "Iteration 2300, Loss: 0.7857\n",
      "Iteration 2400, Loss: 0.7814\n",
      "Iteration 2500, Loss: 0.7774\n",
      "Iteration 2600, Loss: 0.7736\n",
      "Iteration 2700, Loss: 0.7700\n",
      "Iteration 2800, Loss: 0.7665\n",
      "Iteration 2900, Loss: 0.7632\n",
      "Iteration 3000, Loss: 0.7601\n",
      "Iteration 3100, Loss: 0.7571\n",
      "Iteration 3200, Loss: 0.7543\n",
      "Iteration 3300, Loss: 0.7516\n",
      "Iteration 3400, Loss: 0.7490\n",
      "Iteration 3500, Loss: 0.7465\n",
      "Iteration 3600, Loss: 0.7441\n",
      "Iteration 3700, Loss: 0.7418\n",
      "Iteration 3800, Loss: 0.7395\n",
      "Iteration 3900, Loss: 0.7374\n",
      "87 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0544\n",
      "Iteration 200, Loss: 1.0192\n",
      "Iteration 300, Loss: 0.9902\n",
      "Iteration 400, Loss: 0.9654\n",
      "Iteration 500, Loss: 0.9440\n",
      "Iteration 600, Loss: 0.9252\n",
      "Iteration 700, Loss: 0.9086\n",
      "Iteration 800, Loss: 0.8938\n",
      "Iteration 900, Loss: 0.8805\n",
      "Iteration 1000, Loss: 0.8684\n",
      "Iteration 1100, Loss: 0.8574\n",
      "Iteration 1200, Loss: 0.8473\n",
      "Iteration 1300, Loss: 0.8381\n",
      "Iteration 1400, Loss: 0.8296\n",
      "Iteration 1500, Loss: 0.8217\n",
      "Iteration 1600, Loss: 0.8144\n",
      "Iteration 1700, Loss: 0.8076\n",
      "Iteration 1800, Loss: 0.8013\n",
      "Iteration 1900, Loss: 0.7954\n",
      "Iteration 2000, Loss: 0.7898\n",
      "Iteration 2100, Loss: 0.7846\n",
      "Iteration 2200, Loss: 0.7797\n",
      "Iteration 2300, Loss: 0.7751\n",
      "Iteration 2400, Loss: 0.7707\n",
      "Iteration 2500, Loss: 0.7666\n",
      "Iteration 2600, Loss: 0.7627\n",
      "Iteration 2700, Loss: 0.7589\n",
      "Iteration 2800, Loss: 0.7554\n",
      "Iteration 2900, Loss: 0.7521\n",
      "Iteration 3000, Loss: 0.7488\n",
      "Iteration 3100, Loss: 0.7458\n",
      "Iteration 3200, Loss: 0.7429\n",
      "Iteration 3300, Loss: 0.7400\n",
      "Iteration 3400, Loss: 0.7373\n",
      "Iteration 3500, Loss: 0.7348\n",
      "Iteration 3600, Loss: 0.7323\n",
      "Iteration 3700, Loss: 0.7299\n",
      "Iteration 3800, Loss: 0.7276\n",
      "Iteration 3900, Loss: 0.7254\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0524\n",
      "Iteration 200, Loss: 1.0152\n",
      "Iteration 300, Loss: 0.9841\n",
      "Iteration 400, Loss: 0.9577\n",
      "Iteration 500, Loss: 0.9349\n",
      "Iteration 600, Loss: 0.9150\n",
      "Iteration 700, Loss: 0.8975\n",
      "Iteration 800, Loss: 0.8819\n",
      "Iteration 900, Loss: 0.8680\n",
      "Iteration 1000, Loss: 0.8554\n",
      "Iteration 1100, Loss: 0.8440\n",
      "Iteration 1200, Loss: 0.8336\n",
      "Iteration 1300, Loss: 0.8242\n",
      "Iteration 1400, Loss: 0.8155\n",
      "Iteration 1500, Loss: 0.8076\n",
      "Iteration 1600, Loss: 0.8002\n",
      "Iteration 1700, Loss: 0.7934\n",
      "Iteration 1800, Loss: 0.7871\n",
      "Iteration 1900, Loss: 0.7813\n",
      "Iteration 2000, Loss: 0.7759\n",
      "Iteration 2100, Loss: 0.7708\n",
      "Iteration 2200, Loss: 0.7660\n",
      "Iteration 2300, Loss: 0.7615\n",
      "Iteration 2400, Loss: 0.7573\n",
      "Iteration 2500, Loss: 0.7534\n",
      "Iteration 2600, Loss: 0.7496\n",
      "Iteration 2700, Loss: 0.7461\n",
      "Iteration 2800, Loss: 0.7428\n",
      "Iteration 2900, Loss: 0.7396\n",
      "Iteration 3000, Loss: 0.7366\n",
      "Iteration 3100, Loss: 0.7338\n",
      "Iteration 3200, Loss: 0.7310\n",
      "Iteration 3300, Loss: 0.7285\n",
      "Iteration 3400, Loss: 0.7260\n",
      "Iteration 3500, Loss: 0.7236\n",
      "Iteration 3600, Loss: 0.7214\n",
      "Iteration 3700, Loss: 0.7192\n",
      "Iteration 3800, Loss: 0.7171\n",
      "Iteration 3900, Loss: 0.7151\n",
      "88 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0527\n",
      "Iteration 200, Loss: 1.0154\n",
      "Iteration 300, Loss: 0.9839\n",
      "Iteration 400, Loss: 0.9570\n",
      "Iteration 500, Loss: 0.9336\n",
      "Iteration 600, Loss: 0.9130\n",
      "Iteration 700, Loss: 0.8949\n",
      "Iteration 800, Loss: 0.8786\n",
      "Iteration 900, Loss: 0.8639\n",
      "Iteration 1000, Loss: 0.8507\n",
      "Iteration 1100, Loss: 0.8387\n",
      "Iteration 1200, Loss: 0.8277\n",
      "Iteration 1300, Loss: 0.8176\n",
      "Iteration 1400, Loss: 0.8084\n",
      "Iteration 1500, Loss: 0.7998\n",
      "Iteration 1600, Loss: 0.7919\n",
      "Iteration 1700, Loss: 0.7846\n",
      "Iteration 1800, Loss: 0.7778\n",
      "Iteration 1900, Loss: 0.7714\n",
      "Iteration 2000, Loss: 0.7654\n",
      "Iteration 2100, Loss: 0.7599\n",
      "Iteration 2200, Loss: 0.7546\n",
      "Iteration 2300, Loss: 0.7497\n",
      "Iteration 2400, Loss: 0.7450\n",
      "Iteration 2500, Loss: 0.7406\n",
      "Iteration 2600, Loss: 0.7365\n",
      "Iteration 2700, Loss: 0.7326\n",
      "Iteration 2800, Loss: 0.7289\n",
      "Iteration 2900, Loss: 0.7254\n",
      "Iteration 3000, Loss: 0.7220\n",
      "Iteration 3100, Loss: 0.7188\n",
      "Iteration 3200, Loss: 0.7157\n",
      "Iteration 3300, Loss: 0.7128\n",
      "Iteration 3400, Loss: 0.7100\n",
      "Iteration 3500, Loss: 0.7074\n",
      "Iteration 3600, Loss: 0.7048\n",
      "Iteration 3700, Loss: 0.7024\n",
      "Iteration 3800, Loss: 0.7000\n",
      "Iteration 3900, Loss: 0.6978\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0547\n",
      "Iteration 200, Loss: 1.0198\n",
      "Iteration 300, Loss: 0.9910\n",
      "Iteration 400, Loss: 0.9668\n",
      "Iteration 500, Loss: 0.9459\n",
      "Iteration 600, Loss: 0.9277\n",
      "Iteration 700, Loss: 0.9117\n",
      "Iteration 800, Loss: 0.8975\n",
      "Iteration 900, Loss: 0.8847\n",
      "Iteration 1000, Loss: 0.8732\n",
      "Iteration 1100, Loss: 0.8627\n",
      "Iteration 1200, Loss: 0.8532\n",
      "Iteration 1300, Loss: 0.8445\n",
      "Iteration 1400, Loss: 0.8365\n",
      "Iteration 1500, Loss: 0.8292\n",
      "Iteration 1600, Loss: 0.8223\n",
      "Iteration 1700, Loss: 0.8160\n",
      "Iteration 1800, Loss: 0.8101\n",
      "Iteration 1900, Loss: 0.8046\n",
      "Iteration 2000, Loss: 0.7996\n",
      "Iteration 2100, Loss: 0.7947\n",
      "Iteration 2200, Loss: 0.7902\n",
      "Iteration 2300, Loss: 0.7860\n",
      "Iteration 2400, Loss: 0.7820\n",
      "Iteration 2500, Loss: 0.7782\n",
      "Iteration 2600, Loss: 0.7746\n",
      "Iteration 2700, Loss: 0.7712\n",
      "Iteration 2800, Loss: 0.7680\n",
      "Iteration 2900, Loss: 0.7650\n",
      "Iteration 3000, Loss: 0.7621\n",
      "Iteration 3100, Loss: 0.7594\n",
      "Iteration 3200, Loss: 0.7567\n",
      "Iteration 3300, Loss: 0.7542\n",
      "Iteration 3400, Loss: 0.7518\n",
      "Iteration 3500, Loss: 0.7495\n",
      "Iteration 3600, Loss: 0.7473\n",
      "Iteration 3700, Loss: 0.7452\n",
      "Iteration 3800, Loss: 0.7431\n",
      "Iteration 3900, Loss: 0.7412\n",
      "89 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0531\n",
      "Iteration 200, Loss: 1.0162\n",
      "Iteration 300, Loss: 0.9852\n",
      "Iteration 400, Loss: 0.9588\n",
      "Iteration 500, Loss: 0.9360\n",
      "Iteration 600, Loss: 0.9160\n",
      "Iteration 700, Loss: 0.8984\n",
      "Iteration 800, Loss: 0.8825\n",
      "Iteration 900, Loss: 0.8683\n",
      "Iteration 1000, Loss: 0.8555\n",
      "Iteration 1100, Loss: 0.8440\n",
      "Iteration 1200, Loss: 0.8334\n",
      "Iteration 1300, Loss: 0.8237\n",
      "Iteration 1400, Loss: 0.8148\n",
      "Iteration 1500, Loss: 0.8065\n",
      "Iteration 1600, Loss: 0.7989\n",
      "Iteration 1700, Loss: 0.7919\n",
      "Iteration 1800, Loss: 0.7853\n",
      "Iteration 1900, Loss: 0.7792\n",
      "Iteration 2000, Loss: 0.7735\n",
      "Iteration 2100, Loss: 0.7681\n",
      "Iteration 2200, Loss: 0.7631\n",
      "Iteration 2300, Loss: 0.7584\n",
      "Iteration 2400, Loss: 0.7539\n",
      "Iteration 2500, Loss: 0.7497\n",
      "Iteration 2600, Loss: 0.7457\n",
      "Iteration 2700, Loss: 0.7420\n",
      "Iteration 2800, Loss: 0.7384\n",
      "Iteration 2900, Loss: 0.7351\n",
      "Iteration 3000, Loss: 0.7318\n",
      "Iteration 3100, Loss: 0.7288\n",
      "Iteration 3200, Loss: 0.7258\n",
      "Iteration 3300, Loss: 0.7230\n",
      "Iteration 3400, Loss: 0.7203\n",
      "Iteration 3500, Loss: 0.7178\n",
      "Iteration 3600, Loss: 0.7153\n",
      "Iteration 3700, Loss: 0.7129\n",
      "Iteration 3800, Loss: 0.7106\n",
      "Iteration 3900, Loss: 0.7085\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0538\n",
      "Iteration 200, Loss: 1.0184\n",
      "Iteration 300, Loss: 0.9890\n",
      "Iteration 400, Loss: 0.9642\n",
      "Iteration 500, Loss: 0.9427\n",
      "Iteration 600, Loss: 0.9239\n",
      "Iteration 700, Loss: 0.9073\n",
      "Iteration 800, Loss: 0.8925\n",
      "Iteration 900, Loss: 0.8792\n",
      "Iteration 1000, Loss: 0.8672\n",
      "Iteration 1100, Loss: 0.8563\n",
      "Iteration 1200, Loss: 0.8463\n",
      "Iteration 1300, Loss: 0.8372\n",
      "Iteration 1400, Loss: 0.8288\n",
      "Iteration 1500, Loss: 0.8211\n",
      "Iteration 1600, Loss: 0.8140\n",
      "Iteration 1700, Loss: 0.8073\n",
      "Iteration 1800, Loss: 0.8011\n",
      "Iteration 1900, Loss: 0.7954\n",
      "Iteration 2000, Loss: 0.7900\n",
      "Iteration 2100, Loss: 0.7850\n",
      "Iteration 2200, Loss: 0.7803\n",
      "Iteration 2300, Loss: 0.7758\n",
      "Iteration 2400, Loss: 0.7716\n",
      "Iteration 2500, Loss: 0.7677\n",
      "Iteration 2600, Loss: 0.7639\n",
      "Iteration 2700, Loss: 0.7604\n",
      "Iteration 2800, Loss: 0.7570\n",
      "Iteration 2900, Loss: 0.7539\n",
      "Iteration 3000, Loss: 0.7509\n",
      "Iteration 3100, Loss: 0.7479\n",
      "Iteration 3200, Loss: 0.7452\n",
      "Iteration 3300, Loss: 0.7426\n",
      "Iteration 3400, Loss: 0.7400\n",
      "Iteration 3500, Loss: 0.7376\n",
      "Iteration 3600, Loss: 0.7353\n",
      "Iteration 3700, Loss: 0.7331\n",
      "Iteration 3800, Loss: 0.7309\n",
      "Iteration 3900, Loss: 0.7289\n",
      "90 270\n",
      "Iteration 0, Loss: 1.0215\n",
      "Iteration 100, Loss: 0.6152\n",
      "Iteration 200, Loss: 0.5862\n",
      "Iteration 300, Loss: 0.5702\n",
      "Iteration 400, Loss: 0.5593\n",
      "Iteration 500, Loss: 0.5512\n",
      "Iteration 600, Loss: 0.5447\n",
      "Iteration 700, Loss: 0.5392\n",
      "Iteration 800, Loss: 0.5346\n",
      "Iteration 900, Loss: 0.5306\n",
      "Iteration 1000, Loss: 0.5270\n",
      "Iteration 1100, Loss: 0.5238\n",
      "Iteration 1200, Loss: 0.5209\n",
      "Iteration 1300, Loss: 0.5183\n",
      "Iteration 1400, Loss: 0.5159\n",
      "Iteration 1500, Loss: 0.5136\n",
      "Iteration 1600, Loss: 0.5115\n",
      "Iteration 1700, Loss: 0.5095\n",
      "Iteration 1800, Loss: 0.5077\n",
      "Iteration 1900, Loss: 0.5060\n",
      "Iteration 2000, Loss: 0.5043\n",
      "Iteration 2100, Loss: 0.5028\n",
      "Iteration 2200, Loss: 0.5013\n",
      "Iteration 2300, Loss: 0.5000\n",
      "Iteration 2400, Loss: 0.4986\n",
      "Iteration 2500, Loss: 0.4973\n",
      "Iteration 2600, Loss: 0.4962\n",
      "Iteration 2700, Loss: 0.4950\n",
      "Iteration 2800, Loss: 0.4939\n",
      "Iteration 2900, Loss: 0.4929\n",
      "Iteration 3000, Loss: 0.4919\n",
      "Iteration 3100, Loss: 0.4909\n",
      "Iteration 3200, Loss: 0.4900\n",
      "Iteration 3300, Loss: 0.4891\n",
      "Iteration 3400, Loss: 0.4883\n",
      "Iteration 3500, Loss: 0.4874\n",
      "Iteration 3600, Loss: 0.4867\n",
      "Iteration 3700, Loss: 0.4859\n",
      "Iteration 3800, Loss: 0.4851\n",
      "Iteration 3900, Loss: 0.4844\n",
      "Iteration 4000, Loss: 0.4837\n",
      "Iteration 4100, Loss: 0.4830\n",
      "Iteration 4200, Loss: 0.4824\n",
      "Iteration 4300, Loss: 0.4818\n",
      "Iteration 4400, Loss: 0.4812\n",
      "Iteration 4500, Loss: 0.4806\n",
      "Iteration 4600, Loss: 0.4800\n",
      "Iteration 4700, Loss: 0.4795\n",
      "Iteration 4800, Loss: 0.4789\n",
      "Iteration 4900, Loss: 0.4784\n",
      "Iteration 0, Loss: 1.0210\n",
      "Iteration 100, Loss: 0.6297\n",
      "Iteration 200, Loss: 0.5971\n",
      "Iteration 300, Loss: 0.5794\n",
      "Iteration 400, Loss: 0.5674\n",
      "Iteration 500, Loss: 0.5584\n",
      "Iteration 600, Loss: 0.5513\n",
      "Iteration 700, Loss: 0.5454\n",
      "Iteration 800, Loss: 0.5405\n",
      "Iteration 900, Loss: 0.5362\n",
      "Iteration 1000, Loss: 0.5325\n",
      "Iteration 1100, Loss: 0.5292\n",
      "Iteration 1200, Loss: 0.5263\n",
      "Iteration 1300, Loss: 0.5236\n",
      "Iteration 1400, Loss: 0.5212\n",
      "Iteration 1500, Loss: 0.5189\n",
      "Iteration 1600, Loss: 0.5169\n",
      "Iteration 1700, Loss: 0.5150\n",
      "Iteration 1800, Loss: 0.5132\n",
      "Iteration 1900, Loss: 0.5116\n",
      "Iteration 2000, Loss: 0.5100\n",
      "Iteration 2100, Loss: 0.5086\n",
      "Iteration 2200, Loss: 0.5072\n",
      "Iteration 2300, Loss: 0.5059\n",
      "Iteration 2400, Loss: 0.5047\n",
      "Iteration 2500, Loss: 0.5035\n",
      "Iteration 2600, Loss: 0.5024\n",
      "Iteration 2700, Loss: 0.5014\n",
      "Iteration 2800, Loss: 0.5004\n",
      "Iteration 2900, Loss: 0.4994\n",
      "Iteration 3000, Loss: 0.4985\n",
      "Iteration 3100, Loss: 0.4976\n",
      "Iteration 3200, Loss: 0.4967\n",
      "Iteration 3300, Loss: 0.4959\n",
      "Iteration 3400, Loss: 0.4952\n",
      "Iteration 3500, Loss: 0.4944\n",
      "Iteration 3600, Loss: 0.4937\n",
      "Iteration 3700, Loss: 0.4930\n",
      "Iteration 3800, Loss: 0.4923\n",
      "Iteration 3900, Loss: 0.4917\n",
      "Iteration 4000, Loss: 0.4910\n",
      "Iteration 4100, Loss: 0.4904\n",
      "Iteration 4200, Loss: 0.4898\n",
      "Iteration 4300, Loss: 0.4893\n",
      "Iteration 4400, Loss: 0.4887\n",
      "Iteration 4500, Loss: 0.4882\n",
      "Iteration 4600, Loss: 0.4876\n",
      "Iteration 4700, Loss: 0.4871\n",
      "Iteration 4800, Loss: 0.4866\n",
      "Iteration 4900, Loss: 0.4862\n",
      "91 270\n",
      "Iteration 0, Loss: 1.0173\n",
      "Iteration 100, Loss: 0.6181\n",
      "Iteration 200, Loss: 0.5880\n",
      "Iteration 300, Loss: 0.5714\n",
      "Iteration 400, Loss: 0.5600\n",
      "Iteration 500, Loss: 0.5514\n",
      "Iteration 600, Loss: 0.5445\n",
      "Iteration 700, Loss: 0.5388\n",
      "Iteration 800, Loss: 0.5340\n",
      "Iteration 900, Loss: 0.5298\n",
      "Iteration 1000, Loss: 0.5261\n",
      "Iteration 1100, Loss: 0.5228\n",
      "Iteration 1200, Loss: 0.5199\n",
      "Iteration 1300, Loss: 0.5172\n",
      "Iteration 1400, Loss: 0.5147\n",
      "Iteration 1500, Loss: 0.5125\n",
      "Iteration 1600, Loss: 0.5105\n",
      "Iteration 1700, Loss: 0.5085\n",
      "Iteration 1800, Loss: 0.5067\n",
      "Iteration 1900, Loss: 0.5050\n",
      "Iteration 2000, Loss: 0.5034\n",
      "Iteration 2100, Loss: 0.5019\n",
      "Iteration 2200, Loss: 0.5005\n",
      "Iteration 2300, Loss: 0.4992\n",
      "Iteration 2400, Loss: 0.4979\n",
      "Iteration 2500, Loss: 0.4967\n",
      "Iteration 2600, Loss: 0.4956\n",
      "Iteration 2700, Loss: 0.4945\n",
      "Iteration 2800, Loss: 0.4934\n",
      "Iteration 2900, Loss: 0.4925\n",
      "Iteration 3000, Loss: 0.4916\n",
      "Iteration 3100, Loss: 0.4906\n",
      "Iteration 3200, Loss: 0.4898\n",
      "Iteration 3300, Loss: 0.4889\n",
      "Iteration 3400, Loss: 0.4881\n",
      "Iteration 3500, Loss: 0.4873\n",
      "Iteration 3600, Loss: 0.4866\n",
      "Iteration 3700, Loss: 0.4859\n",
      "Iteration 3800, Loss: 0.4852\n",
      "Iteration 3900, Loss: 0.4846\n",
      "Iteration 4000, Loss: 0.4839\n",
      "Iteration 4100, Loss: 0.4832\n",
      "Iteration 4200, Loss: 0.4826\n",
      "Iteration 4300, Loss: 0.4820\n",
      "Iteration 4400, Loss: 0.4814\n",
      "Iteration 4500, Loss: 0.4809\n",
      "Iteration 4600, Loss: 0.4803\n",
      "Iteration 4700, Loss: 0.4798\n",
      "Iteration 4800, Loss: 0.4793\n",
      "Iteration 4900, Loss: 0.4788\n",
      "Iteration 0, Loss: 1.0223\n",
      "Iteration 100, Loss: 0.6291\n",
      "Iteration 200, Loss: 0.5979\n",
      "Iteration 300, Loss: 0.5812\n",
      "Iteration 400, Loss: 0.5700\n",
      "Iteration 500, Loss: 0.5617\n",
      "Iteration 600, Loss: 0.5551\n",
      "Iteration 700, Loss: 0.5497\n",
      "Iteration 800, Loss: 0.5452\n",
      "Iteration 900, Loss: 0.5413\n",
      "Iteration 1000, Loss: 0.5379\n",
      "Iteration 1100, Loss: 0.5350\n",
      "Iteration 1200, Loss: 0.5322\n",
      "Iteration 1300, Loss: 0.5298\n",
      "Iteration 1400, Loss: 0.5276\n",
      "Iteration 1500, Loss: 0.5255\n",
      "Iteration 1600, Loss: 0.5237\n",
      "Iteration 1700, Loss: 0.5219\n",
      "Iteration 1800, Loss: 0.5203\n",
      "Iteration 1900, Loss: 0.5188\n",
      "Iteration 2000, Loss: 0.5173\n",
      "Iteration 2100, Loss: 0.5160\n",
      "Iteration 2200, Loss: 0.5147\n",
      "Iteration 2300, Loss: 0.5135\n",
      "Iteration 2400, Loss: 0.5124\n",
      "Iteration 2500, Loss: 0.5112\n",
      "Iteration 2600, Loss: 0.5102\n",
      "Iteration 2700, Loss: 0.5092\n",
      "Iteration 2800, Loss: 0.5082\n",
      "Iteration 2900, Loss: 0.5073\n",
      "Iteration 3000, Loss: 0.5064\n",
      "Iteration 3100, Loss: 0.5055\n",
      "Iteration 3200, Loss: 0.5047\n",
      "Iteration 3300, Loss: 0.5039\n",
      "Iteration 3400, Loss: 0.5031\n",
      "Iteration 3500, Loss: 0.5024\n",
      "Iteration 3600, Loss: 0.5017\n",
      "Iteration 3700, Loss: 0.5010\n",
      "Iteration 3800, Loss: 0.5003\n",
      "Iteration 3900, Loss: 0.4997\n",
      "Iteration 4000, Loss: 0.4991\n",
      "Iteration 4100, Loss: 0.4985\n",
      "Iteration 4200, Loss: 0.4979\n",
      "Iteration 4300, Loss: 0.4973\n",
      "Iteration 4400, Loss: 0.4968\n",
      "Iteration 4500, Loss: 0.4962\n",
      "Iteration 4600, Loss: 0.4957\n",
      "Iteration 4700, Loss: 0.4952\n",
      "Iteration 4800, Loss: 0.4947\n",
      "Iteration 4900, Loss: 0.4943\n",
      "92 270\n",
      "Iteration 0, Loss: 1.0141\n",
      "Iteration 100, Loss: 0.6079\n",
      "Iteration 200, Loss: 0.5761\n",
      "Iteration 300, Loss: 0.5589\n",
      "Iteration 400, Loss: 0.5474\n",
      "Iteration 500, Loss: 0.5390\n",
      "Iteration 600, Loss: 0.5324\n",
      "Iteration 700, Loss: 0.5271\n",
      "Iteration 800, Loss: 0.5226\n",
      "Iteration 900, Loss: 0.5188\n",
      "Iteration 1000, Loss: 0.5154\n",
      "Iteration 1100, Loss: 0.5124\n",
      "Iteration 1200, Loss: 0.5097\n",
      "Iteration 1300, Loss: 0.5073\n",
      "Iteration 1400, Loss: 0.5051\n",
      "Iteration 1500, Loss: 0.5031\n",
      "Iteration 1600, Loss: 0.5013\n",
      "Iteration 1700, Loss: 0.4996\n",
      "Iteration 1800, Loss: 0.4979\n",
      "Iteration 1900, Loss: 0.4964\n",
      "Iteration 2000, Loss: 0.4950\n",
      "Iteration 2100, Loss: 0.4937\n",
      "Iteration 2200, Loss: 0.4925\n",
      "Iteration 2300, Loss: 0.4913\n",
      "Iteration 2400, Loss: 0.4902\n",
      "Iteration 2500, Loss: 0.4892\n",
      "Iteration 2600, Loss: 0.4881\n",
      "Iteration 2700, Loss: 0.4872\n",
      "Iteration 2800, Loss: 0.4862\n",
      "Iteration 2900, Loss: 0.4854\n",
      "Iteration 3000, Loss: 0.4845\n",
      "Iteration 3100, Loss: 0.4837\n",
      "Iteration 3200, Loss: 0.4829\n",
      "Iteration 3300, Loss: 0.4822\n",
      "Iteration 3400, Loss: 0.4815\n",
      "Iteration 3500, Loss: 0.4808\n",
      "Iteration 3600, Loss: 0.4801\n",
      "Iteration 3700, Loss: 0.4794\n",
      "Iteration 3800, Loss: 0.4788\n",
      "Iteration 3900, Loss: 0.4782\n",
      "Iteration 4000, Loss: 0.4776\n",
      "Iteration 4100, Loss: 0.4770\n",
      "Iteration 4200, Loss: 0.4765\n",
      "Iteration 4300, Loss: 0.4759\n",
      "Iteration 4400, Loss: 0.4754\n",
      "Iteration 4500, Loss: 0.4749\n",
      "Iteration 4600, Loss: 0.4744\n",
      "Iteration 4700, Loss: 0.4739\n",
      "Iteration 4800, Loss: 0.4735\n",
      "Iteration 4900, Loss: 0.4730\n",
      "Iteration 0, Loss: 1.0282\n",
      "Iteration 100, Loss: 0.6387\n",
      "Iteration 200, Loss: 0.6096\n",
      "Iteration 300, Loss: 0.5937\n",
      "Iteration 400, Loss: 0.5827\n",
      "Iteration 500, Loss: 0.5744\n",
      "Iteration 600, Loss: 0.5680\n",
      "Iteration 700, Loss: 0.5625\n",
      "Iteration 800, Loss: 0.5579\n",
      "Iteration 900, Loss: 0.5539\n",
      "Iteration 1000, Loss: 0.5504\n",
      "Iteration 1100, Loss: 0.5472\n",
      "Iteration 1200, Loss: 0.5444\n",
      "Iteration 1300, Loss: 0.5418\n",
      "Iteration 1400, Loss: 0.5394\n",
      "Iteration 1500, Loss: 0.5372\n",
      "Iteration 1600, Loss: 0.5352\n",
      "Iteration 1700, Loss: 0.5333\n",
      "Iteration 1800, Loss: 0.5316\n",
      "Iteration 1900, Loss: 0.5299\n",
      "Iteration 2000, Loss: 0.5283\n",
      "Iteration 2100, Loss: 0.5268\n",
      "Iteration 2200, Loss: 0.5254\n",
      "Iteration 2300, Loss: 0.5241\n",
      "Iteration 2400, Loss: 0.5228\n",
      "Iteration 2500, Loss: 0.5216\n",
      "Iteration 2600, Loss: 0.5205\n",
      "Iteration 2700, Loss: 0.5194\n",
      "Iteration 2800, Loss: 0.5183\n",
      "Iteration 2900, Loss: 0.5173\n",
      "Iteration 3000, Loss: 0.5164\n",
      "Iteration 3100, Loss: 0.5154\n",
      "Iteration 3200, Loss: 0.5146\n",
      "Iteration 3300, Loss: 0.5137\n",
      "Iteration 3400, Loss: 0.5129\n",
      "Iteration 3500, Loss: 0.5121\n",
      "Iteration 3600, Loss: 0.5113\n",
      "Iteration 3700, Loss: 0.5106\n",
      "Iteration 3800, Loss: 0.5099\n",
      "Iteration 3900, Loss: 0.5092\n",
      "Iteration 4000, Loss: 0.5086\n",
      "Iteration 4100, Loss: 0.5079\n",
      "Iteration 4200, Loss: 0.5073\n",
      "Iteration 4300, Loss: 0.5067\n",
      "Iteration 4400, Loss: 0.5061\n",
      "Iteration 4500, Loss: 0.5055\n",
      "Iteration 4600, Loss: 0.5050\n",
      "Iteration 4700, Loss: 0.5045\n",
      "Iteration 4800, Loss: 0.5040\n",
      "Iteration 4900, Loss: 0.5035\n",
      "93 270\n",
      "Iteration 0, Loss: 1.0177\n",
      "Iteration 100, Loss: 0.6161\n",
      "Iteration 200, Loss: 0.5853\n",
      "Iteration 300, Loss: 0.5688\n",
      "Iteration 400, Loss: 0.5576\n",
      "Iteration 500, Loss: 0.5494\n",
      "Iteration 600, Loss: 0.5428\n",
      "Iteration 700, Loss: 0.5374\n",
      "Iteration 800, Loss: 0.5328\n",
      "Iteration 900, Loss: 0.5289\n",
      "Iteration 1000, Loss: 0.5254\n",
      "Iteration 1100, Loss: 0.5223\n",
      "Iteration 1200, Loss: 0.5195\n",
      "Iteration 1300, Loss: 0.5170\n",
      "Iteration 1400, Loss: 0.5147\n",
      "Iteration 1500, Loss: 0.5126\n",
      "Iteration 1600, Loss: 0.5106\n",
      "Iteration 1700, Loss: 0.5088\n",
      "Iteration 1800, Loss: 0.5071\n",
      "Iteration 1900, Loss: 0.5056\n",
      "Iteration 2000, Loss: 0.5041\n",
      "Iteration 2100, Loss: 0.5027\n",
      "Iteration 2200, Loss: 0.5013\n",
      "Iteration 2300, Loss: 0.5000\n",
      "Iteration 2400, Loss: 0.4988\n",
      "Iteration 2500, Loss: 0.4977\n",
      "Iteration 2600, Loss: 0.4966\n",
      "Iteration 2700, Loss: 0.4956\n",
      "Iteration 2800, Loss: 0.4946\n",
      "Iteration 2900, Loss: 0.4937\n",
      "Iteration 3000, Loss: 0.4928\n",
      "Iteration 3100, Loss: 0.4919\n",
      "Iteration 3200, Loss: 0.4911\n",
      "Iteration 3300, Loss: 0.4903\n",
      "Iteration 3400, Loss: 0.4895\n",
      "Iteration 3500, Loss: 0.4888\n",
      "Iteration 3600, Loss: 0.4881\n",
      "Iteration 3700, Loss: 0.4874\n",
      "Iteration 3800, Loss: 0.4868\n",
      "Iteration 3900, Loss: 0.4861\n",
      "Iteration 4000, Loss: 0.4855\n",
      "Iteration 4100, Loss: 0.4849\n",
      "Iteration 4200, Loss: 0.4843\n",
      "Iteration 4300, Loss: 0.4838\n",
      "Iteration 4400, Loss: 0.4833\n",
      "Iteration 4500, Loss: 0.4827\n",
      "Iteration 4600, Loss: 0.4822\n",
      "Iteration 4700, Loss: 0.4817\n",
      "Iteration 4800, Loss: 0.4812\n",
      "Iteration 4900, Loss: 0.4808\n",
      "Iteration 0, Loss: 1.0250\n",
      "Iteration 100, Loss: 0.6305\n",
      "Iteration 200, Loss: 0.6005\n",
      "Iteration 300, Loss: 0.5840\n",
      "Iteration 400, Loss: 0.5728\n",
      "Iteration 500, Loss: 0.5643\n",
      "Iteration 600, Loss: 0.5576\n",
      "Iteration 700, Loss: 0.5522\n",
      "Iteration 800, Loss: 0.5476\n",
      "Iteration 900, Loss: 0.5437\n",
      "Iteration 1000, Loss: 0.5403\n",
      "Iteration 1100, Loss: 0.5373\n",
      "Iteration 1200, Loss: 0.5346\n",
      "Iteration 1300, Loss: 0.5322\n",
      "Iteration 1400, Loss: 0.5300\n",
      "Iteration 1500, Loss: 0.5280\n",
      "Iteration 1600, Loss: 0.5261\n",
      "Iteration 1700, Loss: 0.5244\n",
      "Iteration 1800, Loss: 0.5228\n",
      "Iteration 1900, Loss: 0.5213\n",
      "Iteration 2000, Loss: 0.5199\n",
      "Iteration 2100, Loss: 0.5186\n",
      "Iteration 2200, Loss: 0.5174\n",
      "Iteration 2300, Loss: 0.5162\n",
      "Iteration 2400, Loss: 0.5151\n",
      "Iteration 2500, Loss: 0.5141\n",
      "Iteration 2600, Loss: 0.5131\n",
      "Iteration 2700, Loss: 0.5122\n",
      "Iteration 2800, Loss: 0.5112\n",
      "Iteration 2900, Loss: 0.5104\n",
      "Iteration 3000, Loss: 0.5096\n",
      "Iteration 3100, Loss: 0.5088\n",
      "Iteration 3200, Loss: 0.5080\n",
      "Iteration 3300, Loss: 0.5073\n",
      "Iteration 3400, Loss: 0.5066\n",
      "Iteration 3500, Loss: 0.5059\n",
      "Iteration 3600, Loss: 0.5052\n",
      "Iteration 3700, Loss: 0.5046\n",
      "Iteration 3800, Loss: 0.5040\n",
      "Iteration 3900, Loss: 0.5034\n",
      "Iteration 4000, Loss: 0.5028\n",
      "Iteration 4100, Loss: 0.5023\n",
      "Iteration 4200, Loss: 0.5017\n",
      "Iteration 4300, Loss: 0.5012\n",
      "Iteration 4400, Loss: 0.5007\n",
      "Iteration 4500, Loss: 0.5003\n",
      "Iteration 4600, Loss: 0.4998\n",
      "Iteration 4700, Loss: 0.4993\n",
      "Iteration 4800, Loss: 0.4989\n",
      "Iteration 4900, Loss: 0.4984\n",
      "94 270\n",
      "Iteration 0, Loss: 1.0192\n",
      "Iteration 100, Loss: 0.6174\n",
      "Iteration 200, Loss: 0.5869\n",
      "Iteration 300, Loss: 0.5710\n",
      "Iteration 400, Loss: 0.5604\n",
      "Iteration 500, Loss: 0.5525\n",
      "Iteration 600, Loss: 0.5463\n",
      "Iteration 700, Loss: 0.5412\n",
      "Iteration 800, Loss: 0.5368\n",
      "Iteration 900, Loss: 0.5331\n",
      "Iteration 1000, Loss: 0.5297\n",
      "Iteration 1100, Loss: 0.5268\n",
      "Iteration 1200, Loss: 0.5242\n",
      "Iteration 1300, Loss: 0.5218\n",
      "Iteration 1400, Loss: 0.5196\n",
      "Iteration 1500, Loss: 0.5177\n",
      "Iteration 1600, Loss: 0.5158\n",
      "Iteration 1700, Loss: 0.5141\n",
      "Iteration 1800, Loss: 0.5125\n",
      "Iteration 1900, Loss: 0.5110\n",
      "Iteration 2000, Loss: 0.5096\n",
      "Iteration 2100, Loss: 0.5083\n",
      "Iteration 2200, Loss: 0.5070\n",
      "Iteration 2300, Loss: 0.5058\n",
      "Iteration 2400, Loss: 0.5047\n",
      "Iteration 2500, Loss: 0.5036\n",
      "Iteration 2600, Loss: 0.5026\n",
      "Iteration 2700, Loss: 0.5016\n",
      "Iteration 2800, Loss: 0.5007\n",
      "Iteration 2900, Loss: 0.4998\n",
      "Iteration 3000, Loss: 0.4989\n",
      "Iteration 3100, Loss: 0.4981\n",
      "Iteration 3200, Loss: 0.4973\n",
      "Iteration 3300, Loss: 0.4965\n",
      "Iteration 3400, Loss: 0.4958\n",
      "Iteration 3500, Loss: 0.4951\n",
      "Iteration 3600, Loss: 0.4944\n",
      "Iteration 3700, Loss: 0.4937\n",
      "Iteration 3800, Loss: 0.4931\n",
      "Iteration 3900, Loss: 0.4925\n",
      "Iteration 4000, Loss: 0.4919\n",
      "Iteration 4100, Loss: 0.4913\n",
      "Iteration 4200, Loss: 0.4907\n",
      "Iteration 4300, Loss: 0.4902\n",
      "Iteration 4400, Loss: 0.4896\n",
      "Iteration 4500, Loss: 0.4891\n",
      "Iteration 4600, Loss: 0.4886\n",
      "Iteration 4700, Loss: 0.4882\n",
      "Iteration 4800, Loss: 0.4877\n",
      "Iteration 4900, Loss: 0.4872\n",
      "Iteration 0, Loss: 1.0215\n",
      "Iteration 100, Loss: 0.6316\n",
      "Iteration 200, Loss: 0.6021\n",
      "Iteration 300, Loss: 0.5853\n",
      "Iteration 400, Loss: 0.5738\n",
      "Iteration 500, Loss: 0.5652\n",
      "Iteration 600, Loss: 0.5584\n",
      "Iteration 700, Loss: 0.5528\n",
      "Iteration 800, Loss: 0.5480\n",
      "Iteration 900, Loss: 0.5440\n",
      "Iteration 1000, Loss: 0.5404\n",
      "Iteration 1100, Loss: 0.5371\n",
      "Iteration 1200, Loss: 0.5342\n",
      "Iteration 1300, Loss: 0.5316\n",
      "Iteration 1400, Loss: 0.5292\n",
      "Iteration 1500, Loss: 0.5269\n",
      "Iteration 1600, Loss: 0.5249\n",
      "Iteration 1700, Loss: 0.5229\n",
      "Iteration 1800, Loss: 0.5212\n",
      "Iteration 1900, Loss: 0.5195\n",
      "Iteration 2000, Loss: 0.5179\n",
      "Iteration 2100, Loss: 0.5164\n",
      "Iteration 2200, Loss: 0.5150\n",
      "Iteration 2300, Loss: 0.5137\n",
      "Iteration 2400, Loss: 0.5124\n",
      "Iteration 2500, Loss: 0.5112\n",
      "Iteration 2600, Loss: 0.5100\n",
      "Iteration 2700, Loss: 0.5089\n",
      "Iteration 2800, Loss: 0.5079\n",
      "Iteration 2900, Loss: 0.5069\n",
      "Iteration 3000, Loss: 0.5059\n",
      "Iteration 3100, Loss: 0.5050\n",
      "Iteration 3200, Loss: 0.5041\n",
      "Iteration 3300, Loss: 0.5033\n",
      "Iteration 3400, Loss: 0.5024\n",
      "Iteration 3500, Loss: 0.5016\n",
      "Iteration 3600, Loss: 0.5009\n",
      "Iteration 3700, Loss: 0.5002\n",
      "Iteration 3800, Loss: 0.4994\n",
      "Iteration 3900, Loss: 0.4987\n",
      "Iteration 4000, Loss: 0.4981\n",
      "Iteration 4100, Loss: 0.4974\n",
      "Iteration 4200, Loss: 0.4968\n",
      "Iteration 4300, Loss: 0.4962\n",
      "Iteration 4400, Loss: 0.4956\n",
      "Iteration 4500, Loss: 0.4950\n",
      "Iteration 4600, Loss: 0.4945\n",
      "Iteration 4700, Loss: 0.4939\n",
      "Iteration 4800, Loss: 0.4934\n",
      "Iteration 4900, Loss: 0.4929\n",
      "95 270\n",
      "Iteration 0, Loss: 1.0251\n",
      "Iteration 100, Loss: 0.6418\n",
      "Iteration 200, Loss: 0.6085\n",
      "Iteration 300, Loss: 0.5901\n",
      "Iteration 400, Loss: 0.5777\n",
      "Iteration 500, Loss: 0.5685\n",
      "Iteration 600, Loss: 0.5612\n",
      "Iteration 700, Loss: 0.5552\n",
      "Iteration 800, Loss: 0.5501\n",
      "Iteration 900, Loss: 0.5457\n",
      "Iteration 1000, Loss: 0.5418\n",
      "Iteration 1100, Loss: 0.5383\n",
      "Iteration 1200, Loss: 0.5352\n",
      "Iteration 1300, Loss: 0.5324\n",
      "Iteration 1400, Loss: 0.5297\n",
      "Iteration 1500, Loss: 0.5274\n",
      "Iteration 1600, Loss: 0.5251\n",
      "Iteration 1700, Loss: 0.5230\n",
      "Iteration 1800, Loss: 0.5211\n",
      "Iteration 1900, Loss: 0.5193\n",
      "Iteration 2000, Loss: 0.5175\n",
      "Iteration 2100, Loss: 0.5159\n",
      "Iteration 2200, Loss: 0.5144\n",
      "Iteration 2300, Loss: 0.5128\n",
      "Iteration 2400, Loss: 0.5114\n",
      "Iteration 2500, Loss: 0.5101\n",
      "Iteration 2600, Loss: 0.5088\n",
      "Iteration 2700, Loss: 0.5075\n",
      "Iteration 2800, Loss: 0.5063\n",
      "Iteration 2900, Loss: 0.5052\n",
      "Iteration 3000, Loss: 0.5041\n",
      "Iteration 3100, Loss: 0.5030\n",
      "Iteration 3200, Loss: 0.5020\n",
      "Iteration 3300, Loss: 0.5010\n",
      "Iteration 3400, Loss: 0.5000\n",
      "Iteration 3500, Loss: 0.4991\n",
      "Iteration 3600, Loss: 0.4982\n",
      "Iteration 3700, Loss: 0.4973\n",
      "Iteration 3800, Loss: 0.4964\n",
      "Iteration 3900, Loss: 0.4956\n",
      "Iteration 4000, Loss: 0.4948\n",
      "Iteration 4100, Loss: 0.4940\n",
      "Iteration 4200, Loss: 0.4932\n",
      "Iteration 4300, Loss: 0.4924\n",
      "Iteration 4400, Loss: 0.4917\n",
      "Iteration 4500, Loss: 0.4910\n",
      "Iteration 4600, Loss: 0.4903\n",
      "Iteration 4700, Loss: 0.4896\n",
      "Iteration 4800, Loss: 0.4889\n",
      "Iteration 4900, Loss: 0.4883\n",
      "Iteration 0, Loss: 1.0120\n",
      "Iteration 100, Loss: 0.6058\n",
      "Iteration 200, Loss: 0.5779\n",
      "Iteration 300, Loss: 0.5622\n",
      "Iteration 400, Loss: 0.5511\n",
      "Iteration 500, Loss: 0.5425\n",
      "Iteration 600, Loss: 0.5355\n",
      "Iteration 700, Loss: 0.5295\n",
      "Iteration 800, Loss: 0.5243\n",
      "Iteration 900, Loss: 0.5197\n",
      "Iteration 1000, Loss: 0.5157\n",
      "Iteration 1100, Loss: 0.5120\n",
      "Iteration 1200, Loss: 0.5086\n",
      "Iteration 1300, Loss: 0.5056\n",
      "Iteration 1400, Loss: 0.5027\n",
      "Iteration 1500, Loss: 0.5000\n",
      "Iteration 1600, Loss: 0.4975\n",
      "Iteration 1700, Loss: 0.4952\n",
      "Iteration 1800, Loss: 0.4930\n",
      "Iteration 1900, Loss: 0.4910\n",
      "Iteration 2000, Loss: 0.4890\n",
      "Iteration 2100, Loss: 0.4871\n",
      "Iteration 2200, Loss: 0.4854\n",
      "Iteration 2300, Loss: 0.4837\n",
      "Iteration 2400, Loss: 0.4821\n",
      "Iteration 2500, Loss: 0.4806\n",
      "Iteration 2600, Loss: 0.4791\n",
      "Iteration 2700, Loss: 0.4777\n",
      "Iteration 2800, Loss: 0.4763\n",
      "Iteration 2900, Loss: 0.4750\n",
      "Iteration 3000, Loss: 0.4738\n",
      "Iteration 3100, Loss: 0.4726\n",
      "Iteration 3200, Loss: 0.4714\n",
      "Iteration 3300, Loss: 0.4703\n",
      "Iteration 3400, Loss: 0.4692\n",
      "Iteration 3500, Loss: 0.4681\n",
      "Iteration 3600, Loss: 0.4671\n",
      "Iteration 3700, Loss: 0.4661\n",
      "Iteration 3800, Loss: 0.4652\n",
      "Iteration 3900, Loss: 0.4643\n",
      "Iteration 4000, Loss: 0.4634\n",
      "Iteration 4100, Loss: 0.4625\n",
      "Iteration 4200, Loss: 0.4616\n",
      "Iteration 4300, Loss: 0.4608\n",
      "Iteration 4400, Loss: 0.4600\n",
      "Iteration 4500, Loss: 0.4592\n",
      "Iteration 4600, Loss: 0.4584\n",
      "Iteration 4700, Loss: 0.4577\n",
      "Iteration 4800, Loss: 0.4570\n",
      "Iteration 4900, Loss: 0.4563\n",
      "96 270\n",
      "Iteration 0, Loss: 1.0192\n",
      "Iteration 100, Loss: 0.6085\n",
      "Iteration 200, Loss: 0.5750\n",
      "Iteration 300, Loss: 0.5567\n",
      "Iteration 400, Loss: 0.5442\n",
      "Iteration 500, Loss: 0.5348\n",
      "Iteration 600, Loss: 0.5275\n",
      "Iteration 700, Loss: 0.5214\n",
      "Iteration 800, Loss: 0.5162\n",
      "Iteration 900, Loss: 0.5118\n",
      "Iteration 1000, Loss: 0.5079\n",
      "Iteration 1100, Loss: 0.5044\n",
      "Iteration 1200, Loss: 0.5013\n",
      "Iteration 1300, Loss: 0.4985\n",
      "Iteration 1400, Loss: 0.4958\n",
      "Iteration 1500, Loss: 0.4934\n",
      "Iteration 1600, Loss: 0.4912\n",
      "Iteration 1700, Loss: 0.4891\n",
      "Iteration 1800, Loss: 0.4871\n",
      "Iteration 1900, Loss: 0.4853\n",
      "Iteration 2000, Loss: 0.4836\n",
      "Iteration 2100, Loss: 0.4820\n",
      "Iteration 2200, Loss: 0.4804\n",
      "Iteration 2300, Loss: 0.4789\n",
      "Iteration 2400, Loss: 0.4775\n",
      "Iteration 2500, Loss: 0.4762\n",
      "Iteration 2600, Loss: 0.4749\n",
      "Iteration 2700, Loss: 0.4737\n",
      "Iteration 2800, Loss: 0.4725\n",
      "Iteration 2900, Loss: 0.4713\n",
      "Iteration 3000, Loss: 0.4702\n",
      "Iteration 3100, Loss: 0.4692\n",
      "Iteration 3200, Loss: 0.4682\n",
      "Iteration 3300, Loss: 0.4672\n",
      "Iteration 3400, Loss: 0.4662\n",
      "Iteration 3500, Loss: 0.4653\n",
      "Iteration 3600, Loss: 0.4644\n",
      "Iteration 3700, Loss: 0.4636\n",
      "Iteration 3800, Loss: 0.4627\n",
      "Iteration 3900, Loss: 0.4619\n",
      "Iteration 4000, Loss: 0.4611\n",
      "Iteration 4100, Loss: 0.4603\n",
      "Iteration 4200, Loss: 0.4596\n",
      "Iteration 4300, Loss: 0.4588\n",
      "Iteration 4400, Loss: 0.4582\n",
      "Iteration 4500, Loss: 0.4575\n",
      "Iteration 4600, Loss: 0.4567\n",
      "Iteration 4700, Loss: 0.4561\n",
      "Iteration 4800, Loss: 0.4554\n",
      "Iteration 4900, Loss: 0.4548\n",
      "Iteration 0, Loss: 1.0201\n",
      "Iteration 100, Loss: 0.6381\n",
      "Iteration 200, Loss: 0.6093\n",
      "Iteration 300, Loss: 0.5926\n",
      "Iteration 400, Loss: 0.5808\n",
      "Iteration 500, Loss: 0.5718\n",
      "Iteration 600, Loss: 0.5645\n",
      "Iteration 700, Loss: 0.5583\n",
      "Iteration 800, Loss: 0.5531\n",
      "Iteration 900, Loss: 0.5485\n",
      "Iteration 1000, Loss: 0.5444\n",
      "Iteration 1100, Loss: 0.5407\n",
      "Iteration 1200, Loss: 0.5374\n",
      "Iteration 1300, Loss: 0.5343\n",
      "Iteration 1400, Loss: 0.5315\n",
      "Iteration 1500, Loss: 0.5289\n",
      "Iteration 1600, Loss: 0.5265\n",
      "Iteration 1700, Loss: 0.5242\n",
      "Iteration 1800, Loss: 0.5221\n",
      "Iteration 1900, Loss: 0.5201\n",
      "Iteration 2000, Loss: 0.5182\n",
      "Iteration 2100, Loss: 0.5164\n",
      "Iteration 2200, Loss: 0.5147\n",
      "Iteration 2300, Loss: 0.5131\n",
      "Iteration 2400, Loss: 0.5116\n",
      "Iteration 2500, Loss: 0.5101\n",
      "Iteration 2600, Loss: 0.5087\n",
      "Iteration 2700, Loss: 0.5073\n",
      "Iteration 2800, Loss: 0.5060\n",
      "Iteration 2900, Loss: 0.5047\n",
      "Iteration 3000, Loss: 0.5035\n",
      "Iteration 3100, Loss: 0.5024\n",
      "Iteration 3200, Loss: 0.5013\n",
      "Iteration 3300, Loss: 0.5002\n",
      "Iteration 3400, Loss: 0.4991\n",
      "Iteration 3500, Loss: 0.4981\n",
      "Iteration 3600, Loss: 0.4971\n",
      "Iteration 3700, Loss: 0.4961\n",
      "Iteration 3800, Loss: 0.4952\n",
      "Iteration 3900, Loss: 0.4943\n",
      "Iteration 4000, Loss: 0.4934\n",
      "Iteration 4100, Loss: 0.4926\n",
      "Iteration 4200, Loss: 0.4917\n",
      "Iteration 4300, Loss: 0.4909\n",
      "Iteration 4400, Loss: 0.4902\n",
      "Iteration 4500, Loss: 0.4894\n",
      "Iteration 4600, Loss: 0.4886\n",
      "Iteration 4700, Loss: 0.4879\n",
      "Iteration 4800, Loss: 0.4872\n",
      "Iteration 4900, Loss: 0.4865\n",
      "97 270\n",
      "Iteration 0, Loss: 1.0184\n",
      "Iteration 100, Loss: 0.6199\n",
      "Iteration 200, Loss: 0.5884\n",
      "Iteration 300, Loss: 0.5702\n",
      "Iteration 400, Loss: 0.5576\n",
      "Iteration 500, Loss: 0.5479\n",
      "Iteration 600, Loss: 0.5402\n",
      "Iteration 700, Loss: 0.5337\n",
      "Iteration 800, Loss: 0.5282\n",
      "Iteration 900, Loss: 0.5234\n",
      "Iteration 1000, Loss: 0.5191\n",
      "Iteration 1100, Loss: 0.5153\n",
      "Iteration 1200, Loss: 0.5118\n",
      "Iteration 1300, Loss: 0.5086\n",
      "Iteration 1400, Loss: 0.5056\n",
      "Iteration 1500, Loss: 0.5028\n",
      "Iteration 1600, Loss: 0.5003\n",
      "Iteration 1700, Loss: 0.4978\n",
      "Iteration 1800, Loss: 0.4956\n",
      "Iteration 1900, Loss: 0.4934\n",
      "Iteration 2000, Loss: 0.4914\n",
      "Iteration 2100, Loss: 0.4894\n",
      "Iteration 2200, Loss: 0.4876\n",
      "Iteration 2300, Loss: 0.4858\n",
      "Iteration 2400, Loss: 0.4841\n",
      "Iteration 2500, Loss: 0.4825\n",
      "Iteration 2600, Loss: 0.4809\n",
      "Iteration 2700, Loss: 0.4794\n",
      "Iteration 2800, Loss: 0.4779\n",
      "Iteration 2900, Loss: 0.4765\n",
      "Iteration 3000, Loss: 0.4751\n",
      "Iteration 3100, Loss: 0.4738\n",
      "Iteration 3200, Loss: 0.4726\n",
      "Iteration 3300, Loss: 0.4713\n",
      "Iteration 3400, Loss: 0.4701\n",
      "Iteration 3500, Loss: 0.4690\n",
      "Iteration 3600, Loss: 0.4678\n",
      "Iteration 3700, Loss: 0.4667\n",
      "Iteration 3800, Loss: 0.4657\n",
      "Iteration 3900, Loss: 0.4647\n",
      "Iteration 4000, Loss: 0.4636\n",
      "Iteration 4100, Loss: 0.4627\n",
      "Iteration 4200, Loss: 0.4617\n",
      "Iteration 4300, Loss: 0.4608\n",
      "Iteration 4400, Loss: 0.4599\n",
      "Iteration 4500, Loss: 0.4590\n",
      "Iteration 4600, Loss: 0.4581\n",
      "Iteration 4700, Loss: 0.4573\n",
      "Iteration 4800, Loss: 0.4565\n",
      "Iteration 4900, Loss: 0.4557\n",
      "Iteration 0, Loss: 1.0202\n",
      "Iteration 100, Loss: 0.6236\n",
      "Iteration 200, Loss: 0.5930\n",
      "Iteration 300, Loss: 0.5766\n",
      "Iteration 400, Loss: 0.5652\n",
      "Iteration 500, Loss: 0.5566\n",
      "Iteration 600, Loss: 0.5497\n",
      "Iteration 700, Loss: 0.5440\n",
      "Iteration 800, Loss: 0.5391\n",
      "Iteration 900, Loss: 0.5348\n",
      "Iteration 1000, Loss: 0.5310\n",
      "Iteration 1100, Loss: 0.5276\n",
      "Iteration 1200, Loss: 0.5245\n",
      "Iteration 1300, Loss: 0.5217\n",
      "Iteration 1400, Loss: 0.5191\n",
      "Iteration 1500, Loss: 0.5168\n",
      "Iteration 1600, Loss: 0.5145\n",
      "Iteration 1700, Loss: 0.5125\n",
      "Iteration 1800, Loss: 0.5105\n",
      "Iteration 1900, Loss: 0.5087\n",
      "Iteration 2000, Loss: 0.5069\n",
      "Iteration 2100, Loss: 0.5053\n",
      "Iteration 2200, Loss: 0.5038\n",
      "Iteration 2300, Loss: 0.5023\n",
      "Iteration 2400, Loss: 0.5009\n",
      "Iteration 2500, Loss: 0.4996\n",
      "Iteration 2600, Loss: 0.4983\n",
      "Iteration 2700, Loss: 0.4971\n",
      "Iteration 2800, Loss: 0.4959\n",
      "Iteration 2900, Loss: 0.4948\n",
      "Iteration 3000, Loss: 0.4937\n",
      "Iteration 3100, Loss: 0.4926\n",
      "Iteration 3200, Loss: 0.4916\n",
      "Iteration 3300, Loss: 0.4907\n",
      "Iteration 3400, Loss: 0.4897\n",
      "Iteration 3500, Loss: 0.4888\n",
      "Iteration 3600, Loss: 0.4880\n",
      "Iteration 3700, Loss: 0.4871\n",
      "Iteration 3800, Loss: 0.4863\n",
      "Iteration 3900, Loss: 0.4855\n",
      "Iteration 4000, Loss: 0.4847\n",
      "Iteration 4100, Loss: 0.4839\n",
      "Iteration 4200, Loss: 0.4832\n",
      "Iteration 4300, Loss: 0.4825\n",
      "Iteration 4400, Loss: 0.4818\n",
      "Iteration 4500, Loss: 0.4812\n",
      "Iteration 4600, Loss: 0.4805\n",
      "Iteration 4700, Loss: 0.4798\n",
      "Iteration 4800, Loss: 0.4792\n",
      "Iteration 4900, Loss: 0.4786\n",
      "98 270\n",
      "Iteration 0, Loss: 1.0120\n",
      "Iteration 100, Loss: 0.6240\n",
      "Iteration 200, Loss: 0.5961\n",
      "Iteration 300, Loss: 0.5798\n",
      "Iteration 400, Loss: 0.5682\n",
      "Iteration 500, Loss: 0.5594\n",
      "Iteration 600, Loss: 0.5522\n",
      "Iteration 700, Loss: 0.5462\n",
      "Iteration 800, Loss: 0.5410\n",
      "Iteration 900, Loss: 0.5365\n",
      "Iteration 1000, Loss: 0.5325\n",
      "Iteration 1100, Loss: 0.5289\n",
      "Iteration 1200, Loss: 0.5256\n",
      "Iteration 1300, Loss: 0.5226\n",
      "Iteration 1400, Loss: 0.5198\n",
      "Iteration 1500, Loss: 0.5172\n",
      "Iteration 1600, Loss: 0.5148\n",
      "Iteration 1700, Loss: 0.5125\n",
      "Iteration 1800, Loss: 0.5104\n",
      "Iteration 1900, Loss: 0.5084\n",
      "Iteration 2000, Loss: 0.5065\n",
      "Iteration 2100, Loss: 0.5047\n",
      "Iteration 2200, Loss: 0.5030\n",
      "Iteration 2300, Loss: 0.5014\n",
      "Iteration 2400, Loss: 0.4998\n",
      "Iteration 2500, Loss: 0.4984\n",
      "Iteration 2600, Loss: 0.4970\n",
      "Iteration 2700, Loss: 0.4956\n",
      "Iteration 2800, Loss: 0.4943\n",
      "Iteration 2900, Loss: 0.4930\n",
      "Iteration 3000, Loss: 0.4918\n",
      "Iteration 3100, Loss: 0.4907\n",
      "Iteration 3200, Loss: 0.4896\n",
      "Iteration 3300, Loss: 0.4885\n",
      "Iteration 3400, Loss: 0.4874\n",
      "Iteration 3500, Loss: 0.4864\n",
      "Iteration 3600, Loss: 0.4854\n",
      "Iteration 3700, Loss: 0.4845\n",
      "Iteration 3800, Loss: 0.4836\n",
      "Iteration 3900, Loss: 0.4827\n",
      "Iteration 4000, Loss: 0.4818\n",
      "Iteration 4100, Loss: 0.4809\n",
      "Iteration 4200, Loss: 0.4801\n",
      "Iteration 4300, Loss: 0.4793\n",
      "Iteration 4400, Loss: 0.4785\n",
      "Iteration 4500, Loss: 0.4778\n",
      "Iteration 4600, Loss: 0.4770\n",
      "Iteration 4700, Loss: 0.4763\n",
      "Iteration 4800, Loss: 0.4756\n",
      "Iteration 4900, Loss: 0.4749\n",
      "Iteration 0, Loss: 1.0279\n",
      "Iteration 100, Loss: 0.6175\n",
      "Iteration 200, Loss: 0.5835\n",
      "Iteration 300, Loss: 0.5655\n",
      "Iteration 400, Loss: 0.5531\n",
      "Iteration 500, Loss: 0.5438\n",
      "Iteration 600, Loss: 0.5365\n",
      "Iteration 700, Loss: 0.5303\n",
      "Iteration 800, Loss: 0.5252\n",
      "Iteration 900, Loss: 0.5206\n",
      "Iteration 1000, Loss: 0.5167\n",
      "Iteration 1100, Loss: 0.5131\n",
      "Iteration 1200, Loss: 0.5099\n",
      "Iteration 1300, Loss: 0.5070\n",
      "Iteration 1400, Loss: 0.5043\n",
      "Iteration 1500, Loss: 0.5019\n",
      "Iteration 1600, Loss: 0.4996\n",
      "Iteration 1700, Loss: 0.4974\n",
      "Iteration 1800, Loss: 0.4955\n",
      "Iteration 1900, Loss: 0.4936\n",
      "Iteration 2000, Loss: 0.4918\n",
      "Iteration 2100, Loss: 0.4902\n",
      "Iteration 2200, Loss: 0.4886\n",
      "Iteration 2300, Loss: 0.4871\n",
      "Iteration 2400, Loss: 0.4857\n",
      "Iteration 2500, Loss: 0.4843\n",
      "Iteration 2600, Loss: 0.4831\n",
      "Iteration 2700, Loss: 0.4818\n",
      "Iteration 2800, Loss: 0.4806\n",
      "Iteration 2900, Loss: 0.4795\n",
      "Iteration 3000, Loss: 0.4784\n",
      "Iteration 3100, Loss: 0.4774\n",
      "Iteration 3200, Loss: 0.4763\n",
      "Iteration 3300, Loss: 0.4753\n",
      "Iteration 3400, Loss: 0.4744\n",
      "Iteration 3500, Loss: 0.4735\n",
      "Iteration 3600, Loss: 0.4726\n",
      "Iteration 3700, Loss: 0.4717\n",
      "Iteration 3800, Loss: 0.4709\n",
      "Iteration 3900, Loss: 0.4700\n",
      "Iteration 4000, Loss: 0.4692\n",
      "Iteration 4100, Loss: 0.4685\n",
      "Iteration 4200, Loss: 0.4677\n",
      "Iteration 4300, Loss: 0.4670\n",
      "Iteration 4400, Loss: 0.4663\n",
      "Iteration 4500, Loss: 0.4656\n",
      "Iteration 4600, Loss: 0.4649\n",
      "Iteration 4700, Loss: 0.4642\n",
      "Iteration 4800, Loss: 0.4636\n",
      "Iteration 4900, Loss: 0.4630\n",
      "99 270\n",
      "Iteration 0, Loss: 1.0173\n",
      "Iteration 100, Loss: 0.6267\n",
      "Iteration 200, Loss: 0.5938\n",
      "Iteration 300, Loss: 0.5755\n",
      "Iteration 400, Loss: 0.5630\n",
      "Iteration 500, Loss: 0.5537\n",
      "Iteration 600, Loss: 0.5463\n",
      "Iteration 700, Loss: 0.5402\n",
      "Iteration 800, Loss: 0.5351\n",
      "Iteration 900, Loss: 0.5307\n",
      "Iteration 1000, Loss: 0.5269\n",
      "Iteration 1100, Loss: 0.5235\n",
      "Iteration 1200, Loss: 0.5205\n",
      "Iteration 1300, Loss: 0.5177\n",
      "Iteration 1400, Loss: 0.5152\n",
      "Iteration 1500, Loss: 0.5130\n",
      "Iteration 1600, Loss: 0.5108\n",
      "Iteration 1700, Loss: 0.5088\n",
      "Iteration 1800, Loss: 0.5070\n",
      "Iteration 1900, Loss: 0.5052\n",
      "Iteration 2000, Loss: 0.5036\n",
      "Iteration 2100, Loss: 0.5020\n",
      "Iteration 2200, Loss: 0.5006\n",
      "Iteration 2300, Loss: 0.4992\n",
      "Iteration 2400, Loss: 0.4979\n",
      "Iteration 2500, Loss: 0.4966\n",
      "Iteration 2600, Loss: 0.4954\n",
      "Iteration 2700, Loss: 0.4942\n",
      "Iteration 2800, Loss: 0.4931\n",
      "Iteration 2900, Loss: 0.4920\n",
      "Iteration 3000, Loss: 0.4910\n",
      "Iteration 3100, Loss: 0.4900\n",
      "Iteration 3200, Loss: 0.4890\n",
      "Iteration 3300, Loss: 0.4881\n",
      "Iteration 3400, Loss: 0.4872\n",
      "Iteration 3500, Loss: 0.4864\n",
      "Iteration 3600, Loss: 0.4855\n",
      "Iteration 3700, Loss: 0.4847\n",
      "Iteration 3800, Loss: 0.4839\n",
      "Iteration 3900, Loss: 0.4832\n",
      "Iteration 4000, Loss: 0.4824\n",
      "Iteration 4100, Loss: 0.4817\n",
      "Iteration 4200, Loss: 0.4810\n",
      "Iteration 4300, Loss: 0.4803\n",
      "Iteration 4400, Loss: 0.4796\n",
      "Iteration 4500, Loss: 0.4790\n",
      "Iteration 4600, Loss: 0.4783\n",
      "Iteration 4700, Loss: 0.4777\n",
      "Iteration 4800, Loss: 0.4771\n",
      "Iteration 4900, Loss: 0.4765\n",
      "Iteration 0, Loss: 1.0230\n",
      "Iteration 100, Loss: 0.6196\n",
      "Iteration 200, Loss: 0.5907\n",
      "Iteration 300, Loss: 0.5748\n",
      "Iteration 400, Loss: 0.5637\n",
      "Iteration 500, Loss: 0.5553\n",
      "Iteration 600, Loss: 0.5485\n",
      "Iteration 700, Loss: 0.5428\n",
      "Iteration 800, Loss: 0.5379\n",
      "Iteration 900, Loss: 0.5336\n",
      "Iteration 1000, Loss: 0.5298\n",
      "Iteration 1100, Loss: 0.5264\n",
      "Iteration 1200, Loss: 0.5233\n",
      "Iteration 1300, Loss: 0.5205\n",
      "Iteration 1400, Loss: 0.5178\n",
      "Iteration 1500, Loss: 0.5154\n",
      "Iteration 1600, Loss: 0.5131\n",
      "Iteration 1700, Loss: 0.5110\n",
      "Iteration 1800, Loss: 0.5090\n",
      "Iteration 1900, Loss: 0.5071\n",
      "Iteration 2000, Loss: 0.5053\n",
      "Iteration 2100, Loss: 0.5036\n",
      "Iteration 2200, Loss: 0.5020\n",
      "Iteration 2300, Loss: 0.5004\n",
      "Iteration 2400, Loss: 0.4990\n",
      "Iteration 2500, Loss: 0.4976\n",
      "Iteration 2600, Loss: 0.4962\n",
      "Iteration 2700, Loss: 0.4949\n",
      "Iteration 2800, Loss: 0.4937\n",
      "Iteration 2900, Loss: 0.4925\n",
      "Iteration 3000, Loss: 0.4913\n",
      "Iteration 3100, Loss: 0.4902\n",
      "Iteration 3200, Loss: 0.4892\n",
      "Iteration 3300, Loss: 0.4881\n",
      "Iteration 3400, Loss: 0.4871\n",
      "Iteration 3500, Loss: 0.4862\n",
      "Iteration 3600, Loss: 0.4852\n",
      "Iteration 3700, Loss: 0.4843\n",
      "Iteration 3800, Loss: 0.4834\n",
      "Iteration 3900, Loss: 0.4826\n",
      "Iteration 4000, Loss: 0.4817\n",
      "Iteration 4100, Loss: 0.4809\n",
      "Iteration 4200, Loss: 0.4802\n",
      "Iteration 4300, Loss: 0.4794\n",
      "Iteration 4400, Loss: 0.4786\n",
      "Iteration 4500, Loss: 0.4779\n",
      "Iteration 4600, Loss: 0.4772\n",
      "Iteration 4700, Loss: 0.4765\n",
      "Iteration 4800, Loss: 0.4758\n",
      "Iteration 4900, Loss: 0.4752\n",
      "100 270\n",
      "Iteration 0, Loss: 1.0132\n",
      "Iteration 100, Loss: 0.6073\n",
      "Iteration 200, Loss: 0.5773\n",
      "Iteration 300, Loss: 0.5603\n",
      "Iteration 400, Loss: 0.5484\n",
      "Iteration 500, Loss: 0.5394\n",
      "Iteration 600, Loss: 0.5320\n",
      "Iteration 700, Loss: 0.5259\n",
      "Iteration 800, Loss: 0.5206\n",
      "Iteration 900, Loss: 0.5160\n",
      "Iteration 1000, Loss: 0.5118\n",
      "Iteration 1100, Loss: 0.5081\n",
      "Iteration 1200, Loss: 0.5048\n",
      "Iteration 1300, Loss: 0.5017\n",
      "Iteration 1400, Loss: 0.4989\n",
      "Iteration 1500, Loss: 0.4962\n",
      "Iteration 1600, Loss: 0.4937\n",
      "Iteration 1700, Loss: 0.4914\n",
      "Iteration 1800, Loss: 0.4892\n",
      "Iteration 1900, Loss: 0.4871\n",
      "Iteration 2000, Loss: 0.4852\n",
      "Iteration 2100, Loss: 0.4833\n",
      "Iteration 2200, Loss: 0.4815\n",
      "Iteration 2300, Loss: 0.4798\n",
      "Iteration 2400, Loss: 0.4782\n",
      "Iteration 2500, Loss: 0.4767\n",
      "Iteration 2600, Loss: 0.4752\n",
      "Iteration 2700, Loss: 0.4738\n",
      "Iteration 2800, Loss: 0.4724\n",
      "Iteration 2900, Loss: 0.4711\n",
      "Iteration 3000, Loss: 0.4698\n",
      "Iteration 3100, Loss: 0.4686\n",
      "Iteration 3200, Loss: 0.4674\n",
      "Iteration 3300, Loss: 0.4663\n",
      "Iteration 3400, Loss: 0.4652\n",
      "Iteration 3500, Loss: 0.4641\n",
      "Iteration 3600, Loss: 0.4631\n",
      "Iteration 3700, Loss: 0.4620\n",
      "Iteration 3800, Loss: 0.4611\n",
      "Iteration 3900, Loss: 0.4601\n",
      "Iteration 4000, Loss: 0.4592\n",
      "Iteration 4100, Loss: 0.4583\n",
      "Iteration 4200, Loss: 0.4574\n",
      "Iteration 4300, Loss: 0.4566\n",
      "Iteration 4400, Loss: 0.4557\n",
      "Iteration 4500, Loss: 0.4549\n",
      "Iteration 4600, Loss: 0.4542\n",
      "Iteration 4700, Loss: 0.4534\n",
      "Iteration 4800, Loss: 0.4526\n",
      "Iteration 4900, Loss: 0.4519\n",
      "Iteration 0, Loss: 1.0241\n",
      "Iteration 100, Loss: 0.6366\n",
      "Iteration 200, Loss: 0.6050\n",
      "Iteration 300, Loss: 0.5874\n",
      "Iteration 400, Loss: 0.5751\n",
      "Iteration 500, Loss: 0.5658\n",
      "Iteration 600, Loss: 0.5583\n",
      "Iteration 700, Loss: 0.5521\n",
      "Iteration 800, Loss: 0.5467\n",
      "Iteration 900, Loss: 0.5421\n",
      "Iteration 1000, Loss: 0.5380\n",
      "Iteration 1100, Loss: 0.5344\n",
      "Iteration 1200, Loss: 0.5311\n",
      "Iteration 1300, Loss: 0.5280\n",
      "Iteration 1400, Loss: 0.5253\n",
      "Iteration 1500, Loss: 0.5227\n",
      "Iteration 1600, Loss: 0.5203\n",
      "Iteration 1700, Loss: 0.5181\n",
      "Iteration 1800, Loss: 0.5160\n",
      "Iteration 1900, Loss: 0.5140\n",
      "Iteration 2000, Loss: 0.5121\n",
      "Iteration 2100, Loss: 0.5104\n",
      "Iteration 2200, Loss: 0.5087\n",
      "Iteration 2300, Loss: 0.5071\n",
      "Iteration 2400, Loss: 0.5056\n",
      "Iteration 2500, Loss: 0.5042\n",
      "Iteration 2600, Loss: 0.5027\n",
      "Iteration 2700, Loss: 0.5014\n",
      "Iteration 2800, Loss: 0.5001\n",
      "Iteration 2900, Loss: 0.4989\n",
      "Iteration 3000, Loss: 0.4977\n",
      "Iteration 3100, Loss: 0.4965\n",
      "Iteration 3200, Loss: 0.4954\n",
      "Iteration 3300, Loss: 0.4944\n",
      "Iteration 3400, Loss: 0.4933\n",
      "Iteration 3500, Loss: 0.4923\n",
      "Iteration 3600, Loss: 0.4913\n",
      "Iteration 3700, Loss: 0.4904\n",
      "Iteration 3800, Loss: 0.4894\n",
      "Iteration 3900, Loss: 0.4886\n",
      "Iteration 4000, Loss: 0.4877\n",
      "Iteration 4100, Loss: 0.4868\n",
      "Iteration 4200, Loss: 0.4860\n",
      "Iteration 4300, Loss: 0.4852\n",
      "Iteration 4400, Loss: 0.4844\n",
      "Iteration 4500, Loss: 0.4836\n",
      "Iteration 4600, Loss: 0.4829\n",
      "Iteration 4700, Loss: 0.4822\n",
      "Iteration 4800, Loss: 0.4814\n",
      "Iteration 4900, Loss: 0.4808\n",
      "101 270\n",
      "Iteration 0, Loss: 1.0248\n",
      "Iteration 100, Loss: 0.6394\n",
      "Iteration 200, Loss: 0.6061\n",
      "Iteration 300, Loss: 0.5870\n",
      "Iteration 400, Loss: 0.5738\n",
      "Iteration 500, Loss: 0.5639\n",
      "Iteration 600, Loss: 0.5559\n",
      "Iteration 700, Loss: 0.5492\n",
      "Iteration 800, Loss: 0.5436\n",
      "Iteration 900, Loss: 0.5387\n",
      "Iteration 1000, Loss: 0.5343\n",
      "Iteration 1100, Loss: 0.5304\n",
      "Iteration 1200, Loss: 0.5268\n",
      "Iteration 1300, Loss: 0.5236\n",
      "Iteration 1400, Loss: 0.5206\n",
      "Iteration 1500, Loss: 0.5178\n",
      "Iteration 1600, Loss: 0.5152\n",
      "Iteration 1700, Loss: 0.5128\n",
      "Iteration 1800, Loss: 0.5105\n",
      "Iteration 1900, Loss: 0.5084\n",
      "Iteration 2000, Loss: 0.5063\n",
      "Iteration 2100, Loss: 0.5044\n",
      "Iteration 2200, Loss: 0.5026\n",
      "Iteration 2300, Loss: 0.5008\n",
      "Iteration 2400, Loss: 0.4992\n",
      "Iteration 2500, Loss: 0.4976\n",
      "Iteration 2600, Loss: 0.4961\n",
      "Iteration 2700, Loss: 0.4946\n",
      "Iteration 2800, Loss: 0.4932\n",
      "Iteration 2900, Loss: 0.4919\n",
      "Iteration 3000, Loss: 0.4906\n",
      "Iteration 3100, Loss: 0.4893\n",
      "Iteration 3200, Loss: 0.4881\n",
      "Iteration 3300, Loss: 0.4869\n",
      "Iteration 3400, Loss: 0.4858\n",
      "Iteration 3500, Loss: 0.4848\n",
      "Iteration 3600, Loss: 0.4837\n",
      "Iteration 3700, Loss: 0.4827\n",
      "Iteration 3800, Loss: 0.4817\n",
      "Iteration 3900, Loss: 0.4807\n",
      "Iteration 4000, Loss: 0.4798\n",
      "Iteration 4100, Loss: 0.4789\n",
      "Iteration 4200, Loss: 0.4780\n",
      "Iteration 4300, Loss: 0.4771\n",
      "Iteration 4400, Loss: 0.4763\n",
      "Iteration 4500, Loss: 0.4755\n",
      "Iteration 4600, Loss: 0.4747\n",
      "Iteration 4700, Loss: 0.4739\n",
      "Iteration 4800, Loss: 0.4731\n",
      "Iteration 4900, Loss: 0.4724\n",
      "Iteration 0, Loss: 1.0138\n",
      "Iteration 100, Loss: 0.6062\n",
      "Iteration 200, Loss: 0.5771\n",
      "Iteration 300, Loss: 0.5611\n",
      "Iteration 400, Loss: 0.5500\n",
      "Iteration 500, Loss: 0.5416\n",
      "Iteration 600, Loss: 0.5348\n",
      "Iteration 700, Loss: 0.5292\n",
      "Iteration 800, Loss: 0.5243\n",
      "Iteration 900, Loss: 0.5201\n",
      "Iteration 1000, Loss: 0.5164\n",
      "Iteration 1100, Loss: 0.5130\n",
      "Iteration 1200, Loss: 0.5100\n",
      "Iteration 1300, Loss: 0.5072\n",
      "Iteration 1400, Loss: 0.5046\n",
      "Iteration 1500, Loss: 0.5023\n",
      "Iteration 1600, Loss: 0.5001\n",
      "Iteration 1700, Loss: 0.4981\n",
      "Iteration 1800, Loss: 0.4961\n",
      "Iteration 1900, Loss: 0.4943\n",
      "Iteration 2000, Loss: 0.4926\n",
      "Iteration 2100, Loss: 0.4909\n",
      "Iteration 2200, Loss: 0.4894\n",
      "Iteration 2300, Loss: 0.4879\n",
      "Iteration 2400, Loss: 0.4865\n",
      "Iteration 2500, Loss: 0.4852\n",
      "Iteration 2600, Loss: 0.4839\n",
      "Iteration 2700, Loss: 0.4826\n",
      "Iteration 2800, Loss: 0.4814\n",
      "Iteration 2900, Loss: 0.4803\n",
      "Iteration 3000, Loss: 0.4792\n",
      "Iteration 3100, Loss: 0.4781\n",
      "Iteration 3200, Loss: 0.4771\n",
      "Iteration 3300, Loss: 0.4761\n",
      "Iteration 3400, Loss: 0.4751\n",
      "Iteration 3500, Loss: 0.4742\n",
      "Iteration 3600, Loss: 0.4733\n",
      "Iteration 3700, Loss: 0.4724\n",
      "Iteration 3800, Loss: 0.4716\n",
      "Iteration 3900, Loss: 0.4707\n",
      "Iteration 4000, Loss: 0.4699\n",
      "Iteration 4100, Loss: 0.4691\n",
      "Iteration 4200, Loss: 0.4684\n",
      "Iteration 4300, Loss: 0.4676\n",
      "Iteration 4400, Loss: 0.4669\n",
      "Iteration 4500, Loss: 0.4662\n",
      "Iteration 4600, Loss: 0.4655\n",
      "Iteration 4700, Loss: 0.4648\n",
      "Iteration 4800, Loss: 0.4642\n",
      "Iteration 4900, Loss: 0.4635\n",
      "102 270\n",
      "Iteration 0, Loss: 1.0205\n",
      "Iteration 100, Loss: 0.6152\n",
      "Iteration 200, Loss: 0.5823\n",
      "Iteration 300, Loss: 0.5641\n",
      "Iteration 400, Loss: 0.5515\n",
      "Iteration 500, Loss: 0.5417\n",
      "Iteration 600, Loss: 0.5339\n",
      "Iteration 700, Loss: 0.5272\n",
      "Iteration 800, Loss: 0.5215\n",
      "Iteration 900, Loss: 0.5166\n",
      "Iteration 1000, Loss: 0.5122\n",
      "Iteration 1100, Loss: 0.5083\n",
      "Iteration 1200, Loss: 0.5047\n",
      "Iteration 1300, Loss: 0.5015\n",
      "Iteration 1400, Loss: 0.4985\n",
      "Iteration 1500, Loss: 0.4958\n",
      "Iteration 1600, Loss: 0.4932\n",
      "Iteration 1700, Loss: 0.4909\n",
      "Iteration 1800, Loss: 0.4886\n",
      "Iteration 1900, Loss: 0.4866\n",
      "Iteration 2000, Loss: 0.4846\n",
      "Iteration 2100, Loss: 0.4828\n",
      "Iteration 2200, Loss: 0.4810\n",
      "Iteration 2300, Loss: 0.4794\n",
      "Iteration 2400, Loss: 0.4778\n",
      "Iteration 2500, Loss: 0.4763\n",
      "Iteration 2600, Loss: 0.4749\n",
      "Iteration 2700, Loss: 0.4736\n",
      "Iteration 2800, Loss: 0.4722\n",
      "Iteration 2900, Loss: 0.4710\n",
      "Iteration 3000, Loss: 0.4698\n",
      "Iteration 3100, Loss: 0.4686\n",
      "Iteration 3200, Loss: 0.4675\n",
      "Iteration 3300, Loss: 0.4664\n",
      "Iteration 3400, Loss: 0.4654\n",
      "Iteration 3500, Loss: 0.4644\n",
      "Iteration 3600, Loss: 0.4635\n",
      "Iteration 3700, Loss: 0.4625\n",
      "Iteration 3800, Loss: 0.4616\n",
      "Iteration 3900, Loss: 0.4608\n",
      "Iteration 4000, Loss: 0.4599\n",
      "Iteration 4100, Loss: 0.4591\n",
      "Iteration 4200, Loss: 0.4583\n",
      "Iteration 4300, Loss: 0.4575\n",
      "Iteration 4400, Loss: 0.4568\n",
      "Iteration 4500, Loss: 0.4561\n",
      "Iteration 4600, Loss: 0.4553\n",
      "Iteration 4700, Loss: 0.4546\n",
      "Iteration 4800, Loss: 0.4540\n",
      "Iteration 4900, Loss: 0.4533\n",
      "Iteration 0, Loss: 1.0201\n",
      "Iteration 100, Loss: 0.6263\n",
      "Iteration 200, Loss: 0.5968\n",
      "Iteration 300, Loss: 0.5804\n",
      "Iteration 400, Loss: 0.5693\n",
      "Iteration 500, Loss: 0.5610\n",
      "Iteration 600, Loss: 0.5543\n",
      "Iteration 700, Loss: 0.5488\n",
      "Iteration 800, Loss: 0.5440\n",
      "Iteration 900, Loss: 0.5399\n",
      "Iteration 1000, Loss: 0.5362\n",
      "Iteration 1100, Loss: 0.5329\n",
      "Iteration 1200, Loss: 0.5299\n",
      "Iteration 1300, Loss: 0.5271\n",
      "Iteration 1400, Loss: 0.5244\n",
      "Iteration 1500, Loss: 0.5220\n",
      "Iteration 1600, Loss: 0.5198\n",
      "Iteration 1700, Loss: 0.5176\n",
      "Iteration 1800, Loss: 0.5155\n",
      "Iteration 1900, Loss: 0.5136\n",
      "Iteration 2000, Loss: 0.5118\n",
      "Iteration 2100, Loss: 0.5100\n",
      "Iteration 2200, Loss: 0.5084\n",
      "Iteration 2300, Loss: 0.5067\n",
      "Iteration 2400, Loss: 0.5052\n",
      "Iteration 2500, Loss: 0.5037\n",
      "Iteration 2600, Loss: 0.5022\n",
      "Iteration 2700, Loss: 0.5008\n",
      "Iteration 2800, Loss: 0.4995\n",
      "Iteration 2900, Loss: 0.4982\n",
      "Iteration 3000, Loss: 0.4969\n",
      "Iteration 3100, Loss: 0.4957\n",
      "Iteration 3200, Loss: 0.4945\n",
      "Iteration 3300, Loss: 0.4933\n",
      "Iteration 3400, Loss: 0.4922\n",
      "Iteration 3500, Loss: 0.4911\n",
      "Iteration 3600, Loss: 0.4901\n",
      "Iteration 3700, Loss: 0.4890\n",
      "Iteration 3800, Loss: 0.4880\n",
      "Iteration 3900, Loss: 0.4870\n",
      "Iteration 4000, Loss: 0.4861\n",
      "Iteration 4100, Loss: 0.4852\n",
      "Iteration 4200, Loss: 0.4843\n",
      "Iteration 4300, Loss: 0.4834\n",
      "Iteration 4400, Loss: 0.4825\n",
      "Iteration 4500, Loss: 0.4816\n",
      "Iteration 4600, Loss: 0.4808\n",
      "Iteration 4700, Loss: 0.4800\n",
      "Iteration 4800, Loss: 0.4792\n",
      "Iteration 4900, Loss: 0.4784\n",
      "103 270\n",
      "Iteration 0, Loss: 1.0224\n",
      "Iteration 100, Loss: 0.6290\n",
      "Iteration 200, Loss: 0.5958\n",
      "Iteration 300, Loss: 0.5767\n",
      "Iteration 400, Loss: 0.5633\n",
      "Iteration 500, Loss: 0.5531\n",
      "Iteration 600, Loss: 0.5450\n",
      "Iteration 700, Loss: 0.5383\n",
      "Iteration 800, Loss: 0.5326\n",
      "Iteration 900, Loss: 0.5277\n",
      "Iteration 1000, Loss: 0.5233\n",
      "Iteration 1100, Loss: 0.5195\n",
      "Iteration 1200, Loss: 0.5161\n",
      "Iteration 1300, Loss: 0.5130\n",
      "Iteration 1400, Loss: 0.5101\n",
      "Iteration 1500, Loss: 0.5075\n",
      "Iteration 1600, Loss: 0.5051\n",
      "Iteration 1700, Loss: 0.5029\n",
      "Iteration 1800, Loss: 0.5008\n",
      "Iteration 1900, Loss: 0.4988\n",
      "Iteration 2000, Loss: 0.4970\n",
      "Iteration 2100, Loss: 0.4952\n",
      "Iteration 2200, Loss: 0.4936\n",
      "Iteration 2300, Loss: 0.4920\n",
      "Iteration 2400, Loss: 0.4905\n",
      "Iteration 2500, Loss: 0.4891\n",
      "Iteration 2600, Loss: 0.4878\n",
      "Iteration 2700, Loss: 0.4865\n",
      "Iteration 2800, Loss: 0.4853\n",
      "Iteration 2900, Loss: 0.4841\n",
      "Iteration 3000, Loss: 0.4830\n",
      "Iteration 3100, Loss: 0.4819\n",
      "Iteration 3200, Loss: 0.4809\n",
      "Iteration 3300, Loss: 0.4798\n",
      "Iteration 3400, Loss: 0.4788\n",
      "Iteration 3500, Loss: 0.4779\n",
      "Iteration 3600, Loss: 0.4770\n",
      "Iteration 3700, Loss: 0.4761\n",
      "Iteration 3800, Loss: 0.4753\n",
      "Iteration 3900, Loss: 0.4744\n",
      "Iteration 4000, Loss: 0.4736\n",
      "Iteration 4100, Loss: 0.4729\n",
      "Iteration 4200, Loss: 0.4721\n",
      "Iteration 4300, Loss: 0.4714\n",
      "Iteration 4400, Loss: 0.4706\n",
      "Iteration 4500, Loss: 0.4699\n",
      "Iteration 4600, Loss: 0.4693\n",
      "Iteration 4700, Loss: 0.4686\n",
      "Iteration 4800, Loss: 0.4679\n",
      "Iteration 4900, Loss: 0.4673\n",
      "Iteration 0, Loss: 1.0156\n",
      "Iteration 100, Loss: 0.6110\n",
      "Iteration 200, Loss: 0.5794\n",
      "Iteration 300, Loss: 0.5618\n",
      "Iteration 400, Loss: 0.5497\n",
      "Iteration 500, Loss: 0.5404\n",
      "Iteration 600, Loss: 0.5329\n",
      "Iteration 700, Loss: 0.5266\n",
      "Iteration 800, Loss: 0.5212\n",
      "Iteration 900, Loss: 0.5164\n",
      "Iteration 1000, Loss: 0.5121\n",
      "Iteration 1100, Loss: 0.5083\n",
      "Iteration 1200, Loss: 0.5048\n",
      "Iteration 1300, Loss: 0.5016\n",
      "Iteration 1400, Loss: 0.4987\n",
      "Iteration 1500, Loss: 0.4959\n",
      "Iteration 1600, Loss: 0.4934\n",
      "Iteration 1700, Loss: 0.4910\n",
      "Iteration 1800, Loss: 0.4887\n",
      "Iteration 1900, Loss: 0.4866\n",
      "Iteration 2000, Loss: 0.4846\n",
      "Iteration 2100, Loss: 0.4827\n",
      "Iteration 2200, Loss: 0.4809\n",
      "Iteration 2300, Loss: 0.4791\n",
      "Iteration 2400, Loss: 0.4775\n",
      "Iteration 2500, Loss: 0.4759\n",
      "Iteration 2600, Loss: 0.4744\n",
      "Iteration 2700, Loss: 0.4730\n",
      "Iteration 2800, Loss: 0.4716\n",
      "Iteration 2900, Loss: 0.4702\n",
      "Iteration 3000, Loss: 0.4690\n",
      "Iteration 3100, Loss: 0.4677\n",
      "Iteration 3200, Loss: 0.4665\n",
      "Iteration 3300, Loss: 0.4654\n",
      "Iteration 3400, Loss: 0.4643\n",
      "Iteration 3500, Loss: 0.4632\n",
      "Iteration 3600, Loss: 0.4622\n",
      "Iteration 3700, Loss: 0.4611\n",
      "Iteration 3800, Loss: 0.4602\n",
      "Iteration 3900, Loss: 0.4592\n",
      "Iteration 4000, Loss: 0.4583\n",
      "Iteration 4100, Loss: 0.4574\n",
      "Iteration 4200, Loss: 0.4565\n",
      "Iteration 4300, Loss: 0.4557\n",
      "Iteration 4400, Loss: 0.4548\n",
      "Iteration 4500, Loss: 0.4540\n",
      "Iteration 4600, Loss: 0.4532\n",
      "Iteration 4700, Loss: 0.4525\n",
      "Iteration 4800, Loss: 0.4517\n",
      "Iteration 4900, Loss: 0.4510\n",
      "104 270\n",
      "Iteration 0, Loss: 1.0179\n",
      "Iteration 100, Loss: 0.6163\n",
      "Iteration 200, Loss: 0.5855\n",
      "Iteration 300, Loss: 0.5683\n",
      "Iteration 400, Loss: 0.5565\n",
      "Iteration 500, Loss: 0.5476\n",
      "Iteration 600, Loss: 0.5405\n",
      "Iteration 700, Loss: 0.5345\n",
      "Iteration 800, Loss: 0.5294\n",
      "Iteration 900, Loss: 0.5251\n",
      "Iteration 1000, Loss: 0.5211\n",
      "Iteration 1100, Loss: 0.5176\n",
      "Iteration 1200, Loss: 0.5144\n",
      "Iteration 1300, Loss: 0.5115\n",
      "Iteration 1400, Loss: 0.5088\n",
      "Iteration 1500, Loss: 0.5063\n",
      "Iteration 1600, Loss: 0.5040\n",
      "Iteration 1700, Loss: 0.5018\n",
      "Iteration 1800, Loss: 0.4998\n",
      "Iteration 1900, Loss: 0.4978\n",
      "Iteration 2000, Loss: 0.4959\n",
      "Iteration 2100, Loss: 0.4941\n",
      "Iteration 2200, Loss: 0.4924\n",
      "Iteration 2300, Loss: 0.4908\n",
      "Iteration 2400, Loss: 0.4893\n",
      "Iteration 2500, Loss: 0.4878\n",
      "Iteration 2600, Loss: 0.4864\n",
      "Iteration 2700, Loss: 0.4850\n",
      "Iteration 2800, Loss: 0.4837\n",
      "Iteration 2900, Loss: 0.4824\n",
      "Iteration 3000, Loss: 0.4812\n",
      "Iteration 3100, Loss: 0.4800\n",
      "Iteration 3200, Loss: 0.4789\n",
      "Iteration 3300, Loss: 0.4778\n",
      "Iteration 3400, Loss: 0.4767\n",
      "Iteration 3500, Loss: 0.4756\n",
      "Iteration 3600, Loss: 0.4746\n",
      "Iteration 3700, Loss: 0.4736\n",
      "Iteration 3800, Loss: 0.4727\n",
      "Iteration 3900, Loss: 0.4717\n",
      "Iteration 4000, Loss: 0.4708\n",
      "Iteration 4100, Loss: 0.4700\n",
      "Iteration 4200, Loss: 0.4691\n",
      "Iteration 4300, Loss: 0.4682\n",
      "Iteration 4400, Loss: 0.4674\n",
      "Iteration 4500, Loss: 0.4666\n",
      "Iteration 4600, Loss: 0.4658\n",
      "Iteration 4700, Loss: 0.4651\n",
      "Iteration 4800, Loss: 0.4644\n",
      "Iteration 4900, Loss: 0.4636\n",
      "Iteration 0, Loss: 1.0215\n",
      "Iteration 100, Loss: 0.6261\n",
      "Iteration 200, Loss: 0.5946\n",
      "Iteration 300, Loss: 0.5773\n",
      "Iteration 400, Loss: 0.5652\n",
      "Iteration 500, Loss: 0.5561\n",
      "Iteration 600, Loss: 0.5486\n",
      "Iteration 700, Loss: 0.5424\n",
      "Iteration 800, Loss: 0.5370\n",
      "Iteration 900, Loss: 0.5324\n",
      "Iteration 1000, Loss: 0.5282\n",
      "Iteration 1100, Loss: 0.5244\n",
      "Iteration 1200, Loss: 0.5210\n",
      "Iteration 1300, Loss: 0.5179\n",
      "Iteration 1400, Loss: 0.5150\n",
      "Iteration 1500, Loss: 0.5123\n",
      "Iteration 1600, Loss: 0.5099\n",
      "Iteration 1700, Loss: 0.5075\n",
      "Iteration 1800, Loss: 0.5054\n",
      "Iteration 1900, Loss: 0.5033\n",
      "Iteration 2000, Loss: 0.5014\n",
      "Iteration 2100, Loss: 0.4995\n",
      "Iteration 2200, Loss: 0.4978\n",
      "Iteration 2300, Loss: 0.4961\n",
      "Iteration 2400, Loss: 0.4945\n",
      "Iteration 2500, Loss: 0.4930\n",
      "Iteration 2600, Loss: 0.4915\n",
      "Iteration 2700, Loss: 0.4901\n",
      "Iteration 2800, Loss: 0.4888\n",
      "Iteration 2900, Loss: 0.4875\n",
      "Iteration 3000, Loss: 0.4863\n",
      "Iteration 3100, Loss: 0.4851\n",
      "Iteration 3200, Loss: 0.4839\n",
      "Iteration 3300, Loss: 0.4828\n",
      "Iteration 3400, Loss: 0.4817\n",
      "Iteration 3500, Loss: 0.4807\n",
      "Iteration 3600, Loss: 0.4797\n",
      "Iteration 3700, Loss: 0.4787\n",
      "Iteration 3800, Loss: 0.4777\n",
      "Iteration 3900, Loss: 0.4768\n",
      "Iteration 4000, Loss: 0.4759\n",
      "Iteration 4100, Loss: 0.4750\n",
      "Iteration 4200, Loss: 0.4742\n",
      "Iteration 4300, Loss: 0.4734\n",
      "Iteration 4400, Loss: 0.4726\n",
      "Iteration 4500, Loss: 0.4718\n",
      "Iteration 4600, Loss: 0.4710\n",
      "Iteration 4700, Loss: 0.4703\n",
      "Iteration 4800, Loss: 0.4696\n",
      "Iteration 4900, Loss: 0.4689\n",
      "105 270\n",
      "Iteration 0, Loss: 1.0546\n",
      "Iteration 100, Loss: 0.6562\n",
      "Iteration 200, Loss: 0.6213\n",
      "Iteration 300, Loss: 0.6036\n",
      "Iteration 400, Loss: 0.5914\n",
      "Iteration 500, Loss: 0.5824\n",
      "Iteration 600, Loss: 0.5752\n",
      "Iteration 700, Loss: 0.5692\n",
      "Iteration 800, Loss: 0.5639\n",
      "Iteration 900, Loss: 0.5596\n",
      "Iteration 1000, Loss: 0.5556\n",
      "Iteration 1100, Loss: 0.5521\n",
      "Iteration 1200, Loss: 0.5488\n",
      "Iteration 1300, Loss: 0.5460\n",
      "Iteration 1400, Loss: 0.5433\n",
      "Iteration 1500, Loss: 0.5409\n",
      "Iteration 1600, Loss: 0.5386\n",
      "Iteration 1700, Loss: 0.5365\n",
      "Iteration 1800, Loss: 0.5346\n",
      "Iteration 1900, Loss: 0.5328\n",
      "Iteration 2000, Loss: 0.5310\n",
      "Iteration 2100, Loss: 0.5293\n",
      "Iteration 2200, Loss: 0.5277\n",
      "Iteration 2300, Loss: 0.5262\n",
      "Iteration 2400, Loss: 0.5248\n",
      "Iteration 2500, Loss: 0.5235\n",
      "Iteration 2600, Loss: 0.5222\n",
      "Iteration 2700, Loss: 0.5210\n",
      "Iteration 2800, Loss: 0.5197\n",
      "Iteration 2900, Loss: 0.5186\n",
      "Iteration 3000, Loss: 0.5175\n",
      "Iteration 3100, Loss: 0.5165\n",
      "Iteration 3200, Loss: 0.5155\n",
      "Iteration 3300, Loss: 0.5145\n",
      "Iteration 3400, Loss: 0.5135\n",
      "Iteration 3500, Loss: 0.5126\n",
      "Iteration 3600, Loss: 0.5117\n",
      "Iteration 3700, Loss: 0.5109\n",
      "Iteration 3800, Loss: 0.5100\n",
      "Iteration 3900, Loss: 0.5093\n",
      "Iteration 4000, Loss: 0.5085\n",
      "Iteration 4100, Loss: 0.5077\n",
      "Iteration 4200, Loss: 0.5070\n",
      "Iteration 4300, Loss: 0.5063\n",
      "Iteration 4400, Loss: 0.5056\n",
      "Iteration 4500, Loss: 0.5049\n",
      "Iteration 4600, Loss: 0.5042\n",
      "Iteration 4700, Loss: 0.5036\n",
      "Iteration 4800, Loss: 0.5029\n",
      "Iteration 4900, Loss: 0.5023\n",
      "Iteration 0, Loss: 1.0563\n",
      "Iteration 100, Loss: 0.6560\n",
      "Iteration 200, Loss: 0.6213\n",
      "Iteration 300, Loss: 0.6033\n",
      "Iteration 400, Loss: 0.5910\n",
      "Iteration 500, Loss: 0.5815\n",
      "Iteration 600, Loss: 0.5740\n",
      "Iteration 700, Loss: 0.5677\n",
      "Iteration 800, Loss: 0.5623\n",
      "Iteration 900, Loss: 0.5577\n",
      "Iteration 1000, Loss: 0.5536\n",
      "Iteration 1100, Loss: 0.5500\n",
      "Iteration 1200, Loss: 0.5467\n",
      "Iteration 1300, Loss: 0.5437\n",
      "Iteration 1400, Loss: 0.5410\n",
      "Iteration 1500, Loss: 0.5385\n",
      "Iteration 1600, Loss: 0.5362\n",
      "Iteration 1700, Loss: 0.5341\n",
      "Iteration 1800, Loss: 0.5320\n",
      "Iteration 1900, Loss: 0.5302\n",
      "Iteration 2000, Loss: 0.5284\n",
      "Iteration 2100, Loss: 0.5267\n",
      "Iteration 2200, Loss: 0.5252\n",
      "Iteration 2300, Loss: 0.5237\n",
      "Iteration 2400, Loss: 0.5223\n",
      "Iteration 2500, Loss: 0.5208\n",
      "Iteration 2600, Loss: 0.5196\n",
      "Iteration 2700, Loss: 0.5183\n",
      "Iteration 2800, Loss: 0.5171\n",
      "Iteration 2900, Loss: 0.5159\n",
      "Iteration 3000, Loss: 0.5149\n",
      "Iteration 3100, Loss: 0.5138\n",
      "Iteration 3200, Loss: 0.5127\n",
      "Iteration 3300, Loss: 0.5118\n",
      "Iteration 3400, Loss: 0.5108\n",
      "Iteration 3500, Loss: 0.5098\n",
      "Iteration 3600, Loss: 0.5090\n",
      "Iteration 3700, Loss: 0.5081\n",
      "Iteration 3800, Loss: 0.5072\n",
      "Iteration 3900, Loss: 0.5064\n",
      "Iteration 4000, Loss: 0.5056\n",
      "Iteration 4100, Loss: 0.5048\n",
      "Iteration 4200, Loss: 0.5041\n",
      "Iteration 4300, Loss: 0.5033\n",
      "Iteration 4400, Loss: 0.5026\n",
      "Iteration 4500, Loss: 0.5018\n",
      "Iteration 4600, Loss: 0.5012\n",
      "Iteration 4700, Loss: 0.5005\n",
      "Iteration 4800, Loss: 0.4999\n",
      "Iteration 4900, Loss: 0.4992\n",
      "106 270\n",
      "Iteration 0, Loss: 1.0540\n",
      "Iteration 100, Loss: 0.6461\n",
      "Iteration 200, Loss: 0.6118\n",
      "Iteration 300, Loss: 0.5943\n",
      "Iteration 400, Loss: 0.5827\n",
      "Iteration 500, Loss: 0.5742\n",
      "Iteration 600, Loss: 0.5672\n",
      "Iteration 700, Loss: 0.5616\n",
      "Iteration 800, Loss: 0.5568\n",
      "Iteration 900, Loss: 0.5526\n",
      "Iteration 1000, Loss: 0.5490\n",
      "Iteration 1100, Loss: 0.5458\n",
      "Iteration 1200, Loss: 0.5428\n",
      "Iteration 1300, Loss: 0.5401\n",
      "Iteration 1400, Loss: 0.5376\n",
      "Iteration 1500, Loss: 0.5352\n",
      "Iteration 1600, Loss: 0.5332\n",
      "Iteration 1700, Loss: 0.5312\n",
      "Iteration 1800, Loss: 0.5293\n",
      "Iteration 1900, Loss: 0.5275\n",
      "Iteration 2000, Loss: 0.5258\n",
      "Iteration 2100, Loss: 0.5242\n",
      "Iteration 2200, Loss: 0.5228\n",
      "Iteration 2300, Loss: 0.5214\n",
      "Iteration 2400, Loss: 0.5200\n",
      "Iteration 2500, Loss: 0.5188\n",
      "Iteration 2600, Loss: 0.5176\n",
      "Iteration 2700, Loss: 0.5164\n",
      "Iteration 2800, Loss: 0.5153\n",
      "Iteration 2900, Loss: 0.5143\n",
      "Iteration 3000, Loss: 0.5131\n",
      "Iteration 3100, Loss: 0.5121\n",
      "Iteration 3200, Loss: 0.5111\n",
      "Iteration 3300, Loss: 0.5102\n",
      "Iteration 3400, Loss: 0.5093\n",
      "Iteration 3500, Loss: 0.5084\n",
      "Iteration 3600, Loss: 0.5076\n",
      "Iteration 3700, Loss: 0.5068\n",
      "Iteration 3800, Loss: 0.5060\n",
      "Iteration 3900, Loss: 0.5052\n",
      "Iteration 4000, Loss: 0.5044\n",
      "Iteration 4100, Loss: 0.5037\n",
      "Iteration 4200, Loss: 0.5030\n",
      "Iteration 4300, Loss: 0.5023\n",
      "Iteration 4400, Loss: 0.5016\n",
      "Iteration 4500, Loss: 0.5010\n",
      "Iteration 4600, Loss: 0.5003\n",
      "Iteration 4700, Loss: 0.4997\n",
      "Iteration 4800, Loss: 0.4990\n",
      "Iteration 4900, Loss: 0.4984\n",
      "Iteration 0, Loss: 1.0565\n",
      "Iteration 100, Loss: 0.6666\n",
      "Iteration 200, Loss: 0.6303\n",
      "Iteration 300, Loss: 0.6108\n",
      "Iteration 400, Loss: 0.5976\n",
      "Iteration 500, Loss: 0.5877\n",
      "Iteration 600, Loss: 0.5799\n",
      "Iteration 700, Loss: 0.5732\n",
      "Iteration 800, Loss: 0.5677\n",
      "Iteration 900, Loss: 0.5628\n",
      "Iteration 1000, Loss: 0.5586\n",
      "Iteration 1100, Loss: 0.5549\n",
      "Iteration 1200, Loss: 0.5515\n",
      "Iteration 1300, Loss: 0.5484\n",
      "Iteration 1400, Loss: 0.5457\n",
      "Iteration 1500, Loss: 0.5431\n",
      "Iteration 1600, Loss: 0.5407\n",
      "Iteration 1700, Loss: 0.5386\n",
      "Iteration 1800, Loss: 0.5366\n",
      "Iteration 1900, Loss: 0.5347\n",
      "Iteration 2000, Loss: 0.5329\n",
      "Iteration 2100, Loss: 0.5312\n",
      "Iteration 2200, Loss: 0.5296\n",
      "Iteration 2300, Loss: 0.5282\n",
      "Iteration 2400, Loss: 0.5267\n",
      "Iteration 2500, Loss: 0.5253\n",
      "Iteration 2600, Loss: 0.5240\n",
      "Iteration 2700, Loss: 0.5228\n",
      "Iteration 2800, Loss: 0.5216\n",
      "Iteration 2900, Loss: 0.5204\n",
      "Iteration 3000, Loss: 0.5193\n",
      "Iteration 3100, Loss: 0.5182\n",
      "Iteration 3200, Loss: 0.5172\n",
      "Iteration 3300, Loss: 0.5162\n",
      "Iteration 3400, Loss: 0.5153\n",
      "Iteration 3500, Loss: 0.5144\n",
      "Iteration 3600, Loss: 0.5135\n",
      "Iteration 3700, Loss: 0.5126\n",
      "Iteration 3800, Loss: 0.5118\n",
      "Iteration 3900, Loss: 0.5110\n",
      "Iteration 4000, Loss: 0.5102\n",
      "Iteration 4100, Loss: 0.5094\n",
      "Iteration 4200, Loss: 0.5087\n",
      "Iteration 4300, Loss: 0.5080\n",
      "Iteration 4400, Loss: 0.5072\n",
      "Iteration 4500, Loss: 0.5066\n",
      "Iteration 4600, Loss: 0.5059\n",
      "Iteration 4700, Loss: 0.5053\n",
      "Iteration 4800, Loss: 0.5046\n",
      "Iteration 4900, Loss: 0.5040\n",
      "107 270\n",
      "Iteration 0, Loss: 1.0558\n",
      "Iteration 100, Loss: 0.6608\n",
      "Iteration 200, Loss: 0.6261\n",
      "Iteration 300, Loss: 0.6081\n",
      "Iteration 400, Loss: 0.5959\n",
      "Iteration 500, Loss: 0.5866\n",
      "Iteration 600, Loss: 0.5791\n",
      "Iteration 700, Loss: 0.5729\n",
      "Iteration 800, Loss: 0.5676\n",
      "Iteration 900, Loss: 0.5629\n",
      "Iteration 1000, Loss: 0.5587\n",
      "Iteration 1100, Loss: 0.5551\n",
      "Iteration 1200, Loss: 0.5518\n",
      "Iteration 1300, Loss: 0.5488\n",
      "Iteration 1400, Loss: 0.5461\n",
      "Iteration 1500, Loss: 0.5436\n",
      "Iteration 1600, Loss: 0.5412\n",
      "Iteration 1700, Loss: 0.5390\n",
      "Iteration 1800, Loss: 0.5369\n",
      "Iteration 1900, Loss: 0.5350\n",
      "Iteration 2000, Loss: 0.5332\n",
      "Iteration 2100, Loss: 0.5315\n",
      "Iteration 2200, Loss: 0.5299\n",
      "Iteration 2300, Loss: 0.5284\n",
      "Iteration 2400, Loss: 0.5270\n",
      "Iteration 2500, Loss: 0.5256\n",
      "Iteration 2600, Loss: 0.5243\n",
      "Iteration 2700, Loss: 0.5231\n",
      "Iteration 2800, Loss: 0.5218\n",
      "Iteration 2900, Loss: 0.5207\n",
      "Iteration 3000, Loss: 0.5196\n",
      "Iteration 3100, Loss: 0.5186\n",
      "Iteration 3200, Loss: 0.5175\n",
      "Iteration 3300, Loss: 0.5165\n",
      "Iteration 3400, Loss: 0.5156\n",
      "Iteration 3500, Loss: 0.5147\n",
      "Iteration 3600, Loss: 0.5138\n",
      "Iteration 3700, Loss: 0.5130\n",
      "Iteration 3800, Loss: 0.5122\n",
      "Iteration 3900, Loss: 0.5114\n",
      "Iteration 4000, Loss: 0.5106\n",
      "Iteration 4100, Loss: 0.5099\n",
      "Iteration 4200, Loss: 0.5092\n",
      "Iteration 4300, Loss: 0.5085\n",
      "Iteration 4400, Loss: 0.5078\n",
      "Iteration 4500, Loss: 0.5071\n",
      "Iteration 4600, Loss: 0.5065\n",
      "Iteration 4700, Loss: 0.5059\n",
      "Iteration 4800, Loss: 0.5053\n",
      "Iteration 4900, Loss: 0.5047\n",
      "Iteration 0, Loss: 1.0536\n",
      "Iteration 100, Loss: 0.6548\n",
      "Iteration 200, Loss: 0.6198\n",
      "Iteration 300, Loss: 0.6016\n",
      "Iteration 400, Loss: 0.5893\n",
      "Iteration 500, Loss: 0.5802\n",
      "Iteration 600, Loss: 0.5730\n",
      "Iteration 700, Loss: 0.5671\n",
      "Iteration 800, Loss: 0.5620\n",
      "Iteration 900, Loss: 0.5577\n",
      "Iteration 1000, Loss: 0.5538\n",
      "Iteration 1100, Loss: 0.5505\n",
      "Iteration 1200, Loss: 0.5476\n",
      "Iteration 1300, Loss: 0.5446\n",
      "Iteration 1400, Loss: 0.5422\n",
      "Iteration 1500, Loss: 0.5398\n",
      "Iteration 1600, Loss: 0.5377\n",
      "Iteration 1700, Loss: 0.5357\n",
      "Iteration 1800, Loss: 0.5338\n",
      "Iteration 1900, Loss: 0.5320\n",
      "Iteration 2000, Loss: 0.5304\n",
      "Iteration 2100, Loss: 0.5288\n",
      "Iteration 2200, Loss: 0.5273\n",
      "Iteration 2300, Loss: 0.5259\n",
      "Iteration 2400, Loss: 0.5245\n",
      "Iteration 2500, Loss: 0.5232\n",
      "Iteration 2600, Loss: 0.5220\n",
      "Iteration 2700, Loss: 0.5208\n",
      "Iteration 2800, Loss: 0.5197\n",
      "Iteration 2900, Loss: 0.5186\n",
      "Iteration 3000, Loss: 0.5176\n",
      "Iteration 3100, Loss: 0.5166\n",
      "Iteration 3200, Loss: 0.5156\n",
      "Iteration 3300, Loss: 0.5147\n",
      "Iteration 3400, Loss: 0.5137\n",
      "Iteration 3500, Loss: 0.5129\n",
      "Iteration 3600, Loss: 0.5121\n",
      "Iteration 3700, Loss: 0.5112\n",
      "Iteration 3800, Loss: 0.5103\n",
      "Iteration 3900, Loss: 0.5096\n",
      "Iteration 4000, Loss: 0.5088\n",
      "Iteration 4100, Loss: 0.5081\n",
      "Iteration 4200, Loss: 0.5073\n",
      "Iteration 4300, Loss: 0.5066\n",
      "Iteration 4400, Loss: 0.5060\n",
      "Iteration 4500, Loss: 0.5053\n",
      "Iteration 4600, Loss: 0.5047\n",
      "Iteration 4700, Loss: 0.5040\n",
      "Iteration 4800, Loss: 0.5034\n",
      "Iteration 4900, Loss: 0.5028\n",
      "108 270\n",
      "Iteration 0, Loss: 1.0552\n",
      "Iteration 100, Loss: 0.6576\n",
      "Iteration 200, Loss: 0.6243\n",
      "Iteration 300, Loss: 0.6065\n",
      "Iteration 400, Loss: 0.5948\n",
      "Iteration 500, Loss: 0.5857\n",
      "Iteration 600, Loss: 0.5786\n",
      "Iteration 700, Loss: 0.5727\n",
      "Iteration 800, Loss: 0.5678\n",
      "Iteration 900, Loss: 0.5636\n",
      "Iteration 1000, Loss: 0.5599\n",
      "Iteration 1100, Loss: 0.5565\n",
      "Iteration 1200, Loss: 0.5535\n",
      "Iteration 1300, Loss: 0.5508\n",
      "Iteration 1400, Loss: 0.5484\n",
      "Iteration 1500, Loss: 0.5461\n",
      "Iteration 1600, Loss: 0.5440\n",
      "Iteration 1700, Loss: 0.5421\n",
      "Iteration 1800, Loss: 0.5402\n",
      "Iteration 1900, Loss: 0.5385\n",
      "Iteration 2000, Loss: 0.5369\n",
      "Iteration 2100, Loss: 0.5354\n",
      "Iteration 2200, Loss: 0.5340\n",
      "Iteration 2300, Loss: 0.5326\n",
      "Iteration 2400, Loss: 0.5313\n",
      "Iteration 2500, Loss: 0.5300\n",
      "Iteration 2600, Loss: 0.5289\n",
      "Iteration 2700, Loss: 0.5278\n",
      "Iteration 2800, Loss: 0.5266\n",
      "Iteration 2900, Loss: 0.5256\n",
      "Iteration 3000, Loss: 0.5245\n",
      "Iteration 3100, Loss: 0.5235\n",
      "Iteration 3200, Loss: 0.5226\n",
      "Iteration 3300, Loss: 0.5217\n",
      "Iteration 3400, Loss: 0.5208\n",
      "Iteration 3500, Loss: 0.5199\n",
      "Iteration 3600, Loss: 0.5191\n",
      "Iteration 3700, Loss: 0.5183\n",
      "Iteration 3800, Loss: 0.5175\n",
      "Iteration 3900, Loss: 0.5168\n",
      "Iteration 4000, Loss: 0.5161\n",
      "Iteration 4100, Loss: 0.5153\n",
      "Iteration 4200, Loss: 0.5146\n",
      "Iteration 4300, Loss: 0.5139\n",
      "Iteration 4400, Loss: 0.5133\n",
      "Iteration 4500, Loss: 0.5126\n",
      "Iteration 4600, Loss: 0.5120\n",
      "Iteration 4700, Loss: 0.5114\n",
      "Iteration 4800, Loss: 0.5108\n",
      "Iteration 4900, Loss: 0.5102\n",
      "Iteration 0, Loss: 1.0529\n",
      "Iteration 100, Loss: 0.6569\n",
      "Iteration 200, Loss: 0.6214\n",
      "Iteration 300, Loss: 0.6033\n",
      "Iteration 400, Loss: 0.5911\n",
      "Iteration 500, Loss: 0.5818\n",
      "Iteration 600, Loss: 0.5744\n",
      "Iteration 700, Loss: 0.5682\n",
      "Iteration 800, Loss: 0.5629\n",
      "Iteration 900, Loss: 0.5584\n",
      "Iteration 1000, Loss: 0.5544\n",
      "Iteration 1100, Loss: 0.5508\n",
      "Iteration 1200, Loss: 0.5476\n",
      "Iteration 1300, Loss: 0.5447\n",
      "Iteration 1400, Loss: 0.5420\n",
      "Iteration 1500, Loss: 0.5395\n",
      "Iteration 1600, Loss: 0.5372\n",
      "Iteration 1700, Loss: 0.5351\n",
      "Iteration 1800, Loss: 0.5332\n",
      "Iteration 1900, Loss: 0.5313\n",
      "Iteration 2000, Loss: 0.5295\n",
      "Iteration 2100, Loss: 0.5279\n",
      "Iteration 2200, Loss: 0.5264\n",
      "Iteration 2300, Loss: 0.5249\n",
      "Iteration 2400, Loss: 0.5235\n",
      "Iteration 2500, Loss: 0.5222\n",
      "Iteration 2600, Loss: 0.5209\n",
      "Iteration 2700, Loss: 0.5197\n",
      "Iteration 2800, Loss: 0.5185\n",
      "Iteration 2900, Loss: 0.5175\n",
      "Iteration 3000, Loss: 0.5164\n",
      "Iteration 3100, Loss: 0.5154\n",
      "Iteration 3200, Loss: 0.5144\n",
      "Iteration 3300, Loss: 0.5134\n",
      "Iteration 3400, Loss: 0.5125\n",
      "Iteration 3500, Loss: 0.5116\n",
      "Iteration 3600, Loss: 0.5108\n",
      "Iteration 3700, Loss: 0.5100\n",
      "Iteration 3800, Loss: 0.5092\n",
      "Iteration 3900, Loss: 0.5084\n",
      "Iteration 4000, Loss: 0.5076\n",
      "Iteration 4100, Loss: 0.5069\n",
      "Iteration 4200, Loss: 0.5062\n",
      "Iteration 4300, Loss: 0.5055\n",
      "Iteration 4400, Loss: 0.5049\n",
      "Iteration 4500, Loss: 0.5043\n",
      "Iteration 4600, Loss: 0.5036\n",
      "Iteration 4700, Loss: 0.5030\n",
      "Iteration 4800, Loss: 0.5024\n",
      "Iteration 4900, Loss: 0.5019\n",
      "109 270\n",
      "Iteration 0, Loss: 1.0581\n",
      "Iteration 100, Loss: 0.6597\n",
      "Iteration 200, Loss: 0.6228\n",
      "Iteration 300, Loss: 0.6033\n",
      "Iteration 400, Loss: 0.5899\n",
      "Iteration 500, Loss: 0.5798\n",
      "Iteration 600, Loss: 0.5717\n",
      "Iteration 700, Loss: 0.5650\n",
      "Iteration 800, Loss: 0.5593\n",
      "Iteration 900, Loss: 0.5543\n",
      "Iteration 1000, Loss: 0.5499\n",
      "Iteration 1100, Loss: 0.5461\n",
      "Iteration 1200, Loss: 0.5426\n",
      "Iteration 1300, Loss: 0.5394\n",
      "Iteration 1400, Loss: 0.5365\n",
      "Iteration 1500, Loss: 0.5339\n",
      "Iteration 1600, Loss: 0.5315\n",
      "Iteration 1700, Loss: 0.5292\n",
      "Iteration 1800, Loss: 0.5270\n",
      "Iteration 1900, Loss: 0.5251\n",
      "Iteration 2000, Loss: 0.5232\n",
      "Iteration 2100, Loss: 0.5215\n",
      "Iteration 2200, Loss: 0.5198\n",
      "Iteration 2300, Loss: 0.5182\n",
      "Iteration 2400, Loss: 0.5167\n",
      "Iteration 2500, Loss: 0.5152\n",
      "Iteration 2600, Loss: 0.5138\n",
      "Iteration 2700, Loss: 0.5125\n",
      "Iteration 2800, Loss: 0.5112\n",
      "Iteration 2900, Loss: 0.5100\n",
      "Iteration 3000, Loss: 0.5088\n",
      "Iteration 3100, Loss: 0.5078\n",
      "Iteration 3200, Loss: 0.5066\n",
      "Iteration 3300, Loss: 0.5056\n",
      "Iteration 3400, Loss: 0.5046\n",
      "Iteration 3500, Loss: 0.5036\n",
      "Iteration 3600, Loss: 0.5027\n",
      "Iteration 3700, Loss: 0.5017\n",
      "Iteration 3800, Loss: 0.5009\n",
      "Iteration 3900, Loss: 0.5000\n",
      "Iteration 4000, Loss: 0.4992\n",
      "Iteration 4100, Loss: 0.4984\n",
      "Iteration 4200, Loss: 0.4976\n",
      "Iteration 4300, Loss: 0.4969\n",
      "Iteration 4400, Loss: 0.4961\n",
      "Iteration 4500, Loss: 0.4954\n",
      "Iteration 4600, Loss: 0.4947\n",
      "Iteration 4700, Loss: 0.4940\n",
      "Iteration 4800, Loss: 0.4934\n",
      "Iteration 4900, Loss: 0.4927\n",
      "Iteration 0, Loss: 1.0518\n",
      "Iteration 100, Loss: 0.6551\n",
      "Iteration 200, Loss: 0.6225\n",
      "Iteration 300, Loss: 0.6063\n",
      "Iteration 400, Loss: 0.5956\n",
      "Iteration 500, Loss: 0.5875\n",
      "Iteration 600, Loss: 0.5812\n",
      "Iteration 700, Loss: 0.5760\n",
      "Iteration 800, Loss: 0.5715\n",
      "Iteration 900, Loss: 0.5678\n",
      "Iteration 1000, Loss: 0.5645\n",
      "Iteration 1100, Loss: 0.5615\n",
      "Iteration 1200, Loss: 0.5588\n",
      "Iteration 1300, Loss: 0.5564\n",
      "Iteration 1400, Loss: 0.5543\n",
      "Iteration 1500, Loss: 0.5522\n",
      "Iteration 1600, Loss: 0.5503\n",
      "Iteration 1700, Loss: 0.5485\n",
      "Iteration 1800, Loss: 0.5469\n",
      "Iteration 1900, Loss: 0.5453\n",
      "Iteration 2000, Loss: 0.5438\n",
      "Iteration 2100, Loss: 0.5424\n",
      "Iteration 2200, Loss: 0.5411\n",
      "Iteration 2300, Loss: 0.5399\n",
      "Iteration 2400, Loss: 0.5386\n",
      "Iteration 2500, Loss: 0.5375\n",
      "Iteration 2600, Loss: 0.5364\n",
      "Iteration 2700, Loss: 0.5354\n",
      "Iteration 2800, Loss: 0.5344\n",
      "Iteration 2900, Loss: 0.5335\n",
      "Iteration 3000, Loss: 0.5325\n",
      "Iteration 3100, Loss: 0.5316\n",
      "Iteration 3200, Loss: 0.5308\n",
      "Iteration 3300, Loss: 0.5299\n",
      "Iteration 3400, Loss: 0.5291\n",
      "Iteration 3500, Loss: 0.5283\n",
      "Iteration 3600, Loss: 0.5276\n",
      "Iteration 3700, Loss: 0.5269\n",
      "Iteration 3800, Loss: 0.5261\n",
      "Iteration 3900, Loss: 0.5254\n",
      "Iteration 4000, Loss: 0.5248\n",
      "Iteration 4100, Loss: 0.5241\n",
      "Iteration 4200, Loss: 0.5235\n",
      "Iteration 4300, Loss: 0.5229\n",
      "Iteration 4400, Loss: 0.5223\n",
      "Iteration 4500, Loss: 0.5217\n",
      "Iteration 4600, Loss: 0.5211\n",
      "Iteration 4700, Loss: 0.5206\n",
      "Iteration 4800, Loss: 0.5201\n",
      "Iteration 4900, Loss: 0.5195\n",
      "110 270\n",
      "Iteration 0, Loss: 1.0531\n",
      "Iteration 100, Loss: 0.6749\n",
      "Iteration 200, Loss: 0.6393\n",
      "Iteration 300, Loss: 0.6192\n",
      "Iteration 400, Loss: 0.6054\n",
      "Iteration 500, Loss: 0.5947\n",
      "Iteration 600, Loss: 0.5861\n",
      "Iteration 700, Loss: 0.5790\n",
      "Iteration 800, Loss: 0.5729\n",
      "Iteration 900, Loss: 0.5676\n",
      "Iteration 1000, Loss: 0.5629\n",
      "Iteration 1100, Loss: 0.5588\n",
      "Iteration 1200, Loss: 0.5550\n",
      "Iteration 1300, Loss: 0.5516\n",
      "Iteration 1400, Loss: 0.5484\n",
      "Iteration 1500, Loss: 0.5455\n",
      "Iteration 1600, Loss: 0.5428\n",
      "Iteration 1700, Loss: 0.5403\n",
      "Iteration 1800, Loss: 0.5380\n",
      "Iteration 1900, Loss: 0.5357\n",
      "Iteration 2000, Loss: 0.5337\n",
      "Iteration 2100, Loss: 0.5317\n",
      "Iteration 2200, Loss: 0.5298\n",
      "Iteration 2300, Loss: 0.5280\n",
      "Iteration 2400, Loss: 0.5264\n",
      "Iteration 2500, Loss: 0.5248\n",
      "Iteration 2600, Loss: 0.5232\n",
      "Iteration 2700, Loss: 0.5218\n",
      "Iteration 2800, Loss: 0.5204\n",
      "Iteration 2900, Loss: 0.5190\n",
      "Iteration 3000, Loss: 0.5178\n",
      "Iteration 3100, Loss: 0.5165\n",
      "Iteration 3200, Loss: 0.5152\n",
      "Iteration 3300, Loss: 0.5141\n",
      "Iteration 3400, Loss: 0.5130\n",
      "Iteration 3500, Loss: 0.5118\n",
      "Iteration 3600, Loss: 0.5108\n",
      "Iteration 3700, Loss: 0.5098\n",
      "Iteration 3800, Loss: 0.5088\n",
      "Iteration 3900, Loss: 0.5078\n",
      "Iteration 4000, Loss: 0.5069\n",
      "Iteration 4100, Loss: 0.5060\n",
      "Iteration 4200, Loss: 0.5051\n",
      "Iteration 4300, Loss: 0.5043\n",
      "Iteration 4400, Loss: 0.5034\n",
      "Iteration 4500, Loss: 0.5026\n",
      "Iteration 4600, Loss: 0.5018\n",
      "Iteration 4700, Loss: 0.5010\n",
      "Iteration 4800, Loss: 0.5002\n",
      "Iteration 4900, Loss: 0.4995\n",
      "Iteration 0, Loss: 1.0527\n",
      "Iteration 100, Loss: 0.6381\n",
      "Iteration 200, Loss: 0.6035\n",
      "Iteration 300, Loss: 0.5860\n",
      "Iteration 400, Loss: 0.5744\n",
      "Iteration 500, Loss: 0.5657\n",
      "Iteration 600, Loss: 0.5589\n",
      "Iteration 700, Loss: 0.5532\n",
      "Iteration 800, Loss: 0.5482\n",
      "Iteration 900, Loss: 0.5440\n",
      "Iteration 1000, Loss: 0.5402\n",
      "Iteration 1100, Loss: 0.5368\n",
      "Iteration 1200, Loss: 0.5337\n",
      "Iteration 1300, Loss: 0.5309\n",
      "Iteration 1400, Loss: 0.5283\n",
      "Iteration 1500, Loss: 0.5259\n",
      "Iteration 1600, Loss: 0.5237\n",
      "Iteration 1700, Loss: 0.5216\n",
      "Iteration 1800, Loss: 0.5196\n",
      "Iteration 1900, Loss: 0.5177\n",
      "Iteration 2000, Loss: 0.5160\n",
      "Iteration 2100, Loss: 0.5143\n",
      "Iteration 2200, Loss: 0.5127\n",
      "Iteration 2300, Loss: 0.5112\n",
      "Iteration 2400, Loss: 0.5097\n",
      "Iteration 2500, Loss: 0.5083\n",
      "Iteration 2600, Loss: 0.5071\n",
      "Iteration 2700, Loss: 0.5057\n",
      "Iteration 2800, Loss: 0.5044\n",
      "Iteration 2900, Loss: 0.5033\n",
      "Iteration 3000, Loss: 0.5021\n",
      "Iteration 3100, Loss: 0.5010\n",
      "Iteration 3200, Loss: 0.4999\n",
      "Iteration 3300, Loss: 0.4989\n",
      "Iteration 3400, Loss: 0.4979\n",
      "Iteration 3500, Loss: 0.4969\n",
      "Iteration 3600, Loss: 0.4960\n",
      "Iteration 3700, Loss: 0.4951\n",
      "Iteration 3800, Loss: 0.4941\n",
      "Iteration 3900, Loss: 0.4933\n",
      "Iteration 4000, Loss: 0.4924\n",
      "Iteration 4100, Loss: 0.4916\n",
      "Iteration 4200, Loss: 0.4908\n",
      "Iteration 4300, Loss: 0.4900\n",
      "Iteration 4400, Loss: 0.4892\n",
      "Iteration 4500, Loss: 0.4885\n",
      "Iteration 4600, Loss: 0.4877\n",
      "Iteration 4700, Loss: 0.4870\n",
      "Iteration 4800, Loss: 0.4863\n",
      "Iteration 4900, Loss: 0.4856\n",
      "111 270\n",
      "Iteration 0, Loss: 1.0522\n",
      "Iteration 100, Loss: 0.6527\n",
      "Iteration 200, Loss: 0.6177\n",
      "Iteration 300, Loss: 0.6000\n",
      "Iteration 400, Loss: 0.5881\n",
      "Iteration 500, Loss: 0.5792\n",
      "Iteration 600, Loss: 0.5720\n",
      "Iteration 700, Loss: 0.5661\n",
      "Iteration 800, Loss: 0.5610\n",
      "Iteration 900, Loss: 0.5565\n",
      "Iteration 1000, Loss: 0.5526\n",
      "Iteration 1100, Loss: 0.5490\n",
      "Iteration 1200, Loss: 0.5458\n",
      "Iteration 1300, Loss: 0.5428\n",
      "Iteration 1400, Loss: 0.5400\n",
      "Iteration 1500, Loss: 0.5375\n",
      "Iteration 1600, Loss: 0.5351\n",
      "Iteration 1700, Loss: 0.5329\n",
      "Iteration 1800, Loss: 0.5308\n",
      "Iteration 1900, Loss: 0.5289\n",
      "Iteration 2000, Loss: 0.5270\n",
      "Iteration 2100, Loss: 0.5252\n",
      "Iteration 2200, Loss: 0.5235\n",
      "Iteration 2300, Loss: 0.5219\n",
      "Iteration 2400, Loss: 0.5203\n",
      "Iteration 2500, Loss: 0.5188\n",
      "Iteration 2600, Loss: 0.5174\n",
      "Iteration 2700, Loss: 0.5160\n",
      "Iteration 2800, Loss: 0.5147\n",
      "Iteration 2900, Loss: 0.5135\n",
      "Iteration 3000, Loss: 0.5122\n",
      "Iteration 3100, Loss: 0.5111\n",
      "Iteration 3200, Loss: 0.5099\n",
      "Iteration 3300, Loss: 0.5088\n",
      "Iteration 3400, Loss: 0.5077\n",
      "Iteration 3500, Loss: 0.5066\n",
      "Iteration 3600, Loss: 0.5056\n",
      "Iteration 3700, Loss: 0.5046\n",
      "Iteration 3800, Loss: 0.5037\n",
      "Iteration 3900, Loss: 0.5028\n",
      "Iteration 4000, Loss: 0.5019\n",
      "Iteration 4100, Loss: 0.5009\n",
      "Iteration 4200, Loss: 0.5001\n",
      "Iteration 4300, Loss: 0.4992\n",
      "Iteration 4400, Loss: 0.4984\n",
      "Iteration 4500, Loss: 0.4976\n",
      "Iteration 4600, Loss: 0.4969\n",
      "Iteration 4700, Loss: 0.4961\n",
      "Iteration 4800, Loss: 0.4953\n",
      "Iteration 4900, Loss: 0.4946\n",
      "Iteration 0, Loss: 1.0540\n",
      "Iteration 100, Loss: 0.6560\n",
      "Iteration 200, Loss: 0.6202\n",
      "Iteration 300, Loss: 0.6005\n",
      "Iteration 400, Loss: 0.5868\n",
      "Iteration 500, Loss: 0.5763\n",
      "Iteration 600, Loss: 0.5677\n",
      "Iteration 700, Loss: 0.5606\n",
      "Iteration 800, Loss: 0.5543\n",
      "Iteration 900, Loss: 0.5489\n",
      "Iteration 1000, Loss: 0.5441\n",
      "Iteration 1100, Loss: 0.5398\n",
      "Iteration 1200, Loss: 0.5359\n",
      "Iteration 1300, Loss: 0.5325\n",
      "Iteration 1400, Loss: 0.5292\n",
      "Iteration 1500, Loss: 0.5262\n",
      "Iteration 1600, Loss: 0.5234\n",
      "Iteration 1700, Loss: 0.5209\n",
      "Iteration 1800, Loss: 0.5185\n",
      "Iteration 1900, Loss: 0.5161\n",
      "Iteration 2000, Loss: 0.5140\n",
      "Iteration 2100, Loss: 0.5120\n",
      "Iteration 2200, Loss: 0.5101\n",
      "Iteration 2300, Loss: 0.5083\n",
      "Iteration 2400, Loss: 0.5065\n",
      "Iteration 2500, Loss: 0.5049\n",
      "Iteration 2600, Loss: 0.5033\n",
      "Iteration 2700, Loss: 0.5018\n",
      "Iteration 2800, Loss: 0.5003\n",
      "Iteration 2900, Loss: 0.4989\n",
      "Iteration 3000, Loss: 0.4976\n",
      "Iteration 3100, Loss: 0.4963\n",
      "Iteration 3200, Loss: 0.4950\n",
      "Iteration 3300, Loss: 0.4938\n",
      "Iteration 3400, Loss: 0.4927\n",
      "Iteration 3500, Loss: 0.4916\n",
      "Iteration 3600, Loss: 0.4905\n",
      "Iteration 3700, Loss: 0.4894\n",
      "Iteration 3800, Loss: 0.4884\n",
      "Iteration 3900, Loss: 0.4875\n",
      "Iteration 4000, Loss: 0.4865\n",
      "Iteration 4100, Loss: 0.4856\n",
      "Iteration 4200, Loss: 0.4847\n",
      "Iteration 4300, Loss: 0.4838\n",
      "Iteration 4400, Loss: 0.4829\n",
      "Iteration 4500, Loss: 0.4821\n",
      "Iteration 4600, Loss: 0.4813\n",
      "Iteration 4700, Loss: 0.4805\n",
      "Iteration 4800, Loss: 0.4797\n",
      "Iteration 4900, Loss: 0.4790\n",
      "112 270\n",
      "Iteration 0, Loss: 1.0465\n",
      "Iteration 100, Loss: 0.6324\n",
      "Iteration 200, Loss: 0.5959\n",
      "Iteration 300, Loss: 0.5780\n",
      "Iteration 400, Loss: 0.5663\n",
      "Iteration 500, Loss: 0.5578\n",
      "Iteration 600, Loss: 0.5511\n",
      "Iteration 700, Loss: 0.5457\n",
      "Iteration 800, Loss: 0.5411\n",
      "Iteration 900, Loss: 0.5372\n",
      "Iteration 1000, Loss: 0.5338\n",
      "Iteration 1100, Loss: 0.5307\n",
      "Iteration 1200, Loss: 0.5279\n",
      "Iteration 1300, Loss: 0.5253\n",
      "Iteration 1400, Loss: 0.5230\n",
      "Iteration 1500, Loss: 0.5209\n",
      "Iteration 1600, Loss: 0.5189\n",
      "Iteration 1700, Loss: 0.5169\n",
      "Iteration 1800, Loss: 0.5151\n",
      "Iteration 1900, Loss: 0.5134\n",
      "Iteration 2000, Loss: 0.5118\n",
      "Iteration 2100, Loss: 0.5102\n",
      "Iteration 2200, Loss: 0.5087\n",
      "Iteration 2300, Loss: 0.5073\n",
      "Iteration 2400, Loss: 0.5059\n",
      "Iteration 2500, Loss: 0.5047\n",
      "Iteration 2600, Loss: 0.5034\n",
      "Iteration 2700, Loss: 0.5021\n",
      "Iteration 2800, Loss: 0.5010\n",
      "Iteration 2900, Loss: 0.4998\n",
      "Iteration 3000, Loss: 0.4987\n",
      "Iteration 3100, Loss: 0.4976\n",
      "Iteration 3200, Loss: 0.4966\n",
      "Iteration 3300, Loss: 0.4956\n",
      "Iteration 3400, Loss: 0.4946\n",
      "Iteration 3500, Loss: 0.4936\n",
      "Iteration 3600, Loss: 0.4927\n",
      "Iteration 3700, Loss: 0.4918\n",
      "Iteration 3800, Loss: 0.4909\n",
      "Iteration 3900, Loss: 0.4900\n",
      "Iteration 4000, Loss: 0.4892\n",
      "Iteration 4100, Loss: 0.4883\n",
      "Iteration 4200, Loss: 0.4875\n",
      "Iteration 4300, Loss: 0.4867\n",
      "Iteration 4400, Loss: 0.4859\n",
      "Iteration 4500, Loss: 0.4852\n",
      "Iteration 4600, Loss: 0.4844\n",
      "Iteration 4700, Loss: 0.4837\n",
      "Iteration 4800, Loss: 0.4830\n",
      "Iteration 4900, Loss: 0.4823\n",
      "Iteration 0, Loss: 1.0562\n",
      "Iteration 100, Loss: 0.6786\n",
      "Iteration 200, Loss: 0.6437\n",
      "Iteration 300, Loss: 0.6244\n",
      "Iteration 400, Loss: 0.6104\n",
      "Iteration 500, Loss: 0.5993\n",
      "Iteration 600, Loss: 0.5903\n",
      "Iteration 700, Loss: 0.5827\n",
      "Iteration 800, Loss: 0.5761\n",
      "Iteration 900, Loss: 0.5704\n",
      "Iteration 1000, Loss: 0.5654\n",
      "Iteration 1100, Loss: 0.5608\n",
      "Iteration 1200, Loss: 0.5568\n",
      "Iteration 1300, Loss: 0.5532\n",
      "Iteration 1400, Loss: 0.5498\n",
      "Iteration 1500, Loss: 0.5467\n",
      "Iteration 1600, Loss: 0.5440\n",
      "Iteration 1700, Loss: 0.5413\n",
      "Iteration 1800, Loss: 0.5389\n",
      "Iteration 1900, Loss: 0.5366\n",
      "Iteration 2000, Loss: 0.5345\n",
      "Iteration 2100, Loss: 0.5324\n",
      "Iteration 2200, Loss: 0.5306\n",
      "Iteration 2300, Loss: 0.5288\n",
      "Iteration 2400, Loss: 0.5270\n",
      "Iteration 2500, Loss: 0.5254\n",
      "Iteration 2600, Loss: 0.5239\n",
      "Iteration 2700, Loss: 0.5224\n",
      "Iteration 2800, Loss: 0.5210\n",
      "Iteration 2900, Loss: 0.5197\n",
      "Iteration 3000, Loss: 0.5184\n",
      "Iteration 3100, Loss: 0.5172\n",
      "Iteration 3200, Loss: 0.5160\n",
      "Iteration 3300, Loss: 0.5148\n",
      "Iteration 3400, Loss: 0.5137\n",
      "Iteration 3500, Loss: 0.5127\n",
      "Iteration 3600, Loss: 0.5116\n",
      "Iteration 3700, Loss: 0.5106\n",
      "Iteration 3800, Loss: 0.5096\n",
      "Iteration 3900, Loss: 0.5087\n",
      "Iteration 4000, Loss: 0.5078\n",
      "Iteration 4100, Loss: 0.5069\n",
      "Iteration 4200, Loss: 0.5061\n",
      "Iteration 4300, Loss: 0.5052\n",
      "Iteration 4400, Loss: 0.5044\n",
      "Iteration 4500, Loss: 0.5036\n",
      "Iteration 4600, Loss: 0.5028\n",
      "Iteration 4700, Loss: 0.5021\n",
      "Iteration 4800, Loss: 0.5013\n",
      "Iteration 4900, Loss: 0.5006\n",
      "113 270\n",
      "Iteration 0, Loss: 1.0538\n",
      "Iteration 100, Loss: 0.6713\n",
      "Iteration 200, Loss: 0.6374\n",
      "Iteration 300, Loss: 0.6185\n",
      "Iteration 400, Loss: 0.6052\n",
      "Iteration 500, Loss: 0.5949\n",
      "Iteration 600, Loss: 0.5866\n",
      "Iteration 700, Loss: 0.5797\n",
      "Iteration 800, Loss: 0.5739\n",
      "Iteration 900, Loss: 0.5687\n",
      "Iteration 1000, Loss: 0.5643\n",
      "Iteration 1100, Loss: 0.5603\n",
      "Iteration 1200, Loss: 0.5567\n",
      "Iteration 1300, Loss: 0.5534\n",
      "Iteration 1400, Loss: 0.5504\n",
      "Iteration 1500, Loss: 0.5476\n",
      "Iteration 1600, Loss: 0.5450\n",
      "Iteration 1700, Loss: 0.5426\n",
      "Iteration 1800, Loss: 0.5403\n",
      "Iteration 1900, Loss: 0.5382\n",
      "Iteration 2000, Loss: 0.5362\n",
      "Iteration 2100, Loss: 0.5343\n",
      "Iteration 2200, Loss: 0.5324\n",
      "Iteration 2300, Loss: 0.5307\n",
      "Iteration 2400, Loss: 0.5291\n",
      "Iteration 2500, Loss: 0.5275\n",
      "Iteration 2600, Loss: 0.5260\n",
      "Iteration 2700, Loss: 0.5246\n",
      "Iteration 2800, Loss: 0.5232\n",
      "Iteration 2900, Loss: 0.5219\n",
      "Iteration 3000, Loss: 0.5206\n",
      "Iteration 3100, Loss: 0.5194\n",
      "Iteration 3200, Loss: 0.5182\n",
      "Iteration 3300, Loss: 0.5170\n",
      "Iteration 3400, Loss: 0.5159\n",
      "Iteration 3500, Loss: 0.5148\n",
      "Iteration 3600, Loss: 0.5137\n",
      "Iteration 3700, Loss: 0.5127\n",
      "Iteration 3800, Loss: 0.5117\n",
      "Iteration 3900, Loss: 0.5107\n",
      "Iteration 4000, Loss: 0.5098\n",
      "Iteration 4100, Loss: 0.5089\n",
      "Iteration 4200, Loss: 0.5080\n",
      "Iteration 4300, Loss: 0.5071\n",
      "Iteration 4400, Loss: 0.5063\n",
      "Iteration 4500, Loss: 0.5055\n",
      "Iteration 4600, Loss: 0.5046\n",
      "Iteration 4700, Loss: 0.5039\n",
      "Iteration 4800, Loss: 0.5031\n",
      "Iteration 4900, Loss: 0.5023\n",
      "Iteration 0, Loss: 1.0543\n",
      "Iteration 100, Loss: 0.6418\n",
      "Iteration 200, Loss: 0.6039\n",
      "Iteration 300, Loss: 0.5854\n",
      "Iteration 400, Loss: 0.5731\n",
      "Iteration 500, Loss: 0.5641\n",
      "Iteration 600, Loss: 0.5567\n",
      "Iteration 700, Loss: 0.5507\n",
      "Iteration 800, Loss: 0.5455\n",
      "Iteration 900, Loss: 0.5409\n",
      "Iteration 1000, Loss: 0.5368\n",
      "Iteration 1100, Loss: 0.5332\n",
      "Iteration 1200, Loss: 0.5299\n",
      "Iteration 1300, Loss: 0.5269\n",
      "Iteration 1400, Loss: 0.5241\n",
      "Iteration 1500, Loss: 0.5215\n",
      "Iteration 1600, Loss: 0.5191\n",
      "Iteration 1700, Loss: 0.5169\n",
      "Iteration 1800, Loss: 0.5148\n",
      "Iteration 1900, Loss: 0.5128\n",
      "Iteration 2000, Loss: 0.5109\n",
      "Iteration 2100, Loss: 0.5091\n",
      "Iteration 2200, Loss: 0.5074\n",
      "Iteration 2300, Loss: 0.5058\n",
      "Iteration 2400, Loss: 0.5043\n",
      "Iteration 2500, Loss: 0.5029\n",
      "Iteration 2600, Loss: 0.5014\n",
      "Iteration 2700, Loss: 0.5000\n",
      "Iteration 2800, Loss: 0.4987\n",
      "Iteration 2900, Loss: 0.4975\n",
      "Iteration 3000, Loss: 0.4963\n",
      "Iteration 3100, Loss: 0.4951\n",
      "Iteration 3200, Loss: 0.4940\n",
      "Iteration 3300, Loss: 0.4929\n",
      "Iteration 3400, Loss: 0.4918\n",
      "Iteration 3500, Loss: 0.4908\n",
      "Iteration 3600, Loss: 0.4898\n",
      "Iteration 3700, Loss: 0.4888\n",
      "Iteration 3800, Loss: 0.4879\n",
      "Iteration 3900, Loss: 0.4870\n",
      "Iteration 4000, Loss: 0.4861\n",
      "Iteration 4100, Loss: 0.4852\n",
      "Iteration 4200, Loss: 0.4844\n",
      "Iteration 4300, Loss: 0.4835\n",
      "Iteration 4400, Loss: 0.4827\n",
      "Iteration 4500, Loss: 0.4820\n",
      "Iteration 4600, Loss: 0.4812\n",
      "Iteration 4700, Loss: 0.4805\n",
      "Iteration 4800, Loss: 0.4797\n",
      "Iteration 4900, Loss: 0.4790\n",
      "114 270\n",
      "Iteration 0, Loss: 1.0520\n",
      "Iteration 100, Loss: 0.6488\n",
      "Iteration 200, Loss: 0.6108\n",
      "Iteration 300, Loss: 0.5911\n",
      "Iteration 400, Loss: 0.5778\n",
      "Iteration 500, Loss: 0.5679\n",
      "Iteration 600, Loss: 0.5601\n",
      "Iteration 700, Loss: 0.5535\n",
      "Iteration 800, Loss: 0.5480\n",
      "Iteration 900, Loss: 0.5431\n",
      "Iteration 1000, Loss: 0.5389\n",
      "Iteration 1100, Loss: 0.5351\n",
      "Iteration 1200, Loss: 0.5316\n",
      "Iteration 1300, Loss: 0.5284\n",
      "Iteration 1400, Loss: 0.5255\n",
      "Iteration 1500, Loss: 0.5229\n",
      "Iteration 1600, Loss: 0.5204\n",
      "Iteration 1700, Loss: 0.5181\n",
      "Iteration 1800, Loss: 0.5159\n",
      "Iteration 1900, Loss: 0.5139\n",
      "Iteration 2000, Loss: 0.5120\n",
      "Iteration 2100, Loss: 0.5102\n",
      "Iteration 2200, Loss: 0.5084\n",
      "Iteration 2300, Loss: 0.5068\n",
      "Iteration 2400, Loss: 0.5052\n",
      "Iteration 2500, Loss: 0.5037\n",
      "Iteration 2600, Loss: 0.5023\n",
      "Iteration 2700, Loss: 0.5009\n",
      "Iteration 2800, Loss: 0.4996\n",
      "Iteration 2900, Loss: 0.4983\n",
      "Iteration 3000, Loss: 0.4971\n",
      "Iteration 3100, Loss: 0.4960\n",
      "Iteration 3200, Loss: 0.4948\n",
      "Iteration 3300, Loss: 0.4937\n",
      "Iteration 3400, Loss: 0.4926\n",
      "Iteration 3500, Loss: 0.4916\n",
      "Iteration 3600, Loss: 0.4906\n",
      "Iteration 3700, Loss: 0.4897\n",
      "Iteration 3800, Loss: 0.4887\n",
      "Iteration 3900, Loss: 0.4878\n",
      "Iteration 4000, Loss: 0.4869\n",
      "Iteration 4100, Loss: 0.4861\n",
      "Iteration 4200, Loss: 0.4853\n",
      "Iteration 4300, Loss: 0.4845\n",
      "Iteration 4400, Loss: 0.4836\n",
      "Iteration 4500, Loss: 0.4829\n",
      "Iteration 4600, Loss: 0.4821\n",
      "Iteration 4700, Loss: 0.4814\n",
      "Iteration 4800, Loss: 0.4807\n",
      "Iteration 4900, Loss: 0.4800\n",
      "Iteration 0, Loss: 1.0546\n",
      "Iteration 100, Loss: 0.6618\n",
      "Iteration 200, Loss: 0.6281\n",
      "Iteration 300, Loss: 0.6108\n",
      "Iteration 400, Loss: 0.5989\n",
      "Iteration 500, Loss: 0.5897\n",
      "Iteration 600, Loss: 0.5822\n",
      "Iteration 700, Loss: 0.5760\n",
      "Iteration 800, Loss: 0.5705\n",
      "Iteration 900, Loss: 0.5658\n",
      "Iteration 1000, Loss: 0.5616\n",
      "Iteration 1100, Loss: 0.5578\n",
      "Iteration 1200, Loss: 0.5543\n",
      "Iteration 1300, Loss: 0.5512\n",
      "Iteration 1400, Loss: 0.5483\n",
      "Iteration 1500, Loss: 0.5456\n",
      "Iteration 1600, Loss: 0.5431\n",
      "Iteration 1700, Loss: 0.5408\n",
      "Iteration 1800, Loss: 0.5386\n",
      "Iteration 1900, Loss: 0.5366\n",
      "Iteration 2000, Loss: 0.5346\n",
      "Iteration 2100, Loss: 0.5328\n",
      "Iteration 2200, Loss: 0.5310\n",
      "Iteration 2300, Loss: 0.5294\n",
      "Iteration 2400, Loss: 0.5277\n",
      "Iteration 2500, Loss: 0.5262\n",
      "Iteration 2600, Loss: 0.5248\n",
      "Iteration 2700, Loss: 0.5233\n",
      "Iteration 2800, Loss: 0.5220\n",
      "Iteration 2900, Loss: 0.5207\n",
      "Iteration 3000, Loss: 0.5194\n",
      "Iteration 3100, Loss: 0.5183\n",
      "Iteration 3200, Loss: 0.5171\n",
      "Iteration 3300, Loss: 0.5159\n",
      "Iteration 3400, Loss: 0.5148\n",
      "Iteration 3500, Loss: 0.5138\n",
      "Iteration 3600, Loss: 0.5128\n",
      "Iteration 3700, Loss: 0.5117\n",
      "Iteration 3800, Loss: 0.5108\n",
      "Iteration 3900, Loss: 0.5098\n",
      "Iteration 4000, Loss: 0.5089\n",
      "Iteration 4100, Loss: 0.5080\n",
      "Iteration 4200, Loss: 0.5072\n",
      "Iteration 4300, Loss: 0.5063\n",
      "Iteration 4400, Loss: 0.5055\n",
      "Iteration 4500, Loss: 0.5047\n",
      "Iteration 4600, Loss: 0.5039\n",
      "Iteration 4700, Loss: 0.5032\n",
      "Iteration 4800, Loss: 0.5024\n",
      "Iteration 4900, Loss: 0.5017\n",
      "115 270\n",
      "Iteration 0, Loss: 1.0563\n",
      "Iteration 100, Loss: 0.6446\n",
      "Iteration 200, Loss: 0.6075\n",
      "Iteration 300, Loss: 0.5887\n",
      "Iteration 400, Loss: 0.5764\n",
      "Iteration 500, Loss: 0.5670\n",
      "Iteration 600, Loss: 0.5596\n",
      "Iteration 700, Loss: 0.5534\n",
      "Iteration 800, Loss: 0.5480\n",
      "Iteration 900, Loss: 0.5433\n",
      "Iteration 1000, Loss: 0.5393\n",
      "Iteration 1100, Loss: 0.5356\n",
      "Iteration 1200, Loss: 0.5322\n",
      "Iteration 1300, Loss: 0.5291\n",
      "Iteration 1400, Loss: 0.5262\n",
      "Iteration 1500, Loss: 0.5236\n",
      "Iteration 1600, Loss: 0.5211\n",
      "Iteration 1700, Loss: 0.5188\n",
      "Iteration 1800, Loss: 0.5166\n",
      "Iteration 1900, Loss: 0.5146\n",
      "Iteration 2000, Loss: 0.5126\n",
      "Iteration 2100, Loss: 0.5108\n",
      "Iteration 2200, Loss: 0.5090\n",
      "Iteration 2300, Loss: 0.5074\n",
      "Iteration 2400, Loss: 0.5057\n",
      "Iteration 2500, Loss: 0.5042\n",
      "Iteration 2600, Loss: 0.5027\n",
      "Iteration 2700, Loss: 0.5013\n",
      "Iteration 2800, Loss: 0.4999\n",
      "Iteration 2900, Loss: 0.4986\n",
      "Iteration 3000, Loss: 0.4974\n",
      "Iteration 3100, Loss: 0.4961\n",
      "Iteration 3200, Loss: 0.4949\n",
      "Iteration 3300, Loss: 0.4938\n",
      "Iteration 3400, Loss: 0.4927\n",
      "Iteration 3500, Loss: 0.4916\n",
      "Iteration 3600, Loss: 0.4905\n",
      "Iteration 3700, Loss: 0.4895\n",
      "Iteration 3800, Loss: 0.4886\n",
      "Iteration 3900, Loss: 0.4875\n",
      "Iteration 4000, Loss: 0.4866\n",
      "Iteration 4100, Loss: 0.4857\n",
      "Iteration 4200, Loss: 0.4848\n",
      "Iteration 4300, Loss: 0.4839\n",
      "Iteration 4400, Loss: 0.4831\n",
      "Iteration 4500, Loss: 0.4823\n",
      "Iteration 4600, Loss: 0.4814\n",
      "Iteration 4700, Loss: 0.4806\n",
      "Iteration 4800, Loss: 0.4799\n",
      "Iteration 4900, Loss: 0.4791\n",
      "Iteration 0, Loss: 1.0548\n",
      "Iteration 100, Loss: 0.6689\n",
      "Iteration 200, Loss: 0.6355\n",
      "Iteration 300, Loss: 0.6172\n",
      "Iteration 400, Loss: 0.6043\n",
      "Iteration 500, Loss: 0.5944\n",
      "Iteration 600, Loss: 0.5863\n",
      "Iteration 700, Loss: 0.5796\n",
      "Iteration 800, Loss: 0.5738\n",
      "Iteration 900, Loss: 0.5687\n",
      "Iteration 1000, Loss: 0.5643\n",
      "Iteration 1100, Loss: 0.5602\n",
      "Iteration 1200, Loss: 0.5567\n",
      "Iteration 1300, Loss: 0.5534\n",
      "Iteration 1400, Loss: 0.5504\n",
      "Iteration 1500, Loss: 0.5476\n",
      "Iteration 1600, Loss: 0.5451\n",
      "Iteration 1700, Loss: 0.5427\n",
      "Iteration 1800, Loss: 0.5405\n",
      "Iteration 1900, Loss: 0.5384\n",
      "Iteration 2000, Loss: 0.5365\n",
      "Iteration 2100, Loss: 0.5346\n",
      "Iteration 2200, Loss: 0.5329\n",
      "Iteration 2300, Loss: 0.5312\n",
      "Iteration 2400, Loss: 0.5296\n",
      "Iteration 2500, Loss: 0.5281\n",
      "Iteration 2600, Loss: 0.5267\n",
      "Iteration 2700, Loss: 0.5253\n",
      "Iteration 2800, Loss: 0.5240\n",
      "Iteration 2900, Loss: 0.5227\n",
      "Iteration 3000, Loss: 0.5215\n",
      "Iteration 3100, Loss: 0.5203\n",
      "Iteration 3200, Loss: 0.5191\n",
      "Iteration 3300, Loss: 0.5181\n",
      "Iteration 3400, Loss: 0.5170\n",
      "Iteration 3500, Loss: 0.5160\n",
      "Iteration 3600, Loss: 0.5150\n",
      "Iteration 3700, Loss: 0.5141\n",
      "Iteration 3800, Loss: 0.5131\n",
      "Iteration 3900, Loss: 0.5122\n",
      "Iteration 4000, Loss: 0.5114\n",
      "Iteration 4100, Loss: 0.5105\n",
      "Iteration 4200, Loss: 0.5097\n",
      "Iteration 4300, Loss: 0.5088\n",
      "Iteration 4400, Loss: 0.5081\n",
      "Iteration 4500, Loss: 0.5073\n",
      "Iteration 4600, Loss: 0.5066\n",
      "Iteration 4700, Loss: 0.5058\n",
      "Iteration 4800, Loss: 0.5051\n",
      "Iteration 4900, Loss: 0.5044\n",
      "116 270\n",
      "Iteration 0, Loss: 1.0514\n",
      "Iteration 100, Loss: 0.6556\n",
      "Iteration 200, Loss: 0.6196\n",
      "Iteration 300, Loss: 0.6002\n",
      "Iteration 400, Loss: 0.5867\n",
      "Iteration 500, Loss: 0.5763\n",
      "Iteration 600, Loss: 0.5679\n",
      "Iteration 700, Loss: 0.5610\n",
      "Iteration 800, Loss: 0.5551\n",
      "Iteration 900, Loss: 0.5499\n",
      "Iteration 1000, Loss: 0.5453\n",
      "Iteration 1100, Loss: 0.5413\n",
      "Iteration 1200, Loss: 0.5376\n",
      "Iteration 1300, Loss: 0.5342\n",
      "Iteration 1400, Loss: 0.5311\n",
      "Iteration 1500, Loss: 0.5283\n",
      "Iteration 1600, Loss: 0.5257\n",
      "Iteration 1700, Loss: 0.5232\n",
      "Iteration 1800, Loss: 0.5210\n",
      "Iteration 1900, Loss: 0.5188\n",
      "Iteration 2000, Loss: 0.5167\n",
      "Iteration 2100, Loss: 0.5148\n",
      "Iteration 2200, Loss: 0.5129\n",
      "Iteration 2300, Loss: 0.5112\n",
      "Iteration 2400, Loss: 0.5095\n",
      "Iteration 2500, Loss: 0.5079\n",
      "Iteration 2600, Loss: 0.5064\n",
      "Iteration 2700, Loss: 0.5049\n",
      "Iteration 2800, Loss: 0.5035\n",
      "Iteration 2900, Loss: 0.5021\n",
      "Iteration 3000, Loss: 0.5008\n",
      "Iteration 3100, Loss: 0.4996\n",
      "Iteration 3200, Loss: 0.4984\n",
      "Iteration 3300, Loss: 0.4972\n",
      "Iteration 3400, Loss: 0.4961\n",
      "Iteration 3500, Loss: 0.4950\n",
      "Iteration 3600, Loss: 0.4939\n",
      "Iteration 3700, Loss: 0.4928\n",
      "Iteration 3800, Loss: 0.4918\n",
      "Iteration 3900, Loss: 0.4908\n",
      "Iteration 4000, Loss: 0.4899\n",
      "Iteration 4100, Loss: 0.4890\n",
      "Iteration 4200, Loss: 0.4881\n",
      "Iteration 4300, Loss: 0.4872\n",
      "Iteration 4400, Loss: 0.4863\n",
      "Iteration 4500, Loss: 0.4855\n",
      "Iteration 4600, Loss: 0.4846\n",
      "Iteration 4700, Loss: 0.4838\n",
      "Iteration 4800, Loss: 0.4830\n",
      "Iteration 4900, Loss: 0.4823\n",
      "Iteration 0, Loss: 1.0573\n",
      "Iteration 100, Loss: 0.6578\n",
      "Iteration 200, Loss: 0.6243\n",
      "Iteration 300, Loss: 0.6073\n",
      "Iteration 400, Loss: 0.5958\n",
      "Iteration 500, Loss: 0.5871\n",
      "Iteration 600, Loss: 0.5802\n",
      "Iteration 700, Loss: 0.5744\n",
      "Iteration 800, Loss: 0.5694\n",
      "Iteration 900, Loss: 0.5650\n",
      "Iteration 1000, Loss: 0.5611\n",
      "Iteration 1100, Loss: 0.5577\n",
      "Iteration 1200, Loss: 0.5544\n",
      "Iteration 1300, Loss: 0.5515\n",
      "Iteration 1400, Loss: 0.5488\n",
      "Iteration 1500, Loss: 0.5462\n",
      "Iteration 1600, Loss: 0.5439\n",
      "Iteration 1700, Loss: 0.5417\n",
      "Iteration 1800, Loss: 0.5397\n",
      "Iteration 1900, Loss: 0.5377\n",
      "Iteration 2000, Loss: 0.5358\n",
      "Iteration 2100, Loss: 0.5341\n",
      "Iteration 2200, Loss: 0.5324\n",
      "Iteration 2300, Loss: 0.5308\n",
      "Iteration 2400, Loss: 0.5293\n",
      "Iteration 2500, Loss: 0.5278\n",
      "Iteration 2600, Loss: 0.5264\n",
      "Iteration 2700, Loss: 0.5251\n",
      "Iteration 2800, Loss: 0.5237\n",
      "Iteration 2900, Loss: 0.5225\n",
      "Iteration 3000, Loss: 0.5212\n",
      "Iteration 3100, Loss: 0.5200\n",
      "Iteration 3200, Loss: 0.5189\n",
      "Iteration 3300, Loss: 0.5178\n",
      "Iteration 3400, Loss: 0.5167\n",
      "Iteration 3500, Loss: 0.5157\n",
      "Iteration 3600, Loss: 0.5147\n",
      "Iteration 3700, Loss: 0.5137\n",
      "Iteration 3800, Loss: 0.5127\n",
      "Iteration 3900, Loss: 0.5118\n",
      "Iteration 4000, Loss: 0.5109\n",
      "Iteration 4100, Loss: 0.5100\n",
      "Iteration 4200, Loss: 0.5091\n",
      "Iteration 4300, Loss: 0.5083\n",
      "Iteration 4400, Loss: 0.5075\n",
      "Iteration 4500, Loss: 0.5067\n",
      "Iteration 4600, Loss: 0.5058\n",
      "Iteration 4700, Loss: 0.5052\n",
      "Iteration 4800, Loss: 0.5043\n",
      "Iteration 4900, Loss: 0.5036\n",
      "117 270\n",
      "Iteration 0, Loss: 1.0530\n",
      "Iteration 100, Loss: 0.6536\n",
      "Iteration 200, Loss: 0.6165\n",
      "Iteration 300, Loss: 0.5972\n",
      "Iteration 400, Loss: 0.5839\n",
      "Iteration 500, Loss: 0.5737\n",
      "Iteration 600, Loss: 0.5656\n",
      "Iteration 700, Loss: 0.5588\n",
      "Iteration 800, Loss: 0.5529\n",
      "Iteration 900, Loss: 0.5478\n",
      "Iteration 1000, Loss: 0.5432\n",
      "Iteration 1100, Loss: 0.5391\n",
      "Iteration 1200, Loss: 0.5354\n",
      "Iteration 1300, Loss: 0.5319\n",
      "Iteration 1400, Loss: 0.5288\n",
      "Iteration 1500, Loss: 0.5258\n",
      "Iteration 1600, Loss: 0.5230\n",
      "Iteration 1700, Loss: 0.5204\n",
      "Iteration 1800, Loss: 0.5179\n",
      "Iteration 1900, Loss: 0.5156\n",
      "Iteration 2000, Loss: 0.5135\n",
      "Iteration 2100, Loss: 0.5114\n",
      "Iteration 2200, Loss: 0.5095\n",
      "Iteration 2300, Loss: 0.5076\n",
      "Iteration 2400, Loss: 0.5058\n",
      "Iteration 2500, Loss: 0.5041\n",
      "Iteration 2600, Loss: 0.5025\n",
      "Iteration 2700, Loss: 0.5009\n",
      "Iteration 2800, Loss: 0.4994\n",
      "Iteration 2900, Loss: 0.4980\n",
      "Iteration 3000, Loss: 0.4965\n",
      "Iteration 3100, Loss: 0.4951\n",
      "Iteration 3200, Loss: 0.4939\n",
      "Iteration 3300, Loss: 0.4926\n",
      "Iteration 3400, Loss: 0.4913\n",
      "Iteration 3500, Loss: 0.4901\n",
      "Iteration 3600, Loss: 0.4890\n",
      "Iteration 3700, Loss: 0.4878\n",
      "Iteration 3800, Loss: 0.4868\n",
      "Iteration 3900, Loss: 0.4857\n",
      "Iteration 4000, Loss: 0.4847\n",
      "Iteration 4100, Loss: 0.4837\n",
      "Iteration 4200, Loss: 0.4827\n",
      "Iteration 4300, Loss: 0.4818\n",
      "Iteration 4400, Loss: 0.4808\n",
      "Iteration 4500, Loss: 0.4799\n",
      "Iteration 4600, Loss: 0.4791\n",
      "Iteration 4700, Loss: 0.4782\n",
      "Iteration 4800, Loss: 0.4774\n",
      "Iteration 4900, Loss: 0.4765\n",
      "Iteration 0, Loss: 1.0539\n",
      "Iteration 100, Loss: 0.6594\n",
      "Iteration 200, Loss: 0.6245\n",
      "Iteration 300, Loss: 0.6067\n",
      "Iteration 400, Loss: 0.5946\n",
      "Iteration 500, Loss: 0.5854\n",
      "Iteration 600, Loss: 0.5781\n",
      "Iteration 700, Loss: 0.5720\n",
      "Iteration 800, Loss: 0.5668\n",
      "Iteration 900, Loss: 0.5623\n",
      "Iteration 1000, Loss: 0.5583\n",
      "Iteration 1100, Loss: 0.5547\n",
      "Iteration 1200, Loss: 0.5515\n",
      "Iteration 1300, Loss: 0.5487\n",
      "Iteration 1400, Loss: 0.5461\n",
      "Iteration 1500, Loss: 0.5436\n",
      "Iteration 1600, Loss: 0.5413\n",
      "Iteration 1700, Loss: 0.5392\n",
      "Iteration 1800, Loss: 0.5372\n",
      "Iteration 1900, Loss: 0.5354\n",
      "Iteration 2000, Loss: 0.5337\n",
      "Iteration 2100, Loss: 0.5321\n",
      "Iteration 2200, Loss: 0.5305\n",
      "Iteration 2300, Loss: 0.5291\n",
      "Iteration 2400, Loss: 0.5276\n",
      "Iteration 2500, Loss: 0.5263\n",
      "Iteration 2600, Loss: 0.5250\n",
      "Iteration 2700, Loss: 0.5238\n",
      "Iteration 2800, Loss: 0.5226\n",
      "Iteration 2900, Loss: 0.5215\n",
      "Iteration 3000, Loss: 0.5204\n",
      "Iteration 3100, Loss: 0.5193\n",
      "Iteration 3200, Loss: 0.5183\n",
      "Iteration 3300, Loss: 0.5174\n",
      "Iteration 3400, Loss: 0.5165\n",
      "Iteration 3500, Loss: 0.5155\n",
      "Iteration 3600, Loss: 0.5147\n",
      "Iteration 3700, Loss: 0.5138\n",
      "Iteration 3800, Loss: 0.5129\n",
      "Iteration 3900, Loss: 0.5121\n",
      "Iteration 4000, Loss: 0.5114\n",
      "Iteration 4100, Loss: 0.5106\n",
      "Iteration 4200, Loss: 0.5098\n",
      "Iteration 4300, Loss: 0.5091\n",
      "Iteration 4400, Loss: 0.5084\n",
      "Iteration 4500, Loss: 0.5077\n",
      "Iteration 4600, Loss: 0.5070\n",
      "Iteration 4700, Loss: 0.5064\n",
      "Iteration 4800, Loss: 0.5057\n",
      "Iteration 4900, Loss: 0.5051\n",
      "118 270\n",
      "Iteration 0, Loss: 1.0543\n",
      "Iteration 100, Loss: 0.6593\n",
      "Iteration 200, Loss: 0.6224\n",
      "Iteration 300, Loss: 0.6026\n",
      "Iteration 400, Loss: 0.5888\n",
      "Iteration 500, Loss: 0.5783\n",
      "Iteration 600, Loss: 0.5701\n",
      "Iteration 700, Loss: 0.5632\n",
      "Iteration 800, Loss: 0.5573\n",
      "Iteration 900, Loss: 0.5523\n",
      "Iteration 1000, Loss: 0.5478\n",
      "Iteration 1100, Loss: 0.5439\n",
      "Iteration 1200, Loss: 0.5403\n",
      "Iteration 1300, Loss: 0.5371\n",
      "Iteration 1400, Loss: 0.5342\n",
      "Iteration 1500, Loss: 0.5314\n",
      "Iteration 1600, Loss: 0.5289\n",
      "Iteration 1700, Loss: 0.5266\n",
      "Iteration 1800, Loss: 0.5243\n",
      "Iteration 1900, Loss: 0.5222\n",
      "Iteration 2000, Loss: 0.5203\n",
      "Iteration 2100, Loss: 0.5184\n",
      "Iteration 2200, Loss: 0.5166\n",
      "Iteration 2300, Loss: 0.5150\n",
      "Iteration 2400, Loss: 0.5134\n",
      "Iteration 2500, Loss: 0.5118\n",
      "Iteration 2600, Loss: 0.5104\n",
      "Iteration 2700, Loss: 0.5090\n",
      "Iteration 2800, Loss: 0.5077\n",
      "Iteration 2900, Loss: 0.5064\n",
      "Iteration 3000, Loss: 0.5052\n",
      "Iteration 3100, Loss: 0.5040\n",
      "Iteration 3200, Loss: 0.5029\n",
      "Iteration 3300, Loss: 0.5017\n",
      "Iteration 3400, Loss: 0.5007\n",
      "Iteration 3500, Loss: 0.4996\n",
      "Iteration 3600, Loss: 0.4986\n",
      "Iteration 3700, Loss: 0.4976\n",
      "Iteration 3800, Loss: 0.4967\n",
      "Iteration 3900, Loss: 0.4958\n",
      "Iteration 4000, Loss: 0.4949\n",
      "Iteration 4100, Loss: 0.4940\n",
      "Iteration 4200, Loss: 0.4932\n",
      "Iteration 4300, Loss: 0.4923\n",
      "Iteration 4400, Loss: 0.4915\n",
      "Iteration 4500, Loss: 0.4907\n",
      "Iteration 4600, Loss: 0.4900\n",
      "Iteration 4700, Loss: 0.4893\n",
      "Iteration 4800, Loss: 0.4885\n",
      "Iteration 4900, Loss: 0.4878\n",
      "Iteration 0, Loss: 1.0551\n",
      "Iteration 100, Loss: 0.6510\n",
      "Iteration 200, Loss: 0.6164\n",
      "Iteration 300, Loss: 0.5989\n",
      "Iteration 400, Loss: 0.5871\n",
      "Iteration 500, Loss: 0.5780\n",
      "Iteration 600, Loss: 0.5706\n",
      "Iteration 700, Loss: 0.5644\n",
      "Iteration 800, Loss: 0.5591\n",
      "Iteration 900, Loss: 0.5545\n",
      "Iteration 1000, Loss: 0.5505\n",
      "Iteration 1100, Loss: 0.5467\n",
      "Iteration 1200, Loss: 0.5433\n",
      "Iteration 1300, Loss: 0.5403\n",
      "Iteration 1400, Loss: 0.5374\n",
      "Iteration 1500, Loss: 0.5348\n",
      "Iteration 1600, Loss: 0.5323\n",
      "Iteration 1700, Loss: 0.5301\n",
      "Iteration 1800, Loss: 0.5279\n",
      "Iteration 1900, Loss: 0.5260\n",
      "Iteration 2000, Loss: 0.5240\n",
      "Iteration 2100, Loss: 0.5222\n",
      "Iteration 2200, Loss: 0.5205\n",
      "Iteration 2300, Loss: 0.5189\n",
      "Iteration 2400, Loss: 0.5173\n",
      "Iteration 2500, Loss: 0.5158\n",
      "Iteration 2600, Loss: 0.5144\n",
      "Iteration 2700, Loss: 0.5130\n",
      "Iteration 2800, Loss: 0.5117\n",
      "Iteration 2900, Loss: 0.5104\n",
      "Iteration 3000, Loss: 0.5092\n",
      "Iteration 3100, Loss: 0.5080\n",
      "Iteration 3200, Loss: 0.5069\n",
      "Iteration 3300, Loss: 0.5057\n",
      "Iteration 3400, Loss: 0.5046\n",
      "Iteration 3500, Loss: 0.5036\n",
      "Iteration 3600, Loss: 0.5026\n",
      "Iteration 3700, Loss: 0.5017\n",
      "Iteration 3800, Loss: 0.5007\n",
      "Iteration 3900, Loss: 0.4997\n",
      "Iteration 4000, Loss: 0.4988\n",
      "Iteration 4100, Loss: 0.4980\n",
      "Iteration 4200, Loss: 0.4971\n",
      "Iteration 4300, Loss: 0.4963\n",
      "Iteration 4400, Loss: 0.4955\n",
      "Iteration 4500, Loss: 0.4946\n",
      "Iteration 4600, Loss: 0.4939\n",
      "Iteration 4700, Loss: 0.4931\n",
      "Iteration 4800, Loss: 0.4923\n",
      "Iteration 4900, Loss: 0.4916\n",
      "119 270\n",
      "Iteration 0, Loss: 1.0539\n",
      "Iteration 100, Loss: 0.6637\n",
      "Iteration 200, Loss: 0.6312\n",
      "Iteration 300, Loss: 0.6135\n",
      "Iteration 400, Loss: 0.6013\n",
      "Iteration 500, Loss: 0.5917\n",
      "Iteration 600, Loss: 0.5840\n",
      "Iteration 700, Loss: 0.5776\n",
      "Iteration 800, Loss: 0.5721\n",
      "Iteration 900, Loss: 0.5674\n",
      "Iteration 1000, Loss: 0.5631\n",
      "Iteration 1100, Loss: 0.5593\n",
      "Iteration 1200, Loss: 0.5558\n",
      "Iteration 1300, Loss: 0.5527\n",
      "Iteration 1400, Loss: 0.5499\n",
      "Iteration 1500, Loss: 0.5472\n",
      "Iteration 1600, Loss: 0.5448\n",
      "Iteration 1700, Loss: 0.5425\n",
      "Iteration 1800, Loss: 0.5404\n",
      "Iteration 1900, Loss: 0.5384\n",
      "Iteration 2000, Loss: 0.5365\n",
      "Iteration 2100, Loss: 0.5348\n",
      "Iteration 2200, Loss: 0.5331\n",
      "Iteration 2300, Loss: 0.5315\n",
      "Iteration 2400, Loss: 0.5300\n",
      "Iteration 2500, Loss: 0.5285\n",
      "Iteration 2600, Loss: 0.5271\n",
      "Iteration 2700, Loss: 0.5258\n",
      "Iteration 2800, Loss: 0.5245\n",
      "Iteration 2900, Loss: 0.5232\n",
      "Iteration 3000, Loss: 0.5221\n",
      "Iteration 3100, Loss: 0.5209\n",
      "Iteration 3200, Loss: 0.5198\n",
      "Iteration 3300, Loss: 0.5187\n",
      "Iteration 3400, Loss: 0.5177\n",
      "Iteration 3500, Loss: 0.5167\n",
      "Iteration 3600, Loss: 0.5157\n",
      "Iteration 3700, Loss: 0.5148\n",
      "Iteration 3800, Loss: 0.5139\n",
      "Iteration 3900, Loss: 0.5130\n",
      "Iteration 4000, Loss: 0.5121\n",
      "Iteration 4100, Loss: 0.5112\n",
      "Iteration 4200, Loss: 0.5104\n",
      "Iteration 4300, Loss: 0.5097\n",
      "Iteration 4400, Loss: 0.5089\n",
      "Iteration 4500, Loss: 0.5081\n",
      "Iteration 4600, Loss: 0.5073\n",
      "Iteration 4700, Loss: 0.5066\n",
      "Iteration 4800, Loss: 0.5059\n",
      "Iteration 4900, Loss: 0.5052\n",
      "Iteration 0, Loss: 1.0553\n",
      "Iteration 100, Loss: 0.6506\n",
      "Iteration 200, Loss: 0.6134\n",
      "Iteration 300, Loss: 0.5945\n",
      "Iteration 400, Loss: 0.5817\n",
      "Iteration 500, Loss: 0.5722\n",
      "Iteration 600, Loss: 0.5646\n",
      "Iteration 700, Loss: 0.5583\n",
      "Iteration 800, Loss: 0.5529\n",
      "Iteration 900, Loss: 0.5481\n",
      "Iteration 1000, Loss: 0.5439\n",
      "Iteration 1100, Loss: 0.5402\n",
      "Iteration 1200, Loss: 0.5368\n",
      "Iteration 1300, Loss: 0.5336\n",
      "Iteration 1400, Loss: 0.5307\n",
      "Iteration 1500, Loss: 0.5281\n",
      "Iteration 1600, Loss: 0.5256\n",
      "Iteration 1700, Loss: 0.5233\n",
      "Iteration 1800, Loss: 0.5210\n",
      "Iteration 1900, Loss: 0.5189\n",
      "Iteration 2000, Loss: 0.5170\n",
      "Iteration 2100, Loss: 0.5151\n",
      "Iteration 2200, Loss: 0.5133\n",
      "Iteration 2300, Loss: 0.5116\n",
      "Iteration 2400, Loss: 0.5100\n",
      "Iteration 2500, Loss: 0.5084\n",
      "Iteration 2600, Loss: 0.5069\n",
      "Iteration 2700, Loss: 0.5055\n",
      "Iteration 2800, Loss: 0.5041\n",
      "Iteration 2900, Loss: 0.5027\n",
      "Iteration 3000, Loss: 0.5014\n",
      "Iteration 3100, Loss: 0.5002\n",
      "Iteration 3200, Loss: 0.4990\n",
      "Iteration 3300, Loss: 0.4978\n",
      "Iteration 3400, Loss: 0.4966\n",
      "Iteration 3500, Loss: 0.4955\n",
      "Iteration 3600, Loss: 0.4945\n",
      "Iteration 3700, Loss: 0.4934\n",
      "Iteration 3800, Loss: 0.4924\n",
      "Iteration 3900, Loss: 0.4914\n",
      "Iteration 4000, Loss: 0.4905\n",
      "Iteration 4100, Loss: 0.4895\n",
      "Iteration 4200, Loss: 0.4886\n",
      "Iteration 4300, Loss: 0.4877\n",
      "Iteration 4400, Loss: 0.4869\n",
      "Iteration 4500, Loss: 0.4860\n",
      "Iteration 4600, Loss: 0.4852\n",
      "Iteration 4700, Loss: 0.4844\n",
      "Iteration 4800, Loss: 0.4836\n",
      "Iteration 4900, Loss: 0.4828\n",
      "120 270\n",
      "Iteration 0, Loss: 1.0889\n",
      "Iteration 100, Loss: 0.7829\n",
      "Iteration 200, Loss: 0.7205\n",
      "Iteration 300, Loss: 0.6918\n",
      "Iteration 400, Loss: 0.6742\n",
      "Iteration 500, Loss: 0.6617\n",
      "Iteration 600, Loss: 0.6520\n",
      "Iteration 700, Loss: 0.6443\n",
      "Iteration 800, Loss: 0.6378\n",
      "Iteration 900, Loss: 0.6323\n",
      "Iteration 1000, Loss: 0.6274\n",
      "Iteration 1100, Loss: 0.6232\n",
      "Iteration 1200, Loss: 0.6193\n",
      "Iteration 1300, Loss: 0.6158\n",
      "Iteration 1400, Loss: 0.6126\n",
      "Iteration 1500, Loss: 0.6096\n",
      "Iteration 1600, Loss: 0.6068\n",
      "Iteration 1700, Loss: 0.6043\n",
      "Iteration 1800, Loss: 0.6019\n",
      "Iteration 1900, Loss: 0.5996\n",
      "Iteration 2000, Loss: 0.5974\n",
      "Iteration 2100, Loss: 0.5954\n",
      "Iteration 2200, Loss: 0.5934\n",
      "Iteration 2300, Loss: 0.5916\n",
      "Iteration 2400, Loss: 0.5898\n",
      "Iteration 2500, Loss: 0.5881\n",
      "Iteration 2600, Loss: 0.5865\n",
      "Iteration 2700, Loss: 0.5849\n",
      "Iteration 2800, Loss: 0.5834\n",
      "Iteration 2900, Loss: 0.5820\n",
      "Iteration 3000, Loss: 0.5806\n",
      "Iteration 3100, Loss: 0.5792\n",
      "Iteration 3200, Loss: 0.5780\n",
      "Iteration 3300, Loss: 0.5767\n",
      "Iteration 3400, Loss: 0.5755\n",
      "Iteration 3500, Loss: 0.5743\n",
      "Iteration 3600, Loss: 0.5732\n",
      "Iteration 3700, Loss: 0.5721\n",
      "Iteration 3800, Loss: 0.5710\n",
      "Iteration 3900, Loss: 0.5700\n",
      "Iteration 4000, Loss: 0.5690\n",
      "Iteration 4100, Loss: 0.5680\n",
      "Iteration 4200, Loss: 0.5671\n",
      "Iteration 4300, Loss: 0.5661\n",
      "Iteration 4400, Loss: 0.5652\n",
      "Iteration 4500, Loss: 0.5644\n",
      "Iteration 4600, Loss: 0.5635\n",
      "Iteration 4700, Loss: 0.5627\n",
      "Iteration 4800, Loss: 0.5618\n",
      "Iteration 4900, Loss: 0.5610\n",
      "Iteration 0, Loss: 1.0897\n",
      "Iteration 100, Loss: 0.7901\n",
      "Iteration 200, Loss: 0.7213\n",
      "Iteration 300, Loss: 0.6897\n",
      "Iteration 400, Loss: 0.6702\n",
      "Iteration 500, Loss: 0.6563\n",
      "Iteration 600, Loss: 0.6457\n",
      "Iteration 700, Loss: 0.6371\n",
      "Iteration 800, Loss: 0.6299\n",
      "Iteration 900, Loss: 0.6238\n",
      "Iteration 1000, Loss: 0.6184\n",
      "Iteration 1100, Loss: 0.6136\n",
      "Iteration 1200, Loss: 0.6093\n",
      "Iteration 1300, Loss: 0.6054\n",
      "Iteration 1400, Loss: 0.6019\n",
      "Iteration 1500, Loss: 0.5986\n",
      "Iteration 1600, Loss: 0.5956\n",
      "Iteration 1700, Loss: 0.5927\n",
      "Iteration 1800, Loss: 0.5901\n",
      "Iteration 1900, Loss: 0.5876\n",
      "Iteration 2000, Loss: 0.5853\n",
      "Iteration 2100, Loss: 0.5831\n",
      "Iteration 2200, Loss: 0.5810\n",
      "Iteration 2300, Loss: 0.5790\n",
      "Iteration 2400, Loss: 0.5772\n",
      "Iteration 2500, Loss: 0.5754\n",
      "Iteration 2600, Loss: 0.5737\n",
      "Iteration 2700, Loss: 0.5720\n",
      "Iteration 2800, Loss: 0.5705\n",
      "Iteration 2900, Loss: 0.5690\n",
      "Iteration 3000, Loss: 0.5676\n",
      "Iteration 3100, Loss: 0.5662\n",
      "Iteration 3200, Loss: 0.5649\n",
      "Iteration 3300, Loss: 0.5636\n",
      "Iteration 3400, Loss: 0.5624\n",
      "Iteration 3500, Loss: 0.5612\n",
      "Iteration 3600, Loss: 0.5600\n",
      "Iteration 3700, Loss: 0.5589\n",
      "Iteration 3800, Loss: 0.5579\n",
      "Iteration 3900, Loss: 0.5568\n",
      "Iteration 4000, Loss: 0.5558\n",
      "Iteration 4100, Loss: 0.5548\n",
      "Iteration 4200, Loss: 0.5539\n",
      "Iteration 4300, Loss: 0.5530\n",
      "Iteration 4400, Loss: 0.5521\n",
      "Iteration 4500, Loss: 0.5512\n",
      "Iteration 4600, Loss: 0.5503\n",
      "Iteration 4700, Loss: 0.5495\n",
      "Iteration 4800, Loss: 0.5487\n",
      "Iteration 4900, Loss: 0.5479\n",
      "121 270\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7861\n",
      "Iteration 200, Loss: 0.7211\n",
      "Iteration 300, Loss: 0.6913\n",
      "Iteration 400, Loss: 0.6727\n",
      "Iteration 500, Loss: 0.6595\n",
      "Iteration 600, Loss: 0.6494\n",
      "Iteration 700, Loss: 0.6412\n",
      "Iteration 800, Loss: 0.6343\n",
      "Iteration 900, Loss: 0.6284\n",
      "Iteration 1000, Loss: 0.6232\n",
      "Iteration 1100, Loss: 0.6186\n",
      "Iteration 1200, Loss: 0.6145\n",
      "Iteration 1300, Loss: 0.6107\n",
      "Iteration 1400, Loss: 0.6073\n",
      "Iteration 1500, Loss: 0.6042\n",
      "Iteration 1600, Loss: 0.6012\n",
      "Iteration 1700, Loss: 0.5984\n",
      "Iteration 1800, Loss: 0.5958\n",
      "Iteration 1900, Loss: 0.5934\n",
      "Iteration 2000, Loss: 0.5911\n",
      "Iteration 2100, Loss: 0.5889\n",
      "Iteration 2200, Loss: 0.5869\n",
      "Iteration 2300, Loss: 0.5849\n",
      "Iteration 2400, Loss: 0.5830\n",
      "Iteration 2500, Loss: 0.5812\n",
      "Iteration 2600, Loss: 0.5795\n",
      "Iteration 2700, Loss: 0.5779\n",
      "Iteration 2800, Loss: 0.5763\n",
      "Iteration 2900, Loss: 0.5748\n",
      "Iteration 3000, Loss: 0.5733\n",
      "Iteration 3100, Loss: 0.5719\n",
      "Iteration 3200, Loss: 0.5705\n",
      "Iteration 3300, Loss: 0.5692\n",
      "Iteration 3400, Loss: 0.5679\n",
      "Iteration 3500, Loss: 0.5667\n",
      "Iteration 3600, Loss: 0.5655\n",
      "Iteration 3700, Loss: 0.5644\n",
      "Iteration 3800, Loss: 0.5632\n",
      "Iteration 3900, Loss: 0.5622\n",
      "Iteration 4000, Loss: 0.5611\n",
      "Iteration 4100, Loss: 0.5601\n",
      "Iteration 4200, Loss: 0.5591\n",
      "Iteration 4300, Loss: 0.5581\n",
      "Iteration 4400, Loss: 0.5571\n",
      "Iteration 4500, Loss: 0.5562\n",
      "Iteration 4600, Loss: 0.5553\n",
      "Iteration 4700, Loss: 0.5544\n",
      "Iteration 4800, Loss: 0.5536\n",
      "Iteration 4900, Loss: 0.5527\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7872\n",
      "Iteration 200, Loss: 0.7220\n",
      "Iteration 300, Loss: 0.6917\n",
      "Iteration 400, Loss: 0.6731\n",
      "Iteration 500, Loss: 0.6600\n",
      "Iteration 600, Loss: 0.6499\n",
      "Iteration 700, Loss: 0.6419\n",
      "Iteration 800, Loss: 0.6352\n",
      "Iteration 900, Loss: 0.6296\n",
      "Iteration 1000, Loss: 0.6246\n",
      "Iteration 1100, Loss: 0.6203\n",
      "Iteration 1200, Loss: 0.6165\n",
      "Iteration 1300, Loss: 0.6130\n",
      "Iteration 1400, Loss: 0.6098\n",
      "Iteration 1500, Loss: 0.6070\n",
      "Iteration 1600, Loss: 0.6043\n",
      "Iteration 1700, Loss: 0.6018\n",
      "Iteration 1800, Loss: 0.5995\n",
      "Iteration 1900, Loss: 0.5973\n",
      "Iteration 2000, Loss: 0.5952\n",
      "Iteration 2100, Loss: 0.5933\n",
      "Iteration 2200, Loss: 0.5915\n",
      "Iteration 2300, Loss: 0.5897\n",
      "Iteration 2400, Loss: 0.5880\n",
      "Iteration 2500, Loss: 0.5865\n",
      "Iteration 2600, Loss: 0.5849\n",
      "Iteration 2700, Loss: 0.5835\n",
      "Iteration 2800, Loss: 0.5821\n",
      "Iteration 2900, Loss: 0.5807\n",
      "Iteration 3000, Loss: 0.5794\n",
      "Iteration 3100, Loss: 0.5782\n",
      "Iteration 3200, Loss: 0.5770\n",
      "Iteration 3300, Loss: 0.5758\n",
      "Iteration 3400, Loss: 0.5747\n",
      "Iteration 3500, Loss: 0.5736\n",
      "Iteration 3600, Loss: 0.5726\n",
      "Iteration 3700, Loss: 0.5715\n",
      "Iteration 3800, Loss: 0.5705\n",
      "Iteration 3900, Loss: 0.5696\n",
      "Iteration 4000, Loss: 0.5687\n",
      "Iteration 4100, Loss: 0.5677\n",
      "Iteration 4200, Loss: 0.5669\n",
      "Iteration 4300, Loss: 0.5660\n",
      "Iteration 4400, Loss: 0.5652\n",
      "Iteration 4500, Loss: 0.5643\n",
      "Iteration 4600, Loss: 0.5635\n",
      "Iteration 4700, Loss: 0.5628\n",
      "Iteration 4800, Loss: 0.5620\n",
      "Iteration 4900, Loss: 0.5613\n",
      "122 270\n",
      "Iteration 0, Loss: 1.0886\n",
      "Iteration 100, Loss: 0.7777\n",
      "Iteration 200, Loss: 0.7110\n",
      "Iteration 300, Loss: 0.6796\n",
      "Iteration 400, Loss: 0.6602\n",
      "Iteration 500, Loss: 0.6465\n",
      "Iteration 600, Loss: 0.6361\n",
      "Iteration 700, Loss: 0.6278\n",
      "Iteration 800, Loss: 0.6209\n",
      "Iteration 900, Loss: 0.6151\n",
      "Iteration 1000, Loss: 0.6101\n",
      "Iteration 1100, Loss: 0.6057\n",
      "Iteration 1200, Loss: 0.6018\n",
      "Iteration 1300, Loss: 0.5983\n",
      "Iteration 1400, Loss: 0.5951\n",
      "Iteration 1500, Loss: 0.5922\n",
      "Iteration 1600, Loss: 0.5895\n",
      "Iteration 1700, Loss: 0.5869\n",
      "Iteration 1800, Loss: 0.5846\n",
      "Iteration 1900, Loss: 0.5824\n",
      "Iteration 2000, Loss: 0.5803\n",
      "Iteration 2100, Loss: 0.5784\n",
      "Iteration 2200, Loss: 0.5765\n",
      "Iteration 2300, Loss: 0.5748\n",
      "Iteration 2400, Loss: 0.5731\n",
      "Iteration 2500, Loss: 0.5715\n",
      "Iteration 2600, Loss: 0.5700\n",
      "Iteration 2700, Loss: 0.5685\n",
      "Iteration 2800, Loss: 0.5671\n",
      "Iteration 2900, Loss: 0.5658\n",
      "Iteration 3000, Loss: 0.5645\n",
      "Iteration 3100, Loss: 0.5632\n",
      "Iteration 3200, Loss: 0.5620\n",
      "Iteration 3300, Loss: 0.5608\n",
      "Iteration 3400, Loss: 0.5597\n",
      "Iteration 3500, Loss: 0.5586\n",
      "Iteration 3600, Loss: 0.5575\n",
      "Iteration 3700, Loss: 0.5565\n",
      "Iteration 3800, Loss: 0.5555\n",
      "Iteration 3900, Loss: 0.5545\n",
      "Iteration 4000, Loss: 0.5536\n",
      "Iteration 4100, Loss: 0.5526\n",
      "Iteration 4200, Loss: 0.5517\n",
      "Iteration 4300, Loss: 0.5509\n",
      "Iteration 4400, Loss: 0.5500\n",
      "Iteration 4500, Loss: 0.5492\n",
      "Iteration 4600, Loss: 0.5483\n",
      "Iteration 4700, Loss: 0.5475\n",
      "Iteration 4800, Loss: 0.5468\n",
      "Iteration 4900, Loss: 0.5460\n",
      "Iteration 0, Loss: 1.0899\n",
      "Iteration 100, Loss: 0.7974\n",
      "Iteration 200, Loss: 0.7344\n",
      "Iteration 300, Loss: 0.7059\n",
      "Iteration 400, Loss: 0.6884\n",
      "Iteration 500, Loss: 0.6759\n",
      "Iteration 600, Loss: 0.6662\n",
      "Iteration 700, Loss: 0.6584\n",
      "Iteration 800, Loss: 0.6518\n",
      "Iteration 900, Loss: 0.6461\n",
      "Iteration 1000, Loss: 0.6412\n",
      "Iteration 1100, Loss: 0.6368\n",
      "Iteration 1200, Loss: 0.6328\n",
      "Iteration 1300, Loss: 0.6291\n",
      "Iteration 1400, Loss: 0.6258\n",
      "Iteration 1500, Loss: 0.6227\n",
      "Iteration 1600, Loss: 0.6198\n",
      "Iteration 1700, Loss: 0.6171\n",
      "Iteration 1800, Loss: 0.6146\n",
      "Iteration 1900, Loss: 0.6122\n",
      "Iteration 2000, Loss: 0.6100\n",
      "Iteration 2100, Loss: 0.6078\n",
      "Iteration 2200, Loss: 0.6058\n",
      "Iteration 2300, Loss: 0.6039\n",
      "Iteration 2400, Loss: 0.6021\n",
      "Iteration 2500, Loss: 0.6003\n",
      "Iteration 2600, Loss: 0.5987\n",
      "Iteration 2700, Loss: 0.5971\n",
      "Iteration 2800, Loss: 0.5955\n",
      "Iteration 2900, Loss: 0.5941\n",
      "Iteration 3000, Loss: 0.5927\n",
      "Iteration 3100, Loss: 0.5913\n",
      "Iteration 3200, Loss: 0.5900\n",
      "Iteration 3300, Loss: 0.5887\n",
      "Iteration 3400, Loss: 0.5875\n",
      "Iteration 3500, Loss: 0.5863\n",
      "Iteration 3600, Loss: 0.5852\n",
      "Iteration 3700, Loss: 0.5841\n",
      "Iteration 3800, Loss: 0.5830\n",
      "Iteration 3900, Loss: 0.5820\n",
      "Iteration 4000, Loss: 0.5810\n",
      "Iteration 4100, Loss: 0.5800\n",
      "Iteration 4200, Loss: 0.5791\n",
      "Iteration 4300, Loss: 0.5781\n",
      "Iteration 4400, Loss: 0.5772\n",
      "Iteration 4500, Loss: 0.5764\n",
      "Iteration 4600, Loss: 0.5755\n",
      "Iteration 4700, Loss: 0.5747\n",
      "Iteration 4800, Loss: 0.5739\n",
      "Iteration 4900, Loss: 0.5731\n",
      "123 270\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7934\n",
      "Iteration 200, Loss: 0.7305\n",
      "Iteration 300, Loss: 0.7019\n",
      "Iteration 400, Loss: 0.6846\n",
      "Iteration 500, Loss: 0.6724\n",
      "Iteration 600, Loss: 0.6631\n",
      "Iteration 700, Loss: 0.6558\n",
      "Iteration 800, Loss: 0.6497\n",
      "Iteration 900, Loss: 0.6444\n",
      "Iteration 1000, Loss: 0.6399\n",
      "Iteration 1100, Loss: 0.6359\n",
      "Iteration 1200, Loss: 0.6323\n",
      "Iteration 1300, Loss: 0.6290\n",
      "Iteration 1400, Loss: 0.6260\n",
      "Iteration 1500, Loss: 0.6233\n",
      "Iteration 1600, Loss: 0.6208\n",
      "Iteration 1700, Loss: 0.6184\n",
      "Iteration 1800, Loss: 0.6162\n",
      "Iteration 1900, Loss: 0.6141\n",
      "Iteration 2000, Loss: 0.6121\n",
      "Iteration 2100, Loss: 0.6103\n",
      "Iteration 2200, Loss: 0.6085\n",
      "Iteration 2300, Loss: 0.6068\n",
      "Iteration 2400, Loss: 0.6052\n",
      "Iteration 2500, Loss: 0.6037\n",
      "Iteration 2600, Loss: 0.6022\n",
      "Iteration 2700, Loss: 0.6008\n",
      "Iteration 2800, Loss: 0.5995\n",
      "Iteration 2900, Loss: 0.5982\n",
      "Iteration 3000, Loss: 0.5969\n",
      "Iteration 3100, Loss: 0.5957\n",
      "Iteration 3200, Loss: 0.5945\n",
      "Iteration 3300, Loss: 0.5934\n",
      "Iteration 3400, Loss: 0.5923\n",
      "Iteration 3500, Loss: 0.5912\n",
      "Iteration 3600, Loss: 0.5902\n",
      "Iteration 3700, Loss: 0.5892\n",
      "Iteration 3800, Loss: 0.5882\n",
      "Iteration 3900, Loss: 0.5873\n",
      "Iteration 4000, Loss: 0.5864\n",
      "Iteration 4100, Loss: 0.5855\n",
      "Iteration 4200, Loss: 0.5846\n",
      "Iteration 4300, Loss: 0.5838\n",
      "Iteration 4400, Loss: 0.5829\n",
      "Iteration 4500, Loss: 0.5821\n",
      "Iteration 4600, Loss: 0.5813\n",
      "Iteration 4700, Loss: 0.5806\n",
      "Iteration 4800, Loss: 0.5798\n",
      "Iteration 4900, Loss: 0.5791\n",
      "Iteration 0, Loss: 1.0895\n",
      "Iteration 100, Loss: 0.7828\n",
      "Iteration 200, Loss: 0.7151\n",
      "Iteration 300, Loss: 0.6839\n",
      "Iteration 400, Loss: 0.6647\n",
      "Iteration 500, Loss: 0.6510\n",
      "Iteration 600, Loss: 0.6404\n",
      "Iteration 700, Loss: 0.6319\n",
      "Iteration 800, Loss: 0.6247\n",
      "Iteration 900, Loss: 0.6186\n",
      "Iteration 1000, Loss: 0.6132\n",
      "Iteration 1100, Loss: 0.6084\n",
      "Iteration 1200, Loss: 0.6041\n",
      "Iteration 1300, Loss: 0.6002\n",
      "Iteration 1400, Loss: 0.5967\n",
      "Iteration 1500, Loss: 0.5934\n",
      "Iteration 1600, Loss: 0.5904\n",
      "Iteration 1700, Loss: 0.5875\n",
      "Iteration 1800, Loss: 0.5849\n",
      "Iteration 1900, Loss: 0.5824\n",
      "Iteration 2000, Loss: 0.5800\n",
      "Iteration 2100, Loss: 0.5778\n",
      "Iteration 2200, Loss: 0.5757\n",
      "Iteration 2300, Loss: 0.5737\n",
      "Iteration 2400, Loss: 0.5719\n",
      "Iteration 2500, Loss: 0.5700\n",
      "Iteration 2600, Loss: 0.5683\n",
      "Iteration 2700, Loss: 0.5667\n",
      "Iteration 2800, Loss: 0.5651\n",
      "Iteration 2900, Loss: 0.5636\n",
      "Iteration 3000, Loss: 0.5621\n",
      "Iteration 3100, Loss: 0.5607\n",
      "Iteration 3200, Loss: 0.5594\n",
      "Iteration 3300, Loss: 0.5581\n",
      "Iteration 3400, Loss: 0.5568\n",
      "Iteration 3500, Loss: 0.5556\n",
      "Iteration 3600, Loss: 0.5545\n",
      "Iteration 3700, Loss: 0.5533\n",
      "Iteration 3800, Loss: 0.5522\n",
      "Iteration 3900, Loss: 0.5512\n",
      "Iteration 4000, Loss: 0.5502\n",
      "Iteration 4100, Loss: 0.5492\n",
      "Iteration 4200, Loss: 0.5482\n",
      "Iteration 4300, Loss: 0.5473\n",
      "Iteration 4400, Loss: 0.5463\n",
      "Iteration 4500, Loss: 0.5454\n",
      "Iteration 4600, Loss: 0.5446\n",
      "Iteration 4700, Loss: 0.5437\n",
      "Iteration 4800, Loss: 0.5429\n",
      "Iteration 4900, Loss: 0.5421\n",
      "124 270\n",
      "Iteration 0, Loss: 1.0896\n",
      "Iteration 100, Loss: 0.7883\n",
      "Iteration 200, Loss: 0.7250\n",
      "Iteration 300, Loss: 0.6973\n",
      "Iteration 400, Loss: 0.6805\n",
      "Iteration 500, Loss: 0.6686\n",
      "Iteration 600, Loss: 0.6594\n",
      "Iteration 700, Loss: 0.6519\n",
      "Iteration 800, Loss: 0.6455\n",
      "Iteration 900, Loss: 0.6400\n",
      "Iteration 1000, Loss: 0.6353\n",
      "Iteration 1100, Loss: 0.6310\n",
      "Iteration 1200, Loss: 0.6271\n",
      "Iteration 1300, Loss: 0.6236\n",
      "Iteration 1400, Loss: 0.6203\n",
      "Iteration 1500, Loss: 0.6173\n",
      "Iteration 1600, Loss: 0.6145\n",
      "Iteration 1700, Loss: 0.6119\n",
      "Iteration 1800, Loss: 0.6094\n",
      "Iteration 1900, Loss: 0.6071\n",
      "Iteration 2000, Loss: 0.6049\n",
      "Iteration 2100, Loss: 0.6028\n",
      "Iteration 2200, Loss: 0.6008\n",
      "Iteration 2300, Loss: 0.5990\n",
      "Iteration 2400, Loss: 0.5972\n",
      "Iteration 2500, Loss: 0.5954\n",
      "Iteration 2600, Loss: 0.5938\n",
      "Iteration 2700, Loss: 0.5922\n",
      "Iteration 2800, Loss: 0.5907\n",
      "Iteration 2900, Loss: 0.5893\n",
      "Iteration 3000, Loss: 0.5879\n",
      "Iteration 3100, Loss: 0.5866\n",
      "Iteration 3200, Loss: 0.5853\n",
      "Iteration 3300, Loss: 0.5841\n",
      "Iteration 3400, Loss: 0.5829\n",
      "Iteration 3500, Loss: 0.5817\n",
      "Iteration 3600, Loss: 0.5806\n",
      "Iteration 3700, Loss: 0.5795\n",
      "Iteration 3800, Loss: 0.5784\n",
      "Iteration 3900, Loss: 0.5774\n",
      "Iteration 4000, Loss: 0.5764\n",
      "Iteration 4100, Loss: 0.5754\n",
      "Iteration 4200, Loss: 0.5745\n",
      "Iteration 4300, Loss: 0.5736\n",
      "Iteration 4400, Loss: 0.5727\n",
      "Iteration 4500, Loss: 0.5718\n",
      "Iteration 4600, Loss: 0.5710\n",
      "Iteration 4700, Loss: 0.5701\n",
      "Iteration 4800, Loss: 0.5693\n",
      "Iteration 4900, Loss: 0.5686\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7848\n",
      "Iteration 200, Loss: 0.7180\n",
      "Iteration 300, Loss: 0.6860\n",
      "Iteration 400, Loss: 0.6662\n",
      "Iteration 500, Loss: 0.6522\n",
      "Iteration 600, Loss: 0.6416\n",
      "Iteration 700, Loss: 0.6331\n",
      "Iteration 800, Loss: 0.6261\n",
      "Iteration 900, Loss: 0.6203\n",
      "Iteration 1000, Loss: 0.6152\n",
      "Iteration 1100, Loss: 0.6108\n",
      "Iteration 1200, Loss: 0.6068\n",
      "Iteration 1300, Loss: 0.6033\n",
      "Iteration 1400, Loss: 0.6001\n",
      "Iteration 1500, Loss: 0.5971\n",
      "Iteration 1600, Loss: 0.5944\n",
      "Iteration 1700, Loss: 0.5919\n",
      "Iteration 1800, Loss: 0.5895\n",
      "Iteration 1900, Loss: 0.5874\n",
      "Iteration 2000, Loss: 0.5853\n",
      "Iteration 2100, Loss: 0.5833\n",
      "Iteration 2200, Loss: 0.5815\n",
      "Iteration 2300, Loss: 0.5798\n",
      "Iteration 2400, Loss: 0.5781\n",
      "Iteration 2500, Loss: 0.5765\n",
      "Iteration 2600, Loss: 0.5750\n",
      "Iteration 2700, Loss: 0.5736\n",
      "Iteration 2800, Loss: 0.5722\n",
      "Iteration 2900, Loss: 0.5708\n",
      "Iteration 3000, Loss: 0.5696\n",
      "Iteration 3100, Loss: 0.5683\n",
      "Iteration 3200, Loss: 0.5671\n",
      "Iteration 3300, Loss: 0.5660\n",
      "Iteration 3400, Loss: 0.5649\n",
      "Iteration 3500, Loss: 0.5638\n",
      "Iteration 3600, Loss: 0.5628\n",
      "Iteration 3700, Loss: 0.5618\n",
      "Iteration 3800, Loss: 0.5608\n",
      "Iteration 3900, Loss: 0.5599\n",
      "Iteration 4000, Loss: 0.5590\n",
      "Iteration 4100, Loss: 0.5581\n",
      "Iteration 4200, Loss: 0.5572\n",
      "Iteration 4300, Loss: 0.5564\n",
      "Iteration 4400, Loss: 0.5555\n",
      "Iteration 4500, Loss: 0.5547\n",
      "Iteration 4600, Loss: 0.5540\n",
      "Iteration 4700, Loss: 0.5532\n",
      "Iteration 4800, Loss: 0.5525\n",
      "Iteration 4900, Loss: 0.5517\n",
      "125 270\n",
      "Iteration 0, Loss: 1.0889\n",
      "Iteration 100, Loss: 0.7864\n",
      "Iteration 200, Loss: 0.7198\n",
      "Iteration 300, Loss: 0.6877\n",
      "Iteration 400, Loss: 0.6673\n",
      "Iteration 500, Loss: 0.6527\n",
      "Iteration 600, Loss: 0.6414\n",
      "Iteration 700, Loss: 0.6322\n",
      "Iteration 800, Loss: 0.6246\n",
      "Iteration 900, Loss: 0.6180\n",
      "Iteration 1000, Loss: 0.6123\n",
      "Iteration 1100, Loss: 0.6072\n",
      "Iteration 1200, Loss: 0.6026\n",
      "Iteration 1300, Loss: 0.5985\n",
      "Iteration 1400, Loss: 0.5947\n",
      "Iteration 1500, Loss: 0.5912\n",
      "Iteration 1600, Loss: 0.5879\n",
      "Iteration 1700, Loss: 0.5849\n",
      "Iteration 1800, Loss: 0.5820\n",
      "Iteration 1900, Loss: 0.5794\n",
      "Iteration 2000, Loss: 0.5768\n",
      "Iteration 2100, Loss: 0.5744\n",
      "Iteration 2200, Loss: 0.5722\n",
      "Iteration 2300, Loss: 0.5700\n",
      "Iteration 2400, Loss: 0.5679\n",
      "Iteration 2500, Loss: 0.5660\n",
      "Iteration 2600, Loss: 0.5641\n",
      "Iteration 2700, Loss: 0.5623\n",
      "Iteration 2800, Loss: 0.5606\n",
      "Iteration 2900, Loss: 0.5589\n",
      "Iteration 3000, Loss: 0.5573\n",
      "Iteration 3100, Loss: 0.5557\n",
      "Iteration 3200, Loss: 0.5543\n",
      "Iteration 3300, Loss: 0.5528\n",
      "Iteration 3400, Loss: 0.5514\n",
      "Iteration 3500, Loss: 0.5501\n",
      "Iteration 3600, Loss: 0.5488\n",
      "Iteration 3700, Loss: 0.5475\n",
      "Iteration 3800, Loss: 0.5463\n",
      "Iteration 3900, Loss: 0.5451\n",
      "Iteration 4000, Loss: 0.5440\n",
      "Iteration 4100, Loss: 0.5429\n",
      "Iteration 4200, Loss: 0.5418\n",
      "Iteration 4300, Loss: 0.5407\n",
      "Iteration 4400, Loss: 0.5397\n",
      "Iteration 4500, Loss: 0.5387\n",
      "Iteration 4600, Loss: 0.5377\n",
      "Iteration 4700, Loss: 0.5367\n",
      "Iteration 4800, Loss: 0.5358\n",
      "Iteration 4900, Loss: 0.5349\n",
      "Iteration 0, Loss: 1.0895\n",
      "Iteration 100, Loss: 0.7846\n",
      "Iteration 200, Loss: 0.7202\n",
      "Iteration 300, Loss: 0.6923\n",
      "Iteration 400, Loss: 0.6755\n",
      "Iteration 500, Loss: 0.6638\n",
      "Iteration 600, Loss: 0.6548\n",
      "Iteration 700, Loss: 0.6476\n",
      "Iteration 800, Loss: 0.6416\n",
      "Iteration 900, Loss: 0.6365\n",
      "Iteration 1000, Loss: 0.6320\n",
      "Iteration 1100, Loss: 0.6280\n",
      "Iteration 1200, Loss: 0.6244\n",
      "Iteration 1300, Loss: 0.6211\n",
      "Iteration 1400, Loss: 0.6180\n",
      "Iteration 1500, Loss: 0.6152\n",
      "Iteration 1600, Loss: 0.6126\n",
      "Iteration 1700, Loss: 0.6102\n",
      "Iteration 1800, Loss: 0.6079\n",
      "Iteration 1900, Loss: 0.6058\n",
      "Iteration 2000, Loss: 0.6037\n",
      "Iteration 2100, Loss: 0.6018\n",
      "Iteration 2200, Loss: 0.5999\n",
      "Iteration 2300, Loss: 0.5982\n",
      "Iteration 2400, Loss: 0.5965\n",
      "Iteration 2500, Loss: 0.5949\n",
      "Iteration 2600, Loss: 0.5934\n",
      "Iteration 2700, Loss: 0.5919\n",
      "Iteration 2800, Loss: 0.5905\n",
      "Iteration 2900, Loss: 0.5891\n",
      "Iteration 3000, Loss: 0.5878\n",
      "Iteration 3100, Loss: 0.5865\n",
      "Iteration 3200, Loss: 0.5853\n",
      "Iteration 3300, Loss: 0.5841\n",
      "Iteration 3400, Loss: 0.5830\n",
      "Iteration 3500, Loss: 0.5819\n",
      "Iteration 3600, Loss: 0.5808\n",
      "Iteration 3700, Loss: 0.5797\n",
      "Iteration 3800, Loss: 0.5787\n",
      "Iteration 3900, Loss: 0.5777\n",
      "Iteration 4000, Loss: 0.5768\n",
      "Iteration 4100, Loss: 0.5758\n",
      "Iteration 4200, Loss: 0.5749\n",
      "Iteration 4300, Loss: 0.5740\n",
      "Iteration 4400, Loss: 0.5731\n",
      "Iteration 4500, Loss: 0.5723\n",
      "Iteration 4600, Loss: 0.5715\n",
      "Iteration 4700, Loss: 0.5706\n",
      "Iteration 4800, Loss: 0.5698\n",
      "Iteration 4900, Loss: 0.5691\n",
      "126 270\n",
      "Iteration 0, Loss: 1.0898\n",
      "Iteration 100, Loss: 0.7950\n",
      "Iteration 200, Loss: 0.7297\n",
      "Iteration 300, Loss: 0.6994\n",
      "Iteration 400, Loss: 0.6804\n",
      "Iteration 500, Loss: 0.6667\n",
      "Iteration 600, Loss: 0.6561\n",
      "Iteration 700, Loss: 0.6474\n",
      "Iteration 800, Loss: 0.6401\n",
      "Iteration 900, Loss: 0.6339\n",
      "Iteration 1000, Loss: 0.6284\n",
      "Iteration 1100, Loss: 0.6234\n",
      "Iteration 1200, Loss: 0.6190\n",
      "Iteration 1300, Loss: 0.6150\n",
      "Iteration 1400, Loss: 0.6113\n",
      "Iteration 1500, Loss: 0.6078\n",
      "Iteration 1600, Loss: 0.6046\n",
      "Iteration 1700, Loss: 0.6016\n",
      "Iteration 1800, Loss: 0.5988\n",
      "Iteration 1900, Loss: 0.5962\n",
      "Iteration 2000, Loss: 0.5937\n",
      "Iteration 2100, Loss: 0.5913\n",
      "Iteration 2200, Loss: 0.5890\n",
      "Iteration 2300, Loss: 0.5868\n",
      "Iteration 2400, Loss: 0.5848\n",
      "Iteration 2500, Loss: 0.5828\n",
      "Iteration 2600, Loss: 0.5809\n",
      "Iteration 2700, Loss: 0.5791\n",
      "Iteration 2800, Loss: 0.5774\n",
      "Iteration 2900, Loss: 0.5757\n",
      "Iteration 3000, Loss: 0.5741\n",
      "Iteration 3100, Loss: 0.5725\n",
      "Iteration 3200, Loss: 0.5710\n",
      "Iteration 3300, Loss: 0.5696\n",
      "Iteration 3400, Loss: 0.5682\n",
      "Iteration 3500, Loss: 0.5668\n",
      "Iteration 3600, Loss: 0.5655\n",
      "Iteration 3700, Loss: 0.5642\n",
      "Iteration 3800, Loss: 0.5630\n",
      "Iteration 3900, Loss: 0.5618\n",
      "Iteration 4000, Loss: 0.5606\n",
      "Iteration 4100, Loss: 0.5595\n",
      "Iteration 4200, Loss: 0.5584\n",
      "Iteration 4300, Loss: 0.5574\n",
      "Iteration 4400, Loss: 0.5563\n",
      "Iteration 4500, Loss: 0.5553\n",
      "Iteration 4600, Loss: 0.5543\n",
      "Iteration 4700, Loss: 0.5533\n",
      "Iteration 4800, Loss: 0.5524\n",
      "Iteration 4900, Loss: 0.5515\n",
      "Iteration 0, Loss: 1.0889\n",
      "Iteration 100, Loss: 0.7773\n",
      "Iteration 200, Loss: 0.7116\n",
      "Iteration 300, Loss: 0.6817\n",
      "Iteration 400, Loss: 0.6637\n",
      "Iteration 500, Loss: 0.6510\n",
      "Iteration 600, Loss: 0.6413\n",
      "Iteration 700, Loss: 0.6335\n",
      "Iteration 800, Loss: 0.6270\n",
      "Iteration 900, Loss: 0.6215\n",
      "Iteration 1000, Loss: 0.6167\n",
      "Iteration 1100, Loss: 0.6124\n",
      "Iteration 1200, Loss: 0.6086\n",
      "Iteration 1300, Loss: 0.6051\n",
      "Iteration 1400, Loss: 0.6019\n",
      "Iteration 1500, Loss: 0.5990\n",
      "Iteration 1600, Loss: 0.5963\n",
      "Iteration 1700, Loss: 0.5937\n",
      "Iteration 1800, Loss: 0.5914\n",
      "Iteration 1900, Loss: 0.5891\n",
      "Iteration 2000, Loss: 0.5870\n",
      "Iteration 2100, Loss: 0.5850\n",
      "Iteration 2200, Loss: 0.5831\n",
      "Iteration 2300, Loss: 0.5813\n",
      "Iteration 2400, Loss: 0.5795\n",
      "Iteration 2500, Loss: 0.5779\n",
      "Iteration 2600, Loss: 0.5763\n",
      "Iteration 2700, Loss: 0.5747\n",
      "Iteration 2800, Loss: 0.5733\n",
      "Iteration 2900, Loss: 0.5719\n",
      "Iteration 3000, Loss: 0.5705\n",
      "Iteration 3100, Loss: 0.5692\n",
      "Iteration 3200, Loss: 0.5679\n",
      "Iteration 3300, Loss: 0.5667\n",
      "Iteration 3400, Loss: 0.5655\n",
      "Iteration 3500, Loss: 0.5643\n",
      "Iteration 3600, Loss: 0.5632\n",
      "Iteration 3700, Loss: 0.5621\n",
      "Iteration 3800, Loss: 0.5610\n",
      "Iteration 3900, Loss: 0.5600\n",
      "Iteration 4000, Loss: 0.5590\n",
      "Iteration 4100, Loss: 0.5580\n",
      "Iteration 4200, Loss: 0.5570\n",
      "Iteration 4300, Loss: 0.5561\n",
      "Iteration 4400, Loss: 0.5552\n",
      "Iteration 4500, Loss: 0.5543\n",
      "Iteration 4600, Loss: 0.5534\n",
      "Iteration 4700, Loss: 0.5526\n",
      "Iteration 4800, Loss: 0.5517\n",
      "Iteration 4900, Loss: 0.5509\n",
      "127 270\n",
      "Iteration 0, Loss: 1.0895\n",
      "Iteration 100, Loss: 0.7981\n",
      "Iteration 200, Loss: 0.7350\n",
      "Iteration 300, Loss: 0.7053\n",
      "Iteration 400, Loss: 0.6869\n",
      "Iteration 500, Loss: 0.6739\n",
      "Iteration 600, Loss: 0.6640\n",
      "Iteration 700, Loss: 0.6561\n",
      "Iteration 800, Loss: 0.6495\n",
      "Iteration 900, Loss: 0.6439\n",
      "Iteration 1000, Loss: 0.6391\n",
      "Iteration 1100, Loss: 0.6348\n",
      "Iteration 1200, Loss: 0.6309\n",
      "Iteration 1300, Loss: 0.6274\n",
      "Iteration 1400, Loss: 0.6242\n",
      "Iteration 1500, Loss: 0.6212\n",
      "Iteration 1600, Loss: 0.6184\n",
      "Iteration 1700, Loss: 0.6159\n",
      "Iteration 1800, Loss: 0.6134\n",
      "Iteration 1900, Loss: 0.6111\n",
      "Iteration 2000, Loss: 0.6090\n",
      "Iteration 2100, Loss: 0.6069\n",
      "Iteration 2200, Loss: 0.6050\n",
      "Iteration 2300, Loss: 0.6031\n",
      "Iteration 2400, Loss: 0.6013\n",
      "Iteration 2500, Loss: 0.5996\n",
      "Iteration 2600, Loss: 0.5980\n",
      "Iteration 2700, Loss: 0.5964\n",
      "Iteration 2800, Loss: 0.5949\n",
      "Iteration 2900, Loss: 0.5935\n",
      "Iteration 3000, Loss: 0.5921\n",
      "Iteration 3100, Loss: 0.5908\n",
      "Iteration 3200, Loss: 0.5895\n",
      "Iteration 3300, Loss: 0.5882\n",
      "Iteration 3400, Loss: 0.5870\n",
      "Iteration 3500, Loss: 0.5858\n",
      "Iteration 3600, Loss: 0.5847\n",
      "Iteration 3700, Loss: 0.5836\n",
      "Iteration 3800, Loss: 0.5825\n",
      "Iteration 3900, Loss: 0.5815\n",
      "Iteration 4000, Loss: 0.5804\n",
      "Iteration 4100, Loss: 0.5795\n",
      "Iteration 4200, Loss: 0.5785\n",
      "Iteration 4300, Loss: 0.5776\n",
      "Iteration 4400, Loss: 0.5766\n",
      "Iteration 4500, Loss: 0.5758\n",
      "Iteration 4600, Loss: 0.5749\n",
      "Iteration 4700, Loss: 0.5740\n",
      "Iteration 4800, Loss: 0.5732\n",
      "Iteration 4900, Loss: 0.5724\n",
      "Iteration 0, Loss: 1.0889\n",
      "Iteration 100, Loss: 0.7741\n",
      "Iteration 200, Loss: 0.7064\n",
      "Iteration 300, Loss: 0.6760\n",
      "Iteration 400, Loss: 0.6575\n",
      "Iteration 500, Loss: 0.6442\n",
      "Iteration 600, Loss: 0.6339\n",
      "Iteration 700, Loss: 0.6255\n",
      "Iteration 800, Loss: 0.6185\n",
      "Iteration 900, Loss: 0.6124\n",
      "Iteration 1000, Loss: 0.6071\n",
      "Iteration 1100, Loss: 0.6024\n",
      "Iteration 1200, Loss: 0.5981\n",
      "Iteration 1300, Loss: 0.5942\n",
      "Iteration 1400, Loss: 0.5906\n",
      "Iteration 1500, Loss: 0.5873\n",
      "Iteration 1600, Loss: 0.5843\n",
      "Iteration 1700, Loss: 0.5814\n",
      "Iteration 1800, Loss: 0.5788\n",
      "Iteration 1900, Loss: 0.5763\n",
      "Iteration 2000, Loss: 0.5739\n",
      "Iteration 2100, Loss: 0.5717\n",
      "Iteration 2200, Loss: 0.5695\n",
      "Iteration 2300, Loss: 0.5675\n",
      "Iteration 2400, Loss: 0.5656\n",
      "Iteration 2500, Loss: 0.5638\n",
      "Iteration 2600, Loss: 0.5620\n",
      "Iteration 2700, Loss: 0.5603\n",
      "Iteration 2800, Loss: 0.5587\n",
      "Iteration 2900, Loss: 0.5571\n",
      "Iteration 3000, Loss: 0.5556\n",
      "Iteration 3100, Loss: 0.5542\n",
      "Iteration 3200, Loss: 0.5528\n",
      "Iteration 3300, Loss: 0.5514\n",
      "Iteration 3400, Loss: 0.5501\n",
      "Iteration 3500, Loss: 0.5489\n",
      "Iteration 3600, Loss: 0.5476\n",
      "Iteration 3700, Loss: 0.5464\n",
      "Iteration 3800, Loss: 0.5453\n",
      "Iteration 3900, Loss: 0.5442\n",
      "Iteration 4000, Loss: 0.5431\n",
      "Iteration 4100, Loss: 0.5420\n",
      "Iteration 4200, Loss: 0.5410\n",
      "Iteration 4300, Loss: 0.5400\n",
      "Iteration 4400, Loss: 0.5390\n",
      "Iteration 4500, Loss: 0.5380\n",
      "Iteration 4600, Loss: 0.5371\n",
      "Iteration 4700, Loss: 0.5362\n",
      "Iteration 4800, Loss: 0.5353\n",
      "Iteration 4900, Loss: 0.5344\n",
      "128 270\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7870\n",
      "Iteration 200, Loss: 0.7244\n",
      "Iteration 300, Loss: 0.6964\n",
      "Iteration 400, Loss: 0.6791\n",
      "Iteration 500, Loss: 0.6667\n",
      "Iteration 600, Loss: 0.6572\n",
      "Iteration 700, Loss: 0.6494\n",
      "Iteration 800, Loss: 0.6429\n",
      "Iteration 900, Loss: 0.6372\n",
      "Iteration 1000, Loss: 0.6323\n",
      "Iteration 1100, Loss: 0.6279\n",
      "Iteration 1200, Loss: 0.6239\n",
      "Iteration 1300, Loss: 0.6203\n",
      "Iteration 1400, Loss: 0.6169\n",
      "Iteration 1500, Loss: 0.6138\n",
      "Iteration 1600, Loss: 0.6109\n",
      "Iteration 1700, Loss: 0.6082\n",
      "Iteration 1800, Loss: 0.6056\n",
      "Iteration 1900, Loss: 0.6032\n",
      "Iteration 2000, Loss: 0.6009\n",
      "Iteration 2100, Loss: 0.5987\n",
      "Iteration 2200, Loss: 0.5966\n",
      "Iteration 2300, Loss: 0.5946\n",
      "Iteration 2400, Loss: 0.5927\n",
      "Iteration 2500, Loss: 0.5909\n",
      "Iteration 2600, Loss: 0.5892\n",
      "Iteration 2700, Loss: 0.5875\n",
      "Iteration 2800, Loss: 0.5858\n",
      "Iteration 2900, Loss: 0.5843\n",
      "Iteration 3000, Loss: 0.5828\n",
      "Iteration 3100, Loss: 0.5813\n",
      "Iteration 3200, Loss: 0.5799\n",
      "Iteration 3300, Loss: 0.5785\n",
      "Iteration 3400, Loss: 0.5772\n",
      "Iteration 3500, Loss: 0.5759\n",
      "Iteration 3600, Loss: 0.5747\n",
      "Iteration 3700, Loss: 0.5735\n",
      "Iteration 3800, Loss: 0.5723\n",
      "Iteration 3900, Loss: 0.5712\n",
      "Iteration 4000, Loss: 0.5701\n",
      "Iteration 4100, Loss: 0.5690\n",
      "Iteration 4200, Loss: 0.5680\n",
      "Iteration 4300, Loss: 0.5669\n",
      "Iteration 4400, Loss: 0.5659\n",
      "Iteration 4500, Loss: 0.5650\n",
      "Iteration 4600, Loss: 0.5640\n",
      "Iteration 4700, Loss: 0.5631\n",
      "Iteration 4800, Loss: 0.5622\n",
      "Iteration 4900, Loss: 0.5613\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7850\n",
      "Iteration 200, Loss: 0.7172\n",
      "Iteration 300, Loss: 0.6854\n",
      "Iteration 400, Loss: 0.6657\n",
      "Iteration 500, Loss: 0.6517\n",
      "Iteration 600, Loss: 0.6410\n",
      "Iteration 700, Loss: 0.6324\n",
      "Iteration 800, Loss: 0.6253\n",
      "Iteration 900, Loss: 0.6193\n",
      "Iteration 1000, Loss: 0.6141\n",
      "Iteration 1100, Loss: 0.6095\n",
      "Iteration 1200, Loss: 0.6054\n",
      "Iteration 1300, Loss: 0.6018\n",
      "Iteration 1400, Loss: 0.5984\n",
      "Iteration 1500, Loss: 0.5954\n",
      "Iteration 1600, Loss: 0.5925\n",
      "Iteration 1700, Loss: 0.5899\n",
      "Iteration 1800, Loss: 0.5874\n",
      "Iteration 1900, Loss: 0.5851\n",
      "Iteration 2000, Loss: 0.5829\n",
      "Iteration 2100, Loss: 0.5809\n",
      "Iteration 2200, Loss: 0.5789\n",
      "Iteration 2300, Loss: 0.5771\n",
      "Iteration 2400, Loss: 0.5753\n",
      "Iteration 2500, Loss: 0.5737\n",
      "Iteration 2600, Loss: 0.5720\n",
      "Iteration 2700, Loss: 0.5705\n",
      "Iteration 2800, Loss: 0.5690\n",
      "Iteration 2900, Loss: 0.5676\n",
      "Iteration 3000, Loss: 0.5662\n",
      "Iteration 3100, Loss: 0.5649\n",
      "Iteration 3200, Loss: 0.5636\n",
      "Iteration 3300, Loss: 0.5624\n",
      "Iteration 3400, Loss: 0.5612\n",
      "Iteration 3500, Loss: 0.5601\n",
      "Iteration 3600, Loss: 0.5590\n",
      "Iteration 3700, Loss: 0.5579\n",
      "Iteration 3800, Loss: 0.5568\n",
      "Iteration 3900, Loss: 0.5558\n",
      "Iteration 4000, Loss: 0.5548\n",
      "Iteration 4100, Loss: 0.5539\n",
      "Iteration 4200, Loss: 0.5530\n",
      "Iteration 4300, Loss: 0.5521\n",
      "Iteration 4400, Loss: 0.5512\n",
      "Iteration 4500, Loss: 0.5503\n",
      "Iteration 4600, Loss: 0.5495\n",
      "Iteration 4700, Loss: 0.5487\n",
      "Iteration 4800, Loss: 0.5479\n",
      "Iteration 4900, Loss: 0.5471\n",
      "129 270\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7881\n",
      "Iteration 200, Loss: 0.7219\n",
      "Iteration 300, Loss: 0.6913\n",
      "Iteration 400, Loss: 0.6726\n",
      "Iteration 500, Loss: 0.6594\n",
      "Iteration 600, Loss: 0.6494\n",
      "Iteration 700, Loss: 0.6414\n",
      "Iteration 800, Loss: 0.6348\n",
      "Iteration 900, Loss: 0.6292\n",
      "Iteration 1000, Loss: 0.6244\n",
      "Iteration 1100, Loss: 0.6201\n",
      "Iteration 1200, Loss: 0.6163\n",
      "Iteration 1300, Loss: 0.6128\n",
      "Iteration 1400, Loss: 0.6096\n",
      "Iteration 1500, Loss: 0.6066\n",
      "Iteration 1600, Loss: 0.6039\n",
      "Iteration 1700, Loss: 0.6013\n",
      "Iteration 1800, Loss: 0.5989\n",
      "Iteration 1900, Loss: 0.5966\n",
      "Iteration 2000, Loss: 0.5944\n",
      "Iteration 2100, Loss: 0.5924\n",
      "Iteration 2200, Loss: 0.5904\n",
      "Iteration 2300, Loss: 0.5885\n",
      "Iteration 2400, Loss: 0.5868\n",
      "Iteration 2500, Loss: 0.5850\n",
      "Iteration 2600, Loss: 0.5834\n",
      "Iteration 2700, Loss: 0.5818\n",
      "Iteration 2800, Loss: 0.5803\n",
      "Iteration 2900, Loss: 0.5788\n",
      "Iteration 3000, Loss: 0.5774\n",
      "Iteration 3100, Loss: 0.5761\n",
      "Iteration 3200, Loss: 0.5747\n",
      "Iteration 3300, Loss: 0.5734\n",
      "Iteration 3400, Loss: 0.5722\n",
      "Iteration 3500, Loss: 0.5710\n",
      "Iteration 3600, Loss: 0.5698\n",
      "Iteration 3700, Loss: 0.5687\n",
      "Iteration 3800, Loss: 0.5676\n",
      "Iteration 3900, Loss: 0.5665\n",
      "Iteration 4000, Loss: 0.5655\n",
      "Iteration 4100, Loss: 0.5645\n",
      "Iteration 4200, Loss: 0.5635\n",
      "Iteration 4300, Loss: 0.5625\n",
      "Iteration 4400, Loss: 0.5616\n",
      "Iteration 4500, Loss: 0.5607\n",
      "Iteration 4600, Loss: 0.5598\n",
      "Iteration 4700, Loss: 0.5589\n",
      "Iteration 4800, Loss: 0.5581\n",
      "Iteration 4900, Loss: 0.5572\n",
      "Iteration 0, Loss: 1.0890\n",
      "Iteration 100, Loss: 0.7850\n",
      "Iteration 200, Loss: 0.7204\n",
      "Iteration 300, Loss: 0.6907\n",
      "Iteration 400, Loss: 0.6723\n",
      "Iteration 500, Loss: 0.6590\n",
      "Iteration 600, Loss: 0.6488\n",
      "Iteration 700, Loss: 0.6405\n",
      "Iteration 800, Loss: 0.6336\n",
      "Iteration 900, Loss: 0.6277\n",
      "Iteration 1000, Loss: 0.6226\n",
      "Iteration 1100, Loss: 0.6181\n",
      "Iteration 1200, Loss: 0.6140\n",
      "Iteration 1300, Loss: 0.6103\n",
      "Iteration 1400, Loss: 0.6069\n",
      "Iteration 1500, Loss: 0.6037\n",
      "Iteration 1600, Loss: 0.6009\n",
      "Iteration 1700, Loss: 0.5982\n",
      "Iteration 1800, Loss: 0.5956\n",
      "Iteration 1900, Loss: 0.5933\n",
      "Iteration 2000, Loss: 0.5910\n",
      "Iteration 2100, Loss: 0.5889\n",
      "Iteration 2200, Loss: 0.5869\n",
      "Iteration 2300, Loss: 0.5850\n",
      "Iteration 2400, Loss: 0.5832\n",
      "Iteration 2500, Loss: 0.5815\n",
      "Iteration 2600, Loss: 0.5798\n",
      "Iteration 2700, Loss: 0.5782\n",
      "Iteration 2800, Loss: 0.5767\n",
      "Iteration 2900, Loss: 0.5752\n",
      "Iteration 3000, Loss: 0.5738\n",
      "Iteration 3100, Loss: 0.5725\n",
      "Iteration 3200, Loss: 0.5712\n",
      "Iteration 3300, Loss: 0.5699\n",
      "Iteration 3400, Loss: 0.5687\n",
      "Iteration 3500, Loss: 0.5675\n",
      "Iteration 3600, Loss: 0.5664\n",
      "Iteration 3700, Loss: 0.5652\n",
      "Iteration 3800, Loss: 0.5642\n",
      "Iteration 3900, Loss: 0.5631\n",
      "Iteration 4000, Loss: 0.5621\n",
      "Iteration 4100, Loss: 0.5611\n",
      "Iteration 4200, Loss: 0.5602\n",
      "Iteration 4300, Loss: 0.5592\n",
      "Iteration 4400, Loss: 0.5583\n",
      "Iteration 4500, Loss: 0.5574\n",
      "Iteration 4600, Loss: 0.5566\n",
      "Iteration 4700, Loss: 0.5557\n",
      "Iteration 4800, Loss: 0.5549\n",
      "Iteration 4900, Loss: 0.5541\n",
      "130 270\n",
      "Iteration 0, Loss: 1.0889\n",
      "Iteration 100, Loss: 0.7744\n",
      "Iteration 200, Loss: 0.7061\n",
      "Iteration 300, Loss: 0.6742\n",
      "Iteration 400, Loss: 0.6545\n",
      "Iteration 500, Loss: 0.6405\n",
      "Iteration 600, Loss: 0.6299\n",
      "Iteration 700, Loss: 0.6213\n",
      "Iteration 800, Loss: 0.6141\n",
      "Iteration 900, Loss: 0.6080\n",
      "Iteration 1000, Loss: 0.6026\n",
      "Iteration 1100, Loss: 0.5979\n",
      "Iteration 1200, Loss: 0.5936\n",
      "Iteration 1300, Loss: 0.5897\n",
      "Iteration 1400, Loss: 0.5861\n",
      "Iteration 1500, Loss: 0.5828\n",
      "Iteration 1600, Loss: 0.5798\n",
      "Iteration 1700, Loss: 0.5769\n",
      "Iteration 1800, Loss: 0.5743\n",
      "Iteration 1900, Loss: 0.5717\n",
      "Iteration 2000, Loss: 0.5693\n",
      "Iteration 2100, Loss: 0.5671\n",
      "Iteration 2200, Loss: 0.5649\n",
      "Iteration 2300, Loss: 0.5629\n",
      "Iteration 2400, Loss: 0.5610\n",
      "Iteration 2500, Loss: 0.5591\n",
      "Iteration 2600, Loss: 0.5573\n",
      "Iteration 2700, Loss: 0.5556\n",
      "Iteration 2800, Loss: 0.5540\n",
      "Iteration 2900, Loss: 0.5525\n",
      "Iteration 3000, Loss: 0.5510\n",
      "Iteration 3100, Loss: 0.5495\n",
      "Iteration 3200, Loss: 0.5481\n",
      "Iteration 3300, Loss: 0.5468\n",
      "Iteration 3400, Loss: 0.5455\n",
      "Iteration 3500, Loss: 0.5442\n",
      "Iteration 3600, Loss: 0.5430\n",
      "Iteration 3700, Loss: 0.5418\n",
      "Iteration 3800, Loss: 0.5407\n",
      "Iteration 3900, Loss: 0.5396\n",
      "Iteration 4000, Loss: 0.5385\n",
      "Iteration 4100, Loss: 0.5374\n",
      "Iteration 4200, Loss: 0.5364\n",
      "Iteration 4300, Loss: 0.5354\n",
      "Iteration 4400, Loss: 0.5345\n",
      "Iteration 4500, Loss: 0.5335\n",
      "Iteration 4600, Loss: 0.5326\n",
      "Iteration 4700, Loss: 0.5317\n",
      "Iteration 4800, Loss: 0.5309\n",
      "Iteration 4900, Loss: 0.5300\n",
      "Iteration 0, Loss: 1.0897\n",
      "Iteration 100, Loss: 0.7975\n",
      "Iteration 200, Loss: 0.7355\n",
      "Iteration 300, Loss: 0.7073\n",
      "Iteration 400, Loss: 0.6898\n",
      "Iteration 500, Loss: 0.6772\n",
      "Iteration 600, Loss: 0.6676\n",
      "Iteration 700, Loss: 0.6599\n",
      "Iteration 800, Loss: 0.6534\n",
      "Iteration 900, Loss: 0.6478\n",
      "Iteration 1000, Loss: 0.6430\n",
      "Iteration 1100, Loss: 0.6387\n",
      "Iteration 1200, Loss: 0.6349\n",
      "Iteration 1300, Loss: 0.6314\n",
      "Iteration 1400, Loss: 0.6283\n",
      "Iteration 1500, Loss: 0.6254\n",
      "Iteration 1600, Loss: 0.6227\n",
      "Iteration 1700, Loss: 0.6202\n",
      "Iteration 1800, Loss: 0.6178\n",
      "Iteration 1900, Loss: 0.6156\n",
      "Iteration 2000, Loss: 0.6135\n",
      "Iteration 2100, Loss: 0.6115\n",
      "Iteration 2200, Loss: 0.6096\n",
      "Iteration 2300, Loss: 0.6078\n",
      "Iteration 2400, Loss: 0.6061\n",
      "Iteration 2500, Loss: 0.6044\n",
      "Iteration 2600, Loss: 0.6028\n",
      "Iteration 2700, Loss: 0.6013\n",
      "Iteration 2800, Loss: 0.5998\n",
      "Iteration 2900, Loss: 0.5984\n",
      "Iteration 3000, Loss: 0.5971\n",
      "Iteration 3100, Loss: 0.5958\n",
      "Iteration 3200, Loss: 0.5945\n",
      "Iteration 3300, Loss: 0.5932\n",
      "Iteration 3400, Loss: 0.5921\n",
      "Iteration 3500, Loss: 0.5909\n",
      "Iteration 3600, Loss: 0.5898\n",
      "Iteration 3700, Loss: 0.5887\n",
      "Iteration 3800, Loss: 0.5876\n",
      "Iteration 3900, Loss: 0.5866\n",
      "Iteration 4000, Loss: 0.5856\n",
      "Iteration 4100, Loss: 0.5846\n",
      "Iteration 4200, Loss: 0.5837\n",
      "Iteration 4300, Loss: 0.5827\n",
      "Iteration 4400, Loss: 0.5818\n",
      "Iteration 4500, Loss: 0.5809\n",
      "Iteration 4600, Loss: 0.5801\n",
      "Iteration 4700, Loss: 0.5792\n",
      "Iteration 4800, Loss: 0.5784\n",
      "Iteration 4900, Loss: 0.5776\n",
      "131 270\n",
      "Iteration 0, Loss: 1.0895\n",
      "Iteration 100, Loss: 0.8041\n",
      "Iteration 200, Loss: 0.7414\n",
      "Iteration 300, Loss: 0.7119\n",
      "Iteration 400, Loss: 0.6935\n",
      "Iteration 500, Loss: 0.6803\n",
      "Iteration 600, Loss: 0.6701\n",
      "Iteration 700, Loss: 0.6619\n",
      "Iteration 800, Loss: 0.6551\n",
      "Iteration 900, Loss: 0.6492\n",
      "Iteration 1000, Loss: 0.6441\n",
      "Iteration 1100, Loss: 0.6395\n",
      "Iteration 1200, Loss: 0.6354\n",
      "Iteration 1300, Loss: 0.6317\n",
      "Iteration 1400, Loss: 0.6284\n",
      "Iteration 1500, Loss: 0.6253\n",
      "Iteration 1600, Loss: 0.6224\n",
      "Iteration 1700, Loss: 0.6197\n",
      "Iteration 1800, Loss: 0.6171\n",
      "Iteration 1900, Loss: 0.6148\n",
      "Iteration 2000, Loss: 0.6126\n",
      "Iteration 2100, Loss: 0.6104\n",
      "Iteration 2200, Loss: 0.6084\n",
      "Iteration 2300, Loss: 0.6065\n",
      "Iteration 2400, Loss: 0.6047\n",
      "Iteration 2500, Loss: 0.6030\n",
      "Iteration 2600, Loss: 0.6013\n",
      "Iteration 2700, Loss: 0.5997\n",
      "Iteration 2800, Loss: 0.5982\n",
      "Iteration 2900, Loss: 0.5967\n",
      "Iteration 3000, Loss: 0.5953\n",
      "Iteration 3100, Loss: 0.5940\n",
      "Iteration 3200, Loss: 0.5927\n",
      "Iteration 3300, Loss: 0.5914\n",
      "Iteration 3400, Loss: 0.5902\n",
      "Iteration 3500, Loss: 0.5890\n",
      "Iteration 3600, Loss: 0.5879\n",
      "Iteration 3700, Loss: 0.5868\n",
      "Iteration 3800, Loss: 0.5857\n",
      "Iteration 3900, Loss: 0.5846\n",
      "Iteration 4000, Loss: 0.5836\n",
      "Iteration 4100, Loss: 0.5826\n",
      "Iteration 4200, Loss: 0.5817\n",
      "Iteration 4300, Loss: 0.5808\n",
      "Iteration 4400, Loss: 0.5798\n",
      "Iteration 4500, Loss: 0.5790\n",
      "Iteration 4600, Loss: 0.5781\n",
      "Iteration 4700, Loss: 0.5772\n",
      "Iteration 4800, Loss: 0.5764\n",
      "Iteration 4900, Loss: 0.5756\n",
      "Iteration 0, Loss: 1.0888\n",
      "Iteration 100, Loss: 0.7663\n",
      "Iteration 200, Loss: 0.6991\n",
      "Iteration 300, Loss: 0.6689\n",
      "Iteration 400, Loss: 0.6504\n",
      "Iteration 500, Loss: 0.6372\n",
      "Iteration 600, Loss: 0.6271\n",
      "Iteration 700, Loss: 0.6189\n",
      "Iteration 800, Loss: 0.6120\n",
      "Iteration 900, Loss: 0.6062\n",
      "Iteration 1000, Loss: 0.6011\n",
      "Iteration 1100, Loss: 0.5965\n",
      "Iteration 1200, Loss: 0.5924\n",
      "Iteration 1300, Loss: 0.5886\n",
      "Iteration 1400, Loss: 0.5851\n",
      "Iteration 1500, Loss: 0.5819\n",
      "Iteration 1600, Loss: 0.5789\n",
      "Iteration 1700, Loss: 0.5761\n",
      "Iteration 1800, Loss: 0.5735\n",
      "Iteration 1900, Loss: 0.5710\n",
      "Iteration 2000, Loss: 0.5686\n",
      "Iteration 2100, Loss: 0.5664\n",
      "Iteration 2200, Loss: 0.5642\n",
      "Iteration 2300, Loss: 0.5622\n",
      "Iteration 2400, Loss: 0.5602\n",
      "Iteration 2500, Loss: 0.5583\n",
      "Iteration 2600, Loss: 0.5565\n",
      "Iteration 2700, Loss: 0.5548\n",
      "Iteration 2800, Loss: 0.5531\n",
      "Iteration 2900, Loss: 0.5515\n",
      "Iteration 3000, Loss: 0.5500\n",
      "Iteration 3100, Loss: 0.5485\n",
      "Iteration 3200, Loss: 0.5470\n",
      "Iteration 3300, Loss: 0.5456\n",
      "Iteration 3400, Loss: 0.5443\n",
      "Iteration 3500, Loss: 0.5429\n",
      "Iteration 3600, Loss: 0.5417\n",
      "Iteration 3700, Loss: 0.5404\n",
      "Iteration 3800, Loss: 0.5392\n",
      "Iteration 3900, Loss: 0.5380\n",
      "Iteration 4000, Loss: 0.5369\n",
      "Iteration 4100, Loss: 0.5358\n",
      "Iteration 4200, Loss: 0.5347\n",
      "Iteration 4300, Loss: 0.5336\n",
      "Iteration 4400, Loss: 0.5326\n",
      "Iteration 4500, Loss: 0.5316\n",
      "Iteration 4600, Loss: 0.5306\n",
      "Iteration 4700, Loss: 0.5296\n",
      "Iteration 4800, Loss: 0.5287\n",
      "Iteration 4900, Loss: 0.5277\n",
      "132 270\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7805\n",
      "Iteration 200, Loss: 0.7154\n",
      "Iteration 300, Loss: 0.6860\n",
      "Iteration 400, Loss: 0.6679\n",
      "Iteration 500, Loss: 0.6551\n",
      "Iteration 600, Loss: 0.6454\n",
      "Iteration 700, Loss: 0.6377\n",
      "Iteration 800, Loss: 0.6312\n",
      "Iteration 900, Loss: 0.6256\n",
      "Iteration 1000, Loss: 0.6208\n",
      "Iteration 1100, Loss: 0.6165\n",
      "Iteration 1200, Loss: 0.6126\n",
      "Iteration 1300, Loss: 0.6091\n",
      "Iteration 1400, Loss: 0.6059\n",
      "Iteration 1500, Loss: 0.6029\n",
      "Iteration 1600, Loss: 0.6002\n",
      "Iteration 1700, Loss: 0.5976\n",
      "Iteration 1800, Loss: 0.5951\n",
      "Iteration 1900, Loss: 0.5928\n",
      "Iteration 2000, Loss: 0.5906\n",
      "Iteration 2100, Loss: 0.5886\n",
      "Iteration 2200, Loss: 0.5866\n",
      "Iteration 2300, Loss: 0.5847\n",
      "Iteration 2400, Loss: 0.5829\n",
      "Iteration 2500, Loss: 0.5812\n",
      "Iteration 2600, Loss: 0.5795\n",
      "Iteration 2700, Loss: 0.5779\n",
      "Iteration 2800, Loss: 0.5764\n",
      "Iteration 2900, Loss: 0.5749\n",
      "Iteration 3000, Loss: 0.5735\n",
      "Iteration 3100, Loss: 0.5721\n",
      "Iteration 3200, Loss: 0.5708\n",
      "Iteration 3300, Loss: 0.5695\n",
      "Iteration 3400, Loss: 0.5682\n",
      "Iteration 3500, Loss: 0.5670\n",
      "Iteration 3600, Loss: 0.5658\n",
      "Iteration 3700, Loss: 0.5647\n",
      "Iteration 3800, Loss: 0.5636\n",
      "Iteration 3900, Loss: 0.5625\n",
      "Iteration 4000, Loss: 0.5615\n",
      "Iteration 4100, Loss: 0.5605\n",
      "Iteration 4200, Loss: 0.5595\n",
      "Iteration 4300, Loss: 0.5585\n",
      "Iteration 4400, Loss: 0.5576\n",
      "Iteration 4500, Loss: 0.5567\n",
      "Iteration 4600, Loss: 0.5558\n",
      "Iteration 4700, Loss: 0.5549\n",
      "Iteration 4800, Loss: 0.5540\n",
      "Iteration 4900, Loss: 0.5532\n",
      "Iteration 0, Loss: 1.0894\n",
      "Iteration 100, Loss: 0.7926\n",
      "Iteration 200, Loss: 0.7270\n",
      "Iteration 300, Loss: 0.6965\n",
      "Iteration 400, Loss: 0.6774\n",
      "Iteration 500, Loss: 0.6638\n",
      "Iteration 600, Loss: 0.6534\n",
      "Iteration 700, Loss: 0.6449\n",
      "Iteration 800, Loss: 0.6379\n",
      "Iteration 900, Loss: 0.6318\n",
      "Iteration 1000, Loss: 0.6265\n",
      "Iteration 1100, Loss: 0.6219\n",
      "Iteration 1200, Loss: 0.6177\n",
      "Iteration 1300, Loss: 0.6138\n",
      "Iteration 1400, Loss: 0.6103\n",
      "Iteration 1500, Loss: 0.6071\n",
      "Iteration 1600, Loss: 0.6041\n",
      "Iteration 1700, Loss: 0.6013\n",
      "Iteration 1800, Loss: 0.5987\n",
      "Iteration 1900, Loss: 0.5963\n",
      "Iteration 2000, Loss: 0.5940\n",
      "Iteration 2100, Loss: 0.5917\n",
      "Iteration 2200, Loss: 0.5897\n",
      "Iteration 2300, Loss: 0.5877\n",
      "Iteration 2400, Loss: 0.5858\n",
      "Iteration 2500, Loss: 0.5840\n",
      "Iteration 2600, Loss: 0.5822\n",
      "Iteration 2700, Loss: 0.5806\n",
      "Iteration 2800, Loss: 0.5790\n",
      "Iteration 2900, Loss: 0.5774\n",
      "Iteration 3000, Loss: 0.5760\n",
      "Iteration 3100, Loss: 0.5745\n",
      "Iteration 3200, Loss: 0.5732\n",
      "Iteration 3300, Loss: 0.5718\n",
      "Iteration 3400, Loss: 0.5705\n",
      "Iteration 3500, Loss: 0.5693\n",
      "Iteration 3600, Loss: 0.5681\n",
      "Iteration 3700, Loss: 0.5669\n",
      "Iteration 3800, Loss: 0.5658\n",
      "Iteration 3900, Loss: 0.5647\n",
      "Iteration 4000, Loss: 0.5636\n",
      "Iteration 4100, Loss: 0.5626\n",
      "Iteration 4200, Loss: 0.5615\n",
      "Iteration 4300, Loss: 0.5605\n",
      "Iteration 4400, Loss: 0.5596\n",
      "Iteration 4500, Loss: 0.5586\n",
      "Iteration 4600, Loss: 0.5577\n",
      "Iteration 4700, Loss: 0.5568\n",
      "Iteration 4800, Loss: 0.5559\n",
      "Iteration 4900, Loss: 0.5551\n",
      "133 270\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7841\n",
      "Iteration 200, Loss: 0.7189\n",
      "Iteration 300, Loss: 0.6897\n",
      "Iteration 400, Loss: 0.6718\n",
      "Iteration 500, Loss: 0.6589\n",
      "Iteration 600, Loss: 0.6490\n",
      "Iteration 700, Loss: 0.6410\n",
      "Iteration 800, Loss: 0.6343\n",
      "Iteration 900, Loss: 0.6286\n",
      "Iteration 1000, Loss: 0.6235\n",
      "Iteration 1100, Loss: 0.6191\n",
      "Iteration 1200, Loss: 0.6151\n",
      "Iteration 1300, Loss: 0.6115\n",
      "Iteration 1400, Loss: 0.6082\n",
      "Iteration 1500, Loss: 0.6052\n",
      "Iteration 1600, Loss: 0.6024\n",
      "Iteration 1700, Loss: 0.5997\n",
      "Iteration 1800, Loss: 0.5973\n",
      "Iteration 1900, Loss: 0.5950\n",
      "Iteration 2000, Loss: 0.5928\n",
      "Iteration 2100, Loss: 0.5907\n",
      "Iteration 2200, Loss: 0.5887\n",
      "Iteration 2300, Loss: 0.5868\n",
      "Iteration 2400, Loss: 0.5851\n",
      "Iteration 2500, Loss: 0.5833\n",
      "Iteration 2600, Loss: 0.5817\n",
      "Iteration 2700, Loss: 0.5801\n",
      "Iteration 2800, Loss: 0.5786\n",
      "Iteration 2900, Loss: 0.5771\n",
      "Iteration 3000, Loss: 0.5757\n",
      "Iteration 3100, Loss: 0.5744\n",
      "Iteration 3200, Loss: 0.5731\n",
      "Iteration 3300, Loss: 0.5718\n",
      "Iteration 3400, Loss: 0.5706\n",
      "Iteration 3500, Loss: 0.5694\n",
      "Iteration 3600, Loss: 0.5682\n",
      "Iteration 3700, Loss: 0.5671\n",
      "Iteration 3800, Loss: 0.5660\n",
      "Iteration 3900, Loss: 0.5650\n",
      "Iteration 4000, Loss: 0.5640\n",
      "Iteration 4100, Loss: 0.5630\n",
      "Iteration 4200, Loss: 0.5620\n",
      "Iteration 4300, Loss: 0.5611\n",
      "Iteration 4400, Loss: 0.5602\n",
      "Iteration 4500, Loss: 0.5593\n",
      "Iteration 4600, Loss: 0.5584\n",
      "Iteration 4700, Loss: 0.5575\n",
      "Iteration 4800, Loss: 0.5567\n",
      "Iteration 4900, Loss: 0.5559\n",
      "Iteration 0, Loss: 1.0888\n",
      "Iteration 100, Loss: 0.7884\n",
      "Iteration 200, Loss: 0.7230\n",
      "Iteration 300, Loss: 0.6918\n",
      "Iteration 400, Loss: 0.6725\n",
      "Iteration 500, Loss: 0.6589\n",
      "Iteration 600, Loss: 0.6484\n",
      "Iteration 700, Loss: 0.6400\n",
      "Iteration 800, Loss: 0.6332\n",
      "Iteration 900, Loss: 0.6273\n",
      "Iteration 1000, Loss: 0.6222\n",
      "Iteration 1100, Loss: 0.6177\n",
      "Iteration 1200, Loss: 0.6137\n",
      "Iteration 1300, Loss: 0.6101\n",
      "Iteration 1400, Loss: 0.6067\n",
      "Iteration 1500, Loss: 0.6037\n",
      "Iteration 1600, Loss: 0.6008\n",
      "Iteration 1700, Loss: 0.5981\n",
      "Iteration 1800, Loss: 0.5956\n",
      "Iteration 1900, Loss: 0.5933\n",
      "Iteration 2000, Loss: 0.5911\n",
      "Iteration 2100, Loss: 0.5890\n",
      "Iteration 2200, Loss: 0.5869\n",
      "Iteration 2300, Loss: 0.5850\n",
      "Iteration 2400, Loss: 0.5832\n",
      "Iteration 2500, Loss: 0.5815\n",
      "Iteration 2600, Loss: 0.5798\n",
      "Iteration 2700, Loss: 0.5782\n",
      "Iteration 2800, Loss: 0.5767\n",
      "Iteration 2900, Loss: 0.5752\n",
      "Iteration 3000, Loss: 0.5737\n",
      "Iteration 3100, Loss: 0.5723\n",
      "Iteration 3200, Loss: 0.5710\n",
      "Iteration 3300, Loss: 0.5697\n",
      "Iteration 3400, Loss: 0.5685\n",
      "Iteration 3500, Loss: 0.5673\n",
      "Iteration 3600, Loss: 0.5661\n",
      "Iteration 3700, Loss: 0.5649\n",
      "Iteration 3800, Loss: 0.5638\n",
      "Iteration 3900, Loss: 0.5627\n",
      "Iteration 4000, Loss: 0.5617\n",
      "Iteration 4100, Loss: 0.5607\n",
      "Iteration 4200, Loss: 0.5597\n",
      "Iteration 4300, Loss: 0.5587\n",
      "Iteration 4400, Loss: 0.5577\n",
      "Iteration 4500, Loss: 0.5568\n",
      "Iteration 4600, Loss: 0.5559\n",
      "Iteration 4700, Loss: 0.5550\n",
      "Iteration 4800, Loss: 0.5542\n",
      "Iteration 4900, Loss: 0.5533\n",
      "134 270\n",
      "Iteration 0, Loss: 1.0883\n",
      "Iteration 100, Loss: 0.7743\n",
      "Iteration 200, Loss: 0.7106\n",
      "Iteration 300, Loss: 0.6816\n",
      "Iteration 400, Loss: 0.6639\n",
      "Iteration 500, Loss: 0.6514\n",
      "Iteration 600, Loss: 0.6418\n",
      "Iteration 700, Loss: 0.6339\n",
      "Iteration 800, Loss: 0.6273\n",
      "Iteration 900, Loss: 0.6216\n",
      "Iteration 1000, Loss: 0.6166\n",
      "Iteration 1100, Loss: 0.6121\n",
      "Iteration 1200, Loss: 0.6080\n",
      "Iteration 1300, Loss: 0.6043\n",
      "Iteration 1400, Loss: 0.6009\n",
      "Iteration 1500, Loss: 0.5977\n",
      "Iteration 1600, Loss: 0.5947\n",
      "Iteration 1700, Loss: 0.5919\n",
      "Iteration 1800, Loss: 0.5892\n",
      "Iteration 1900, Loss: 0.5867\n",
      "Iteration 2000, Loss: 0.5844\n",
      "Iteration 2100, Loss: 0.5821\n",
      "Iteration 2200, Loss: 0.5799\n",
      "Iteration 2300, Loss: 0.5779\n",
      "Iteration 2400, Loss: 0.5759\n",
      "Iteration 2500, Loss: 0.5740\n",
      "Iteration 2600, Loss: 0.5722\n",
      "Iteration 2700, Loss: 0.5704\n",
      "Iteration 2800, Loss: 0.5687\n",
      "Iteration 2900, Loss: 0.5671\n",
      "Iteration 3000, Loss: 0.5655\n",
      "Iteration 3100, Loss: 0.5640\n",
      "Iteration 3200, Loss: 0.5626\n",
      "Iteration 3300, Loss: 0.5611\n",
      "Iteration 3400, Loss: 0.5598\n",
      "Iteration 3500, Loss: 0.5584\n",
      "Iteration 3600, Loss: 0.5572\n",
      "Iteration 3700, Loss: 0.5559\n",
      "Iteration 3800, Loss: 0.5547\n",
      "Iteration 3900, Loss: 0.5535\n",
      "Iteration 4000, Loss: 0.5523\n",
      "Iteration 4100, Loss: 0.5512\n",
      "Iteration 4200, Loss: 0.5501\n",
      "Iteration 4300, Loss: 0.5491\n",
      "Iteration 4400, Loss: 0.5480\n",
      "Iteration 4500, Loss: 0.5470\n",
      "Iteration 4600, Loss: 0.5460\n",
      "Iteration 4700, Loss: 0.5451\n",
      "Iteration 4800, Loss: 0.5441\n",
      "Iteration 4900, Loss: 0.5432\n",
      "Iteration 0, Loss: 1.0903\n",
      "Iteration 100, Loss: 0.7973\n",
      "Iteration 200, Loss: 0.7308\n",
      "Iteration 300, Loss: 0.6998\n",
      "Iteration 400, Loss: 0.6804\n",
      "Iteration 500, Loss: 0.6666\n",
      "Iteration 600, Loss: 0.6561\n",
      "Iteration 700, Loss: 0.6477\n",
      "Iteration 800, Loss: 0.6408\n",
      "Iteration 900, Loss: 0.6350\n",
      "Iteration 1000, Loss: 0.6299\n",
      "Iteration 1100, Loss: 0.6255\n",
      "Iteration 1200, Loss: 0.6215\n",
      "Iteration 1300, Loss: 0.6180\n",
      "Iteration 1400, Loss: 0.6147\n",
      "Iteration 1500, Loss: 0.6117\n",
      "Iteration 1600, Loss: 0.6090\n",
      "Iteration 1700, Loss: 0.6064\n",
      "Iteration 1800, Loss: 0.6040\n",
      "Iteration 1900, Loss: 0.6018\n",
      "Iteration 2000, Loss: 0.5997\n",
      "Iteration 2100, Loss: 0.5977\n",
      "Iteration 2200, Loss: 0.5958\n",
      "Iteration 2300, Loss: 0.5940\n",
      "Iteration 2400, Loss: 0.5923\n",
      "Iteration 2500, Loss: 0.5907\n",
      "Iteration 2600, Loss: 0.5891\n",
      "Iteration 2700, Loss: 0.5876\n",
      "Iteration 2800, Loss: 0.5862\n",
      "Iteration 2900, Loss: 0.5848\n",
      "Iteration 3000, Loss: 0.5835\n",
      "Iteration 3100, Loss: 0.5822\n",
      "Iteration 3200, Loss: 0.5810\n",
      "Iteration 3300, Loss: 0.5798\n",
      "Iteration 3400, Loss: 0.5786\n",
      "Iteration 3500, Loss: 0.5775\n",
      "Iteration 3600, Loss: 0.5765\n",
      "Iteration 3700, Loss: 0.5754\n",
      "Iteration 3800, Loss: 0.5744\n",
      "Iteration 3900, Loss: 0.5734\n",
      "Iteration 4000, Loss: 0.5725\n",
      "Iteration 4100, Loss: 0.5716\n",
      "Iteration 4200, Loss: 0.5707\n",
      "Iteration 4300, Loss: 0.5698\n",
      "Iteration 4400, Loss: 0.5689\n",
      "Iteration 4500, Loss: 0.5681\n",
      "Iteration 4600, Loss: 0.5673\n",
      "Iteration 4700, Loss: 0.5665\n",
      "Iteration 4800, Loss: 0.5657\n",
      "Iteration 4900, Loss: 0.5649\n",
      "135 270\n",
      "Iteration 0, Loss: 1.0941\n",
      "Iteration 100, Loss: 0.8715\n",
      "Iteration 200, Loss: 0.7919\n",
      "Iteration 300, Loss: 0.7510\n",
      "Iteration 400, Loss: 0.7263\n",
      "Iteration 500, Loss: 0.7093\n",
      "Iteration 600, Loss: 0.6965\n",
      "Iteration 700, Loss: 0.6865\n",
      "Iteration 800, Loss: 0.6781\n",
      "Iteration 900, Loss: 0.6711\n",
      "Iteration 1000, Loss: 0.6649\n",
      "Iteration 1100, Loss: 0.6595\n",
      "Iteration 1200, Loss: 0.6547\n",
      "Iteration 1300, Loss: 0.6504\n",
      "Iteration 1400, Loss: 0.6465\n",
      "Iteration 1500, Loss: 0.6428\n",
      "Iteration 1600, Loss: 0.6395\n",
      "Iteration 1700, Loss: 0.6364\n",
      "Iteration 1800, Loss: 0.6336\n",
      "Iteration 1900, Loss: 0.6309\n",
      "Iteration 2000, Loss: 0.6284\n",
      "Iteration 2100, Loss: 0.6260\n",
      "Iteration 2200, Loss: 0.6238\n",
      "Iteration 2300, Loss: 0.6216\n",
      "Iteration 2400, Loss: 0.6196\n",
      "Iteration 2500, Loss: 0.6177\n",
      "Iteration 2600, Loss: 0.6159\n",
      "Iteration 2700, Loss: 0.6142\n",
      "Iteration 2800, Loss: 0.6125\n",
      "Iteration 2900, Loss: 0.6109\n",
      "Iteration 3000, Loss: 0.6093\n",
      "Iteration 3100, Loss: 0.6078\n",
      "Iteration 3200, Loss: 0.6064\n",
      "Iteration 3300, Loss: 0.6050\n",
      "Iteration 3400, Loss: 0.6036\n",
      "Iteration 3500, Loss: 0.6023\n",
      "Iteration 3600, Loss: 0.6011\n",
      "Iteration 3700, Loss: 0.5999\n",
      "Iteration 3800, Loss: 0.5987\n",
      "Iteration 3900, Loss: 0.5975\n",
      "Iteration 4000, Loss: 0.5964\n",
      "Iteration 4100, Loss: 0.5953\n",
      "Iteration 4200, Loss: 0.5943\n",
      "Iteration 4300, Loss: 0.5932\n",
      "Iteration 4400, Loss: 0.5922\n",
      "Iteration 4500, Loss: 0.5913\n",
      "Iteration 4600, Loss: 0.5903\n",
      "Iteration 4700, Loss: 0.5894\n",
      "Iteration 4800, Loss: 0.5885\n",
      "Iteration 4900, Loss: 0.5876\n",
      "Iteration 0, Loss: 1.0931\n",
      "Iteration 100, Loss: 0.8510\n",
      "Iteration 200, Loss: 0.7736\n",
      "Iteration 300, Loss: 0.7340\n",
      "Iteration 400, Loss: 0.7095\n",
      "Iteration 500, Loss: 0.6926\n",
      "Iteration 600, Loss: 0.6800\n",
      "Iteration 700, Loss: 0.6700\n",
      "Iteration 800, Loss: 0.6619\n",
      "Iteration 900, Loss: 0.6551\n",
      "Iteration 1000, Loss: 0.6493\n",
      "Iteration 1100, Loss: 0.6442\n",
      "Iteration 1200, Loss: 0.6397\n",
      "Iteration 1300, Loss: 0.6357\n",
      "Iteration 1400, Loss: 0.6320\n",
      "Iteration 1500, Loss: 0.6287\n",
      "Iteration 1600, Loss: 0.6257\n",
      "Iteration 1700, Loss: 0.6228\n",
      "Iteration 1800, Loss: 0.6202\n",
      "Iteration 1900, Loss: 0.6178\n",
      "Iteration 2000, Loss: 0.6155\n",
      "Iteration 2100, Loss: 0.6134\n",
      "Iteration 2200, Loss: 0.6113\n",
      "Iteration 2300, Loss: 0.6094\n",
      "Iteration 2400, Loss: 0.6076\n",
      "Iteration 2500, Loss: 0.6059\n",
      "Iteration 2600, Loss: 0.6042\n",
      "Iteration 2700, Loss: 0.6026\n",
      "Iteration 2800, Loss: 0.6011\n",
      "Iteration 2900, Loss: 0.5996\n",
      "Iteration 3000, Loss: 0.5982\n",
      "Iteration 3100, Loss: 0.5968\n",
      "Iteration 3200, Loss: 0.5955\n",
      "Iteration 3300, Loss: 0.5943\n",
      "Iteration 3400, Loss: 0.5930\n",
      "Iteration 3500, Loss: 0.5919\n",
      "Iteration 3600, Loss: 0.5907\n",
      "Iteration 3700, Loss: 0.5896\n",
      "Iteration 3800, Loss: 0.5885\n",
      "Iteration 3900, Loss: 0.5874\n",
      "Iteration 4000, Loss: 0.5864\n",
      "Iteration 4100, Loss: 0.5854\n",
      "Iteration 4200, Loss: 0.5844\n",
      "Iteration 4300, Loss: 0.5835\n",
      "Iteration 4400, Loss: 0.5826\n",
      "Iteration 4500, Loss: 0.5817\n",
      "Iteration 4600, Loss: 0.5808\n",
      "Iteration 4700, Loss: 0.5799\n",
      "Iteration 4800, Loss: 0.5791\n",
      "Iteration 4900, Loss: 0.5782\n",
      "136 270\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8579\n",
      "Iteration 200, Loss: 0.7772\n",
      "Iteration 300, Loss: 0.7370\n",
      "Iteration 400, Loss: 0.7130\n",
      "Iteration 500, Loss: 0.6965\n",
      "Iteration 600, Loss: 0.6842\n",
      "Iteration 700, Loss: 0.6744\n",
      "Iteration 800, Loss: 0.6664\n",
      "Iteration 900, Loss: 0.6597\n",
      "Iteration 1000, Loss: 0.6539\n",
      "Iteration 1100, Loss: 0.6489\n",
      "Iteration 1200, Loss: 0.6444\n",
      "Iteration 1300, Loss: 0.6404\n",
      "Iteration 1400, Loss: 0.6368\n",
      "Iteration 1500, Loss: 0.6335\n",
      "Iteration 1600, Loss: 0.6305\n",
      "Iteration 1700, Loss: 0.6277\n",
      "Iteration 1800, Loss: 0.6251\n",
      "Iteration 1900, Loss: 0.6227\n",
      "Iteration 2000, Loss: 0.6205\n",
      "Iteration 2100, Loss: 0.6184\n",
      "Iteration 2200, Loss: 0.6164\n",
      "Iteration 2300, Loss: 0.6144\n",
      "Iteration 2400, Loss: 0.6126\n",
      "Iteration 2500, Loss: 0.6109\n",
      "Iteration 2600, Loss: 0.6093\n",
      "Iteration 2700, Loss: 0.6077\n",
      "Iteration 2800, Loss: 0.6062\n",
      "Iteration 2900, Loss: 0.6048\n",
      "Iteration 3000, Loss: 0.6034\n",
      "Iteration 3100, Loss: 0.6021\n",
      "Iteration 3200, Loss: 0.6008\n",
      "Iteration 3300, Loss: 0.5996\n",
      "Iteration 3400, Loss: 0.5984\n",
      "Iteration 3500, Loss: 0.5972\n",
      "Iteration 3600, Loss: 0.5961\n",
      "Iteration 3700, Loss: 0.5950\n",
      "Iteration 3800, Loss: 0.5939\n",
      "Iteration 3900, Loss: 0.5929\n",
      "Iteration 4000, Loss: 0.5919\n",
      "Iteration 4100, Loss: 0.5909\n",
      "Iteration 4200, Loss: 0.5900\n",
      "Iteration 4300, Loss: 0.5890\n",
      "Iteration 4400, Loss: 0.5881\n",
      "Iteration 4500, Loss: 0.5873\n",
      "Iteration 4600, Loss: 0.5864\n",
      "Iteration 4700, Loss: 0.5856\n",
      "Iteration 4800, Loss: 0.5847\n",
      "Iteration 4900, Loss: 0.5839\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8645\n",
      "Iteration 200, Loss: 0.7888\n",
      "Iteration 300, Loss: 0.7497\n",
      "Iteration 400, Loss: 0.7256\n",
      "Iteration 500, Loss: 0.7086\n",
      "Iteration 600, Loss: 0.6958\n",
      "Iteration 700, Loss: 0.6857\n",
      "Iteration 800, Loss: 0.6773\n",
      "Iteration 900, Loss: 0.6702\n",
      "Iteration 1000, Loss: 0.6641\n",
      "Iteration 1100, Loss: 0.6587\n",
      "Iteration 1200, Loss: 0.6538\n",
      "Iteration 1300, Loss: 0.6495\n",
      "Iteration 1400, Loss: 0.6455\n",
      "Iteration 1500, Loss: 0.6419\n",
      "Iteration 1600, Loss: 0.6386\n",
      "Iteration 1700, Loss: 0.6355\n",
      "Iteration 1800, Loss: 0.6326\n",
      "Iteration 1900, Loss: 0.6299\n",
      "Iteration 2000, Loss: 0.6274\n",
      "Iteration 2100, Loss: 0.6251\n",
      "Iteration 2200, Loss: 0.6228\n",
      "Iteration 2300, Loss: 0.6207\n",
      "Iteration 2400, Loss: 0.6187\n",
      "Iteration 2500, Loss: 0.6167\n",
      "Iteration 2600, Loss: 0.6149\n",
      "Iteration 2700, Loss: 0.6132\n",
      "Iteration 2800, Loss: 0.6115\n",
      "Iteration 2900, Loss: 0.6098\n",
      "Iteration 3000, Loss: 0.6083\n",
      "Iteration 3100, Loss: 0.6068\n",
      "Iteration 3200, Loss: 0.6053\n",
      "Iteration 3300, Loss: 0.6039\n",
      "Iteration 3400, Loss: 0.6026\n",
      "Iteration 3500, Loss: 0.6012\n",
      "Iteration 3600, Loss: 0.6000\n",
      "Iteration 3700, Loss: 0.5987\n",
      "Iteration 3800, Loss: 0.5976\n",
      "Iteration 3900, Loss: 0.5964\n",
      "Iteration 4000, Loss: 0.5953\n",
      "Iteration 4100, Loss: 0.5942\n",
      "Iteration 4200, Loss: 0.5931\n",
      "Iteration 4300, Loss: 0.5921\n",
      "Iteration 4400, Loss: 0.5911\n",
      "Iteration 4500, Loss: 0.5901\n",
      "Iteration 4600, Loss: 0.5891\n",
      "Iteration 4700, Loss: 0.5882\n",
      "Iteration 4800, Loss: 0.5873\n",
      "Iteration 4900, Loss: 0.5864\n",
      "137 270\n",
      "Iteration 0, Loss: 1.0940\n",
      "Iteration 100, Loss: 0.8773\n",
      "Iteration 200, Loss: 0.8037\n",
      "Iteration 300, Loss: 0.7650\n",
      "Iteration 400, Loss: 0.7411\n",
      "Iteration 500, Loss: 0.7246\n",
      "Iteration 600, Loss: 0.7121\n",
      "Iteration 700, Loss: 0.7022\n",
      "Iteration 800, Loss: 0.6941\n",
      "Iteration 900, Loss: 0.6873\n",
      "Iteration 1000, Loss: 0.6815\n",
      "Iteration 1100, Loss: 0.6764\n",
      "Iteration 1200, Loss: 0.6719\n",
      "Iteration 1300, Loss: 0.6678\n",
      "Iteration 1400, Loss: 0.6642\n",
      "Iteration 1500, Loss: 0.6608\n",
      "Iteration 1600, Loss: 0.6577\n",
      "Iteration 1700, Loss: 0.6549\n",
      "Iteration 1800, Loss: 0.6523\n",
      "Iteration 1900, Loss: 0.6499\n",
      "Iteration 2000, Loss: 0.6476\n",
      "Iteration 2100, Loss: 0.6454\n",
      "Iteration 2200, Loss: 0.6434\n",
      "Iteration 2300, Loss: 0.6415\n",
      "Iteration 2400, Loss: 0.6397\n",
      "Iteration 2500, Loss: 0.6379\n",
      "Iteration 2600, Loss: 0.6363\n",
      "Iteration 2700, Loss: 0.6347\n",
      "Iteration 2800, Loss: 0.6331\n",
      "Iteration 2900, Loss: 0.6317\n",
      "Iteration 3000, Loss: 0.6303\n",
      "Iteration 3100, Loss: 0.6289\n",
      "Iteration 3200, Loss: 0.6276\n",
      "Iteration 3300, Loss: 0.6264\n",
      "Iteration 3400, Loss: 0.6251\n",
      "Iteration 3500, Loss: 0.6240\n",
      "Iteration 3600, Loss: 0.6228\n",
      "Iteration 3700, Loss: 0.6217\n",
      "Iteration 3800, Loss: 0.6207\n",
      "Iteration 3900, Loss: 0.6196\n",
      "Iteration 4000, Loss: 0.6186\n",
      "Iteration 4100, Loss: 0.6176\n",
      "Iteration 4200, Loss: 0.6166\n",
      "Iteration 4300, Loss: 0.6157\n",
      "Iteration 4400, Loss: 0.6148\n",
      "Iteration 4500, Loss: 0.6139\n",
      "Iteration 4600, Loss: 0.6131\n",
      "Iteration 4700, Loss: 0.6122\n",
      "Iteration 4800, Loss: 0.6114\n",
      "Iteration 4900, Loss: 0.6106\n",
      "Iteration 0, Loss: 1.0933\n",
      "Iteration 100, Loss: 0.8458\n",
      "Iteration 200, Loss: 0.7628\n",
      "Iteration 300, Loss: 0.7211\n",
      "Iteration 400, Loss: 0.6958\n",
      "Iteration 500, Loss: 0.6785\n",
      "Iteration 600, Loss: 0.6656\n",
      "Iteration 700, Loss: 0.6555\n",
      "Iteration 800, Loss: 0.6472\n",
      "Iteration 900, Loss: 0.6401\n",
      "Iteration 1000, Loss: 0.6339\n",
      "Iteration 1100, Loss: 0.6285\n",
      "Iteration 1200, Loss: 0.6237\n",
      "Iteration 1300, Loss: 0.6194\n",
      "Iteration 1400, Loss: 0.6155\n",
      "Iteration 1500, Loss: 0.6120\n",
      "Iteration 1600, Loss: 0.6087\n",
      "Iteration 1700, Loss: 0.6057\n",
      "Iteration 1800, Loss: 0.6029\n",
      "Iteration 1900, Loss: 0.6003\n",
      "Iteration 2000, Loss: 0.5978\n",
      "Iteration 2100, Loss: 0.5955\n",
      "Iteration 2200, Loss: 0.5933\n",
      "Iteration 2300, Loss: 0.5913\n",
      "Iteration 2400, Loss: 0.5893\n",
      "Iteration 2500, Loss: 0.5874\n",
      "Iteration 2600, Loss: 0.5856\n",
      "Iteration 2700, Loss: 0.5839\n",
      "Iteration 2800, Loss: 0.5823\n",
      "Iteration 2900, Loss: 0.5807\n",
      "Iteration 3000, Loss: 0.5792\n",
      "Iteration 3100, Loss: 0.5778\n",
      "Iteration 3200, Loss: 0.5764\n",
      "Iteration 3300, Loss: 0.5750\n",
      "Iteration 3400, Loss: 0.5737\n",
      "Iteration 3500, Loss: 0.5725\n",
      "Iteration 3600, Loss: 0.5712\n",
      "Iteration 3700, Loss: 0.5701\n",
      "Iteration 3800, Loss: 0.5689\n",
      "Iteration 3900, Loss: 0.5678\n",
      "Iteration 4000, Loss: 0.5667\n",
      "Iteration 4100, Loss: 0.5657\n",
      "Iteration 4200, Loss: 0.5646\n",
      "Iteration 4300, Loss: 0.5636\n",
      "Iteration 4400, Loss: 0.5627\n",
      "Iteration 4500, Loss: 0.5617\n",
      "Iteration 4600, Loss: 0.5608\n",
      "Iteration 4700, Loss: 0.5599\n",
      "Iteration 4800, Loss: 0.5590\n",
      "Iteration 4900, Loss: 0.5582\n",
      "138 270\n",
      "Iteration 0, Loss: 1.0932\n",
      "Iteration 100, Loss: 0.8520\n",
      "Iteration 200, Loss: 0.7745\n",
      "Iteration 300, Loss: 0.7360\n",
      "Iteration 400, Loss: 0.7129\n",
      "Iteration 500, Loss: 0.6971\n",
      "Iteration 600, Loss: 0.6855\n",
      "Iteration 700, Loss: 0.6763\n",
      "Iteration 800, Loss: 0.6688\n",
      "Iteration 900, Loss: 0.6625\n",
      "Iteration 1000, Loss: 0.6571\n",
      "Iteration 1100, Loss: 0.6525\n",
      "Iteration 1200, Loss: 0.6483\n",
      "Iteration 1300, Loss: 0.6446\n",
      "Iteration 1400, Loss: 0.6413\n",
      "Iteration 1500, Loss: 0.6383\n",
      "Iteration 1600, Loss: 0.6356\n",
      "Iteration 1700, Loss: 0.6331\n",
      "Iteration 1800, Loss: 0.6307\n",
      "Iteration 1900, Loss: 0.6286\n",
      "Iteration 2000, Loss: 0.6265\n",
      "Iteration 2100, Loss: 0.6246\n",
      "Iteration 2200, Loss: 0.6228\n",
      "Iteration 2300, Loss: 0.6211\n",
      "Iteration 2400, Loss: 0.6195\n",
      "Iteration 2500, Loss: 0.6180\n",
      "Iteration 2600, Loss: 0.6165\n",
      "Iteration 2700, Loss: 0.6151\n",
      "Iteration 2800, Loss: 0.6138\n",
      "Iteration 2900, Loss: 0.6125\n",
      "Iteration 3000, Loss: 0.6113\n",
      "Iteration 3100, Loss: 0.6101\n",
      "Iteration 3200, Loss: 0.6089\n",
      "Iteration 3300, Loss: 0.6079\n",
      "Iteration 3400, Loss: 0.6068\n",
      "Iteration 3500, Loss: 0.6058\n",
      "Iteration 3600, Loss: 0.6048\n",
      "Iteration 3700, Loss: 0.6038\n",
      "Iteration 3800, Loss: 0.6028\n",
      "Iteration 3900, Loss: 0.6019\n",
      "Iteration 4000, Loss: 0.6010\n",
      "Iteration 4100, Loss: 0.6002\n",
      "Iteration 4200, Loss: 0.5993\n",
      "Iteration 4300, Loss: 0.5985\n",
      "Iteration 4400, Loss: 0.5977\n",
      "Iteration 4500, Loss: 0.5969\n",
      "Iteration 4600, Loss: 0.5962\n",
      "Iteration 4700, Loss: 0.5954\n",
      "Iteration 4800, Loss: 0.5947\n",
      "Iteration 4900, Loss: 0.5940\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8702\n",
      "Iteration 200, Loss: 0.7919\n",
      "Iteration 300, Loss: 0.7512\n",
      "Iteration 400, Loss: 0.7259\n",
      "Iteration 500, Loss: 0.7085\n",
      "Iteration 600, Loss: 0.6953\n",
      "Iteration 700, Loss: 0.6848\n",
      "Iteration 800, Loss: 0.6760\n",
      "Iteration 900, Loss: 0.6685\n",
      "Iteration 1000, Loss: 0.6620\n",
      "Iteration 1100, Loss: 0.6562\n",
      "Iteration 1200, Loss: 0.6511\n",
      "Iteration 1300, Loss: 0.6465\n",
      "Iteration 1400, Loss: 0.6422\n",
      "Iteration 1500, Loss: 0.6383\n",
      "Iteration 1600, Loss: 0.6347\n",
      "Iteration 1700, Loss: 0.6313\n",
      "Iteration 1800, Loss: 0.6282\n",
      "Iteration 1900, Loss: 0.6253\n",
      "Iteration 2000, Loss: 0.6226\n",
      "Iteration 2100, Loss: 0.6200\n",
      "Iteration 2200, Loss: 0.6175\n",
      "Iteration 2300, Loss: 0.6152\n",
      "Iteration 2400, Loss: 0.6129\n",
      "Iteration 2500, Loss: 0.6108\n",
      "Iteration 2600, Loss: 0.6088\n",
      "Iteration 2700, Loss: 0.6068\n",
      "Iteration 2800, Loss: 0.6050\n",
      "Iteration 2900, Loss: 0.6032\n",
      "Iteration 3000, Loss: 0.6015\n",
      "Iteration 3100, Loss: 0.5998\n",
      "Iteration 3200, Loss: 0.5982\n",
      "Iteration 3300, Loss: 0.5967\n",
      "Iteration 3400, Loss: 0.5952\n",
      "Iteration 3500, Loss: 0.5937\n",
      "Iteration 3600, Loss: 0.5923\n",
      "Iteration 3700, Loss: 0.5910\n",
      "Iteration 3800, Loss: 0.5897\n",
      "Iteration 3900, Loss: 0.5884\n",
      "Iteration 4000, Loss: 0.5871\n",
      "Iteration 4100, Loss: 0.5859\n",
      "Iteration 4200, Loss: 0.5848\n",
      "Iteration 4300, Loss: 0.5836\n",
      "Iteration 4400, Loss: 0.5825\n",
      "Iteration 4500, Loss: 0.5815\n",
      "Iteration 4600, Loss: 0.5804\n",
      "Iteration 4700, Loss: 0.5794\n",
      "Iteration 4800, Loss: 0.5784\n",
      "Iteration 4900, Loss: 0.5774\n",
      "139 270\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8662\n",
      "Iteration 200, Loss: 0.7864\n",
      "Iteration 300, Loss: 0.7450\n",
      "Iteration 400, Loss: 0.7194\n",
      "Iteration 500, Loss: 0.7016\n",
      "Iteration 600, Loss: 0.6884\n",
      "Iteration 700, Loss: 0.6781\n",
      "Iteration 800, Loss: 0.6696\n",
      "Iteration 900, Loss: 0.6624\n",
      "Iteration 1000, Loss: 0.6562\n",
      "Iteration 1100, Loss: 0.6508\n",
      "Iteration 1200, Loss: 0.6460\n",
      "Iteration 1300, Loss: 0.6417\n",
      "Iteration 1400, Loss: 0.6378\n",
      "Iteration 1500, Loss: 0.6343\n",
      "Iteration 1600, Loss: 0.6311\n",
      "Iteration 1700, Loss: 0.6281\n",
      "Iteration 1800, Loss: 0.6254\n",
      "Iteration 1900, Loss: 0.6228\n",
      "Iteration 2000, Loss: 0.6204\n",
      "Iteration 2100, Loss: 0.6182\n",
      "Iteration 2200, Loss: 0.6160\n",
      "Iteration 2300, Loss: 0.6140\n",
      "Iteration 2400, Loss: 0.6121\n",
      "Iteration 2500, Loss: 0.6103\n",
      "Iteration 2600, Loss: 0.6085\n",
      "Iteration 2700, Loss: 0.6069\n",
      "Iteration 2800, Loss: 0.6053\n",
      "Iteration 2900, Loss: 0.6037\n",
      "Iteration 3000, Loss: 0.6022\n",
      "Iteration 3100, Loss: 0.6008\n",
      "Iteration 3200, Loss: 0.5995\n",
      "Iteration 3300, Loss: 0.5981\n",
      "Iteration 3400, Loss: 0.5968\n",
      "Iteration 3500, Loss: 0.5956\n",
      "Iteration 3600, Loss: 0.5944\n",
      "Iteration 3700, Loss: 0.5932\n",
      "Iteration 3800, Loss: 0.5920\n",
      "Iteration 3900, Loss: 0.5909\n",
      "Iteration 4000, Loss: 0.5899\n",
      "Iteration 4100, Loss: 0.5888\n",
      "Iteration 4200, Loss: 0.5878\n",
      "Iteration 4300, Loss: 0.5868\n",
      "Iteration 4400, Loss: 0.5858\n",
      "Iteration 4500, Loss: 0.5849\n",
      "Iteration 4600, Loss: 0.5840\n",
      "Iteration 4700, Loss: 0.5831\n",
      "Iteration 4800, Loss: 0.5822\n",
      "Iteration 4900, Loss: 0.5813\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8578\n",
      "Iteration 200, Loss: 0.7811\n",
      "Iteration 300, Loss: 0.7426\n",
      "Iteration 400, Loss: 0.7193\n",
      "Iteration 500, Loss: 0.7033\n",
      "Iteration 600, Loss: 0.6913\n",
      "Iteration 700, Loss: 0.6818\n",
      "Iteration 800, Loss: 0.6739\n",
      "Iteration 900, Loss: 0.6671\n",
      "Iteration 1000, Loss: 0.6613\n",
      "Iteration 1100, Loss: 0.6561\n",
      "Iteration 1200, Loss: 0.6515\n",
      "Iteration 1300, Loss: 0.6474\n",
      "Iteration 1400, Loss: 0.6436\n",
      "Iteration 1500, Loss: 0.6402\n",
      "Iteration 1600, Loss: 0.6370\n",
      "Iteration 1700, Loss: 0.6341\n",
      "Iteration 1800, Loss: 0.6313\n",
      "Iteration 1900, Loss: 0.6288\n",
      "Iteration 2000, Loss: 0.6264\n",
      "Iteration 2100, Loss: 0.6241\n",
      "Iteration 2200, Loss: 0.6220\n",
      "Iteration 2300, Loss: 0.6199\n",
      "Iteration 2400, Loss: 0.6179\n",
      "Iteration 2500, Loss: 0.6161\n",
      "Iteration 2600, Loss: 0.6143\n",
      "Iteration 2700, Loss: 0.6126\n",
      "Iteration 2800, Loss: 0.6110\n",
      "Iteration 2900, Loss: 0.6094\n",
      "Iteration 3000, Loss: 0.6079\n",
      "Iteration 3100, Loss: 0.6064\n",
      "Iteration 3200, Loss: 0.6050\n",
      "Iteration 3300, Loss: 0.6037\n",
      "Iteration 3400, Loss: 0.6024\n",
      "Iteration 3500, Loss: 0.6011\n",
      "Iteration 3600, Loss: 0.5999\n",
      "Iteration 3700, Loss: 0.5987\n",
      "Iteration 3800, Loss: 0.5976\n",
      "Iteration 3900, Loss: 0.5964\n",
      "Iteration 4000, Loss: 0.5954\n",
      "Iteration 4100, Loss: 0.5943\n",
      "Iteration 4200, Loss: 0.5933\n",
      "Iteration 4300, Loss: 0.5923\n",
      "Iteration 4400, Loss: 0.5913\n",
      "Iteration 4500, Loss: 0.5903\n",
      "Iteration 4600, Loss: 0.5894\n",
      "Iteration 4700, Loss: 0.5885\n",
      "Iteration 4800, Loss: 0.5876\n",
      "Iteration 4900, Loss: 0.5867\n",
      "140 270\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8583\n",
      "Iteration 200, Loss: 0.7766\n",
      "Iteration 300, Loss: 0.7348\n",
      "Iteration 400, Loss: 0.7092\n",
      "Iteration 500, Loss: 0.6918\n",
      "Iteration 600, Loss: 0.6789\n",
      "Iteration 700, Loss: 0.6686\n",
      "Iteration 800, Loss: 0.6601\n",
      "Iteration 900, Loss: 0.6529\n",
      "Iteration 1000, Loss: 0.6468\n",
      "Iteration 1100, Loss: 0.6413\n",
      "Iteration 1200, Loss: 0.6366\n",
      "Iteration 1300, Loss: 0.6323\n",
      "Iteration 1400, Loss: 0.6284\n",
      "Iteration 1500, Loss: 0.6249\n",
      "Iteration 1600, Loss: 0.6216\n",
      "Iteration 1700, Loss: 0.6186\n",
      "Iteration 1800, Loss: 0.6157\n",
      "Iteration 1900, Loss: 0.6131\n",
      "Iteration 2000, Loss: 0.6106\n",
      "Iteration 2100, Loss: 0.6083\n",
      "Iteration 2200, Loss: 0.6061\n",
      "Iteration 2300, Loss: 0.6040\n",
      "Iteration 2400, Loss: 0.6020\n",
      "Iteration 2500, Loss: 0.6001\n",
      "Iteration 2600, Loss: 0.5983\n",
      "Iteration 2700, Loss: 0.5966\n",
      "Iteration 2800, Loss: 0.5949\n",
      "Iteration 2900, Loss: 0.5933\n",
      "Iteration 3000, Loss: 0.5918\n",
      "Iteration 3100, Loss: 0.5903\n",
      "Iteration 3200, Loss: 0.5888\n",
      "Iteration 3300, Loss: 0.5875\n",
      "Iteration 3400, Loss: 0.5861\n",
      "Iteration 3500, Loss: 0.5848\n",
      "Iteration 3600, Loss: 0.5836\n",
      "Iteration 3700, Loss: 0.5823\n",
      "Iteration 3800, Loss: 0.5812\n",
      "Iteration 3900, Loss: 0.5800\n",
      "Iteration 4000, Loss: 0.5789\n",
      "Iteration 4100, Loss: 0.5778\n",
      "Iteration 4200, Loss: 0.5768\n",
      "Iteration 4300, Loss: 0.5757\n",
      "Iteration 4400, Loss: 0.5747\n",
      "Iteration 4500, Loss: 0.5737\n",
      "Iteration 4600, Loss: 0.5728\n",
      "Iteration 4700, Loss: 0.5718\n",
      "Iteration 4800, Loss: 0.5709\n",
      "Iteration 4900, Loss: 0.5700\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8639\n",
      "Iteration 200, Loss: 0.7879\n",
      "Iteration 300, Loss: 0.7497\n",
      "Iteration 400, Loss: 0.7264\n",
      "Iteration 500, Loss: 0.7102\n",
      "Iteration 600, Loss: 0.6981\n",
      "Iteration 700, Loss: 0.6886\n",
      "Iteration 800, Loss: 0.6806\n",
      "Iteration 900, Loss: 0.6739\n",
      "Iteration 1000, Loss: 0.6681\n",
      "Iteration 1100, Loss: 0.6631\n",
      "Iteration 1200, Loss: 0.6585\n",
      "Iteration 1300, Loss: 0.6544\n",
      "Iteration 1400, Loss: 0.6506\n",
      "Iteration 1500, Loss: 0.6472\n",
      "Iteration 1600, Loss: 0.6441\n",
      "Iteration 1700, Loss: 0.6412\n",
      "Iteration 1800, Loss: 0.6384\n",
      "Iteration 1900, Loss: 0.6359\n",
      "Iteration 2000, Loss: 0.6335\n",
      "Iteration 2100, Loss: 0.6312\n",
      "Iteration 2200, Loss: 0.6291\n",
      "Iteration 2300, Loss: 0.6270\n",
      "Iteration 2400, Loss: 0.6251\n",
      "Iteration 2500, Loss: 0.6233\n",
      "Iteration 2600, Loss: 0.6215\n",
      "Iteration 2700, Loss: 0.6198\n",
      "Iteration 2800, Loss: 0.6182\n",
      "Iteration 2900, Loss: 0.6167\n",
      "Iteration 3000, Loss: 0.6152\n",
      "Iteration 3100, Loss: 0.6138\n",
      "Iteration 3200, Loss: 0.6124\n",
      "Iteration 3300, Loss: 0.6110\n",
      "Iteration 3400, Loss: 0.6097\n",
      "Iteration 3500, Loss: 0.6085\n",
      "Iteration 3600, Loss: 0.6072\n",
      "Iteration 3700, Loss: 0.6061\n",
      "Iteration 3800, Loss: 0.6049\n",
      "Iteration 3900, Loss: 0.6038\n",
      "Iteration 4000, Loss: 0.6027\n",
      "Iteration 4100, Loss: 0.6016\n",
      "Iteration 4200, Loss: 0.6006\n",
      "Iteration 4300, Loss: 0.5996\n",
      "Iteration 4400, Loss: 0.5986\n",
      "Iteration 4500, Loss: 0.5977\n",
      "Iteration 4600, Loss: 0.5967\n",
      "Iteration 4700, Loss: 0.5958\n",
      "Iteration 4800, Loss: 0.5949\n",
      "Iteration 4900, Loss: 0.5941\n",
      "141 270\n",
      "Iteration 0, Loss: 1.0940\n",
      "Iteration 100, Loss: 0.8612\n",
      "Iteration 200, Loss: 0.7805\n",
      "Iteration 300, Loss: 0.7397\n",
      "Iteration 400, Loss: 0.7148\n",
      "Iteration 500, Loss: 0.6976\n",
      "Iteration 600, Loss: 0.6847\n",
      "Iteration 700, Loss: 0.6743\n",
      "Iteration 800, Loss: 0.6658\n",
      "Iteration 900, Loss: 0.6584\n",
      "Iteration 1000, Loss: 0.6520\n",
      "Iteration 1100, Loss: 0.6463\n",
      "Iteration 1200, Loss: 0.6413\n",
      "Iteration 1300, Loss: 0.6367\n",
      "Iteration 1400, Loss: 0.6325\n",
      "Iteration 1500, Loss: 0.6287\n",
      "Iteration 1600, Loss: 0.6252\n",
      "Iteration 1700, Loss: 0.6219\n",
      "Iteration 1800, Loss: 0.6189\n",
      "Iteration 1900, Loss: 0.6160\n",
      "Iteration 2000, Loss: 0.6133\n",
      "Iteration 2100, Loss: 0.6107\n",
      "Iteration 2200, Loss: 0.6083\n",
      "Iteration 2300, Loss: 0.6060\n",
      "Iteration 2400, Loss: 0.6039\n",
      "Iteration 2500, Loss: 0.6018\n",
      "Iteration 2600, Loss: 0.5998\n",
      "Iteration 2700, Loss: 0.5979\n",
      "Iteration 2800, Loss: 0.5961\n",
      "Iteration 2900, Loss: 0.5944\n",
      "Iteration 3000, Loss: 0.5927\n",
      "Iteration 3100, Loss: 0.5911\n",
      "Iteration 3200, Loss: 0.5895\n",
      "Iteration 3300, Loss: 0.5880\n",
      "Iteration 3400, Loss: 0.5865\n",
      "Iteration 3500, Loss: 0.5851\n",
      "Iteration 3600, Loss: 0.5838\n",
      "Iteration 3700, Loss: 0.5824\n",
      "Iteration 3800, Loss: 0.5812\n",
      "Iteration 3900, Loss: 0.5799\n",
      "Iteration 4000, Loss: 0.5787\n",
      "Iteration 4100, Loss: 0.5775\n",
      "Iteration 4200, Loss: 0.5763\n",
      "Iteration 4300, Loss: 0.5752\n",
      "Iteration 4400, Loss: 0.5741\n",
      "Iteration 4500, Loss: 0.5731\n",
      "Iteration 4600, Loss: 0.5720\n",
      "Iteration 4700, Loss: 0.5710\n",
      "Iteration 4800, Loss: 0.5700\n",
      "Iteration 4900, Loss: 0.5690\n",
      "Iteration 0, Loss: 1.0935\n",
      "Iteration 100, Loss: 0.8599\n",
      "Iteration 200, Loss: 0.7825\n",
      "Iteration 300, Loss: 0.7428\n",
      "Iteration 400, Loss: 0.7185\n",
      "Iteration 500, Loss: 0.7019\n",
      "Iteration 600, Loss: 0.6895\n",
      "Iteration 700, Loss: 0.6797\n",
      "Iteration 800, Loss: 0.6717\n",
      "Iteration 900, Loss: 0.6650\n",
      "Iteration 1000, Loss: 0.6592\n",
      "Iteration 1100, Loss: 0.6541\n",
      "Iteration 1200, Loss: 0.6496\n",
      "Iteration 1300, Loss: 0.6456\n",
      "Iteration 1400, Loss: 0.6420\n",
      "Iteration 1500, Loss: 0.6387\n",
      "Iteration 1600, Loss: 0.6357\n",
      "Iteration 1700, Loss: 0.6329\n",
      "Iteration 1800, Loss: 0.6303\n",
      "Iteration 1900, Loss: 0.6279\n",
      "Iteration 2000, Loss: 0.6256\n",
      "Iteration 2100, Loss: 0.6235\n",
      "Iteration 2200, Loss: 0.6215\n",
      "Iteration 2300, Loss: 0.6196\n",
      "Iteration 2400, Loss: 0.6178\n",
      "Iteration 2500, Loss: 0.6161\n",
      "Iteration 2600, Loss: 0.6145\n",
      "Iteration 2700, Loss: 0.6130\n",
      "Iteration 2800, Loss: 0.6115\n",
      "Iteration 2900, Loss: 0.6100\n",
      "Iteration 3000, Loss: 0.6086\n",
      "Iteration 3100, Loss: 0.6073\n",
      "Iteration 3200, Loss: 0.6060\n",
      "Iteration 3300, Loss: 0.6048\n",
      "Iteration 3400, Loss: 0.6035\n",
      "Iteration 3500, Loss: 0.6024\n",
      "Iteration 3600, Loss: 0.6012\n",
      "Iteration 3700, Loss: 0.6001\n",
      "Iteration 3800, Loss: 0.5991\n",
      "Iteration 3900, Loss: 0.5980\n",
      "Iteration 4000, Loss: 0.5970\n",
      "Iteration 4100, Loss: 0.5960\n",
      "Iteration 4200, Loss: 0.5950\n",
      "Iteration 4300, Loss: 0.5941\n",
      "Iteration 4400, Loss: 0.5932\n",
      "Iteration 4500, Loss: 0.5923\n",
      "Iteration 4600, Loss: 0.5914\n",
      "Iteration 4700, Loss: 0.5906\n",
      "Iteration 4800, Loss: 0.5897\n",
      "Iteration 4900, Loss: 0.5889\n",
      "142 270\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8569\n",
      "Iteration 200, Loss: 0.7753\n",
      "Iteration 300, Loss: 0.7333\n",
      "Iteration 400, Loss: 0.7074\n",
      "Iteration 500, Loss: 0.6897\n",
      "Iteration 600, Loss: 0.6764\n",
      "Iteration 700, Loss: 0.6659\n",
      "Iteration 800, Loss: 0.6574\n",
      "Iteration 900, Loss: 0.6500\n",
      "Iteration 1000, Loss: 0.6437\n",
      "Iteration 1100, Loss: 0.6382\n",
      "Iteration 1200, Loss: 0.6332\n",
      "Iteration 1300, Loss: 0.6288\n",
      "Iteration 1400, Loss: 0.6248\n",
      "Iteration 1500, Loss: 0.6212\n",
      "Iteration 1600, Loss: 0.6178\n",
      "Iteration 1700, Loss: 0.6147\n",
      "Iteration 1800, Loss: 0.6118\n",
      "Iteration 1900, Loss: 0.6091\n",
      "Iteration 2000, Loss: 0.6066\n",
      "Iteration 2100, Loss: 0.6042\n",
      "Iteration 2200, Loss: 0.6020\n",
      "Iteration 2300, Loss: 0.5998\n",
      "Iteration 2400, Loss: 0.5978\n",
      "Iteration 2500, Loss: 0.5958\n",
      "Iteration 2600, Loss: 0.5940\n",
      "Iteration 2700, Loss: 0.5922\n",
      "Iteration 2800, Loss: 0.5905\n",
      "Iteration 2900, Loss: 0.5889\n",
      "Iteration 3000, Loss: 0.5873\n",
      "Iteration 3100, Loss: 0.5858\n",
      "Iteration 3200, Loss: 0.5844\n",
      "Iteration 3300, Loss: 0.5830\n",
      "Iteration 3400, Loss: 0.5816\n",
      "Iteration 3500, Loss: 0.5803\n",
      "Iteration 3600, Loss: 0.5790\n",
      "Iteration 3700, Loss: 0.5778\n",
      "Iteration 3800, Loss: 0.5766\n",
      "Iteration 3900, Loss: 0.5754\n",
      "Iteration 4000, Loss: 0.5743\n",
      "Iteration 4100, Loss: 0.5732\n",
      "Iteration 4200, Loss: 0.5721\n",
      "Iteration 4300, Loss: 0.5711\n",
      "Iteration 4400, Loss: 0.5700\n",
      "Iteration 4500, Loss: 0.5690\n",
      "Iteration 4600, Loss: 0.5681\n",
      "Iteration 4700, Loss: 0.5671\n",
      "Iteration 4800, Loss: 0.5662\n",
      "Iteration 4900, Loss: 0.5653\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8662\n",
      "Iteration 200, Loss: 0.7906\n",
      "Iteration 300, Loss: 0.7520\n",
      "Iteration 400, Loss: 0.7285\n",
      "Iteration 500, Loss: 0.7123\n",
      "Iteration 600, Loss: 0.7002\n",
      "Iteration 700, Loss: 0.6906\n",
      "Iteration 800, Loss: 0.6827\n",
      "Iteration 900, Loss: 0.6760\n",
      "Iteration 1000, Loss: 0.6702\n",
      "Iteration 1100, Loss: 0.6651\n",
      "Iteration 1200, Loss: 0.6605\n",
      "Iteration 1300, Loss: 0.6564\n",
      "Iteration 1400, Loss: 0.6527\n",
      "Iteration 1500, Loss: 0.6493\n",
      "Iteration 1600, Loss: 0.6462\n",
      "Iteration 1700, Loss: 0.6433\n",
      "Iteration 1800, Loss: 0.6406\n",
      "Iteration 1900, Loss: 0.6381\n",
      "Iteration 2000, Loss: 0.6358\n",
      "Iteration 2100, Loss: 0.6336\n",
      "Iteration 2200, Loss: 0.6315\n",
      "Iteration 2300, Loss: 0.6295\n",
      "Iteration 2400, Loss: 0.6276\n",
      "Iteration 2500, Loss: 0.6258\n",
      "Iteration 2600, Loss: 0.6241\n",
      "Iteration 2700, Loss: 0.6225\n",
      "Iteration 2800, Loss: 0.6209\n",
      "Iteration 2900, Loss: 0.6194\n",
      "Iteration 3000, Loss: 0.6180\n",
      "Iteration 3100, Loss: 0.6166\n",
      "Iteration 3200, Loss: 0.6152\n",
      "Iteration 3300, Loss: 0.6139\n",
      "Iteration 3400, Loss: 0.6127\n",
      "Iteration 3500, Loss: 0.6114\n",
      "Iteration 3600, Loss: 0.6103\n",
      "Iteration 3700, Loss: 0.6091\n",
      "Iteration 3800, Loss: 0.6080\n",
      "Iteration 3900, Loss: 0.6069\n",
      "Iteration 4000, Loss: 0.6059\n",
      "Iteration 4100, Loss: 0.6048\n",
      "Iteration 4200, Loss: 0.6038\n",
      "Iteration 4300, Loss: 0.6029\n",
      "Iteration 4400, Loss: 0.6019\n",
      "Iteration 4500, Loss: 0.6010\n",
      "Iteration 4600, Loss: 0.6001\n",
      "Iteration 4700, Loss: 0.5992\n",
      "Iteration 4800, Loss: 0.5984\n",
      "Iteration 4900, Loss: 0.5975\n",
      "143 270\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8609\n",
      "Iteration 200, Loss: 0.7842\n",
      "Iteration 300, Loss: 0.7455\n",
      "Iteration 400, Loss: 0.7220\n",
      "Iteration 500, Loss: 0.7059\n",
      "Iteration 600, Loss: 0.6939\n",
      "Iteration 700, Loss: 0.6843\n",
      "Iteration 800, Loss: 0.6765\n",
      "Iteration 900, Loss: 0.6699\n",
      "Iteration 1000, Loss: 0.6642\n",
      "Iteration 1100, Loss: 0.6592\n",
      "Iteration 1200, Loss: 0.6548\n",
      "Iteration 1300, Loss: 0.6508\n",
      "Iteration 1400, Loss: 0.6472\n",
      "Iteration 1500, Loss: 0.6439\n",
      "Iteration 1600, Loss: 0.6408\n",
      "Iteration 1700, Loss: 0.6380\n",
      "Iteration 1800, Loss: 0.6354\n",
      "Iteration 1900, Loss: 0.6329\n",
      "Iteration 2000, Loss: 0.6306\n",
      "Iteration 2100, Loss: 0.6285\n",
      "Iteration 2200, Loss: 0.6265\n",
      "Iteration 2300, Loss: 0.6245\n",
      "Iteration 2400, Loss: 0.6227\n",
      "Iteration 2500, Loss: 0.6210\n",
      "Iteration 2600, Loss: 0.6193\n",
      "Iteration 2700, Loss: 0.6177\n",
      "Iteration 2800, Loss: 0.6161\n",
      "Iteration 2900, Loss: 0.6146\n",
      "Iteration 3000, Loss: 0.6132\n",
      "Iteration 3100, Loss: 0.6119\n",
      "Iteration 3200, Loss: 0.6105\n",
      "Iteration 3300, Loss: 0.6093\n",
      "Iteration 3400, Loss: 0.6080\n",
      "Iteration 3500, Loss: 0.6068\n",
      "Iteration 3600, Loss: 0.6056\n",
      "Iteration 3700, Loss: 0.6045\n",
      "Iteration 3800, Loss: 0.6034\n",
      "Iteration 3900, Loss: 0.6023\n",
      "Iteration 4000, Loss: 0.6013\n",
      "Iteration 4100, Loss: 0.6003\n",
      "Iteration 4200, Loss: 0.5993\n",
      "Iteration 4300, Loss: 0.5983\n",
      "Iteration 4400, Loss: 0.5974\n",
      "Iteration 4500, Loss: 0.5964\n",
      "Iteration 4600, Loss: 0.5955\n",
      "Iteration 4700, Loss: 0.5947\n",
      "Iteration 4800, Loss: 0.5938\n",
      "Iteration 4900, Loss: 0.5930\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8620\n",
      "Iteration 200, Loss: 0.7817\n",
      "Iteration 300, Loss: 0.7401\n",
      "Iteration 400, Loss: 0.7145\n",
      "Iteration 500, Loss: 0.6966\n",
      "Iteration 600, Loss: 0.6832\n",
      "Iteration 700, Loss: 0.6725\n",
      "Iteration 800, Loss: 0.6638\n",
      "Iteration 900, Loss: 0.6564\n",
      "Iteration 1000, Loss: 0.6500\n",
      "Iteration 1100, Loss: 0.6444\n",
      "Iteration 1200, Loss: 0.6394\n",
      "Iteration 1300, Loss: 0.6349\n",
      "Iteration 1400, Loss: 0.6309\n",
      "Iteration 1500, Loss: 0.6272\n",
      "Iteration 1600, Loss: 0.6238\n",
      "Iteration 1700, Loss: 0.6206\n",
      "Iteration 1800, Loss: 0.6177\n",
      "Iteration 1900, Loss: 0.6149\n",
      "Iteration 2000, Loss: 0.6124\n",
      "Iteration 2100, Loss: 0.6099\n",
      "Iteration 2200, Loss: 0.6076\n",
      "Iteration 2300, Loss: 0.6054\n",
      "Iteration 2400, Loss: 0.6034\n",
      "Iteration 2500, Loss: 0.6014\n",
      "Iteration 2600, Loss: 0.5994\n",
      "Iteration 2700, Loss: 0.5976\n",
      "Iteration 2800, Loss: 0.5958\n",
      "Iteration 2900, Loss: 0.5941\n",
      "Iteration 3000, Loss: 0.5925\n",
      "Iteration 3100, Loss: 0.5910\n",
      "Iteration 3200, Loss: 0.5895\n",
      "Iteration 3300, Loss: 0.5880\n",
      "Iteration 3400, Loss: 0.5866\n",
      "Iteration 3500, Loss: 0.5853\n",
      "Iteration 3600, Loss: 0.5840\n",
      "Iteration 3700, Loss: 0.5827\n",
      "Iteration 3800, Loss: 0.5815\n",
      "Iteration 3900, Loss: 0.5803\n",
      "Iteration 4000, Loss: 0.5791\n",
      "Iteration 4100, Loss: 0.5780\n",
      "Iteration 4200, Loss: 0.5769\n",
      "Iteration 4300, Loss: 0.5758\n",
      "Iteration 4400, Loss: 0.5747\n",
      "Iteration 4500, Loss: 0.5737\n",
      "Iteration 4600, Loss: 0.5727\n",
      "Iteration 4700, Loss: 0.5717\n",
      "Iteration 4800, Loss: 0.5708\n",
      "Iteration 4900, Loss: 0.5699\n",
      "144 270\n",
      "Iteration 0, Loss: 1.0931\n",
      "Iteration 100, Loss: 0.8628\n",
      "Iteration 200, Loss: 0.7855\n",
      "Iteration 300, Loss: 0.7460\n",
      "Iteration 400, Loss: 0.7219\n",
      "Iteration 500, Loss: 0.7050\n",
      "Iteration 600, Loss: 0.6924\n",
      "Iteration 700, Loss: 0.6822\n",
      "Iteration 800, Loss: 0.6738\n",
      "Iteration 900, Loss: 0.6667\n",
      "Iteration 1000, Loss: 0.6605\n",
      "Iteration 1100, Loss: 0.6550\n",
      "Iteration 1200, Loss: 0.6502\n",
      "Iteration 1300, Loss: 0.6458\n",
      "Iteration 1400, Loss: 0.6418\n",
      "Iteration 1500, Loss: 0.6382\n",
      "Iteration 1600, Loss: 0.6349\n",
      "Iteration 1700, Loss: 0.6318\n",
      "Iteration 1800, Loss: 0.6290\n",
      "Iteration 1900, Loss: 0.6263\n",
      "Iteration 2000, Loss: 0.6238\n",
      "Iteration 2100, Loss: 0.6214\n",
      "Iteration 2200, Loss: 0.6192\n",
      "Iteration 2300, Loss: 0.6170\n",
      "Iteration 2400, Loss: 0.6150\n",
      "Iteration 2500, Loss: 0.6131\n",
      "Iteration 2600, Loss: 0.6112\n",
      "Iteration 2700, Loss: 0.6095\n",
      "Iteration 2800, Loss: 0.6078\n",
      "Iteration 2900, Loss: 0.6062\n",
      "Iteration 3000, Loss: 0.6046\n",
      "Iteration 3100, Loss: 0.6031\n",
      "Iteration 3200, Loss: 0.6017\n",
      "Iteration 3300, Loss: 0.6003\n",
      "Iteration 3400, Loss: 0.5989\n",
      "Iteration 3500, Loss: 0.5976\n",
      "Iteration 3600, Loss: 0.5963\n",
      "Iteration 3700, Loss: 0.5951\n",
      "Iteration 3800, Loss: 0.5939\n",
      "Iteration 3900, Loss: 0.5927\n",
      "Iteration 4000, Loss: 0.5916\n",
      "Iteration 4100, Loss: 0.5905\n",
      "Iteration 4200, Loss: 0.5894\n",
      "Iteration 4300, Loss: 0.5884\n",
      "Iteration 4400, Loss: 0.5873\n",
      "Iteration 4500, Loss: 0.5863\n",
      "Iteration 4600, Loss: 0.5854\n",
      "Iteration 4700, Loss: 0.5844\n",
      "Iteration 4800, Loss: 0.5835\n",
      "Iteration 4900, Loss: 0.5825\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8593\n",
      "Iteration 200, Loss: 0.7790\n",
      "Iteration 300, Loss: 0.7381\n",
      "Iteration 400, Loss: 0.7133\n",
      "Iteration 500, Loss: 0.6962\n",
      "Iteration 600, Loss: 0.6834\n",
      "Iteration 700, Loss: 0.6732\n",
      "Iteration 800, Loss: 0.6648\n",
      "Iteration 900, Loss: 0.6577\n",
      "Iteration 1000, Loss: 0.6516\n",
      "Iteration 1100, Loss: 0.6462\n",
      "Iteration 1200, Loss: 0.6415\n",
      "Iteration 1300, Loss: 0.6372\n",
      "Iteration 1400, Loss: 0.6334\n",
      "Iteration 1500, Loss: 0.6299\n",
      "Iteration 1600, Loss: 0.6267\n",
      "Iteration 1700, Loss: 0.6237\n",
      "Iteration 1800, Loss: 0.6209\n",
      "Iteration 1900, Loss: 0.6183\n",
      "Iteration 2000, Loss: 0.6159\n",
      "Iteration 2100, Loss: 0.6136\n",
      "Iteration 2200, Loss: 0.6115\n",
      "Iteration 2300, Loss: 0.6094\n",
      "Iteration 2400, Loss: 0.6075\n",
      "Iteration 2500, Loss: 0.6057\n",
      "Iteration 2600, Loss: 0.6039\n",
      "Iteration 2700, Loss: 0.6022\n",
      "Iteration 2800, Loss: 0.6006\n",
      "Iteration 2900, Loss: 0.5991\n",
      "Iteration 3000, Loss: 0.5976\n",
      "Iteration 3100, Loss: 0.5962\n",
      "Iteration 3200, Loss: 0.5949\n",
      "Iteration 3300, Loss: 0.5935\n",
      "Iteration 3400, Loss: 0.5923\n",
      "Iteration 3500, Loss: 0.5910\n",
      "Iteration 3600, Loss: 0.5898\n",
      "Iteration 3700, Loss: 0.5887\n",
      "Iteration 3800, Loss: 0.5876\n",
      "Iteration 3900, Loss: 0.5865\n",
      "Iteration 4000, Loss: 0.5854\n",
      "Iteration 4100, Loss: 0.5844\n",
      "Iteration 4200, Loss: 0.5834\n",
      "Iteration 4300, Loss: 0.5825\n",
      "Iteration 4400, Loss: 0.5815\n",
      "Iteration 4500, Loss: 0.5806\n",
      "Iteration 4600, Loss: 0.5797\n",
      "Iteration 4700, Loss: 0.5789\n",
      "Iteration 4800, Loss: 0.5780\n",
      "Iteration 4900, Loss: 0.5772\n",
      "145 270\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8572\n",
      "Iteration 200, Loss: 0.7813\n",
      "Iteration 300, Loss: 0.7425\n",
      "Iteration 400, Loss: 0.7183\n",
      "Iteration 500, Loss: 0.7017\n",
      "Iteration 600, Loss: 0.6892\n",
      "Iteration 700, Loss: 0.6794\n",
      "Iteration 800, Loss: 0.6713\n",
      "Iteration 900, Loss: 0.6646\n",
      "Iteration 1000, Loss: 0.6588\n",
      "Iteration 1100, Loss: 0.6537\n",
      "Iteration 1200, Loss: 0.6493\n",
      "Iteration 1300, Loss: 0.6452\n",
      "Iteration 1400, Loss: 0.6416\n",
      "Iteration 1500, Loss: 0.6383\n",
      "Iteration 1600, Loss: 0.6353\n",
      "Iteration 1700, Loss: 0.6325\n",
      "Iteration 1800, Loss: 0.6299\n",
      "Iteration 1900, Loss: 0.6275\n",
      "Iteration 2000, Loss: 0.6252\n",
      "Iteration 2100, Loss: 0.6231\n",
      "Iteration 2200, Loss: 0.6211\n",
      "Iteration 2300, Loss: 0.6192\n",
      "Iteration 2400, Loss: 0.6174\n",
      "Iteration 2500, Loss: 0.6157\n",
      "Iteration 2600, Loss: 0.6141\n",
      "Iteration 2700, Loss: 0.6126\n",
      "Iteration 2800, Loss: 0.6111\n",
      "Iteration 2900, Loss: 0.6096\n",
      "Iteration 3000, Loss: 0.6083\n",
      "Iteration 3100, Loss: 0.6069\n",
      "Iteration 3200, Loss: 0.6057\n",
      "Iteration 3300, Loss: 0.6044\n",
      "Iteration 3400, Loss: 0.6032\n",
      "Iteration 3500, Loss: 0.6021\n",
      "Iteration 3600, Loss: 0.6010\n",
      "Iteration 3700, Loss: 0.5999\n",
      "Iteration 3800, Loss: 0.5988\n",
      "Iteration 3900, Loss: 0.5978\n",
      "Iteration 4000, Loss: 0.5968\n",
      "Iteration 4100, Loss: 0.5959\n",
      "Iteration 4200, Loss: 0.5949\n",
      "Iteration 4300, Loss: 0.5940\n",
      "Iteration 4400, Loss: 0.5931\n",
      "Iteration 4500, Loss: 0.5922\n",
      "Iteration 4600, Loss: 0.5913\n",
      "Iteration 4700, Loss: 0.5905\n",
      "Iteration 4800, Loss: 0.5897\n",
      "Iteration 4900, Loss: 0.5889\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8632\n",
      "Iteration 200, Loss: 0.7814\n",
      "Iteration 300, Loss: 0.7398\n",
      "Iteration 400, Loss: 0.7148\n",
      "Iteration 500, Loss: 0.6978\n",
      "Iteration 600, Loss: 0.6850\n",
      "Iteration 700, Loss: 0.6748\n",
      "Iteration 800, Loss: 0.6664\n",
      "Iteration 900, Loss: 0.6593\n",
      "Iteration 1000, Loss: 0.6530\n",
      "Iteration 1100, Loss: 0.6475\n",
      "Iteration 1200, Loss: 0.6426\n",
      "Iteration 1300, Loss: 0.6381\n",
      "Iteration 1400, Loss: 0.6341\n",
      "Iteration 1500, Loss: 0.6304\n",
      "Iteration 1600, Loss: 0.6270\n",
      "Iteration 1700, Loss: 0.6239\n",
      "Iteration 1800, Loss: 0.6209\n",
      "Iteration 1900, Loss: 0.6181\n",
      "Iteration 2000, Loss: 0.6155\n",
      "Iteration 2100, Loss: 0.6130\n",
      "Iteration 2200, Loss: 0.6106\n",
      "Iteration 2300, Loss: 0.6084\n",
      "Iteration 2400, Loss: 0.6063\n",
      "Iteration 2500, Loss: 0.6042\n",
      "Iteration 2600, Loss: 0.6023\n",
      "Iteration 2700, Loss: 0.6004\n",
      "Iteration 2800, Loss: 0.5986\n",
      "Iteration 2900, Loss: 0.5969\n",
      "Iteration 3000, Loss: 0.5952\n",
      "Iteration 3100, Loss: 0.5936\n",
      "Iteration 3200, Loss: 0.5921\n",
      "Iteration 3300, Loss: 0.5906\n",
      "Iteration 3400, Loss: 0.5891\n",
      "Iteration 3500, Loss: 0.5877\n",
      "Iteration 3600, Loss: 0.5864\n",
      "Iteration 3700, Loss: 0.5850\n",
      "Iteration 3800, Loss: 0.5838\n",
      "Iteration 3900, Loss: 0.5825\n",
      "Iteration 4000, Loss: 0.5813\n",
      "Iteration 4100, Loss: 0.5801\n",
      "Iteration 4200, Loss: 0.5790\n",
      "Iteration 4300, Loss: 0.5779\n",
      "Iteration 4400, Loss: 0.5768\n",
      "Iteration 4500, Loss: 0.5757\n",
      "Iteration 4600, Loss: 0.5747\n",
      "Iteration 4700, Loss: 0.5737\n",
      "Iteration 4800, Loss: 0.5727\n",
      "Iteration 4900, Loss: 0.5717\n",
      "146 270\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8585\n",
      "Iteration 200, Loss: 0.7772\n",
      "Iteration 300, Loss: 0.7349\n",
      "Iteration 400, Loss: 0.7089\n",
      "Iteration 500, Loss: 0.6910\n",
      "Iteration 600, Loss: 0.6774\n",
      "Iteration 700, Loss: 0.6666\n",
      "Iteration 800, Loss: 0.6577\n",
      "Iteration 900, Loss: 0.6503\n",
      "Iteration 1000, Loss: 0.6437\n",
      "Iteration 1100, Loss: 0.6379\n",
      "Iteration 1200, Loss: 0.6328\n",
      "Iteration 1300, Loss: 0.6282\n",
      "Iteration 1400, Loss: 0.6240\n",
      "Iteration 1500, Loss: 0.6202\n",
      "Iteration 1600, Loss: 0.6167\n",
      "Iteration 1700, Loss: 0.6135\n",
      "Iteration 1800, Loss: 0.6105\n",
      "Iteration 1900, Loss: 0.6077\n",
      "Iteration 2000, Loss: 0.6051\n",
      "Iteration 2100, Loss: 0.6026\n",
      "Iteration 2200, Loss: 0.6003\n",
      "Iteration 2300, Loss: 0.5981\n",
      "Iteration 2400, Loss: 0.5960\n",
      "Iteration 2500, Loss: 0.5939\n",
      "Iteration 2600, Loss: 0.5920\n",
      "Iteration 2700, Loss: 0.5902\n",
      "Iteration 2800, Loss: 0.5885\n",
      "Iteration 2900, Loss: 0.5868\n",
      "Iteration 3000, Loss: 0.5852\n",
      "Iteration 3100, Loss: 0.5836\n",
      "Iteration 3200, Loss: 0.5821\n",
      "Iteration 3300, Loss: 0.5807\n",
      "Iteration 3400, Loss: 0.5793\n",
      "Iteration 3500, Loss: 0.5779\n",
      "Iteration 3600, Loss: 0.5766\n",
      "Iteration 3700, Loss: 0.5753\n",
      "Iteration 3800, Loss: 0.5741\n",
      "Iteration 3900, Loss: 0.5729\n",
      "Iteration 4000, Loss: 0.5717\n",
      "Iteration 4100, Loss: 0.5706\n",
      "Iteration 4200, Loss: 0.5695\n",
      "Iteration 4300, Loss: 0.5684\n",
      "Iteration 4400, Loss: 0.5673\n",
      "Iteration 4500, Loss: 0.5663\n",
      "Iteration 4600, Loss: 0.5653\n",
      "Iteration 4700, Loss: 0.5644\n",
      "Iteration 4800, Loss: 0.5634\n",
      "Iteration 4900, Loss: 0.5625\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8634\n",
      "Iteration 200, Loss: 0.7878\n",
      "Iteration 300, Loss: 0.7499\n",
      "Iteration 400, Loss: 0.7265\n",
      "Iteration 500, Loss: 0.7104\n",
      "Iteration 600, Loss: 0.6985\n",
      "Iteration 700, Loss: 0.6891\n",
      "Iteration 800, Loss: 0.6813\n",
      "Iteration 900, Loss: 0.6747\n",
      "Iteration 1000, Loss: 0.6690\n",
      "Iteration 1100, Loss: 0.6640\n",
      "Iteration 1200, Loss: 0.6596\n",
      "Iteration 1300, Loss: 0.6556\n",
      "Iteration 1400, Loss: 0.6520\n",
      "Iteration 1500, Loss: 0.6487\n",
      "Iteration 1600, Loss: 0.6457\n",
      "Iteration 1700, Loss: 0.6428\n",
      "Iteration 1800, Loss: 0.6402\n",
      "Iteration 1900, Loss: 0.6377\n",
      "Iteration 2000, Loss: 0.6354\n",
      "Iteration 2100, Loss: 0.6332\n",
      "Iteration 2200, Loss: 0.6312\n",
      "Iteration 2300, Loss: 0.6292\n",
      "Iteration 2400, Loss: 0.6274\n",
      "Iteration 2500, Loss: 0.6256\n",
      "Iteration 2600, Loss: 0.6239\n",
      "Iteration 2700, Loss: 0.6223\n",
      "Iteration 2800, Loss: 0.6207\n",
      "Iteration 2900, Loss: 0.6192\n",
      "Iteration 3000, Loss: 0.6177\n",
      "Iteration 3100, Loss: 0.6164\n",
      "Iteration 3200, Loss: 0.6150\n",
      "Iteration 3300, Loss: 0.6137\n",
      "Iteration 3400, Loss: 0.6125\n",
      "Iteration 3500, Loss: 0.6112\n",
      "Iteration 3600, Loss: 0.6100\n",
      "Iteration 3700, Loss: 0.6089\n",
      "Iteration 3800, Loss: 0.6078\n",
      "Iteration 3900, Loss: 0.6067\n",
      "Iteration 4000, Loss: 0.6057\n",
      "Iteration 4100, Loss: 0.6046\n",
      "Iteration 4200, Loss: 0.6037\n",
      "Iteration 4300, Loss: 0.6027\n",
      "Iteration 4400, Loss: 0.6017\n",
      "Iteration 4500, Loss: 0.6008\n",
      "Iteration 4600, Loss: 0.5999\n",
      "Iteration 4700, Loss: 0.5990\n",
      "Iteration 4800, Loss: 0.5981\n",
      "Iteration 4900, Loss: 0.5973\n",
      "147 270\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8568\n",
      "Iteration 200, Loss: 0.7756\n",
      "Iteration 300, Loss: 0.7347\n",
      "Iteration 400, Loss: 0.7095\n",
      "Iteration 500, Loss: 0.6923\n",
      "Iteration 600, Loss: 0.6794\n",
      "Iteration 700, Loss: 0.6694\n",
      "Iteration 800, Loss: 0.6612\n",
      "Iteration 900, Loss: 0.6542\n",
      "Iteration 1000, Loss: 0.6481\n",
      "Iteration 1100, Loss: 0.6428\n",
      "Iteration 1200, Loss: 0.6381\n",
      "Iteration 1300, Loss: 0.6339\n",
      "Iteration 1400, Loss: 0.6301\n",
      "Iteration 1500, Loss: 0.6265\n",
      "Iteration 1600, Loss: 0.6233\n",
      "Iteration 1700, Loss: 0.6203\n",
      "Iteration 1800, Loss: 0.6175\n",
      "Iteration 1900, Loss: 0.6148\n",
      "Iteration 2000, Loss: 0.6124\n",
      "Iteration 2100, Loss: 0.6101\n",
      "Iteration 2200, Loss: 0.6079\n",
      "Iteration 2300, Loss: 0.6058\n",
      "Iteration 2400, Loss: 0.6038\n",
      "Iteration 2500, Loss: 0.6020\n",
      "Iteration 2600, Loss: 0.6001\n",
      "Iteration 2700, Loss: 0.5984\n",
      "Iteration 2800, Loss: 0.5968\n",
      "Iteration 2900, Loss: 0.5952\n",
      "Iteration 3000, Loss: 0.5936\n",
      "Iteration 3100, Loss: 0.5921\n",
      "Iteration 3200, Loss: 0.5907\n",
      "Iteration 3300, Loss: 0.5893\n",
      "Iteration 3400, Loss: 0.5880\n",
      "Iteration 3500, Loss: 0.5867\n",
      "Iteration 3600, Loss: 0.5855\n",
      "Iteration 3700, Loss: 0.5842\n",
      "Iteration 3800, Loss: 0.5831\n",
      "Iteration 3900, Loss: 0.5819\n",
      "Iteration 4000, Loss: 0.5808\n",
      "Iteration 4100, Loss: 0.5797\n",
      "Iteration 4200, Loss: 0.5787\n",
      "Iteration 4300, Loss: 0.5776\n",
      "Iteration 4400, Loss: 0.5766\n",
      "Iteration 4500, Loss: 0.5757\n",
      "Iteration 4600, Loss: 0.5747\n",
      "Iteration 4700, Loss: 0.5738\n",
      "Iteration 4800, Loss: 0.5729\n",
      "Iteration 4900, Loss: 0.5720\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8651\n",
      "Iteration 200, Loss: 0.7891\n",
      "Iteration 300, Loss: 0.7505\n",
      "Iteration 400, Loss: 0.7267\n",
      "Iteration 500, Loss: 0.7102\n",
      "Iteration 600, Loss: 0.6978\n",
      "Iteration 700, Loss: 0.6880\n",
      "Iteration 800, Loss: 0.6799\n",
      "Iteration 900, Loss: 0.6730\n",
      "Iteration 1000, Loss: 0.6671\n",
      "Iteration 1100, Loss: 0.6619\n",
      "Iteration 1200, Loss: 0.6573\n",
      "Iteration 1300, Loss: 0.6531\n",
      "Iteration 1400, Loss: 0.6493\n",
      "Iteration 1500, Loss: 0.6458\n",
      "Iteration 1600, Loss: 0.6426\n",
      "Iteration 1700, Loss: 0.6397\n",
      "Iteration 1800, Loss: 0.6370\n",
      "Iteration 1900, Loss: 0.6344\n",
      "Iteration 2000, Loss: 0.6320\n",
      "Iteration 2100, Loss: 0.6297\n",
      "Iteration 2200, Loss: 0.6275\n",
      "Iteration 2300, Loss: 0.6255\n",
      "Iteration 2400, Loss: 0.6235\n",
      "Iteration 2500, Loss: 0.6216\n",
      "Iteration 2600, Loss: 0.6198\n",
      "Iteration 2700, Loss: 0.6182\n",
      "Iteration 2800, Loss: 0.6165\n",
      "Iteration 2900, Loss: 0.6150\n",
      "Iteration 3000, Loss: 0.6134\n",
      "Iteration 3100, Loss: 0.6119\n",
      "Iteration 3200, Loss: 0.6105\n",
      "Iteration 3300, Loss: 0.6092\n",
      "Iteration 3400, Loss: 0.6078\n",
      "Iteration 3500, Loss: 0.6066\n",
      "Iteration 3600, Loss: 0.6053\n",
      "Iteration 3700, Loss: 0.6041\n",
      "Iteration 3800, Loss: 0.6029\n",
      "Iteration 3900, Loss: 0.6018\n",
      "Iteration 4000, Loss: 0.6007\n",
      "Iteration 4100, Loss: 0.5996\n",
      "Iteration 4200, Loss: 0.5985\n",
      "Iteration 4300, Loss: 0.5975\n",
      "Iteration 4400, Loss: 0.5965\n",
      "Iteration 4500, Loss: 0.5955\n",
      "Iteration 4600, Loss: 0.5945\n",
      "Iteration 4700, Loss: 0.5936\n",
      "Iteration 4800, Loss: 0.5927\n",
      "Iteration 4900, Loss: 0.5918\n",
      "148 270\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8604\n",
      "Iteration 200, Loss: 0.7823\n",
      "Iteration 300, Loss: 0.7427\n",
      "Iteration 400, Loss: 0.7183\n",
      "Iteration 500, Loss: 0.7011\n",
      "Iteration 600, Loss: 0.6881\n",
      "Iteration 700, Loss: 0.6777\n",
      "Iteration 800, Loss: 0.6690\n",
      "Iteration 900, Loss: 0.6616\n",
      "Iteration 1000, Loss: 0.6551\n",
      "Iteration 1100, Loss: 0.6495\n",
      "Iteration 1200, Loss: 0.6444\n",
      "Iteration 1300, Loss: 0.6399\n",
      "Iteration 1400, Loss: 0.6358\n",
      "Iteration 1500, Loss: 0.6320\n",
      "Iteration 1600, Loss: 0.6285\n",
      "Iteration 1700, Loss: 0.6252\n",
      "Iteration 1800, Loss: 0.6223\n",
      "Iteration 1900, Loss: 0.6194\n",
      "Iteration 2000, Loss: 0.6168\n",
      "Iteration 2100, Loss: 0.6143\n",
      "Iteration 2200, Loss: 0.6119\n",
      "Iteration 2300, Loss: 0.6097\n",
      "Iteration 2400, Loss: 0.6075\n",
      "Iteration 2500, Loss: 0.6055\n",
      "Iteration 2600, Loss: 0.6036\n",
      "Iteration 2700, Loss: 0.6017\n",
      "Iteration 2800, Loss: 0.5999\n",
      "Iteration 2900, Loss: 0.5982\n",
      "Iteration 3000, Loss: 0.5965\n",
      "Iteration 3100, Loss: 0.5949\n",
      "Iteration 3200, Loss: 0.5934\n",
      "Iteration 3300, Loss: 0.5919\n",
      "Iteration 3400, Loss: 0.5904\n",
      "Iteration 3500, Loss: 0.5890\n",
      "Iteration 3600, Loss: 0.5877\n",
      "Iteration 3700, Loss: 0.5863\n",
      "Iteration 3800, Loss: 0.5850\n",
      "Iteration 3900, Loss: 0.5838\n",
      "Iteration 4000, Loss: 0.5826\n",
      "Iteration 4100, Loss: 0.5814\n",
      "Iteration 4200, Loss: 0.5802\n",
      "Iteration 4300, Loss: 0.5791\n",
      "Iteration 4400, Loss: 0.5780\n",
      "Iteration 4500, Loss: 0.5769\n",
      "Iteration 4600, Loss: 0.5759\n",
      "Iteration 4700, Loss: 0.5749\n",
      "Iteration 4800, Loss: 0.5739\n",
      "Iteration 4900, Loss: 0.5729\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8609\n",
      "Iteration 200, Loss: 0.7816\n",
      "Iteration 300, Loss: 0.7408\n",
      "Iteration 400, Loss: 0.7160\n",
      "Iteration 500, Loss: 0.6989\n",
      "Iteration 600, Loss: 0.6863\n",
      "Iteration 700, Loss: 0.6763\n",
      "Iteration 800, Loss: 0.6681\n",
      "Iteration 900, Loss: 0.6613\n",
      "Iteration 1000, Loss: 0.6554\n",
      "Iteration 1100, Loss: 0.6502\n",
      "Iteration 1200, Loss: 0.6456\n",
      "Iteration 1300, Loss: 0.6416\n",
      "Iteration 1400, Loss: 0.6380\n",
      "Iteration 1500, Loss: 0.6347\n",
      "Iteration 1600, Loss: 0.6317\n",
      "Iteration 1700, Loss: 0.6289\n",
      "Iteration 1800, Loss: 0.6263\n",
      "Iteration 1900, Loss: 0.6239\n",
      "Iteration 2000, Loss: 0.6216\n",
      "Iteration 2100, Loss: 0.6196\n",
      "Iteration 2200, Loss: 0.6176\n",
      "Iteration 2300, Loss: 0.6158\n",
      "Iteration 2400, Loss: 0.6140\n",
      "Iteration 2500, Loss: 0.6123\n",
      "Iteration 2600, Loss: 0.6107\n",
      "Iteration 2700, Loss: 0.6092\n",
      "Iteration 2800, Loss: 0.6077\n",
      "Iteration 2900, Loss: 0.6063\n",
      "Iteration 3000, Loss: 0.6050\n",
      "Iteration 3100, Loss: 0.6037\n",
      "Iteration 3200, Loss: 0.6024\n",
      "Iteration 3300, Loss: 0.6012\n",
      "Iteration 3400, Loss: 0.6000\n",
      "Iteration 3500, Loss: 0.5989\n",
      "Iteration 3600, Loss: 0.5978\n",
      "Iteration 3700, Loss: 0.5968\n",
      "Iteration 3800, Loss: 0.5957\n",
      "Iteration 3900, Loss: 0.5947\n",
      "Iteration 4000, Loss: 0.5938\n",
      "Iteration 4100, Loss: 0.5928\n",
      "Iteration 4200, Loss: 0.5919\n",
      "Iteration 4300, Loss: 0.5910\n",
      "Iteration 4400, Loss: 0.5901\n",
      "Iteration 4500, Loss: 0.5892\n",
      "Iteration 4600, Loss: 0.5884\n",
      "Iteration 4700, Loss: 0.5876\n",
      "Iteration 4800, Loss: 0.5868\n",
      "Iteration 4900, Loss: 0.5860\n",
      "149 270\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8495\n",
      "Iteration 200, Loss: 0.7694\n",
      "Iteration 300, Loss: 0.7293\n",
      "Iteration 400, Loss: 0.7050\n",
      "Iteration 500, Loss: 0.6886\n",
      "Iteration 600, Loss: 0.6766\n",
      "Iteration 700, Loss: 0.6672\n",
      "Iteration 800, Loss: 0.6596\n",
      "Iteration 900, Loss: 0.6533\n",
      "Iteration 1000, Loss: 0.6479\n",
      "Iteration 1100, Loss: 0.6432\n",
      "Iteration 1200, Loss: 0.6390\n",
      "Iteration 1300, Loss: 0.6353\n",
      "Iteration 1400, Loss: 0.6320\n",
      "Iteration 1500, Loss: 0.6289\n",
      "Iteration 1600, Loss: 0.6261\n",
      "Iteration 1700, Loss: 0.6235\n",
      "Iteration 1800, Loss: 0.6211\n",
      "Iteration 1900, Loss: 0.6189\n",
      "Iteration 2000, Loss: 0.6168\n",
      "Iteration 2100, Loss: 0.6148\n",
      "Iteration 2200, Loss: 0.6129\n",
      "Iteration 2300, Loss: 0.6111\n",
      "Iteration 2400, Loss: 0.6094\n",
      "Iteration 2500, Loss: 0.6078\n",
      "Iteration 2600, Loss: 0.6063\n",
      "Iteration 2700, Loss: 0.6048\n",
      "Iteration 2800, Loss: 0.6033\n",
      "Iteration 2900, Loss: 0.6020\n",
      "Iteration 3000, Loss: 0.6007\n",
      "Iteration 3100, Loss: 0.5994\n",
      "Iteration 3200, Loss: 0.5982\n",
      "Iteration 3300, Loss: 0.5970\n",
      "Iteration 3400, Loss: 0.5958\n",
      "Iteration 3500, Loss: 0.5947\n",
      "Iteration 3600, Loss: 0.5936\n",
      "Iteration 3700, Loss: 0.5925\n",
      "Iteration 3800, Loss: 0.5915\n",
      "Iteration 3900, Loss: 0.5905\n",
      "Iteration 4000, Loss: 0.5895\n",
      "Iteration 4100, Loss: 0.5886\n",
      "Iteration 4200, Loss: 0.5877\n",
      "Iteration 4300, Loss: 0.5867\n",
      "Iteration 4400, Loss: 0.5859\n",
      "Iteration 4500, Loss: 0.5850\n",
      "Iteration 4600, Loss: 0.5842\n",
      "Iteration 4700, Loss: 0.5833\n",
      "Iteration 4800, Loss: 0.5825\n",
      "Iteration 4900, Loss: 0.5817\n",
      "Iteration 0, Loss: 1.0942\n",
      "Iteration 100, Loss: 0.8729\n",
      "Iteration 200, Loss: 0.7947\n",
      "Iteration 300, Loss: 0.7538\n",
      "Iteration 400, Loss: 0.7285\n",
      "Iteration 500, Loss: 0.7107\n",
      "Iteration 600, Loss: 0.6971\n",
      "Iteration 700, Loss: 0.6863\n",
      "Iteration 800, Loss: 0.6773\n",
      "Iteration 900, Loss: 0.6696\n",
      "Iteration 1000, Loss: 0.6630\n",
      "Iteration 1100, Loss: 0.6570\n",
      "Iteration 1200, Loss: 0.6518\n",
      "Iteration 1300, Loss: 0.6470\n",
      "Iteration 1400, Loss: 0.6427\n",
      "Iteration 1500, Loss: 0.6388\n",
      "Iteration 1600, Loss: 0.6351\n",
      "Iteration 1700, Loss: 0.6318\n",
      "Iteration 1800, Loss: 0.6286\n",
      "Iteration 1900, Loss: 0.6257\n",
      "Iteration 2000, Loss: 0.6230\n",
      "Iteration 2100, Loss: 0.6204\n",
      "Iteration 2200, Loss: 0.6179\n",
      "Iteration 2300, Loss: 0.6156\n",
      "Iteration 2400, Loss: 0.6134\n",
      "Iteration 2500, Loss: 0.6113\n",
      "Iteration 2600, Loss: 0.6093\n",
      "Iteration 2700, Loss: 0.6074\n",
      "Iteration 2800, Loss: 0.6055\n",
      "Iteration 2900, Loss: 0.6038\n",
      "Iteration 3000, Loss: 0.6020\n",
      "Iteration 3100, Loss: 0.6004\n",
      "Iteration 3200, Loss: 0.5988\n",
      "Iteration 3300, Loss: 0.5973\n",
      "Iteration 3400, Loss: 0.5958\n",
      "Iteration 3500, Loss: 0.5944\n",
      "Iteration 3600, Loss: 0.5930\n",
      "Iteration 3700, Loss: 0.5917\n",
      "Iteration 3800, Loss: 0.5904\n",
      "Iteration 3900, Loss: 0.5891\n",
      "Iteration 4000, Loss: 0.5879\n",
      "Iteration 4100, Loss: 0.5867\n",
      "Iteration 4200, Loss: 0.5855\n",
      "Iteration 4300, Loss: 0.5844\n",
      "Iteration 4400, Loss: 0.5833\n",
      "Iteration 4500, Loss: 0.5822\n",
      "Iteration 4600, Loss: 0.5811\n",
      "Iteration 4700, Loss: 0.5801\n",
      "Iteration 4800, Loss: 0.5791\n",
      "Iteration 4900, Loss: 0.5781\n",
      "150 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0227\n",
      "Iteration 200, Loss: 0.9689\n",
      "Iteration 300, Loss: 0.9280\n",
      "Iteration 400, Loss: 0.8955\n",
      "Iteration 500, Loss: 0.8690\n",
      "Iteration 600, Loss: 0.8471\n",
      "Iteration 700, Loss: 0.8286\n",
      "Iteration 800, Loss: 0.8129\n",
      "Iteration 900, Loss: 0.7994\n",
      "Iteration 1000, Loss: 0.7877\n",
      "Iteration 1100, Loss: 0.7774\n",
      "Iteration 1200, Loss: 0.7684\n",
      "Iteration 1300, Loss: 0.7603\n",
      "Iteration 1400, Loss: 0.7532\n",
      "Iteration 1500, Loss: 0.7467\n",
      "Iteration 1600, Loss: 0.7409\n",
      "Iteration 1700, Loss: 0.7356\n",
      "Iteration 1800, Loss: 0.7307\n",
      "Iteration 1900, Loss: 0.7263\n",
      "Iteration 2000, Loss: 0.7222\n",
      "Iteration 2100, Loss: 0.7184\n",
      "Iteration 2200, Loss: 0.7149\n",
      "Iteration 2300, Loss: 0.7116\n",
      "Iteration 2400, Loss: 0.7085\n",
      "Iteration 2500, Loss: 0.7056\n",
      "Iteration 2600, Loss: 0.7029\n",
      "Iteration 2700, Loss: 0.7003\n",
      "Iteration 2800, Loss: 0.6979\n",
      "Iteration 2900, Loss: 0.6956\n",
      "Iteration 3000, Loss: 0.6934\n",
      "Iteration 3100, Loss: 0.6913\n",
      "Iteration 3200, Loss: 0.6894\n",
      "Iteration 3300, Loss: 0.6875\n",
      "Iteration 3400, Loss: 0.6857\n",
      "Iteration 3500, Loss: 0.6839\n",
      "Iteration 3600, Loss: 0.6823\n",
      "Iteration 3700, Loss: 0.6807\n",
      "Iteration 3800, Loss: 0.6791\n",
      "Iteration 3900, Loss: 0.6776\n",
      "Iteration 4000, Loss: 0.6762\n",
      "Iteration 4100, Loss: 0.6748\n",
      "Iteration 4200, Loss: 0.6735\n",
      "Iteration 4300, Loss: 0.6722\n",
      "Iteration 4400, Loss: 0.6709\n",
      "Iteration 4500, Loss: 0.6697\n",
      "Iteration 4600, Loss: 0.6685\n",
      "Iteration 4700, Loss: 0.6674\n",
      "Iteration 4800, Loss: 0.6662\n",
      "Iteration 4900, Loss: 0.6652\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0170\n",
      "Iteration 200, Loss: 0.9623\n",
      "Iteration 300, Loss: 0.9217\n",
      "Iteration 400, Loss: 0.8902\n",
      "Iteration 500, Loss: 0.8649\n",
      "Iteration 600, Loss: 0.8441\n",
      "Iteration 700, Loss: 0.8266\n",
      "Iteration 800, Loss: 0.8118\n",
      "Iteration 900, Loss: 0.7989\n",
      "Iteration 1000, Loss: 0.7876\n",
      "Iteration 1100, Loss: 0.7777\n",
      "Iteration 1200, Loss: 0.7688\n",
      "Iteration 1300, Loss: 0.7608\n",
      "Iteration 1400, Loss: 0.7535\n",
      "Iteration 1500, Loss: 0.7470\n",
      "Iteration 1600, Loss: 0.7410\n",
      "Iteration 1700, Loss: 0.7354\n",
      "Iteration 1800, Loss: 0.7303\n",
      "Iteration 1900, Loss: 0.7256\n",
      "Iteration 2000, Loss: 0.7212\n",
      "Iteration 2100, Loss: 0.7171\n",
      "Iteration 2200, Loss: 0.7133\n",
      "Iteration 2300, Loss: 0.7096\n",
      "Iteration 2400, Loss: 0.7062\n",
      "Iteration 2500, Loss: 0.7030\n",
      "Iteration 2600, Loss: 0.7000\n",
      "Iteration 2700, Loss: 0.6971\n",
      "Iteration 2800, Loss: 0.6944\n",
      "Iteration 2900, Loss: 0.6918\n",
      "Iteration 3000, Loss: 0.6893\n",
      "Iteration 3100, Loss: 0.6869\n",
      "Iteration 3200, Loss: 0.6846\n",
      "Iteration 3300, Loss: 0.6824\n",
      "Iteration 3400, Loss: 0.6803\n",
      "Iteration 3500, Loss: 0.6783\n",
      "Iteration 3600, Loss: 0.6763\n",
      "Iteration 3700, Loss: 0.6744\n",
      "Iteration 3800, Loss: 0.6726\n",
      "Iteration 3900, Loss: 0.6709\n",
      "Iteration 4000, Loss: 0.6692\n",
      "Iteration 4100, Loss: 0.6675\n",
      "Iteration 4200, Loss: 0.6659\n",
      "Iteration 4300, Loss: 0.6644\n",
      "Iteration 4400, Loss: 0.6629\n",
      "Iteration 4500, Loss: 0.6615\n",
      "Iteration 4600, Loss: 0.6601\n",
      "Iteration 4700, Loss: 0.6587\n",
      "Iteration 4800, Loss: 0.6574\n",
      "Iteration 4900, Loss: 0.6561\n",
      "151 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0223\n",
      "Iteration 200, Loss: 0.9697\n",
      "Iteration 300, Loss: 0.9302\n",
      "Iteration 400, Loss: 0.8991\n",
      "Iteration 500, Loss: 0.8739\n",
      "Iteration 600, Loss: 0.8531\n",
      "Iteration 700, Loss: 0.8355\n",
      "Iteration 800, Loss: 0.8205\n",
      "Iteration 900, Loss: 0.8075\n",
      "Iteration 1000, Loss: 0.7962\n",
      "Iteration 1100, Loss: 0.7862\n",
      "Iteration 1200, Loss: 0.7774\n",
      "Iteration 1300, Loss: 0.7694\n",
      "Iteration 1400, Loss: 0.7623\n",
      "Iteration 1500, Loss: 0.7559\n",
      "Iteration 1600, Loss: 0.7500\n",
      "Iteration 1700, Loss: 0.7446\n",
      "Iteration 1800, Loss: 0.7396\n",
      "Iteration 1900, Loss: 0.7351\n",
      "Iteration 2000, Loss: 0.7308\n",
      "Iteration 2100, Loss: 0.7269\n",
      "Iteration 2200, Loss: 0.7232\n",
      "Iteration 2300, Loss: 0.7197\n",
      "Iteration 2400, Loss: 0.7165\n",
      "Iteration 2500, Loss: 0.7134\n",
      "Iteration 2600, Loss: 0.7105\n",
      "Iteration 2700, Loss: 0.7077\n",
      "Iteration 2800, Loss: 0.7051\n",
      "Iteration 2900, Loss: 0.7026\n",
      "Iteration 3000, Loss: 0.7003\n",
      "Iteration 3100, Loss: 0.6980\n",
      "Iteration 3200, Loss: 0.6958\n",
      "Iteration 3300, Loss: 0.6937\n",
      "Iteration 3400, Loss: 0.6918\n",
      "Iteration 3500, Loss: 0.6898\n",
      "Iteration 3600, Loss: 0.6880\n",
      "Iteration 3700, Loss: 0.6862\n",
      "Iteration 3800, Loss: 0.6845\n",
      "Iteration 3900, Loss: 0.6828\n",
      "Iteration 4000, Loss: 0.6812\n",
      "Iteration 4100, Loss: 0.6797\n",
      "Iteration 4200, Loss: 0.6782\n",
      "Iteration 4300, Loss: 0.6767\n",
      "Iteration 4400, Loss: 0.6753\n",
      "Iteration 4500, Loss: 0.6739\n",
      "Iteration 4600, Loss: 0.6726\n",
      "Iteration 4700, Loss: 0.6713\n",
      "Iteration 4800, Loss: 0.6700\n",
      "Iteration 4900, Loss: 0.6688\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0180\n",
      "Iteration 200, Loss: 0.9622\n",
      "Iteration 300, Loss: 0.9204\n",
      "Iteration 400, Loss: 0.8876\n",
      "Iteration 500, Loss: 0.8613\n",
      "Iteration 600, Loss: 0.8394\n",
      "Iteration 700, Loss: 0.8211\n",
      "Iteration 800, Loss: 0.8054\n",
      "Iteration 900, Loss: 0.7919\n",
      "Iteration 1000, Loss: 0.7802\n",
      "Iteration 1100, Loss: 0.7698\n",
      "Iteration 1200, Loss: 0.7607\n",
      "Iteration 1300, Loss: 0.7526\n",
      "Iteration 1400, Loss: 0.7454\n",
      "Iteration 1500, Loss: 0.7389\n",
      "Iteration 1600, Loss: 0.7329\n",
      "Iteration 1700, Loss: 0.7276\n",
      "Iteration 1800, Loss: 0.7226\n",
      "Iteration 1900, Loss: 0.7181\n",
      "Iteration 2000, Loss: 0.7139\n",
      "Iteration 2100, Loss: 0.7100\n",
      "Iteration 2200, Loss: 0.7064\n",
      "Iteration 2300, Loss: 0.7030\n",
      "Iteration 2400, Loss: 0.6999\n",
      "Iteration 2500, Loss: 0.6969\n",
      "Iteration 2600, Loss: 0.6941\n",
      "Iteration 2700, Loss: 0.6915\n",
      "Iteration 2800, Loss: 0.6890\n",
      "Iteration 2900, Loss: 0.6866\n",
      "Iteration 3000, Loss: 0.6843\n",
      "Iteration 3100, Loss: 0.6822\n",
      "Iteration 3200, Loss: 0.6801\n",
      "Iteration 3300, Loss: 0.6782\n",
      "Iteration 3400, Loss: 0.6763\n",
      "Iteration 3500, Loss: 0.6745\n",
      "Iteration 3600, Loss: 0.6728\n",
      "Iteration 3700, Loss: 0.6711\n",
      "Iteration 3800, Loss: 0.6695\n",
      "Iteration 3900, Loss: 0.6680\n",
      "Iteration 4000, Loss: 0.6665\n",
      "Iteration 4100, Loss: 0.6650\n",
      "Iteration 4200, Loss: 0.6636\n",
      "Iteration 4300, Loss: 0.6623\n",
      "Iteration 4400, Loss: 0.6610\n",
      "Iteration 4500, Loss: 0.6597\n",
      "Iteration 4600, Loss: 0.6585\n",
      "Iteration 4700, Loss: 0.6573\n",
      "Iteration 4800, Loss: 0.6562\n",
      "Iteration 4900, Loss: 0.6550\n",
      "152 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0182\n",
      "Iteration 200, Loss: 0.9626\n",
      "Iteration 300, Loss: 0.9208\n",
      "Iteration 400, Loss: 0.8881\n",
      "Iteration 500, Loss: 0.8616\n",
      "Iteration 600, Loss: 0.8399\n",
      "Iteration 700, Loss: 0.8217\n",
      "Iteration 800, Loss: 0.8062\n",
      "Iteration 900, Loss: 0.7928\n",
      "Iteration 1000, Loss: 0.7812\n",
      "Iteration 1100, Loss: 0.7710\n",
      "Iteration 1200, Loss: 0.7620\n",
      "Iteration 1300, Loss: 0.7540\n",
      "Iteration 1400, Loss: 0.7467\n",
      "Iteration 1500, Loss: 0.7402\n",
      "Iteration 1600, Loss: 0.7343\n",
      "Iteration 1700, Loss: 0.7289\n",
      "Iteration 1800, Loss: 0.7240\n",
      "Iteration 1900, Loss: 0.7194\n",
      "Iteration 2000, Loss: 0.7152\n",
      "Iteration 2100, Loss: 0.7112\n",
      "Iteration 2200, Loss: 0.7076\n",
      "Iteration 2300, Loss: 0.7042\n",
      "Iteration 2400, Loss: 0.7010\n",
      "Iteration 2500, Loss: 0.6979\n",
      "Iteration 2600, Loss: 0.6951\n",
      "Iteration 2700, Loss: 0.6924\n",
      "Iteration 2800, Loss: 0.6898\n",
      "Iteration 2900, Loss: 0.6874\n",
      "Iteration 3000, Loss: 0.6851\n",
      "Iteration 3100, Loss: 0.6829\n",
      "Iteration 3200, Loss: 0.6807\n",
      "Iteration 3300, Loss: 0.6787\n",
      "Iteration 3400, Loss: 0.6768\n",
      "Iteration 3500, Loss: 0.6749\n",
      "Iteration 3600, Loss: 0.6731\n",
      "Iteration 3700, Loss: 0.6714\n",
      "Iteration 3800, Loss: 0.6697\n",
      "Iteration 3900, Loss: 0.6681\n",
      "Iteration 4000, Loss: 0.6666\n",
      "Iteration 4100, Loss: 0.6651\n",
      "Iteration 4200, Loss: 0.6636\n",
      "Iteration 4300, Loss: 0.6622\n",
      "Iteration 4400, Loss: 0.6609\n",
      "Iteration 4500, Loss: 0.6595\n",
      "Iteration 4600, Loss: 0.6582\n",
      "Iteration 4700, Loss: 0.6570\n",
      "Iteration 4800, Loss: 0.6558\n",
      "Iteration 4900, Loss: 0.6546\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0218\n",
      "Iteration 200, Loss: 0.9691\n",
      "Iteration 300, Loss: 0.9295\n",
      "Iteration 400, Loss: 0.8984\n",
      "Iteration 500, Loss: 0.8732\n",
      "Iteration 600, Loss: 0.8522\n",
      "Iteration 700, Loss: 0.8346\n",
      "Iteration 800, Loss: 0.8195\n",
      "Iteration 900, Loss: 0.8064\n",
      "Iteration 1000, Loss: 0.7950\n",
      "Iteration 1100, Loss: 0.7849\n",
      "Iteration 1200, Loss: 0.7760\n",
      "Iteration 1300, Loss: 0.7680\n",
      "Iteration 1400, Loss: 0.7609\n",
      "Iteration 1500, Loss: 0.7544\n",
      "Iteration 1600, Loss: 0.7485\n",
      "Iteration 1700, Loss: 0.7431\n",
      "Iteration 1800, Loss: 0.7382\n",
      "Iteration 1900, Loss: 0.7337\n",
      "Iteration 2000, Loss: 0.7294\n",
      "Iteration 2100, Loss: 0.7255\n",
      "Iteration 2200, Loss: 0.7219\n",
      "Iteration 2300, Loss: 0.7185\n",
      "Iteration 2400, Loss: 0.7153\n",
      "Iteration 2500, Loss: 0.7123\n",
      "Iteration 2600, Loss: 0.7095\n",
      "Iteration 2700, Loss: 0.7068\n",
      "Iteration 2800, Loss: 0.7042\n",
      "Iteration 2900, Loss: 0.7018\n",
      "Iteration 3000, Loss: 0.6995\n",
      "Iteration 3100, Loss: 0.6973\n",
      "Iteration 3200, Loss: 0.6952\n",
      "Iteration 3300, Loss: 0.6932\n",
      "Iteration 3400, Loss: 0.6913\n",
      "Iteration 3500, Loss: 0.6894\n",
      "Iteration 3600, Loss: 0.6876\n",
      "Iteration 3700, Loss: 0.6859\n",
      "Iteration 3800, Loss: 0.6842\n",
      "Iteration 3900, Loss: 0.6826\n",
      "Iteration 4000, Loss: 0.6811\n",
      "Iteration 4100, Loss: 0.6796\n",
      "Iteration 4200, Loss: 0.6781\n",
      "Iteration 4300, Loss: 0.6767\n",
      "Iteration 4400, Loss: 0.6753\n",
      "Iteration 4500, Loss: 0.6740\n",
      "Iteration 4600, Loss: 0.6727\n",
      "Iteration 4700, Loss: 0.6715\n",
      "Iteration 4800, Loss: 0.6703\n",
      "Iteration 4900, Loss: 0.6691\n",
      "153 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0193\n",
      "Iteration 200, Loss: 0.9659\n",
      "Iteration 300, Loss: 0.9263\n",
      "Iteration 400, Loss: 0.8953\n",
      "Iteration 500, Loss: 0.8703\n",
      "Iteration 600, Loss: 0.8497\n",
      "Iteration 700, Loss: 0.8323\n",
      "Iteration 800, Loss: 0.8175\n",
      "Iteration 900, Loss: 0.8046\n",
      "Iteration 1000, Loss: 0.7934\n",
      "Iteration 1100, Loss: 0.7834\n",
      "Iteration 1200, Loss: 0.7745\n",
      "Iteration 1300, Loss: 0.7666\n",
      "Iteration 1400, Loss: 0.7593\n",
      "Iteration 1500, Loss: 0.7528\n",
      "Iteration 1600, Loss: 0.7468\n",
      "Iteration 1700, Loss: 0.7413\n",
      "Iteration 1800, Loss: 0.7362\n",
      "Iteration 1900, Loss: 0.7315\n",
      "Iteration 2000, Loss: 0.7271\n",
      "Iteration 2100, Loss: 0.7230\n",
      "Iteration 2200, Loss: 0.7192\n",
      "Iteration 2300, Loss: 0.7156\n",
      "Iteration 2400, Loss: 0.7123\n",
      "Iteration 2500, Loss: 0.7091\n",
      "Iteration 2600, Loss: 0.7060\n",
      "Iteration 2700, Loss: 0.7032\n",
      "Iteration 2800, Loss: 0.7005\n",
      "Iteration 2900, Loss: 0.6979\n",
      "Iteration 3000, Loss: 0.6954\n",
      "Iteration 3100, Loss: 0.6930\n",
      "Iteration 3200, Loss: 0.6907\n",
      "Iteration 3300, Loss: 0.6885\n",
      "Iteration 3400, Loss: 0.6864\n",
      "Iteration 3500, Loss: 0.6844\n",
      "Iteration 3600, Loss: 0.6825\n",
      "Iteration 3700, Loss: 0.6806\n",
      "Iteration 3800, Loss: 0.6788\n",
      "Iteration 3900, Loss: 0.6770\n",
      "Iteration 4000, Loss: 0.6754\n",
      "Iteration 4100, Loss: 0.6737\n",
      "Iteration 4200, Loss: 0.6722\n",
      "Iteration 4300, Loss: 0.6706\n",
      "Iteration 4400, Loss: 0.6691\n",
      "Iteration 4500, Loss: 0.6677\n",
      "Iteration 4600, Loss: 0.6663\n",
      "Iteration 4700, Loss: 0.6649\n",
      "Iteration 4800, Loss: 0.6636\n",
      "Iteration 4900, Loss: 0.6623\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0204\n",
      "Iteration 200, Loss: 0.9652\n",
      "Iteration 300, Loss: 0.9233\n",
      "Iteration 400, Loss: 0.8901\n",
      "Iteration 500, Loss: 0.8631\n",
      "Iteration 600, Loss: 0.8408\n",
      "Iteration 700, Loss: 0.8220\n",
      "Iteration 800, Loss: 0.8060\n",
      "Iteration 900, Loss: 0.7922\n",
      "Iteration 1000, Loss: 0.7803\n",
      "Iteration 1100, Loss: 0.7697\n",
      "Iteration 1200, Loss: 0.7605\n",
      "Iteration 1300, Loss: 0.7522\n",
      "Iteration 1400, Loss: 0.7447\n",
      "Iteration 1500, Loss: 0.7381\n",
      "Iteration 1600, Loss: 0.7320\n",
      "Iteration 1700, Loss: 0.7265\n",
      "Iteration 1800, Loss: 0.7215\n",
      "Iteration 1900, Loss: 0.7169\n",
      "Iteration 2000, Loss: 0.7127\n",
      "Iteration 2100, Loss: 0.7087\n",
      "Iteration 2200, Loss: 0.7051\n",
      "Iteration 2300, Loss: 0.7017\n",
      "Iteration 2400, Loss: 0.6985\n",
      "Iteration 2500, Loss: 0.6955\n",
      "Iteration 2600, Loss: 0.6926\n",
      "Iteration 2700, Loss: 0.6900\n",
      "Iteration 2800, Loss: 0.6874\n",
      "Iteration 2900, Loss: 0.6851\n",
      "Iteration 3000, Loss: 0.6828\n",
      "Iteration 3100, Loss: 0.6806\n",
      "Iteration 3200, Loss: 0.6786\n",
      "Iteration 3300, Loss: 0.6766\n",
      "Iteration 3400, Loss: 0.6747\n",
      "Iteration 3500, Loss: 0.6729\n",
      "Iteration 3600, Loss: 0.6711\n",
      "Iteration 3700, Loss: 0.6695\n",
      "Iteration 3800, Loss: 0.6679\n",
      "Iteration 3900, Loss: 0.6663\n",
      "Iteration 4000, Loss: 0.6648\n",
      "Iteration 4100, Loss: 0.6633\n",
      "Iteration 4200, Loss: 0.6619\n",
      "Iteration 4300, Loss: 0.6606\n",
      "Iteration 4400, Loss: 0.6593\n",
      "Iteration 4500, Loss: 0.6580\n",
      "Iteration 4600, Loss: 0.6567\n",
      "Iteration 4700, Loss: 0.6555\n",
      "Iteration 4800, Loss: 0.6544\n",
      "Iteration 4900, Loss: 0.6532\n",
      "154 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0202\n",
      "Iteration 200, Loss: 0.9676\n",
      "Iteration 300, Loss: 0.9290\n",
      "Iteration 400, Loss: 0.8991\n",
      "Iteration 500, Loss: 0.8752\n",
      "Iteration 600, Loss: 0.8556\n",
      "Iteration 700, Loss: 0.8393\n",
      "Iteration 800, Loss: 0.8254\n",
      "Iteration 900, Loss: 0.8134\n",
      "Iteration 1000, Loss: 0.8030\n",
      "Iteration 1100, Loss: 0.7938\n",
      "Iteration 1200, Loss: 0.7857\n",
      "Iteration 1300, Loss: 0.7784\n",
      "Iteration 1400, Loss: 0.7718\n",
      "Iteration 1500, Loss: 0.7659\n",
      "Iteration 1600, Loss: 0.7605\n",
      "Iteration 1700, Loss: 0.7555\n",
      "Iteration 1800, Loss: 0.7510\n",
      "Iteration 1900, Loss: 0.7467\n",
      "Iteration 2000, Loss: 0.7428\n",
      "Iteration 2100, Loss: 0.7392\n",
      "Iteration 2200, Loss: 0.7358\n",
      "Iteration 2300, Loss: 0.7325\n",
      "Iteration 2400, Loss: 0.7295\n",
      "Iteration 2500, Loss: 0.7267\n",
      "Iteration 2600, Loss: 0.7240\n",
      "Iteration 2700, Loss: 0.7214\n",
      "Iteration 2800, Loss: 0.7190\n",
      "Iteration 2900, Loss: 0.7167\n",
      "Iteration 3000, Loss: 0.7145\n",
      "Iteration 3100, Loss: 0.7124\n",
      "Iteration 3200, Loss: 0.7104\n",
      "Iteration 3300, Loss: 0.7085\n",
      "Iteration 3400, Loss: 0.7066\n",
      "Iteration 3500, Loss: 0.7048\n",
      "Iteration 3600, Loss: 0.7031\n",
      "Iteration 3700, Loss: 0.7015\n",
      "Iteration 3800, Loss: 0.6999\n",
      "Iteration 3900, Loss: 0.6983\n",
      "Iteration 4000, Loss: 0.6968\n",
      "Iteration 4100, Loss: 0.6954\n",
      "Iteration 4200, Loss: 0.6940\n",
      "Iteration 4300, Loss: 0.6926\n",
      "Iteration 4400, Loss: 0.6913\n",
      "Iteration 4500, Loss: 0.6900\n",
      "Iteration 4600, Loss: 0.6888\n",
      "Iteration 4700, Loss: 0.6876\n",
      "Iteration 4800, Loss: 0.6864\n",
      "Iteration 4900, Loss: 0.6853\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0198\n",
      "Iteration 200, Loss: 0.9639\n",
      "Iteration 300, Loss: 0.9211\n",
      "Iteration 400, Loss: 0.8871\n",
      "Iteration 500, Loss: 0.8593\n",
      "Iteration 600, Loss: 0.8362\n",
      "Iteration 700, Loss: 0.8166\n",
      "Iteration 800, Loss: 0.7999\n",
      "Iteration 900, Loss: 0.7854\n",
      "Iteration 1000, Loss: 0.7728\n",
      "Iteration 1100, Loss: 0.7618\n",
      "Iteration 1200, Loss: 0.7520\n",
      "Iteration 1300, Loss: 0.7433\n",
      "Iteration 1400, Loss: 0.7355\n",
      "Iteration 1500, Loss: 0.7285\n",
      "Iteration 1600, Loss: 0.7221\n",
      "Iteration 1700, Loss: 0.7163\n",
      "Iteration 1800, Loss: 0.7110\n",
      "Iteration 1900, Loss: 0.7061\n",
      "Iteration 2000, Loss: 0.7015\n",
      "Iteration 2100, Loss: 0.6974\n",
      "Iteration 2200, Loss: 0.6935\n",
      "Iteration 2300, Loss: 0.6898\n",
      "Iteration 2400, Loss: 0.6864\n",
      "Iteration 2500, Loss: 0.6832\n",
      "Iteration 2600, Loss: 0.6802\n",
      "Iteration 2700, Loss: 0.6774\n",
      "Iteration 2800, Loss: 0.6747\n",
      "Iteration 2900, Loss: 0.6721\n",
      "Iteration 3000, Loss: 0.6697\n",
      "Iteration 3100, Loss: 0.6674\n",
      "Iteration 3200, Loss: 0.6652\n",
      "Iteration 3300, Loss: 0.6631\n",
      "Iteration 3400, Loss: 0.6611\n",
      "Iteration 3500, Loss: 0.6592\n",
      "Iteration 3600, Loss: 0.6573\n",
      "Iteration 3700, Loss: 0.6556\n",
      "Iteration 3800, Loss: 0.6539\n",
      "Iteration 3900, Loss: 0.6522\n",
      "Iteration 4000, Loss: 0.6506\n",
      "Iteration 4100, Loss: 0.6491\n",
      "Iteration 4200, Loss: 0.6476\n",
      "Iteration 4300, Loss: 0.6462\n",
      "Iteration 4400, Loss: 0.6448\n",
      "Iteration 4500, Loss: 0.6435\n",
      "Iteration 4600, Loss: 0.6422\n",
      "Iteration 4700, Loss: 0.6409\n",
      "Iteration 4800, Loss: 0.6397\n",
      "Iteration 4900, Loss: 0.6385\n",
      "155 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0235\n",
      "Iteration 200, Loss: 0.9706\n",
      "Iteration 300, Loss: 0.9306\n",
      "Iteration 400, Loss: 0.8990\n",
      "Iteration 500, Loss: 0.8734\n",
      "Iteration 600, Loss: 0.8522\n",
      "Iteration 700, Loss: 0.8343\n",
      "Iteration 800, Loss: 0.8191\n",
      "Iteration 900, Loss: 0.8058\n",
      "Iteration 1000, Loss: 0.7943\n",
      "Iteration 1100, Loss: 0.7842\n",
      "Iteration 1200, Loss: 0.7752\n",
      "Iteration 1300, Loss: 0.7672\n",
      "Iteration 1400, Loss: 0.7599\n",
      "Iteration 1500, Loss: 0.7534\n",
      "Iteration 1600, Loss: 0.7474\n",
      "Iteration 1700, Loss: 0.7419\n",
      "Iteration 1800, Loss: 0.7369\n",
      "Iteration 1900, Loss: 0.7323\n",
      "Iteration 2000, Loss: 0.7280\n",
      "Iteration 2100, Loss: 0.7240\n",
      "Iteration 2200, Loss: 0.7203\n",
      "Iteration 2300, Loss: 0.7168\n",
      "Iteration 2400, Loss: 0.7135\n",
      "Iteration 2500, Loss: 0.7104\n",
      "Iteration 2600, Loss: 0.7075\n",
      "Iteration 2700, Loss: 0.7047\n",
      "Iteration 2800, Loss: 0.7021\n",
      "Iteration 2900, Loss: 0.6996\n",
      "Iteration 3000, Loss: 0.6972\n",
      "Iteration 3100, Loss: 0.6950\n",
      "Iteration 3200, Loss: 0.6928\n",
      "Iteration 3300, Loss: 0.6907\n",
      "Iteration 3400, Loss: 0.6887\n",
      "Iteration 3500, Loss: 0.6868\n",
      "Iteration 3600, Loss: 0.6850\n",
      "Iteration 3700, Loss: 0.6832\n",
      "Iteration 3800, Loss: 0.6815\n",
      "Iteration 3900, Loss: 0.6798\n",
      "Iteration 4000, Loss: 0.6782\n",
      "Iteration 4100, Loss: 0.6767\n",
      "Iteration 4200, Loss: 0.6752\n",
      "Iteration 4300, Loss: 0.6738\n",
      "Iteration 4400, Loss: 0.6724\n",
      "Iteration 4500, Loss: 0.6710\n",
      "Iteration 4600, Loss: 0.6697\n",
      "Iteration 4700, Loss: 0.6684\n",
      "Iteration 4800, Loss: 0.6672\n",
      "Iteration 4900, Loss: 0.6660\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0161\n",
      "Iteration 200, Loss: 0.9601\n",
      "Iteration 300, Loss: 0.9184\n",
      "Iteration 400, Loss: 0.8858\n",
      "Iteration 500, Loss: 0.8597\n",
      "Iteration 600, Loss: 0.8381\n",
      "Iteration 700, Loss: 0.8200\n",
      "Iteration 800, Loss: 0.8046\n",
      "Iteration 900, Loss: 0.7914\n",
      "Iteration 1000, Loss: 0.7798\n",
      "Iteration 1100, Loss: 0.7697\n",
      "Iteration 1200, Loss: 0.7606\n",
      "Iteration 1300, Loss: 0.7526\n",
      "Iteration 1400, Loss: 0.7454\n",
      "Iteration 1500, Loss: 0.7388\n",
      "Iteration 1600, Loss: 0.7329\n",
      "Iteration 1700, Loss: 0.7275\n",
      "Iteration 1800, Loss: 0.7225\n",
      "Iteration 1900, Loss: 0.7179\n",
      "Iteration 2000, Loss: 0.7137\n",
      "Iteration 2100, Loss: 0.7098\n",
      "Iteration 2200, Loss: 0.7061\n",
      "Iteration 2300, Loss: 0.7027\n",
      "Iteration 2400, Loss: 0.6995\n",
      "Iteration 2500, Loss: 0.6965\n",
      "Iteration 2600, Loss: 0.6936\n",
      "Iteration 2700, Loss: 0.6909\n",
      "Iteration 2800, Loss: 0.6884\n",
      "Iteration 2900, Loss: 0.6859\n",
      "Iteration 3000, Loss: 0.6836\n",
      "Iteration 3100, Loss: 0.6814\n",
      "Iteration 3200, Loss: 0.6793\n",
      "Iteration 3300, Loss: 0.6773\n",
      "Iteration 3400, Loss: 0.6754\n",
      "Iteration 3500, Loss: 0.6735\n",
      "Iteration 3600, Loss: 0.6717\n",
      "Iteration 3700, Loss: 0.6700\n",
      "Iteration 3800, Loss: 0.6683\n",
      "Iteration 3900, Loss: 0.6667\n",
      "Iteration 4000, Loss: 0.6652\n",
      "Iteration 4100, Loss: 0.6637\n",
      "Iteration 4200, Loss: 0.6623\n",
      "Iteration 4300, Loss: 0.6609\n",
      "Iteration 4400, Loss: 0.6595\n",
      "Iteration 4500, Loss: 0.6582\n",
      "Iteration 4600, Loss: 0.6569\n",
      "Iteration 4700, Loss: 0.6557\n",
      "Iteration 4800, Loss: 0.6544\n",
      "Iteration 4900, Loss: 0.6533\n",
      "156 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0262\n",
      "Iteration 200, Loss: 0.9746\n",
      "Iteration 300, Loss: 0.9350\n",
      "Iteration 400, Loss: 0.9034\n",
      "Iteration 500, Loss: 0.8775\n",
      "Iteration 600, Loss: 0.8560\n",
      "Iteration 700, Loss: 0.8377\n",
      "Iteration 800, Loss: 0.8221\n",
      "Iteration 900, Loss: 0.8085\n",
      "Iteration 1000, Loss: 0.7967\n",
      "Iteration 1100, Loss: 0.7863\n",
      "Iteration 1200, Loss: 0.7770\n",
      "Iteration 1300, Loss: 0.7688\n",
      "Iteration 1400, Loss: 0.7613\n",
      "Iteration 1500, Loss: 0.7546\n",
      "Iteration 1600, Loss: 0.7485\n",
      "Iteration 1700, Loss: 0.7430\n",
      "Iteration 1800, Loss: 0.7378\n",
      "Iteration 1900, Loss: 0.7331\n",
      "Iteration 2000, Loss: 0.7287\n",
      "Iteration 2100, Loss: 0.7247\n",
      "Iteration 2200, Loss: 0.7209\n",
      "Iteration 2300, Loss: 0.7173\n",
      "Iteration 2400, Loss: 0.7139\n",
      "Iteration 2500, Loss: 0.7108\n",
      "Iteration 2600, Loss: 0.7078\n",
      "Iteration 2700, Loss: 0.7050\n",
      "Iteration 2800, Loss: 0.7023\n",
      "Iteration 2900, Loss: 0.6997\n",
      "Iteration 3000, Loss: 0.6973\n",
      "Iteration 3100, Loss: 0.6950\n",
      "Iteration 3200, Loss: 0.6927\n",
      "Iteration 3300, Loss: 0.6906\n",
      "Iteration 3400, Loss: 0.6886\n",
      "Iteration 3500, Loss: 0.6866\n",
      "Iteration 3600, Loss: 0.6847\n",
      "Iteration 3700, Loss: 0.6829\n",
      "Iteration 3800, Loss: 0.6811\n",
      "Iteration 3900, Loss: 0.6794\n",
      "Iteration 4000, Loss: 0.6778\n",
      "Iteration 4100, Loss: 0.6762\n",
      "Iteration 4200, Loss: 0.6747\n",
      "Iteration 4300, Loss: 0.6732\n",
      "Iteration 4400, Loss: 0.6717\n",
      "Iteration 4500, Loss: 0.6703\n",
      "Iteration 4600, Loss: 0.6690\n",
      "Iteration 4700, Loss: 0.6676\n",
      "Iteration 4800, Loss: 0.6663\n",
      "Iteration 4900, Loss: 0.6651\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0129\n",
      "Iteration 200, Loss: 0.9556\n",
      "Iteration 300, Loss: 0.9134\n",
      "Iteration 400, Loss: 0.8808\n",
      "Iteration 500, Loss: 0.8549\n",
      "Iteration 600, Loss: 0.8337\n",
      "Iteration 700, Loss: 0.8160\n",
      "Iteration 800, Loss: 0.8011\n",
      "Iteration 900, Loss: 0.7882\n",
      "Iteration 1000, Loss: 0.7771\n",
      "Iteration 1100, Loss: 0.7673\n",
      "Iteration 1200, Loss: 0.7587\n",
      "Iteration 1300, Loss: 0.7509\n",
      "Iteration 1400, Loss: 0.7440\n",
      "Iteration 1500, Loss: 0.7377\n",
      "Iteration 1600, Loss: 0.7320\n",
      "Iteration 1700, Loss: 0.7268\n",
      "Iteration 1800, Loss: 0.7220\n",
      "Iteration 1900, Loss: 0.7176\n",
      "Iteration 2000, Loss: 0.7135\n",
      "Iteration 2100, Loss: 0.7097\n",
      "Iteration 2200, Loss: 0.7062\n",
      "Iteration 2300, Loss: 0.7028\n",
      "Iteration 2400, Loss: 0.6997\n",
      "Iteration 2500, Loss: 0.6968\n",
      "Iteration 2600, Loss: 0.6940\n",
      "Iteration 2700, Loss: 0.6914\n",
      "Iteration 2800, Loss: 0.6889\n",
      "Iteration 2900, Loss: 0.6865\n",
      "Iteration 3000, Loss: 0.6843\n",
      "Iteration 3100, Loss: 0.6821\n",
      "Iteration 3200, Loss: 0.6800\n",
      "Iteration 3300, Loss: 0.6781\n",
      "Iteration 3400, Loss: 0.6762\n",
      "Iteration 3500, Loss: 0.6743\n",
      "Iteration 3600, Loss: 0.6726\n",
      "Iteration 3700, Loss: 0.6709\n",
      "Iteration 3800, Loss: 0.6693\n",
      "Iteration 3900, Loss: 0.6677\n",
      "Iteration 4000, Loss: 0.6662\n",
      "Iteration 4100, Loss: 0.6647\n",
      "Iteration 4200, Loss: 0.6633\n",
      "Iteration 4300, Loss: 0.6619\n",
      "Iteration 4400, Loss: 0.6606\n",
      "Iteration 4500, Loss: 0.6593\n",
      "Iteration 4600, Loss: 0.6580\n",
      "Iteration 4700, Loss: 0.6568\n",
      "Iteration 4800, Loss: 0.6556\n",
      "Iteration 4900, Loss: 0.6545\n",
      "157 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0207\n",
      "Iteration 200, Loss: 0.9677\n",
      "Iteration 300, Loss: 0.9279\n",
      "Iteration 400, Loss: 0.8964\n",
      "Iteration 500, Loss: 0.8709\n",
      "Iteration 600, Loss: 0.8498\n",
      "Iteration 700, Loss: 0.8319\n",
      "Iteration 800, Loss: 0.8166\n",
      "Iteration 900, Loss: 0.8033\n",
      "Iteration 1000, Loss: 0.7917\n",
      "Iteration 1100, Loss: 0.7814\n",
      "Iteration 1200, Loss: 0.7723\n",
      "Iteration 1300, Loss: 0.7641\n",
      "Iteration 1400, Loss: 0.7568\n",
      "Iteration 1500, Loss: 0.7500\n",
      "Iteration 1600, Loss: 0.7439\n",
      "Iteration 1700, Loss: 0.7383\n",
      "Iteration 1800, Loss: 0.7332\n",
      "Iteration 1900, Loss: 0.7284\n",
      "Iteration 2000, Loss: 0.7239\n",
      "Iteration 2100, Loss: 0.7198\n",
      "Iteration 2200, Loss: 0.7159\n",
      "Iteration 2300, Loss: 0.7123\n",
      "Iteration 2400, Loss: 0.7088\n",
      "Iteration 2500, Loss: 0.7056\n",
      "Iteration 2600, Loss: 0.7025\n",
      "Iteration 2700, Loss: 0.6996\n",
      "Iteration 2800, Loss: 0.6968\n",
      "Iteration 2900, Loss: 0.6941\n",
      "Iteration 3000, Loss: 0.6916\n",
      "Iteration 3100, Loss: 0.6892\n",
      "Iteration 3200, Loss: 0.6868\n",
      "Iteration 3300, Loss: 0.6846\n",
      "Iteration 3400, Loss: 0.6824\n",
      "Iteration 3500, Loss: 0.6804\n",
      "Iteration 3600, Loss: 0.6784\n",
      "Iteration 3700, Loss: 0.6764\n",
      "Iteration 3800, Loss: 0.6746\n",
      "Iteration 3900, Loss: 0.6728\n",
      "Iteration 4000, Loss: 0.6710\n",
      "Iteration 4100, Loss: 0.6693\n",
      "Iteration 4200, Loss: 0.6677\n",
      "Iteration 4300, Loss: 0.6661\n",
      "Iteration 4400, Loss: 0.6645\n",
      "Iteration 4500, Loss: 0.6630\n",
      "Iteration 4600, Loss: 0.6616\n",
      "Iteration 4700, Loss: 0.6602\n",
      "Iteration 4800, Loss: 0.6588\n",
      "Iteration 4900, Loss: 0.6574\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0178\n",
      "Iteration 200, Loss: 0.9617\n",
      "Iteration 300, Loss: 0.9198\n",
      "Iteration 400, Loss: 0.8871\n",
      "Iteration 500, Loss: 0.8608\n",
      "Iteration 600, Loss: 0.8392\n",
      "Iteration 700, Loss: 0.8211\n",
      "Iteration 800, Loss: 0.8059\n",
      "Iteration 900, Loss: 0.7927\n",
      "Iteration 1000, Loss: 0.7813\n",
      "Iteration 1100, Loss: 0.7714\n",
      "Iteration 1200, Loss: 0.7627\n",
      "Iteration 1300, Loss: 0.7549\n",
      "Iteration 1400, Loss: 0.7480\n",
      "Iteration 1500, Loss: 0.7417\n",
      "Iteration 1600, Loss: 0.7361\n",
      "Iteration 1700, Loss: 0.7310\n",
      "Iteration 1800, Loss: 0.7263\n",
      "Iteration 1900, Loss: 0.7221\n",
      "Iteration 2000, Loss: 0.7181\n",
      "Iteration 2100, Loss: 0.7145\n",
      "Iteration 2200, Loss: 0.7111\n",
      "Iteration 2300, Loss: 0.7079\n",
      "Iteration 2400, Loss: 0.7050\n",
      "Iteration 2500, Loss: 0.7022\n",
      "Iteration 2600, Loss: 0.6996\n",
      "Iteration 2700, Loss: 0.6972\n",
      "Iteration 2800, Loss: 0.6949\n",
      "Iteration 2900, Loss: 0.6927\n",
      "Iteration 3000, Loss: 0.6906\n",
      "Iteration 3100, Loss: 0.6886\n",
      "Iteration 3200, Loss: 0.6868\n",
      "Iteration 3300, Loss: 0.6850\n",
      "Iteration 3400, Loss: 0.6832\n",
      "Iteration 3500, Loss: 0.6816\n",
      "Iteration 3600, Loss: 0.6800\n",
      "Iteration 3700, Loss: 0.6785\n",
      "Iteration 3800, Loss: 0.6770\n",
      "Iteration 3900, Loss: 0.6756\n",
      "Iteration 4000, Loss: 0.6742\n",
      "Iteration 4100, Loss: 0.6729\n",
      "Iteration 4200, Loss: 0.6717\n",
      "Iteration 4300, Loss: 0.6705\n",
      "Iteration 4400, Loss: 0.6693\n",
      "Iteration 4500, Loss: 0.6681\n",
      "Iteration 4600, Loss: 0.6670\n",
      "Iteration 4700, Loss: 0.6659\n",
      "Iteration 4800, Loss: 0.6649\n",
      "Iteration 4900, Loss: 0.6639\n",
      "158 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0203\n",
      "Iteration 200, Loss: 0.9668\n",
      "Iteration 300, Loss: 0.9267\n",
      "Iteration 400, Loss: 0.8953\n",
      "Iteration 500, Loss: 0.8699\n",
      "Iteration 600, Loss: 0.8489\n",
      "Iteration 700, Loss: 0.8313\n",
      "Iteration 800, Loss: 0.8162\n",
      "Iteration 900, Loss: 0.8033\n",
      "Iteration 1000, Loss: 0.7921\n",
      "Iteration 1100, Loss: 0.7822\n",
      "Iteration 1200, Loss: 0.7734\n",
      "Iteration 1300, Loss: 0.7656\n",
      "Iteration 1400, Loss: 0.7586\n",
      "Iteration 1500, Loss: 0.7523\n",
      "Iteration 1600, Loss: 0.7465\n",
      "Iteration 1700, Loss: 0.7413\n",
      "Iteration 1800, Loss: 0.7365\n",
      "Iteration 1900, Loss: 0.7320\n",
      "Iteration 2000, Loss: 0.7279\n",
      "Iteration 2100, Loss: 0.7241\n",
      "Iteration 2200, Loss: 0.7205\n",
      "Iteration 2300, Loss: 0.7172\n",
      "Iteration 2400, Loss: 0.7141\n",
      "Iteration 2500, Loss: 0.7111\n",
      "Iteration 2600, Loss: 0.7083\n",
      "Iteration 2700, Loss: 0.7057\n",
      "Iteration 2800, Loss: 0.7032\n",
      "Iteration 2900, Loss: 0.7008\n",
      "Iteration 3000, Loss: 0.6986\n",
      "Iteration 3100, Loss: 0.6964\n",
      "Iteration 3200, Loss: 0.6943\n",
      "Iteration 3300, Loss: 0.6924\n",
      "Iteration 3400, Loss: 0.6905\n",
      "Iteration 3500, Loss: 0.6887\n",
      "Iteration 3600, Loss: 0.6869\n",
      "Iteration 3700, Loss: 0.6852\n",
      "Iteration 3800, Loss: 0.6836\n",
      "Iteration 3900, Loss: 0.6820\n",
      "Iteration 4000, Loss: 0.6805\n",
      "Iteration 4100, Loss: 0.6790\n",
      "Iteration 4200, Loss: 0.6776\n",
      "Iteration 4300, Loss: 0.6762\n",
      "Iteration 4400, Loss: 0.6749\n",
      "Iteration 4500, Loss: 0.6736\n",
      "Iteration 4600, Loss: 0.6724\n",
      "Iteration 4700, Loss: 0.6711\n",
      "Iteration 4800, Loss: 0.6699\n",
      "Iteration 4900, Loss: 0.6688\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0194\n",
      "Iteration 200, Loss: 0.9644\n",
      "Iteration 300, Loss: 0.9228\n",
      "Iteration 400, Loss: 0.8903\n",
      "Iteration 500, Loss: 0.8640\n",
      "Iteration 600, Loss: 0.8421\n",
      "Iteration 700, Loss: 0.8237\n",
      "Iteration 800, Loss: 0.8080\n",
      "Iteration 900, Loss: 0.7945\n",
      "Iteration 1000, Loss: 0.7827\n",
      "Iteration 1100, Loss: 0.7723\n",
      "Iteration 1200, Loss: 0.7631\n",
      "Iteration 1300, Loss: 0.7549\n",
      "Iteration 1400, Loss: 0.7475\n",
      "Iteration 1500, Loss: 0.7408\n",
      "Iteration 1600, Loss: 0.7348\n",
      "Iteration 1700, Loss: 0.7292\n",
      "Iteration 1800, Loss: 0.7241\n",
      "Iteration 1900, Loss: 0.7194\n",
      "Iteration 2000, Loss: 0.7151\n",
      "Iteration 2100, Loss: 0.7111\n",
      "Iteration 2200, Loss: 0.7073\n",
      "Iteration 2300, Loss: 0.7038\n",
      "Iteration 2400, Loss: 0.7004\n",
      "Iteration 2500, Loss: 0.6973\n",
      "Iteration 2600, Loss: 0.6944\n",
      "Iteration 2700, Loss: 0.6916\n",
      "Iteration 2800, Loss: 0.6889\n",
      "Iteration 2900, Loss: 0.6864\n",
      "Iteration 3000, Loss: 0.6840\n",
      "Iteration 3100, Loss: 0.6817\n",
      "Iteration 3200, Loss: 0.6795\n",
      "Iteration 3300, Loss: 0.6774\n",
      "Iteration 3400, Loss: 0.6754\n",
      "Iteration 3500, Loss: 0.6735\n",
      "Iteration 3600, Loss: 0.6716\n",
      "Iteration 3700, Loss: 0.6698\n",
      "Iteration 3800, Loss: 0.6681\n",
      "Iteration 3900, Loss: 0.6664\n",
      "Iteration 4000, Loss: 0.6648\n",
      "Iteration 4100, Loss: 0.6633\n",
      "Iteration 4200, Loss: 0.6618\n",
      "Iteration 4300, Loss: 0.6603\n",
      "Iteration 4400, Loss: 0.6589\n",
      "Iteration 4500, Loss: 0.6575\n",
      "Iteration 4600, Loss: 0.6562\n",
      "Iteration 4700, Loss: 0.6549\n",
      "Iteration 4800, Loss: 0.6536\n",
      "Iteration 4900, Loss: 0.6524\n",
      "159 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0206\n",
      "Iteration 200, Loss: 0.9672\n",
      "Iteration 300, Loss: 0.9271\n",
      "Iteration 400, Loss: 0.8956\n",
      "Iteration 500, Loss: 0.8700\n",
      "Iteration 600, Loss: 0.8489\n",
      "Iteration 700, Loss: 0.8310\n",
      "Iteration 800, Loss: 0.8157\n",
      "Iteration 900, Loss: 0.8025\n",
      "Iteration 1000, Loss: 0.7909\n",
      "Iteration 1100, Loss: 0.7807\n",
      "Iteration 1200, Loss: 0.7716\n",
      "Iteration 1300, Loss: 0.7635\n",
      "Iteration 1400, Loss: 0.7563\n",
      "Iteration 1500, Loss: 0.7497\n",
      "Iteration 1600, Loss: 0.7436\n",
      "Iteration 1700, Loss: 0.7382\n",
      "Iteration 1800, Loss: 0.7331\n",
      "Iteration 1900, Loss: 0.7285\n",
      "Iteration 2000, Loss: 0.7242\n",
      "Iteration 2100, Loss: 0.7202\n",
      "Iteration 2200, Loss: 0.7164\n",
      "Iteration 2300, Loss: 0.7129\n",
      "Iteration 2400, Loss: 0.7096\n",
      "Iteration 2500, Loss: 0.7065\n",
      "Iteration 2600, Loss: 0.7035\n",
      "Iteration 2700, Loss: 0.7007\n",
      "Iteration 2800, Loss: 0.6981\n",
      "Iteration 2900, Loss: 0.6956\n",
      "Iteration 3000, Loss: 0.6932\n",
      "Iteration 3100, Loss: 0.6909\n",
      "Iteration 3200, Loss: 0.6887\n",
      "Iteration 3300, Loss: 0.6866\n",
      "Iteration 3400, Loss: 0.6845\n",
      "Iteration 3500, Loss: 0.6826\n",
      "Iteration 3600, Loss: 0.6807\n",
      "Iteration 3700, Loss: 0.6789\n",
      "Iteration 3800, Loss: 0.6772\n",
      "Iteration 3900, Loss: 0.6755\n",
      "Iteration 4000, Loss: 0.6739\n",
      "Iteration 4100, Loss: 0.6723\n",
      "Iteration 4200, Loss: 0.6708\n",
      "Iteration 4300, Loss: 0.6693\n",
      "Iteration 4400, Loss: 0.6679\n",
      "Iteration 4500, Loss: 0.6665\n",
      "Iteration 4600, Loss: 0.6652\n",
      "Iteration 4700, Loss: 0.6639\n",
      "Iteration 4800, Loss: 0.6626\n",
      "Iteration 4900, Loss: 0.6614\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0185\n",
      "Iteration 200, Loss: 0.9629\n",
      "Iteration 300, Loss: 0.9210\n",
      "Iteration 400, Loss: 0.8882\n",
      "Iteration 500, Loss: 0.8620\n",
      "Iteration 600, Loss: 0.8403\n",
      "Iteration 700, Loss: 0.8221\n",
      "Iteration 800, Loss: 0.8067\n",
      "Iteration 900, Loss: 0.7934\n",
      "Iteration 1000, Loss: 0.7819\n",
      "Iteration 1100, Loss: 0.7718\n",
      "Iteration 1200, Loss: 0.7629\n",
      "Iteration 1300, Loss: 0.7549\n",
      "Iteration 1400, Loss: 0.7478\n",
      "Iteration 1500, Loss: 0.7414\n",
      "Iteration 1600, Loss: 0.7356\n",
      "Iteration 1700, Loss: 0.7304\n",
      "Iteration 1800, Loss: 0.7255\n",
      "Iteration 1900, Loss: 0.7211\n",
      "Iteration 2000, Loss: 0.7170\n",
      "Iteration 2100, Loss: 0.7132\n",
      "Iteration 2200, Loss: 0.7096\n",
      "Iteration 2300, Loss: 0.7063\n",
      "Iteration 2400, Loss: 0.7032\n",
      "Iteration 2500, Loss: 0.7003\n",
      "Iteration 2600, Loss: 0.6975\n",
      "Iteration 2700, Loss: 0.6949\n",
      "Iteration 2800, Loss: 0.6924\n",
      "Iteration 2900, Loss: 0.6901\n",
      "Iteration 3000, Loss: 0.6879\n",
      "Iteration 3100, Loss: 0.6857\n",
      "Iteration 3200, Loss: 0.6837\n",
      "Iteration 3300, Loss: 0.6817\n",
      "Iteration 3400, Loss: 0.6798\n",
      "Iteration 3500, Loss: 0.6781\n",
      "Iteration 3600, Loss: 0.6763\n",
      "Iteration 3700, Loss: 0.6747\n",
      "Iteration 3800, Loss: 0.6731\n",
      "Iteration 3900, Loss: 0.6715\n",
      "Iteration 4000, Loss: 0.6700\n",
      "Iteration 4100, Loss: 0.6685\n",
      "Iteration 4200, Loss: 0.6671\n",
      "Iteration 4300, Loss: 0.6658\n",
      "Iteration 4400, Loss: 0.6645\n",
      "Iteration 4500, Loss: 0.6632\n",
      "Iteration 4600, Loss: 0.6619\n",
      "Iteration 4700, Loss: 0.6607\n",
      "Iteration 4800, Loss: 0.6595\n",
      "Iteration 4900, Loss: 0.6584\n",
      "160 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0201\n",
      "Iteration 200, Loss: 0.9661\n",
      "Iteration 300, Loss: 0.9257\n",
      "Iteration 400, Loss: 0.8939\n",
      "Iteration 500, Loss: 0.8682\n",
      "Iteration 600, Loss: 0.8470\n",
      "Iteration 700, Loss: 0.8291\n",
      "Iteration 800, Loss: 0.8138\n",
      "Iteration 900, Loss: 0.8006\n",
      "Iteration 1000, Loss: 0.7890\n",
      "Iteration 1100, Loss: 0.7787\n",
      "Iteration 1200, Loss: 0.7696\n",
      "Iteration 1300, Loss: 0.7614\n",
      "Iteration 1400, Loss: 0.7541\n",
      "Iteration 1500, Loss: 0.7474\n",
      "Iteration 1600, Loss: 0.7412\n",
      "Iteration 1700, Loss: 0.7356\n",
      "Iteration 1800, Loss: 0.7305\n",
      "Iteration 1900, Loss: 0.7257\n",
      "Iteration 2000, Loss: 0.7213\n",
      "Iteration 2100, Loss: 0.7171\n",
      "Iteration 2200, Loss: 0.7133\n",
      "Iteration 2300, Loss: 0.7096\n",
      "Iteration 2400, Loss: 0.7062\n",
      "Iteration 2500, Loss: 0.7030\n",
      "Iteration 2600, Loss: 0.7000\n",
      "Iteration 2700, Loss: 0.6971\n",
      "Iteration 2800, Loss: 0.6944\n",
      "Iteration 2900, Loss: 0.6918\n",
      "Iteration 3000, Loss: 0.6893\n",
      "Iteration 3100, Loss: 0.6869\n",
      "Iteration 3200, Loss: 0.6847\n",
      "Iteration 3300, Loss: 0.6825\n",
      "Iteration 3400, Loss: 0.6804\n",
      "Iteration 3500, Loss: 0.6784\n",
      "Iteration 3600, Loss: 0.6765\n",
      "Iteration 3700, Loss: 0.6747\n",
      "Iteration 3800, Loss: 0.6729\n",
      "Iteration 3900, Loss: 0.6712\n",
      "Iteration 4000, Loss: 0.6696\n",
      "Iteration 4100, Loss: 0.6680\n",
      "Iteration 4200, Loss: 0.6664\n",
      "Iteration 4300, Loss: 0.6649\n",
      "Iteration 4400, Loss: 0.6635\n",
      "Iteration 4500, Loss: 0.6621\n",
      "Iteration 4600, Loss: 0.6607\n",
      "Iteration 4700, Loss: 0.6594\n",
      "Iteration 4800, Loss: 0.6581\n",
      "Iteration 4900, Loss: 0.6569\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0194\n",
      "Iteration 200, Loss: 0.9647\n",
      "Iteration 300, Loss: 0.9236\n",
      "Iteration 400, Loss: 0.8911\n",
      "Iteration 500, Loss: 0.8648\n",
      "Iteration 600, Loss: 0.8430\n",
      "Iteration 700, Loss: 0.8247\n",
      "Iteration 800, Loss: 0.8091\n",
      "Iteration 900, Loss: 0.7957\n",
      "Iteration 1000, Loss: 0.7840\n",
      "Iteration 1100, Loss: 0.7738\n",
      "Iteration 1200, Loss: 0.7647\n",
      "Iteration 1300, Loss: 0.7567\n",
      "Iteration 1400, Loss: 0.7495\n",
      "Iteration 1500, Loss: 0.7430\n",
      "Iteration 1600, Loss: 0.7372\n",
      "Iteration 1700, Loss: 0.7318\n",
      "Iteration 1800, Loss: 0.7270\n",
      "Iteration 1900, Loss: 0.7225\n",
      "Iteration 2000, Loss: 0.7184\n",
      "Iteration 2100, Loss: 0.7146\n",
      "Iteration 2200, Loss: 0.7110\n",
      "Iteration 2300, Loss: 0.7077\n",
      "Iteration 2400, Loss: 0.7046\n",
      "Iteration 2500, Loss: 0.7017\n",
      "Iteration 2600, Loss: 0.6989\n",
      "Iteration 2700, Loss: 0.6963\n",
      "Iteration 2800, Loss: 0.6939\n",
      "Iteration 2900, Loss: 0.6915\n",
      "Iteration 3000, Loss: 0.6893\n",
      "Iteration 3100, Loss: 0.6872\n",
      "Iteration 3200, Loss: 0.6851\n",
      "Iteration 3300, Loss: 0.6832\n",
      "Iteration 3400, Loss: 0.6814\n",
      "Iteration 3500, Loss: 0.6796\n",
      "Iteration 3600, Loss: 0.6778\n",
      "Iteration 3700, Loss: 0.6762\n",
      "Iteration 3800, Loss: 0.6746\n",
      "Iteration 3900, Loss: 0.6731\n",
      "Iteration 4000, Loss: 0.6716\n",
      "Iteration 4100, Loss: 0.6701\n",
      "Iteration 4200, Loss: 0.6687\n",
      "Iteration 4300, Loss: 0.6674\n",
      "Iteration 4400, Loss: 0.6661\n",
      "Iteration 4500, Loss: 0.6648\n",
      "Iteration 4600, Loss: 0.6636\n",
      "Iteration 4700, Loss: 0.6624\n",
      "Iteration 4800, Loss: 0.6612\n",
      "Iteration 4900, Loss: 0.6601\n",
      "161 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0176\n",
      "Iteration 200, Loss: 0.9633\n",
      "Iteration 300, Loss: 0.9228\n",
      "Iteration 400, Loss: 0.8912\n",
      "Iteration 500, Loss: 0.8657\n",
      "Iteration 600, Loss: 0.8446\n",
      "Iteration 700, Loss: 0.8268\n",
      "Iteration 800, Loss: 0.8117\n",
      "Iteration 900, Loss: 0.7986\n",
      "Iteration 1000, Loss: 0.7871\n",
      "Iteration 1100, Loss: 0.7769\n",
      "Iteration 1200, Loss: 0.7678\n",
      "Iteration 1300, Loss: 0.7597\n",
      "Iteration 1400, Loss: 0.7523\n",
      "Iteration 1500, Loss: 0.7457\n",
      "Iteration 1600, Loss: 0.7396\n",
      "Iteration 1700, Loss: 0.7340\n",
      "Iteration 1800, Loss: 0.7288\n",
      "Iteration 1900, Loss: 0.7240\n",
      "Iteration 2000, Loss: 0.7196\n",
      "Iteration 2100, Loss: 0.7155\n",
      "Iteration 2200, Loss: 0.7116\n",
      "Iteration 2300, Loss: 0.7080\n",
      "Iteration 2400, Loss: 0.7046\n",
      "Iteration 2500, Loss: 0.7013\n",
      "Iteration 2600, Loss: 0.6983\n",
      "Iteration 2700, Loss: 0.6954\n",
      "Iteration 2800, Loss: 0.6927\n",
      "Iteration 2900, Loss: 0.6901\n",
      "Iteration 3000, Loss: 0.6876\n",
      "Iteration 3100, Loss: 0.6853\n",
      "Iteration 3200, Loss: 0.6830\n",
      "Iteration 3300, Loss: 0.6808\n",
      "Iteration 3400, Loss: 0.6788\n",
      "Iteration 3500, Loss: 0.6768\n",
      "Iteration 3600, Loss: 0.6749\n",
      "Iteration 3700, Loss: 0.6730\n",
      "Iteration 3800, Loss: 0.6713\n",
      "Iteration 3900, Loss: 0.6696\n",
      "Iteration 4000, Loss: 0.6679\n",
      "Iteration 4100, Loss: 0.6663\n",
      "Iteration 4200, Loss: 0.6648\n",
      "Iteration 4300, Loss: 0.6633\n",
      "Iteration 4400, Loss: 0.6618\n",
      "Iteration 4500, Loss: 0.6604\n",
      "Iteration 4600, Loss: 0.6591\n",
      "Iteration 4700, Loss: 0.6578\n",
      "Iteration 4800, Loss: 0.6565\n",
      "Iteration 4900, Loss: 0.6553\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0214\n",
      "Iteration 200, Loss: 0.9667\n",
      "Iteration 300, Loss: 0.9252\n",
      "Iteration 400, Loss: 0.8924\n",
      "Iteration 500, Loss: 0.8658\n",
      "Iteration 600, Loss: 0.8439\n",
      "Iteration 700, Loss: 0.8255\n",
      "Iteration 800, Loss: 0.8099\n",
      "Iteration 900, Loss: 0.7965\n",
      "Iteration 1000, Loss: 0.7849\n",
      "Iteration 1100, Loss: 0.7748\n",
      "Iteration 1200, Loss: 0.7660\n",
      "Iteration 1300, Loss: 0.7581\n",
      "Iteration 1400, Loss: 0.7511\n",
      "Iteration 1500, Loss: 0.7449\n",
      "Iteration 1600, Loss: 0.7392\n",
      "Iteration 1700, Loss: 0.7340\n",
      "Iteration 1800, Loss: 0.7293\n",
      "Iteration 1900, Loss: 0.7250\n",
      "Iteration 2000, Loss: 0.7211\n",
      "Iteration 2100, Loss: 0.7174\n",
      "Iteration 2200, Loss: 0.7140\n",
      "Iteration 2300, Loss: 0.7108\n",
      "Iteration 2400, Loss: 0.7078\n",
      "Iteration 2500, Loss: 0.7050\n",
      "Iteration 2600, Loss: 0.7023\n",
      "Iteration 2700, Loss: 0.6998\n",
      "Iteration 2800, Loss: 0.6975\n",
      "Iteration 2900, Loss: 0.6952\n",
      "Iteration 3000, Loss: 0.6930\n",
      "Iteration 3100, Loss: 0.6910\n",
      "Iteration 3200, Loss: 0.6890\n",
      "Iteration 3300, Loss: 0.6871\n",
      "Iteration 3400, Loss: 0.6853\n",
      "Iteration 3500, Loss: 0.6836\n",
      "Iteration 3600, Loss: 0.6819\n",
      "Iteration 3700, Loss: 0.6803\n",
      "Iteration 3800, Loss: 0.6788\n",
      "Iteration 3900, Loss: 0.6773\n",
      "Iteration 4000, Loss: 0.6758\n",
      "Iteration 4100, Loss: 0.6744\n",
      "Iteration 4200, Loss: 0.6731\n",
      "Iteration 4300, Loss: 0.6717\n",
      "Iteration 4400, Loss: 0.6705\n",
      "Iteration 4500, Loss: 0.6692\n",
      "Iteration 4600, Loss: 0.6680\n",
      "Iteration 4700, Loss: 0.6668\n",
      "Iteration 4800, Loss: 0.6657\n",
      "Iteration 4900, Loss: 0.6645\n",
      "162 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0195\n",
      "Iteration 200, Loss: 0.9650\n",
      "Iteration 300, Loss: 0.9242\n",
      "Iteration 400, Loss: 0.8923\n",
      "Iteration 500, Loss: 0.8666\n",
      "Iteration 600, Loss: 0.8453\n",
      "Iteration 700, Loss: 0.8274\n",
      "Iteration 800, Loss: 0.8122\n",
      "Iteration 900, Loss: 0.7991\n",
      "Iteration 1000, Loss: 0.7877\n",
      "Iteration 1100, Loss: 0.7777\n",
      "Iteration 1200, Loss: 0.7688\n",
      "Iteration 1300, Loss: 0.7609\n",
      "Iteration 1400, Loss: 0.7538\n",
      "Iteration 1500, Loss: 0.7474\n",
      "Iteration 1600, Loss: 0.7415\n",
      "Iteration 1700, Loss: 0.7363\n",
      "Iteration 1800, Loss: 0.7314\n",
      "Iteration 1900, Loss: 0.7269\n",
      "Iteration 2000, Loss: 0.7228\n",
      "Iteration 2100, Loss: 0.7190\n",
      "Iteration 2200, Loss: 0.7154\n",
      "Iteration 2300, Loss: 0.7120\n",
      "Iteration 2400, Loss: 0.7089\n",
      "Iteration 2500, Loss: 0.7060\n",
      "Iteration 2600, Loss: 0.7032\n",
      "Iteration 2700, Loss: 0.7005\n",
      "Iteration 2800, Loss: 0.6981\n",
      "Iteration 2900, Loss: 0.6957\n",
      "Iteration 3000, Loss: 0.6934\n",
      "Iteration 3100, Loss: 0.6913\n",
      "Iteration 3200, Loss: 0.6893\n",
      "Iteration 3300, Loss: 0.6873\n",
      "Iteration 3400, Loss: 0.6854\n",
      "Iteration 3500, Loss: 0.6836\n",
      "Iteration 3600, Loss: 0.6819\n",
      "Iteration 3700, Loss: 0.6802\n",
      "Iteration 3800, Loss: 0.6786\n",
      "Iteration 3900, Loss: 0.6770\n",
      "Iteration 4000, Loss: 0.6755\n",
      "Iteration 4100, Loss: 0.6741\n",
      "Iteration 4200, Loss: 0.6726\n",
      "Iteration 4300, Loss: 0.6713\n",
      "Iteration 4400, Loss: 0.6700\n",
      "Iteration 4500, Loss: 0.6687\n",
      "Iteration 4600, Loss: 0.6675\n",
      "Iteration 4700, Loss: 0.6663\n",
      "Iteration 4800, Loss: 0.6651\n",
      "Iteration 4900, Loss: 0.6639\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0204\n",
      "Iteration 200, Loss: 0.9663\n",
      "Iteration 300, Loss: 0.9255\n",
      "Iteration 400, Loss: 0.8935\n",
      "Iteration 500, Loss: 0.8675\n",
      "Iteration 600, Loss: 0.8460\n",
      "Iteration 700, Loss: 0.8278\n",
      "Iteration 800, Loss: 0.8123\n",
      "Iteration 900, Loss: 0.7989\n",
      "Iteration 1000, Loss: 0.7872\n",
      "Iteration 1100, Loss: 0.7769\n",
      "Iteration 1200, Loss: 0.7677\n",
      "Iteration 1300, Loss: 0.7595\n",
      "Iteration 1400, Loss: 0.7521\n",
      "Iteration 1500, Loss: 0.7455\n",
      "Iteration 1600, Loss: 0.7394\n",
      "Iteration 1700, Loss: 0.7339\n",
      "Iteration 1800, Loss: 0.7288\n",
      "Iteration 1900, Loss: 0.7241\n",
      "Iteration 2000, Loss: 0.7197\n",
      "Iteration 2100, Loss: 0.7156\n",
      "Iteration 2200, Loss: 0.7119\n",
      "Iteration 2300, Loss: 0.7083\n",
      "Iteration 2400, Loss: 0.7050\n",
      "Iteration 2500, Loss: 0.7018\n",
      "Iteration 2600, Loss: 0.6989\n",
      "Iteration 2700, Loss: 0.6961\n",
      "Iteration 2800, Loss: 0.6934\n",
      "Iteration 2900, Loss: 0.6909\n",
      "Iteration 3000, Loss: 0.6884\n",
      "Iteration 3100, Loss: 0.6861\n",
      "Iteration 3200, Loss: 0.6839\n",
      "Iteration 3300, Loss: 0.6818\n",
      "Iteration 3400, Loss: 0.6798\n",
      "Iteration 3500, Loss: 0.6778\n",
      "Iteration 3600, Loss: 0.6759\n",
      "Iteration 3700, Loss: 0.6741\n",
      "Iteration 3800, Loss: 0.6724\n",
      "Iteration 3900, Loss: 0.6707\n",
      "Iteration 4000, Loss: 0.6690\n",
      "Iteration 4100, Loss: 0.6675\n",
      "Iteration 4200, Loss: 0.6659\n",
      "Iteration 4300, Loss: 0.6644\n",
      "Iteration 4400, Loss: 0.6630\n",
      "Iteration 4500, Loss: 0.6616\n",
      "Iteration 4600, Loss: 0.6603\n",
      "Iteration 4700, Loss: 0.6589\n",
      "Iteration 4800, Loss: 0.6577\n",
      "Iteration 4900, Loss: 0.6564\n",
      "163 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0132\n",
      "Iteration 200, Loss: 0.9559\n",
      "Iteration 300, Loss: 0.9136\n",
      "Iteration 400, Loss: 0.8809\n",
      "Iteration 500, Loss: 0.8546\n",
      "Iteration 600, Loss: 0.8331\n",
      "Iteration 700, Loss: 0.8151\n",
      "Iteration 800, Loss: 0.7998\n",
      "Iteration 900, Loss: 0.7866\n",
      "Iteration 1000, Loss: 0.7750\n",
      "Iteration 1100, Loss: 0.7648\n",
      "Iteration 1200, Loss: 0.7558\n",
      "Iteration 1300, Loss: 0.7477\n",
      "Iteration 1400, Loss: 0.7404\n",
      "Iteration 1500, Loss: 0.7338\n",
      "Iteration 1600, Loss: 0.7278\n",
      "Iteration 1700, Loss: 0.7222\n",
      "Iteration 1800, Loss: 0.7171\n",
      "Iteration 1900, Loss: 0.7124\n",
      "Iteration 2000, Loss: 0.7081\n",
      "Iteration 2100, Loss: 0.7040\n",
      "Iteration 2200, Loss: 0.7002\n",
      "Iteration 2300, Loss: 0.6967\n",
      "Iteration 2400, Loss: 0.6933\n",
      "Iteration 2500, Loss: 0.6902\n",
      "Iteration 2600, Loss: 0.6872\n",
      "Iteration 2700, Loss: 0.6844\n",
      "Iteration 2800, Loss: 0.6817\n",
      "Iteration 2900, Loss: 0.6791\n",
      "Iteration 3000, Loss: 0.6767\n",
      "Iteration 3100, Loss: 0.6744\n",
      "Iteration 3200, Loss: 0.6722\n",
      "Iteration 3300, Loss: 0.6700\n",
      "Iteration 3400, Loss: 0.6680\n",
      "Iteration 3500, Loss: 0.6660\n",
      "Iteration 3600, Loss: 0.6642\n",
      "Iteration 3700, Loss: 0.6623\n",
      "Iteration 3800, Loss: 0.6606\n",
      "Iteration 3900, Loss: 0.6589\n",
      "Iteration 4000, Loss: 0.6573\n",
      "Iteration 4100, Loss: 0.6557\n",
      "Iteration 4200, Loss: 0.6542\n",
      "Iteration 4300, Loss: 0.6527\n",
      "Iteration 4400, Loss: 0.6512\n",
      "Iteration 4500, Loss: 0.6498\n",
      "Iteration 4600, Loss: 0.6485\n",
      "Iteration 4700, Loss: 0.6471\n",
      "Iteration 4800, Loss: 0.6459\n",
      "Iteration 4900, Loss: 0.6446\n",
      "Iteration 0, Loss: 1.0978\n",
      "Iteration 100, Loss: 1.0256\n",
      "Iteration 200, Loss: 0.9741\n",
      "Iteration 300, Loss: 0.9346\n",
      "Iteration 400, Loss: 0.9032\n",
      "Iteration 500, Loss: 0.8776\n",
      "Iteration 600, Loss: 0.8563\n",
      "Iteration 700, Loss: 0.8385\n",
      "Iteration 800, Loss: 0.8232\n",
      "Iteration 900, Loss: 0.8100\n",
      "Iteration 1000, Loss: 0.7986\n",
      "Iteration 1100, Loss: 0.7885\n",
      "Iteration 1200, Loss: 0.7796\n",
      "Iteration 1300, Loss: 0.7717\n",
      "Iteration 1400, Loss: 0.7646\n",
      "Iteration 1500, Loss: 0.7582\n",
      "Iteration 1600, Loss: 0.7524\n",
      "Iteration 1700, Loss: 0.7471\n",
      "Iteration 1800, Loss: 0.7422\n",
      "Iteration 1900, Loss: 0.7378\n",
      "Iteration 2000, Loss: 0.7336\n",
      "Iteration 2100, Loss: 0.7298\n",
      "Iteration 2200, Loss: 0.7262\n",
      "Iteration 2300, Loss: 0.7229\n",
      "Iteration 2400, Loss: 0.7198\n",
      "Iteration 2500, Loss: 0.7168\n",
      "Iteration 2600, Loss: 0.7140\n",
      "Iteration 2700, Loss: 0.7114\n",
      "Iteration 2800, Loss: 0.7089\n",
      "Iteration 2900, Loss: 0.7065\n",
      "Iteration 3000, Loss: 0.7042\n",
      "Iteration 3100, Loss: 0.7021\n",
      "Iteration 3200, Loss: 0.7000\n",
      "Iteration 3300, Loss: 0.6980\n",
      "Iteration 3400, Loss: 0.6961\n",
      "Iteration 3500, Loss: 0.6943\n",
      "Iteration 3600, Loss: 0.6925\n",
      "Iteration 3700, Loss: 0.6908\n",
      "Iteration 3800, Loss: 0.6892\n",
      "Iteration 3900, Loss: 0.6876\n",
      "Iteration 4000, Loss: 0.6861\n",
      "Iteration 4100, Loss: 0.6846\n",
      "Iteration 4200, Loss: 0.6832\n",
      "Iteration 4300, Loss: 0.6818\n",
      "Iteration 4400, Loss: 0.6805\n",
      "Iteration 4500, Loss: 0.6792\n",
      "Iteration 4600, Loss: 0.6779\n",
      "Iteration 4700, Loss: 0.6767\n",
      "Iteration 4800, Loss: 0.6755\n",
      "Iteration 4900, Loss: 0.6743\n",
      "164 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0195\n",
      "Iteration 200, Loss: 0.9638\n",
      "Iteration 300, Loss: 0.9214\n",
      "Iteration 400, Loss: 0.8879\n",
      "Iteration 500, Loss: 0.8605\n",
      "Iteration 600, Loss: 0.8377\n",
      "Iteration 700, Loss: 0.8186\n",
      "Iteration 800, Loss: 0.8022\n",
      "Iteration 900, Loss: 0.7880\n",
      "Iteration 1000, Loss: 0.7757\n",
      "Iteration 1100, Loss: 0.7649\n",
      "Iteration 1200, Loss: 0.7553\n",
      "Iteration 1300, Loss: 0.7468\n",
      "Iteration 1400, Loss: 0.7391\n",
      "Iteration 1500, Loss: 0.7322\n",
      "Iteration 1600, Loss: 0.7259\n",
      "Iteration 1700, Loss: 0.7202\n",
      "Iteration 1800, Loss: 0.7150\n",
      "Iteration 1900, Loss: 0.7102\n",
      "Iteration 2000, Loss: 0.7057\n",
      "Iteration 2100, Loss: 0.7016\n",
      "Iteration 2200, Loss: 0.6978\n",
      "Iteration 2300, Loss: 0.6942\n",
      "Iteration 2400, Loss: 0.6909\n",
      "Iteration 2500, Loss: 0.6877\n",
      "Iteration 2600, Loss: 0.6848\n",
      "Iteration 2700, Loss: 0.6820\n",
      "Iteration 2800, Loss: 0.6793\n",
      "Iteration 2900, Loss: 0.6768\n",
      "Iteration 3000, Loss: 0.6744\n",
      "Iteration 3100, Loss: 0.6721\n",
      "Iteration 3200, Loss: 0.6699\n",
      "Iteration 3300, Loss: 0.6679\n",
      "Iteration 3400, Loss: 0.6659\n",
      "Iteration 3500, Loss: 0.6639\n",
      "Iteration 3600, Loss: 0.6621\n",
      "Iteration 3700, Loss: 0.6603\n",
      "Iteration 3800, Loss: 0.6586\n",
      "Iteration 3900, Loss: 0.6570\n",
      "Iteration 4000, Loss: 0.6554\n",
      "Iteration 4100, Loss: 0.6538\n",
      "Iteration 4200, Loss: 0.6524\n",
      "Iteration 4300, Loss: 0.6509\n",
      "Iteration 4400, Loss: 0.6495\n",
      "Iteration 4500, Loss: 0.6482\n",
      "Iteration 4600, Loss: 0.6469\n",
      "Iteration 4700, Loss: 0.6456\n",
      "Iteration 4800, Loss: 0.6443\n",
      "Iteration 4900, Loss: 0.6431\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0200\n",
      "Iteration 200, Loss: 0.9669\n",
      "Iteration 300, Loss: 0.9276\n",
      "Iteration 400, Loss: 0.8971\n",
      "Iteration 500, Loss: 0.8725\n",
      "Iteration 600, Loss: 0.8525\n",
      "Iteration 700, Loss: 0.8356\n",
      "Iteration 800, Loss: 0.8212\n",
      "Iteration 900, Loss: 0.8089\n",
      "Iteration 1000, Loss: 0.7981\n",
      "Iteration 1100, Loss: 0.7887\n",
      "Iteration 1200, Loss: 0.7803\n",
      "Iteration 1300, Loss: 0.7728\n",
      "Iteration 1400, Loss: 0.7661\n",
      "Iteration 1500, Loss: 0.7600\n",
      "Iteration 1600, Loss: 0.7544\n",
      "Iteration 1700, Loss: 0.7494\n",
      "Iteration 1800, Loss: 0.7447\n",
      "Iteration 1900, Loss: 0.7404\n",
      "Iteration 2000, Loss: 0.7363\n",
      "Iteration 2100, Loss: 0.7326\n",
      "Iteration 2200, Loss: 0.7291\n",
      "Iteration 2300, Loss: 0.7258\n",
      "Iteration 2400, Loss: 0.7227\n",
      "Iteration 2500, Loss: 0.7198\n",
      "Iteration 2600, Loss: 0.7170\n",
      "Iteration 2700, Loss: 0.7144\n",
      "Iteration 2800, Loss: 0.7119\n",
      "Iteration 2900, Loss: 0.7095\n",
      "Iteration 3000, Loss: 0.7072\n",
      "Iteration 3100, Loss: 0.7051\n",
      "Iteration 3200, Loss: 0.7030\n",
      "Iteration 3300, Loss: 0.7010\n",
      "Iteration 3400, Loss: 0.6991\n",
      "Iteration 3500, Loss: 0.6972\n",
      "Iteration 3600, Loss: 0.6955\n",
      "Iteration 3700, Loss: 0.6938\n",
      "Iteration 3800, Loss: 0.6921\n",
      "Iteration 3900, Loss: 0.6905\n",
      "Iteration 4000, Loss: 0.6889\n",
      "Iteration 4100, Loss: 0.6874\n",
      "Iteration 4200, Loss: 0.6860\n",
      "Iteration 4300, Loss: 0.6846\n",
      "Iteration 4400, Loss: 0.6832\n",
      "Iteration 4500, Loss: 0.6819\n",
      "Iteration 4600, Loss: 0.6806\n",
      "Iteration 4700, Loss: 0.6793\n",
      "Iteration 4800, Loss: 0.6781\n",
      "Iteration 4900, Loss: 0.6769\n",
      "165 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0527\n",
      "Iteration 200, Loss: 1.0158\n",
      "Iteration 300, Loss: 0.9850\n",
      "Iteration 400, Loss: 0.9585\n",
      "Iteration 500, Loss: 0.9359\n",
      "Iteration 600, Loss: 0.9160\n",
      "Iteration 700, Loss: 0.8983\n",
      "Iteration 800, Loss: 0.8826\n",
      "Iteration 900, Loss: 0.8685\n",
      "Iteration 1000, Loss: 0.8559\n",
      "Iteration 1100, Loss: 0.8444\n",
      "Iteration 1200, Loss: 0.8340\n",
      "Iteration 1300, Loss: 0.8245\n",
      "Iteration 1400, Loss: 0.8158\n",
      "Iteration 1500, Loss: 0.8077\n",
      "Iteration 1600, Loss: 0.8002\n",
      "Iteration 1700, Loss: 0.7933\n",
      "Iteration 1800, Loss: 0.7869\n",
      "Iteration 1900, Loss: 0.7809\n",
      "Iteration 2000, Loss: 0.7753\n",
      "Iteration 2100, Loss: 0.7701\n",
      "Iteration 2200, Loss: 0.7651\n",
      "Iteration 2300, Loss: 0.7605\n",
      "Iteration 2400, Loss: 0.7561\n",
      "Iteration 2500, Loss: 0.7521\n",
      "Iteration 2600, Loss: 0.7482\n",
      "Iteration 2700, Loss: 0.7445\n",
      "Iteration 2800, Loss: 0.7410\n",
      "Iteration 2900, Loss: 0.7377\n",
      "Iteration 3000, Loss: 0.7346\n",
      "Iteration 3100, Loss: 0.7316\n",
      "Iteration 3200, Loss: 0.7287\n",
      "Iteration 3300, Loss: 0.7260\n",
      "Iteration 3400, Loss: 0.7234\n",
      "Iteration 3500, Loss: 0.7209\n",
      "Iteration 3600, Loss: 0.7185\n",
      "Iteration 3700, Loss: 0.7162\n",
      "Iteration 3800, Loss: 0.7140\n",
      "Iteration 3900, Loss: 0.7118\n",
      "Iteration 4000, Loss: 0.7098\n",
      "Iteration 4100, Loss: 0.7078\n",
      "Iteration 4200, Loss: 0.7059\n",
      "Iteration 4300, Loss: 0.7041\n",
      "Iteration 4400, Loss: 0.7023\n",
      "Iteration 4500, Loss: 0.7006\n",
      "Iteration 4600, Loss: 0.6989\n",
      "Iteration 4700, Loss: 0.6973\n",
      "Iteration 4800, Loss: 0.6958\n",
      "Iteration 4900, Loss: 0.6942\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0549\n",
      "Iteration 200, Loss: 1.0199\n",
      "Iteration 300, Loss: 0.9908\n",
      "Iteration 400, Loss: 0.9662\n",
      "Iteration 500, Loss: 0.9448\n",
      "Iteration 600, Loss: 0.9261\n",
      "Iteration 700, Loss: 0.9095\n",
      "Iteration 800, Loss: 0.8947\n",
      "Iteration 900, Loss: 0.8814\n",
      "Iteration 1000, Loss: 0.8693\n",
      "Iteration 1100, Loss: 0.8582\n",
      "Iteration 1200, Loss: 0.8483\n",
      "Iteration 1300, Loss: 0.8391\n",
      "Iteration 1400, Loss: 0.8306\n",
      "Iteration 1500, Loss: 0.8229\n",
      "Iteration 1600, Loss: 0.8156\n",
      "Iteration 1700, Loss: 0.8089\n",
      "Iteration 1800, Loss: 0.8026\n",
      "Iteration 1900, Loss: 0.7968\n",
      "Iteration 2000, Loss: 0.7913\n",
      "Iteration 2100, Loss: 0.7862\n",
      "Iteration 2200, Loss: 0.7814\n",
      "Iteration 2300, Loss: 0.7769\n",
      "Iteration 2400, Loss: 0.7725\n",
      "Iteration 2500, Loss: 0.7685\n",
      "Iteration 2600, Loss: 0.7647\n",
      "Iteration 2700, Loss: 0.7611\n",
      "Iteration 2800, Loss: 0.7576\n",
      "Iteration 2900, Loss: 0.7543\n",
      "Iteration 3000, Loss: 0.7512\n",
      "Iteration 3100, Loss: 0.7482\n",
      "Iteration 3200, Loss: 0.7454\n",
      "Iteration 3300, Loss: 0.7427\n",
      "Iteration 3400, Loss: 0.7401\n",
      "Iteration 3500, Loss: 0.7376\n",
      "Iteration 3600, Loss: 0.7352\n",
      "Iteration 3700, Loss: 0.7330\n",
      "Iteration 3800, Loss: 0.7308\n",
      "Iteration 3900, Loss: 0.7287\n",
      "Iteration 4000, Loss: 0.7266\n",
      "Iteration 4100, Loss: 0.7247\n",
      "Iteration 4200, Loss: 0.7228\n",
      "Iteration 4300, Loss: 0.7210\n",
      "Iteration 4400, Loss: 0.7192\n",
      "Iteration 4500, Loss: 0.7175\n",
      "Iteration 4600, Loss: 0.7158\n",
      "Iteration 4700, Loss: 0.7142\n",
      "Iteration 4800, Loss: 0.7127\n",
      "Iteration 4900, Loss: 0.7111\n",
      "166 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0571\n",
      "Iteration 200, Loss: 1.0236\n",
      "Iteration 300, Loss: 0.9953\n",
      "Iteration 400, Loss: 0.9714\n",
      "Iteration 500, Loss: 0.9504\n",
      "Iteration 600, Loss: 0.9321\n",
      "Iteration 700, Loss: 0.9157\n",
      "Iteration 800, Loss: 0.9010\n",
      "Iteration 900, Loss: 0.8878\n",
      "Iteration 1000, Loss: 0.8759\n",
      "Iteration 1100, Loss: 0.8649\n",
      "Iteration 1200, Loss: 0.8549\n",
      "Iteration 1300, Loss: 0.8458\n",
      "Iteration 1400, Loss: 0.8373\n",
      "Iteration 1500, Loss: 0.8295\n",
      "Iteration 1600, Loss: 0.8223\n",
      "Iteration 1700, Loss: 0.8156\n",
      "Iteration 1800, Loss: 0.8093\n",
      "Iteration 1900, Loss: 0.8034\n",
      "Iteration 2000, Loss: 0.7980\n",
      "Iteration 2100, Loss: 0.7929\n",
      "Iteration 2200, Loss: 0.7880\n",
      "Iteration 2300, Loss: 0.7835\n",
      "Iteration 2400, Loss: 0.7792\n",
      "Iteration 2500, Loss: 0.7751\n",
      "Iteration 2600, Loss: 0.7713\n",
      "Iteration 2700, Loss: 0.7677\n",
      "Iteration 2800, Loss: 0.7642\n",
      "Iteration 2900, Loss: 0.7609\n",
      "Iteration 3000, Loss: 0.7578\n",
      "Iteration 3100, Loss: 0.7548\n",
      "Iteration 3200, Loss: 0.7519\n",
      "Iteration 3300, Loss: 0.7492\n",
      "Iteration 3400, Loss: 0.7466\n",
      "Iteration 3500, Loss: 0.7441\n",
      "Iteration 3600, Loss: 0.7417\n",
      "Iteration 3700, Loss: 0.7393\n",
      "Iteration 3800, Loss: 0.7371\n",
      "Iteration 3900, Loss: 0.7350\n",
      "Iteration 4000, Loss: 0.7329\n",
      "Iteration 4100, Loss: 0.7309\n",
      "Iteration 4200, Loss: 0.7290\n",
      "Iteration 4300, Loss: 0.7271\n",
      "Iteration 4400, Loss: 0.7253\n",
      "Iteration 4500, Loss: 0.7236\n",
      "Iteration 4600, Loss: 0.7219\n",
      "Iteration 4700, Loss: 0.7202\n",
      "Iteration 4800, Loss: 0.7186\n",
      "Iteration 4900, Loss: 0.7171\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0501\n",
      "Iteration 200, Loss: 1.0115\n",
      "Iteration 300, Loss: 0.9796\n",
      "Iteration 400, Loss: 0.9528\n",
      "Iteration 500, Loss: 0.9297\n",
      "Iteration 600, Loss: 0.9096\n",
      "Iteration 700, Loss: 0.8918\n",
      "Iteration 800, Loss: 0.8761\n",
      "Iteration 900, Loss: 0.8620\n",
      "Iteration 1000, Loss: 0.8495\n",
      "Iteration 1100, Loss: 0.8380\n",
      "Iteration 1200, Loss: 0.8276\n",
      "Iteration 1300, Loss: 0.8181\n",
      "Iteration 1400, Loss: 0.8093\n",
      "Iteration 1500, Loss: 0.8013\n",
      "Iteration 1600, Loss: 0.7939\n",
      "Iteration 1700, Loss: 0.7870\n",
      "Iteration 1800, Loss: 0.7806\n",
      "Iteration 1900, Loss: 0.7747\n",
      "Iteration 2000, Loss: 0.7692\n",
      "Iteration 2100, Loss: 0.7640\n",
      "Iteration 2200, Loss: 0.7591\n",
      "Iteration 2300, Loss: 0.7545\n",
      "Iteration 2400, Loss: 0.7502\n",
      "Iteration 2500, Loss: 0.7462\n",
      "Iteration 2600, Loss: 0.7423\n",
      "Iteration 2700, Loss: 0.7387\n",
      "Iteration 2800, Loss: 0.7352\n",
      "Iteration 2900, Loss: 0.7320\n",
      "Iteration 3000, Loss: 0.7288\n",
      "Iteration 3100, Loss: 0.7259\n",
      "Iteration 3200, Loss: 0.7230\n",
      "Iteration 3300, Loss: 0.7203\n",
      "Iteration 3400, Loss: 0.7177\n",
      "Iteration 3500, Loss: 0.7153\n",
      "Iteration 3600, Loss: 0.7129\n",
      "Iteration 3700, Loss: 0.7106\n",
      "Iteration 3800, Loss: 0.7084\n",
      "Iteration 3900, Loss: 0.7063\n",
      "Iteration 4000, Loss: 0.7043\n",
      "Iteration 4100, Loss: 0.7023\n",
      "Iteration 4200, Loss: 0.7005\n",
      "Iteration 4300, Loss: 0.6987\n",
      "Iteration 4400, Loss: 0.6969\n",
      "Iteration 4500, Loss: 0.6952\n",
      "Iteration 4600, Loss: 0.6936\n",
      "Iteration 4700, Loss: 0.6920\n",
      "Iteration 4800, Loss: 0.6905\n",
      "Iteration 4900, Loss: 0.6890\n",
      "167 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0534\n",
      "Iteration 200, Loss: 1.0171\n",
      "Iteration 300, Loss: 0.9868\n",
      "Iteration 400, Loss: 0.9612\n",
      "Iteration 500, Loss: 0.9389\n",
      "Iteration 600, Loss: 0.9196\n",
      "Iteration 700, Loss: 0.9023\n",
      "Iteration 800, Loss: 0.8871\n",
      "Iteration 900, Loss: 0.8733\n",
      "Iteration 1000, Loss: 0.8610\n",
      "Iteration 1100, Loss: 0.8497\n",
      "Iteration 1200, Loss: 0.8394\n",
      "Iteration 1300, Loss: 0.8300\n",
      "Iteration 1400, Loss: 0.8214\n",
      "Iteration 1500, Loss: 0.8134\n",
      "Iteration 1600, Loss: 0.8060\n",
      "Iteration 1700, Loss: 0.7992\n",
      "Iteration 1800, Loss: 0.7928\n",
      "Iteration 1900, Loss: 0.7868\n",
      "Iteration 2000, Loss: 0.7812\n",
      "Iteration 2100, Loss: 0.7759\n",
      "Iteration 2200, Loss: 0.7709\n",
      "Iteration 2300, Loss: 0.7663\n",
      "Iteration 2400, Loss: 0.7618\n",
      "Iteration 2500, Loss: 0.7576\n",
      "Iteration 2600, Loss: 0.7536\n",
      "Iteration 2700, Loss: 0.7498\n",
      "Iteration 2800, Loss: 0.7462\n",
      "Iteration 2900, Loss: 0.7428\n",
      "Iteration 3000, Loss: 0.7395\n",
      "Iteration 3100, Loss: 0.7364\n",
      "Iteration 3200, Loss: 0.7334\n",
      "Iteration 3300, Loss: 0.7305\n",
      "Iteration 3400, Loss: 0.7278\n",
      "Iteration 3500, Loss: 0.7251\n",
      "Iteration 3600, Loss: 0.7226\n",
      "Iteration 3700, Loss: 0.7201\n",
      "Iteration 3800, Loss: 0.7178\n",
      "Iteration 3900, Loss: 0.7155\n",
      "Iteration 4000, Loss: 0.7133\n",
      "Iteration 4100, Loss: 0.7112\n",
      "Iteration 4200, Loss: 0.7092\n",
      "Iteration 4300, Loss: 0.7072\n",
      "Iteration 4400, Loss: 0.7053\n",
      "Iteration 4500, Loss: 0.7034\n",
      "Iteration 4600, Loss: 0.7017\n",
      "Iteration 4700, Loss: 0.6999\n",
      "Iteration 4800, Loss: 0.6982\n",
      "Iteration 4900, Loss: 0.6966\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0539\n",
      "Iteration 200, Loss: 1.0182\n",
      "Iteration 300, Loss: 0.9884\n",
      "Iteration 400, Loss: 0.9631\n",
      "Iteration 500, Loss: 0.9412\n",
      "Iteration 600, Loss: 0.9219\n",
      "Iteration 700, Loss: 0.9049\n",
      "Iteration 800, Loss: 0.8898\n",
      "Iteration 900, Loss: 0.8762\n",
      "Iteration 1000, Loss: 0.8639\n",
      "Iteration 1100, Loss: 0.8526\n",
      "Iteration 1200, Loss: 0.8425\n",
      "Iteration 1300, Loss: 0.8332\n",
      "Iteration 1400, Loss: 0.8247\n",
      "Iteration 1500, Loss: 0.8169\n",
      "Iteration 1600, Loss: 0.8097\n",
      "Iteration 1700, Loss: 0.8030\n",
      "Iteration 1800, Loss: 0.7968\n",
      "Iteration 1900, Loss: 0.7910\n",
      "Iteration 2000, Loss: 0.7857\n",
      "Iteration 2100, Loss: 0.7806\n",
      "Iteration 2200, Loss: 0.7759\n",
      "Iteration 2300, Loss: 0.7714\n",
      "Iteration 2400, Loss: 0.7673\n",
      "Iteration 2500, Loss: 0.7633\n",
      "Iteration 2600, Loss: 0.7597\n",
      "Iteration 2700, Loss: 0.7562\n",
      "Iteration 2800, Loss: 0.7528\n",
      "Iteration 2900, Loss: 0.7497\n",
      "Iteration 3000, Loss: 0.7468\n",
      "Iteration 3100, Loss: 0.7439\n",
      "Iteration 3200, Loss: 0.7412\n",
      "Iteration 3300, Loss: 0.7387\n",
      "Iteration 3400, Loss: 0.7363\n",
      "Iteration 3500, Loss: 0.7339\n",
      "Iteration 3600, Loss: 0.7317\n",
      "Iteration 3700, Loss: 0.7295\n",
      "Iteration 3800, Loss: 0.7275\n",
      "Iteration 3900, Loss: 0.7255\n",
      "Iteration 4000, Loss: 0.7236\n",
      "Iteration 4100, Loss: 0.7217\n",
      "Iteration 4200, Loss: 0.7200\n",
      "Iteration 4300, Loss: 0.7183\n",
      "Iteration 4400, Loss: 0.7166\n",
      "Iteration 4500, Loss: 0.7150\n",
      "Iteration 4600, Loss: 0.7135\n",
      "Iteration 4700, Loss: 0.7120\n",
      "Iteration 4800, Loss: 0.7106\n",
      "Iteration 4900, Loss: 0.7092\n",
      "168 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0521\n",
      "Iteration 200, Loss: 1.0146\n",
      "Iteration 300, Loss: 0.9835\n",
      "Iteration 400, Loss: 0.9572\n",
      "Iteration 500, Loss: 0.9346\n",
      "Iteration 600, Loss: 0.9148\n",
      "Iteration 700, Loss: 0.8974\n",
      "Iteration 800, Loss: 0.8820\n",
      "Iteration 900, Loss: 0.8681\n",
      "Iteration 1000, Loss: 0.8557\n",
      "Iteration 1100, Loss: 0.8446\n",
      "Iteration 1200, Loss: 0.8344\n",
      "Iteration 1300, Loss: 0.8250\n",
      "Iteration 1400, Loss: 0.8165\n",
      "Iteration 1500, Loss: 0.8086\n",
      "Iteration 1600, Loss: 0.8014\n",
      "Iteration 1700, Loss: 0.7947\n",
      "Iteration 1800, Loss: 0.7885\n",
      "Iteration 1900, Loss: 0.7827\n",
      "Iteration 2000, Loss: 0.7772\n",
      "Iteration 2100, Loss: 0.7722\n",
      "Iteration 2200, Loss: 0.7674\n",
      "Iteration 2300, Loss: 0.7629\n",
      "Iteration 2400, Loss: 0.7587\n",
      "Iteration 2500, Loss: 0.7548\n",
      "Iteration 2600, Loss: 0.7510\n",
      "Iteration 2700, Loss: 0.7474\n",
      "Iteration 2800, Loss: 0.7441\n",
      "Iteration 2900, Loss: 0.7408\n",
      "Iteration 3000, Loss: 0.7378\n",
      "Iteration 3100, Loss: 0.7349\n",
      "Iteration 3200, Loss: 0.7322\n",
      "Iteration 3300, Loss: 0.7295\n",
      "Iteration 3400, Loss: 0.7270\n",
      "Iteration 3500, Loss: 0.7246\n",
      "Iteration 3600, Loss: 0.7223\n",
      "Iteration 3700, Loss: 0.7201\n",
      "Iteration 3800, Loss: 0.7180\n",
      "Iteration 3900, Loss: 0.7160\n",
      "Iteration 4000, Loss: 0.7140\n",
      "Iteration 4100, Loss: 0.7121\n",
      "Iteration 4200, Loss: 0.7103\n",
      "Iteration 4300, Loss: 0.7085\n",
      "Iteration 4400, Loss: 0.7069\n",
      "Iteration 4500, Loss: 0.7052\n",
      "Iteration 4600, Loss: 0.7037\n",
      "Iteration 4700, Loss: 0.7021\n",
      "Iteration 4800, Loss: 0.7007\n",
      "Iteration 4900, Loss: 0.6992\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0550\n",
      "Iteration 200, Loss: 1.0199\n",
      "Iteration 300, Loss: 0.9908\n",
      "Iteration 400, Loss: 0.9660\n",
      "Iteration 500, Loss: 0.9444\n",
      "Iteration 600, Loss: 0.9255\n",
      "Iteration 700, Loss: 0.9088\n",
      "Iteration 800, Loss: 0.8938\n",
      "Iteration 900, Loss: 0.8803\n",
      "Iteration 1000, Loss: 0.8681\n",
      "Iteration 1100, Loss: 0.8570\n",
      "Iteration 1200, Loss: 0.8468\n",
      "Iteration 1300, Loss: 0.8375\n",
      "Iteration 1400, Loss: 0.8289\n",
      "Iteration 1500, Loss: 0.8210\n",
      "Iteration 1600, Loss: 0.8137\n",
      "Iteration 1700, Loss: 0.8070\n",
      "Iteration 1800, Loss: 0.8006\n",
      "Iteration 1900, Loss: 0.7947\n",
      "Iteration 2000, Loss: 0.7892\n",
      "Iteration 2100, Loss: 0.7840\n",
      "Iteration 2200, Loss: 0.7791\n",
      "Iteration 2300, Loss: 0.7745\n",
      "Iteration 2400, Loss: 0.7701\n",
      "Iteration 2500, Loss: 0.7661\n",
      "Iteration 2600, Loss: 0.7622\n",
      "Iteration 2700, Loss: 0.7586\n",
      "Iteration 2800, Loss: 0.7551\n",
      "Iteration 2900, Loss: 0.7518\n",
      "Iteration 3000, Loss: 0.7486\n",
      "Iteration 3100, Loss: 0.7456\n",
      "Iteration 3200, Loss: 0.7427\n",
      "Iteration 3300, Loss: 0.7400\n",
      "Iteration 3400, Loss: 0.7374\n",
      "Iteration 3500, Loss: 0.7349\n",
      "Iteration 3600, Loss: 0.7324\n",
      "Iteration 3700, Loss: 0.7301\n",
      "Iteration 3800, Loss: 0.7279\n",
      "Iteration 3900, Loss: 0.7257\n",
      "Iteration 4000, Loss: 0.7236\n",
      "Iteration 4100, Loss: 0.7216\n",
      "Iteration 4200, Loss: 0.7197\n",
      "Iteration 4300, Loss: 0.7179\n",
      "Iteration 4400, Loss: 0.7161\n",
      "Iteration 4500, Loss: 0.7143\n",
      "Iteration 4600, Loss: 0.7126\n",
      "Iteration 4700, Loss: 0.7110\n",
      "Iteration 4800, Loss: 0.7094\n",
      "Iteration 4900, Loss: 0.7078\n",
      "169 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0533\n",
      "Iteration 200, Loss: 1.0172\n",
      "Iteration 300, Loss: 0.9873\n",
      "Iteration 400, Loss: 0.9620\n",
      "Iteration 500, Loss: 0.9400\n",
      "Iteration 600, Loss: 0.9208\n",
      "Iteration 700, Loss: 0.9039\n",
      "Iteration 800, Loss: 0.8890\n",
      "Iteration 900, Loss: 0.8755\n",
      "Iteration 1000, Loss: 0.8634\n",
      "Iteration 1100, Loss: 0.8525\n",
      "Iteration 1200, Loss: 0.8425\n",
      "Iteration 1300, Loss: 0.8334\n",
      "Iteration 1400, Loss: 0.8251\n",
      "Iteration 1500, Loss: 0.8174\n",
      "Iteration 1600, Loss: 0.8103\n",
      "Iteration 1700, Loss: 0.8037\n",
      "Iteration 1800, Loss: 0.7976\n",
      "Iteration 1900, Loss: 0.7919\n",
      "Iteration 2000, Loss: 0.7865\n",
      "Iteration 2100, Loss: 0.7815\n",
      "Iteration 2200, Loss: 0.7769\n",
      "Iteration 2300, Loss: 0.7725\n",
      "Iteration 2400, Loss: 0.7683\n",
      "Iteration 2500, Loss: 0.7644\n",
      "Iteration 2600, Loss: 0.7607\n",
      "Iteration 2700, Loss: 0.7572\n",
      "Iteration 2800, Loss: 0.7539\n",
      "Iteration 2900, Loss: 0.7508\n",
      "Iteration 3000, Loss: 0.7478\n",
      "Iteration 3100, Loss: 0.7449\n",
      "Iteration 3200, Loss: 0.7422\n",
      "Iteration 3300, Loss: 0.7396\n",
      "Iteration 3400, Loss: 0.7371\n",
      "Iteration 3500, Loss: 0.7347\n",
      "Iteration 3600, Loss: 0.7324\n",
      "Iteration 3700, Loss: 0.7302\n",
      "Iteration 3800, Loss: 0.7281\n",
      "Iteration 3900, Loss: 0.7261\n",
      "Iteration 4000, Loss: 0.7242\n",
      "Iteration 4100, Loss: 0.7223\n",
      "Iteration 4200, Loss: 0.7205\n",
      "Iteration 4300, Loss: 0.7187\n",
      "Iteration 4400, Loss: 0.7171\n",
      "Iteration 4500, Loss: 0.7154\n",
      "Iteration 4600, Loss: 0.7138\n",
      "Iteration 4700, Loss: 0.7123\n",
      "Iteration 4800, Loss: 0.7109\n",
      "Iteration 4900, Loss: 0.7094\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0543\n",
      "Iteration 200, Loss: 1.0187\n",
      "Iteration 300, Loss: 0.9887\n",
      "Iteration 400, Loss: 0.9629\n",
      "Iteration 500, Loss: 0.9407\n",
      "Iteration 600, Loss: 0.9212\n",
      "Iteration 700, Loss: 0.9038\n",
      "Iteration 800, Loss: 0.8884\n",
      "Iteration 900, Loss: 0.8745\n",
      "Iteration 1000, Loss: 0.8618\n",
      "Iteration 1100, Loss: 0.8503\n",
      "Iteration 1200, Loss: 0.8399\n",
      "Iteration 1300, Loss: 0.8302\n",
      "Iteration 1400, Loss: 0.8214\n",
      "Iteration 1500, Loss: 0.8131\n",
      "Iteration 1600, Loss: 0.8055\n",
      "Iteration 1700, Loss: 0.7985\n",
      "Iteration 1800, Loss: 0.7919\n",
      "Iteration 1900, Loss: 0.7858\n",
      "Iteration 2000, Loss: 0.7801\n",
      "Iteration 2100, Loss: 0.7747\n",
      "Iteration 2200, Loss: 0.7697\n",
      "Iteration 2300, Loss: 0.7649\n",
      "Iteration 2400, Loss: 0.7605\n",
      "Iteration 2500, Loss: 0.7563\n",
      "Iteration 2600, Loss: 0.7523\n",
      "Iteration 2700, Loss: 0.7485\n",
      "Iteration 2800, Loss: 0.7449\n",
      "Iteration 2900, Loss: 0.7415\n",
      "Iteration 3000, Loss: 0.7382\n",
      "Iteration 3100, Loss: 0.7352\n",
      "Iteration 3200, Loss: 0.7322\n",
      "Iteration 3300, Loss: 0.7294\n",
      "Iteration 3400, Loss: 0.7267\n",
      "Iteration 3500, Loss: 0.7241\n",
      "Iteration 3600, Loss: 0.7216\n",
      "Iteration 3700, Loss: 0.7192\n",
      "Iteration 3800, Loss: 0.7169\n",
      "Iteration 3900, Loss: 0.7147\n",
      "Iteration 4000, Loss: 0.7126\n",
      "Iteration 4100, Loss: 0.7105\n",
      "Iteration 4200, Loss: 0.7085\n",
      "Iteration 4300, Loss: 0.7066\n",
      "Iteration 4400, Loss: 0.7047\n",
      "Iteration 4500, Loss: 0.7029\n",
      "Iteration 4600, Loss: 0.7012\n",
      "Iteration 4700, Loss: 0.6995\n",
      "Iteration 4800, Loss: 0.6978\n",
      "Iteration 4900, Loss: 0.6962\n",
      "170 270\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0528\n",
      "Iteration 200, Loss: 1.0164\n",
      "Iteration 300, Loss: 0.9863\n",
      "Iteration 400, Loss: 0.9607\n",
      "Iteration 500, Loss: 0.9387\n",
      "Iteration 600, Loss: 0.9194\n",
      "Iteration 700, Loss: 0.9024\n",
      "Iteration 800, Loss: 0.8872\n",
      "Iteration 900, Loss: 0.8736\n",
      "Iteration 1000, Loss: 0.8614\n",
      "Iteration 1100, Loss: 0.8503\n",
      "Iteration 1200, Loss: 0.8403\n",
      "Iteration 1300, Loss: 0.8310\n",
      "Iteration 1400, Loss: 0.8225\n",
      "Iteration 1500, Loss: 0.8147\n",
      "Iteration 1600, Loss: 0.8075\n",
      "Iteration 1700, Loss: 0.8008\n",
      "Iteration 1800, Loss: 0.7946\n",
      "Iteration 1900, Loss: 0.7888\n",
      "Iteration 2000, Loss: 0.7834\n",
      "Iteration 2100, Loss: 0.7783\n",
      "Iteration 2200, Loss: 0.7735\n",
      "Iteration 2300, Loss: 0.7690\n",
      "Iteration 2400, Loss: 0.7647\n",
      "Iteration 2500, Loss: 0.7607\n",
      "Iteration 2600, Loss: 0.7569\n",
      "Iteration 2700, Loss: 0.7533\n",
      "Iteration 2800, Loss: 0.7499\n",
      "Iteration 2900, Loss: 0.7467\n",
      "Iteration 3000, Loss: 0.7436\n",
      "Iteration 3100, Loss: 0.7407\n",
      "Iteration 3200, Loss: 0.7379\n",
      "Iteration 3300, Loss: 0.7352\n",
      "Iteration 3400, Loss: 0.7326\n",
      "Iteration 3500, Loss: 0.7302\n",
      "Iteration 3600, Loss: 0.7278\n",
      "Iteration 3700, Loss: 0.7256\n",
      "Iteration 3800, Loss: 0.7234\n",
      "Iteration 3900, Loss: 0.7213\n",
      "Iteration 4000, Loss: 0.7193\n",
      "Iteration 4100, Loss: 0.7174\n",
      "Iteration 4200, Loss: 0.7155\n",
      "Iteration 4300, Loss: 0.7137\n",
      "Iteration 4400, Loss: 0.7119\n",
      "Iteration 4500, Loss: 0.7102\n",
      "Iteration 4600, Loss: 0.7086\n",
      "Iteration 4700, Loss: 0.7070\n",
      "Iteration 4800, Loss: 0.7054\n",
      "Iteration 4900, Loss: 0.7039\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0540\n",
      "Iteration 200, Loss: 1.0180\n",
      "Iteration 300, Loss: 0.9878\n",
      "Iteration 400, Loss: 0.9622\n",
      "Iteration 500, Loss: 0.9400\n",
      "Iteration 600, Loss: 0.9205\n",
      "Iteration 700, Loss: 0.9033\n",
      "Iteration 800, Loss: 0.8878\n",
      "Iteration 900, Loss: 0.8740\n",
      "Iteration 1000, Loss: 0.8615\n",
      "Iteration 1100, Loss: 0.8501\n",
      "Iteration 1200, Loss: 0.8397\n",
      "Iteration 1300, Loss: 0.8301\n",
      "Iteration 1400, Loss: 0.8214\n",
      "Iteration 1500, Loss: 0.8133\n",
      "Iteration 1600, Loss: 0.8059\n",
      "Iteration 1700, Loss: 0.7989\n",
      "Iteration 1800, Loss: 0.7925\n",
      "Iteration 1900, Loss: 0.7865\n",
      "Iteration 2000, Loss: 0.7808\n",
      "Iteration 2100, Loss: 0.7756\n",
      "Iteration 2200, Loss: 0.7707\n",
      "Iteration 2300, Loss: 0.7660\n",
      "Iteration 2400, Loss: 0.7617\n",
      "Iteration 2500, Loss: 0.7576\n",
      "Iteration 2600, Loss: 0.7537\n",
      "Iteration 2700, Loss: 0.7500\n",
      "Iteration 2800, Loss: 0.7465\n",
      "Iteration 2900, Loss: 0.7432\n",
      "Iteration 3000, Loss: 0.7401\n",
      "Iteration 3100, Loss: 0.7371\n",
      "Iteration 3200, Loss: 0.7342\n",
      "Iteration 3300, Loss: 0.7315\n",
      "Iteration 3400, Loss: 0.7289\n",
      "Iteration 3500, Loss: 0.7264\n",
      "Iteration 3600, Loss: 0.7240\n",
      "Iteration 3700, Loss: 0.7217\n",
      "Iteration 3800, Loss: 0.7195\n",
      "Iteration 3900, Loss: 0.7174\n",
      "Iteration 4000, Loss: 0.7153\n",
      "Iteration 4100, Loss: 0.7134\n",
      "Iteration 4200, Loss: 0.7114\n",
      "Iteration 4300, Loss: 0.7096\n",
      "Iteration 4400, Loss: 0.7078\n",
      "Iteration 4500, Loss: 0.7061\n",
      "Iteration 4600, Loss: 0.7044\n",
      "Iteration 4700, Loss: 0.7028\n",
      "Iteration 4800, Loss: 0.7013\n",
      "Iteration 4900, Loss: 0.6997\n",
      "171 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0507\n",
      "Iteration 200, Loss: 1.0126\n",
      "Iteration 300, Loss: 0.9812\n",
      "Iteration 400, Loss: 0.9546\n",
      "Iteration 500, Loss: 0.9319\n",
      "Iteration 600, Loss: 0.9120\n",
      "Iteration 700, Loss: 0.8945\n",
      "Iteration 800, Loss: 0.8789\n",
      "Iteration 900, Loss: 0.8650\n",
      "Iteration 1000, Loss: 0.8525\n",
      "Iteration 1100, Loss: 0.8411\n",
      "Iteration 1200, Loss: 0.8308\n",
      "Iteration 1300, Loss: 0.8214\n",
      "Iteration 1400, Loss: 0.8127\n",
      "Iteration 1500, Loss: 0.8048\n",
      "Iteration 1600, Loss: 0.7974\n",
      "Iteration 1700, Loss: 0.7906\n",
      "Iteration 1800, Loss: 0.7844\n",
      "Iteration 1900, Loss: 0.7785\n",
      "Iteration 2000, Loss: 0.7731\n",
      "Iteration 2100, Loss: 0.7680\n",
      "Iteration 2200, Loss: 0.7632\n",
      "Iteration 2300, Loss: 0.7587\n",
      "Iteration 2400, Loss: 0.7545\n",
      "Iteration 2500, Loss: 0.7505\n",
      "Iteration 2600, Loss: 0.7467\n",
      "Iteration 2700, Loss: 0.7432\n",
      "Iteration 2800, Loss: 0.7398\n",
      "Iteration 2900, Loss: 0.7367\n",
      "Iteration 3000, Loss: 0.7337\n",
      "Iteration 3100, Loss: 0.7308\n",
      "Iteration 3200, Loss: 0.7281\n",
      "Iteration 3300, Loss: 0.7255\n",
      "Iteration 3400, Loss: 0.7230\n",
      "Iteration 3500, Loss: 0.7206\n",
      "Iteration 3600, Loss: 0.7183\n",
      "Iteration 3700, Loss: 0.7162\n",
      "Iteration 3800, Loss: 0.7141\n",
      "Iteration 3900, Loss: 0.7121\n",
      "Iteration 4000, Loss: 0.7102\n",
      "Iteration 4100, Loss: 0.7083\n",
      "Iteration 4200, Loss: 0.7066\n",
      "Iteration 4300, Loss: 0.7048\n",
      "Iteration 4400, Loss: 0.7032\n",
      "Iteration 4500, Loss: 0.7016\n",
      "Iteration 4600, Loss: 0.7000\n",
      "Iteration 4700, Loss: 0.6985\n",
      "Iteration 4800, Loss: 0.6971\n",
      "Iteration 4900, Loss: 0.6957\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0561\n",
      "Iteration 200, Loss: 1.0218\n",
      "Iteration 300, Loss: 0.9931\n",
      "Iteration 400, Loss: 0.9685\n",
      "Iteration 500, Loss: 0.9472\n",
      "Iteration 600, Loss: 0.9284\n",
      "Iteration 700, Loss: 0.9118\n",
      "Iteration 800, Loss: 0.8968\n",
      "Iteration 900, Loss: 0.8834\n",
      "Iteration 1000, Loss: 0.8712\n",
      "Iteration 1100, Loss: 0.8600\n",
      "Iteration 1200, Loss: 0.8498\n",
      "Iteration 1300, Loss: 0.8404\n",
      "Iteration 1400, Loss: 0.8318\n",
      "Iteration 1500, Loss: 0.8238\n",
      "Iteration 1600, Loss: 0.8164\n",
      "Iteration 1700, Loss: 0.8095\n",
      "Iteration 1800, Loss: 0.8030\n",
      "Iteration 1900, Loss: 0.7971\n",
      "Iteration 2000, Loss: 0.7914\n",
      "Iteration 2100, Loss: 0.7861\n",
      "Iteration 2200, Loss: 0.7812\n",
      "Iteration 2300, Loss: 0.7765\n",
      "Iteration 2400, Loss: 0.7720\n",
      "Iteration 2500, Loss: 0.7678\n",
      "Iteration 2600, Loss: 0.7638\n",
      "Iteration 2700, Loss: 0.7601\n",
      "Iteration 2800, Loss: 0.7565\n",
      "Iteration 2900, Loss: 0.7530\n",
      "Iteration 3000, Loss: 0.7498\n",
      "Iteration 3100, Loss: 0.7466\n",
      "Iteration 3200, Loss: 0.7437\n",
      "Iteration 3300, Loss: 0.7408\n",
      "Iteration 3400, Loss: 0.7381\n",
      "Iteration 3500, Loss: 0.7355\n",
      "Iteration 3600, Loss: 0.7329\n",
      "Iteration 3700, Loss: 0.7305\n",
      "Iteration 3800, Loss: 0.7282\n",
      "Iteration 3900, Loss: 0.7259\n",
      "Iteration 4000, Loss: 0.7238\n",
      "Iteration 4100, Loss: 0.7216\n",
      "Iteration 4200, Loss: 0.7196\n",
      "Iteration 4300, Loss: 0.7177\n",
      "Iteration 4400, Loss: 0.7158\n",
      "Iteration 4500, Loss: 0.7139\n",
      "Iteration 4600, Loss: 0.7121\n",
      "Iteration 4700, Loss: 0.7104\n",
      "Iteration 4800, Loss: 0.7087\n",
      "Iteration 4900, Loss: 0.7071\n",
      "172 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0547\n",
      "Iteration 200, Loss: 1.0188\n",
      "Iteration 300, Loss: 0.9887\n",
      "Iteration 400, Loss: 0.9630\n",
      "Iteration 500, Loss: 0.9407\n",
      "Iteration 600, Loss: 0.9211\n",
      "Iteration 700, Loss: 0.9037\n",
      "Iteration 800, Loss: 0.8882\n",
      "Iteration 900, Loss: 0.8742\n",
      "Iteration 1000, Loss: 0.8615\n",
      "Iteration 1100, Loss: 0.8500\n",
      "Iteration 1200, Loss: 0.8395\n",
      "Iteration 1300, Loss: 0.8299\n",
      "Iteration 1400, Loss: 0.8210\n",
      "Iteration 1500, Loss: 0.8128\n",
      "Iteration 1600, Loss: 0.8053\n",
      "Iteration 1700, Loss: 0.7983\n",
      "Iteration 1800, Loss: 0.7917\n",
      "Iteration 1900, Loss: 0.7856\n",
      "Iteration 2000, Loss: 0.7799\n",
      "Iteration 2100, Loss: 0.7746\n",
      "Iteration 2200, Loss: 0.7696\n",
      "Iteration 2300, Loss: 0.7650\n",
      "Iteration 2400, Loss: 0.7605\n",
      "Iteration 2500, Loss: 0.7564\n",
      "Iteration 2600, Loss: 0.7524\n",
      "Iteration 2700, Loss: 0.7487\n",
      "Iteration 2800, Loss: 0.7451\n",
      "Iteration 2900, Loss: 0.7418\n",
      "Iteration 3000, Loss: 0.7386\n",
      "Iteration 3100, Loss: 0.7356\n",
      "Iteration 3200, Loss: 0.7327\n",
      "Iteration 3300, Loss: 0.7299\n",
      "Iteration 3400, Loss: 0.7272\n",
      "Iteration 3500, Loss: 0.7247\n",
      "Iteration 3600, Loss: 0.7222\n",
      "Iteration 3700, Loss: 0.7199\n",
      "Iteration 3800, Loss: 0.7176\n",
      "Iteration 3900, Loss: 0.7154\n",
      "Iteration 4000, Loss: 0.7133\n",
      "Iteration 4100, Loss: 0.7113\n",
      "Iteration 4200, Loss: 0.7094\n",
      "Iteration 4300, Loss: 0.7075\n",
      "Iteration 4400, Loss: 0.7057\n",
      "Iteration 4500, Loss: 0.7039\n",
      "Iteration 4600, Loss: 0.7022\n",
      "Iteration 4700, Loss: 0.7006\n",
      "Iteration 4800, Loss: 0.6990\n",
      "Iteration 4900, Loss: 0.6974\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0528\n",
      "Iteration 200, Loss: 1.0164\n",
      "Iteration 300, Loss: 0.9865\n",
      "Iteration 400, Loss: 0.9611\n",
      "Iteration 500, Loss: 0.9393\n",
      "Iteration 600, Loss: 0.9202\n",
      "Iteration 700, Loss: 0.9034\n",
      "Iteration 800, Loss: 0.8886\n",
      "Iteration 900, Loss: 0.8752\n",
      "Iteration 1000, Loss: 0.8631\n",
      "Iteration 1100, Loss: 0.8523\n",
      "Iteration 1200, Loss: 0.8423\n",
      "Iteration 1300, Loss: 0.8332\n",
      "Iteration 1400, Loss: 0.8249\n",
      "Iteration 1500, Loss: 0.8172\n",
      "Iteration 1600, Loss: 0.8101\n",
      "Iteration 1700, Loss: 0.8035\n",
      "Iteration 1800, Loss: 0.7974\n",
      "Iteration 1900, Loss: 0.7916\n",
      "Iteration 2000, Loss: 0.7863\n",
      "Iteration 2100, Loss: 0.7813\n",
      "Iteration 2200, Loss: 0.7766\n",
      "Iteration 2300, Loss: 0.7722\n",
      "Iteration 2400, Loss: 0.7680\n",
      "Iteration 2500, Loss: 0.7640\n",
      "Iteration 2600, Loss: 0.7603\n",
      "Iteration 2700, Loss: 0.7568\n",
      "Iteration 2800, Loss: 0.7534\n",
      "Iteration 2900, Loss: 0.7502\n",
      "Iteration 3000, Loss: 0.7472\n",
      "Iteration 3100, Loss: 0.7443\n",
      "Iteration 3200, Loss: 0.7415\n",
      "Iteration 3300, Loss: 0.7388\n",
      "Iteration 3400, Loss: 0.7363\n",
      "Iteration 3500, Loss: 0.7339\n",
      "Iteration 3600, Loss: 0.7316\n",
      "Iteration 3700, Loss: 0.7293\n",
      "Iteration 3800, Loss: 0.7272\n",
      "Iteration 3900, Loss: 0.7251\n",
      "Iteration 4000, Loss: 0.7231\n",
      "Iteration 4100, Loss: 0.7211\n",
      "Iteration 4200, Loss: 0.7193\n",
      "Iteration 4300, Loss: 0.7175\n",
      "Iteration 4400, Loss: 0.7157\n",
      "Iteration 4500, Loss: 0.7140\n",
      "Iteration 4600, Loss: 0.7124\n",
      "Iteration 4700, Loss: 0.7108\n",
      "Iteration 4800, Loss: 0.7093\n",
      "Iteration 4900, Loss: 0.7078\n",
      "173 270\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0575\n",
      "Iteration 200, Loss: 1.0241\n",
      "Iteration 300, Loss: 0.9957\n",
      "Iteration 400, Loss: 0.9714\n",
      "Iteration 500, Loss: 0.9502\n",
      "Iteration 600, Loss: 0.9316\n",
      "Iteration 700, Loss: 0.9148\n",
      "Iteration 800, Loss: 0.8999\n",
      "Iteration 900, Loss: 0.8863\n",
      "Iteration 1000, Loss: 0.8740\n",
      "Iteration 1100, Loss: 0.8629\n",
      "Iteration 1200, Loss: 0.8527\n",
      "Iteration 1300, Loss: 0.8433\n",
      "Iteration 1400, Loss: 0.8346\n",
      "Iteration 1500, Loss: 0.8266\n",
      "Iteration 1600, Loss: 0.8192\n",
      "Iteration 1700, Loss: 0.8123\n",
      "Iteration 1800, Loss: 0.8059\n",
      "Iteration 1900, Loss: 0.7999\n",
      "Iteration 2000, Loss: 0.7943\n",
      "Iteration 2100, Loss: 0.7891\n",
      "Iteration 2200, Loss: 0.7842\n",
      "Iteration 2300, Loss: 0.7796\n",
      "Iteration 2400, Loss: 0.7753\n",
      "Iteration 2500, Loss: 0.7712\n",
      "Iteration 2600, Loss: 0.7673\n",
      "Iteration 2700, Loss: 0.7636\n",
      "Iteration 2800, Loss: 0.7601\n",
      "Iteration 2900, Loss: 0.7568\n",
      "Iteration 3000, Loss: 0.7537\n",
      "Iteration 3100, Loss: 0.7507\n",
      "Iteration 3200, Loss: 0.7478\n",
      "Iteration 3300, Loss: 0.7450\n",
      "Iteration 3400, Loss: 0.7424\n",
      "Iteration 3500, Loss: 0.7399\n",
      "Iteration 3600, Loss: 0.7376\n",
      "Iteration 3700, Loss: 0.7353\n",
      "Iteration 3800, Loss: 0.7331\n",
      "Iteration 3900, Loss: 0.7309\n",
      "Iteration 4000, Loss: 0.7289\n",
      "Iteration 4100, Loss: 0.7269\n",
      "Iteration 4200, Loss: 0.7250\n",
      "Iteration 4300, Loss: 0.7232\n",
      "Iteration 4400, Loss: 0.7214\n",
      "Iteration 4500, Loss: 0.7197\n",
      "Iteration 4600, Loss: 0.7181\n",
      "Iteration 4700, Loss: 0.7165\n",
      "Iteration 4800, Loss: 0.7149\n",
      "Iteration 4900, Loss: 0.7134\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0497\n",
      "Iteration 200, Loss: 1.0112\n",
      "Iteration 300, Loss: 0.9791\n",
      "Iteration 400, Loss: 0.9523\n",
      "Iteration 500, Loss: 0.9293\n",
      "Iteration 600, Loss: 0.9093\n",
      "Iteration 700, Loss: 0.8918\n",
      "Iteration 800, Loss: 0.8763\n",
      "Iteration 900, Loss: 0.8623\n",
      "Iteration 1000, Loss: 0.8498\n",
      "Iteration 1100, Loss: 0.8386\n",
      "Iteration 1200, Loss: 0.8283\n",
      "Iteration 1300, Loss: 0.8189\n",
      "Iteration 1400, Loss: 0.8103\n",
      "Iteration 1500, Loss: 0.8024\n",
      "Iteration 1600, Loss: 0.7951\n",
      "Iteration 1700, Loss: 0.7883\n",
      "Iteration 1800, Loss: 0.7820\n",
      "Iteration 1900, Loss: 0.7762\n",
      "Iteration 2000, Loss: 0.7707\n",
      "Iteration 2100, Loss: 0.7656\n",
      "Iteration 2200, Loss: 0.7608\n",
      "Iteration 2300, Loss: 0.7562\n",
      "Iteration 2400, Loss: 0.7520\n",
      "Iteration 2500, Loss: 0.7480\n",
      "Iteration 2600, Loss: 0.7442\n",
      "Iteration 2700, Loss: 0.7406\n",
      "Iteration 2800, Loss: 0.7372\n",
      "Iteration 2900, Loss: 0.7339\n",
      "Iteration 3000, Loss: 0.7308\n",
      "Iteration 3100, Loss: 0.7279\n",
      "Iteration 3200, Loss: 0.7250\n",
      "Iteration 3300, Loss: 0.7223\n",
      "Iteration 3400, Loss: 0.7198\n",
      "Iteration 3500, Loss: 0.7173\n",
      "Iteration 3600, Loss: 0.7149\n",
      "Iteration 3700, Loss: 0.7127\n",
      "Iteration 3800, Loss: 0.7105\n",
      "Iteration 3900, Loss: 0.7084\n",
      "Iteration 4000, Loss: 0.7063\n",
      "Iteration 4100, Loss: 0.7044\n",
      "Iteration 4200, Loss: 0.7025\n",
      "Iteration 4300, Loss: 0.7007\n",
      "Iteration 4400, Loss: 0.6989\n",
      "Iteration 4500, Loss: 0.6972\n",
      "Iteration 4600, Loss: 0.6955\n",
      "Iteration 4700, Loss: 0.6939\n",
      "Iteration 4800, Loss: 0.6924\n",
      "Iteration 4900, Loss: 0.6909\n",
      "174 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0510\n",
      "Iteration 200, Loss: 1.0133\n",
      "Iteration 300, Loss: 0.9827\n",
      "Iteration 400, Loss: 0.9566\n",
      "Iteration 500, Loss: 0.9343\n",
      "Iteration 600, Loss: 0.9148\n",
      "Iteration 700, Loss: 0.8977\n",
      "Iteration 800, Loss: 0.8825\n",
      "Iteration 900, Loss: 0.8688\n",
      "Iteration 1000, Loss: 0.8566\n",
      "Iteration 1100, Loss: 0.8455\n",
      "Iteration 1200, Loss: 0.8355\n",
      "Iteration 1300, Loss: 0.8262\n",
      "Iteration 1400, Loss: 0.8178\n",
      "Iteration 1500, Loss: 0.8100\n",
      "Iteration 1600, Loss: 0.8028\n",
      "Iteration 1700, Loss: 0.7962\n",
      "Iteration 1800, Loss: 0.7899\n",
      "Iteration 1900, Loss: 0.7842\n",
      "Iteration 2000, Loss: 0.7788\n",
      "Iteration 2100, Loss: 0.7737\n",
      "Iteration 2200, Loss: 0.7689\n",
      "Iteration 2300, Loss: 0.7645\n",
      "Iteration 2400, Loss: 0.7602\n",
      "Iteration 2500, Loss: 0.7561\n",
      "Iteration 2600, Loss: 0.7523\n",
      "Iteration 2700, Loss: 0.7487\n",
      "Iteration 2800, Loss: 0.7452\n",
      "Iteration 2900, Loss: 0.7419\n",
      "Iteration 3000, Loss: 0.7388\n",
      "Iteration 3100, Loss: 0.7358\n",
      "Iteration 3200, Loss: 0.7330\n",
      "Iteration 3300, Loss: 0.7302\n",
      "Iteration 3400, Loss: 0.7276\n",
      "Iteration 3500, Loss: 0.7251\n",
      "Iteration 3600, Loss: 0.7227\n",
      "Iteration 3700, Loss: 0.7204\n",
      "Iteration 3800, Loss: 0.7181\n",
      "Iteration 3900, Loss: 0.7160\n",
      "Iteration 4000, Loss: 0.7139\n",
      "Iteration 4100, Loss: 0.7119\n",
      "Iteration 4200, Loss: 0.7100\n",
      "Iteration 4300, Loss: 0.7081\n",
      "Iteration 4400, Loss: 0.7063\n",
      "Iteration 4500, Loss: 0.7046\n",
      "Iteration 4600, Loss: 0.7029\n",
      "Iteration 4700, Loss: 0.7012\n",
      "Iteration 4800, Loss: 0.6996\n",
      "Iteration 4900, Loss: 0.6981\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0560\n",
      "Iteration 200, Loss: 1.0214\n",
      "Iteration 300, Loss: 0.9921\n",
      "Iteration 400, Loss: 0.9670\n",
      "Iteration 500, Loss: 0.9452\n",
      "Iteration 600, Loss: 0.9259\n",
      "Iteration 700, Loss: 0.9088\n",
      "Iteration 800, Loss: 0.8934\n",
      "Iteration 900, Loss: 0.8795\n",
      "Iteration 1000, Loss: 0.8671\n",
      "Iteration 1100, Loss: 0.8557\n",
      "Iteration 1200, Loss: 0.8454\n",
      "Iteration 1300, Loss: 0.8359\n",
      "Iteration 1400, Loss: 0.8271\n",
      "Iteration 1500, Loss: 0.8190\n",
      "Iteration 1600, Loss: 0.8116\n",
      "Iteration 1700, Loss: 0.8046\n",
      "Iteration 1800, Loss: 0.7982\n",
      "Iteration 1900, Loss: 0.7922\n",
      "Iteration 2000, Loss: 0.7866\n",
      "Iteration 2100, Loss: 0.7813\n",
      "Iteration 2200, Loss: 0.7764\n",
      "Iteration 2300, Loss: 0.7717\n",
      "Iteration 2400, Loss: 0.7674\n",
      "Iteration 2500, Loss: 0.7633\n",
      "Iteration 2600, Loss: 0.7595\n",
      "Iteration 2700, Loss: 0.7558\n",
      "Iteration 2800, Loss: 0.7524\n",
      "Iteration 2900, Loss: 0.7491\n",
      "Iteration 3000, Loss: 0.7459\n",
      "Iteration 3100, Loss: 0.7430\n",
      "Iteration 3200, Loss: 0.7401\n",
      "Iteration 3300, Loss: 0.7374\n",
      "Iteration 3400, Loss: 0.7349\n",
      "Iteration 3500, Loss: 0.7324\n",
      "Iteration 3600, Loss: 0.7300\n",
      "Iteration 3700, Loss: 0.7278\n",
      "Iteration 3800, Loss: 0.7256\n",
      "Iteration 3900, Loss: 0.7236\n",
      "Iteration 4000, Loss: 0.7216\n",
      "Iteration 4100, Loss: 0.7196\n",
      "Iteration 4200, Loss: 0.7178\n",
      "Iteration 4300, Loss: 0.7159\n",
      "Iteration 4400, Loss: 0.7142\n",
      "Iteration 4500, Loss: 0.7125\n",
      "Iteration 4600, Loss: 0.7109\n",
      "Iteration 4700, Loss: 0.7094\n",
      "Iteration 4800, Loss: 0.7078\n",
      "Iteration 4900, Loss: 0.7063\n",
      "175 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0542\n",
      "Iteration 200, Loss: 1.0184\n",
      "Iteration 300, Loss: 0.9887\n",
      "Iteration 400, Loss: 0.9635\n",
      "Iteration 500, Loss: 0.9418\n",
      "Iteration 600, Loss: 0.9228\n",
      "Iteration 700, Loss: 0.9060\n",
      "Iteration 800, Loss: 0.8910\n",
      "Iteration 900, Loss: 0.8776\n",
      "Iteration 1000, Loss: 0.8655\n",
      "Iteration 1100, Loss: 0.8545\n",
      "Iteration 1200, Loss: 0.8444\n",
      "Iteration 1300, Loss: 0.8352\n",
      "Iteration 1400, Loss: 0.8267\n",
      "Iteration 1500, Loss: 0.8189\n",
      "Iteration 1600, Loss: 0.8117\n",
      "Iteration 1700, Loss: 0.8050\n",
      "Iteration 1800, Loss: 0.7987\n",
      "Iteration 1900, Loss: 0.7929\n",
      "Iteration 2000, Loss: 0.7875\n",
      "Iteration 2100, Loss: 0.7824\n",
      "Iteration 2200, Loss: 0.7776\n",
      "Iteration 2300, Loss: 0.7731\n",
      "Iteration 2400, Loss: 0.7689\n",
      "Iteration 2500, Loss: 0.7649\n",
      "Iteration 2600, Loss: 0.7611\n",
      "Iteration 2700, Loss: 0.7575\n",
      "Iteration 2800, Loss: 0.7541\n",
      "Iteration 2900, Loss: 0.7508\n",
      "Iteration 3000, Loss: 0.7477\n",
      "Iteration 3100, Loss: 0.7448\n",
      "Iteration 3200, Loss: 0.7420\n",
      "Iteration 3300, Loss: 0.7393\n",
      "Iteration 3400, Loss: 0.7367\n",
      "Iteration 3500, Loss: 0.7342\n",
      "Iteration 3600, Loss: 0.7319\n",
      "Iteration 3700, Loss: 0.7296\n",
      "Iteration 3800, Loss: 0.7274\n",
      "Iteration 3900, Loss: 0.7253\n",
      "Iteration 4000, Loss: 0.7233\n",
      "Iteration 4100, Loss: 0.7213\n",
      "Iteration 4200, Loss: 0.7194\n",
      "Iteration 4300, Loss: 0.7176\n",
      "Iteration 4400, Loss: 0.7158\n",
      "Iteration 4500, Loss: 0.7141\n",
      "Iteration 4600, Loss: 0.7125\n",
      "Iteration 4700, Loss: 0.7109\n",
      "Iteration 4800, Loss: 0.7093\n",
      "Iteration 4900, Loss: 0.7078\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0532\n",
      "Iteration 200, Loss: 1.0168\n",
      "Iteration 300, Loss: 0.9865\n",
      "Iteration 400, Loss: 0.9607\n",
      "Iteration 500, Loss: 0.9383\n",
      "Iteration 600, Loss: 0.9189\n",
      "Iteration 700, Loss: 0.9015\n",
      "Iteration 800, Loss: 0.8860\n",
      "Iteration 900, Loss: 0.8721\n",
      "Iteration 1000, Loss: 0.8595\n",
      "Iteration 1100, Loss: 0.8482\n",
      "Iteration 1200, Loss: 0.8378\n",
      "Iteration 1300, Loss: 0.8283\n",
      "Iteration 1400, Loss: 0.8195\n",
      "Iteration 1500, Loss: 0.8114\n",
      "Iteration 1600, Loss: 0.8039\n",
      "Iteration 1700, Loss: 0.7970\n",
      "Iteration 1800, Loss: 0.7906\n",
      "Iteration 1900, Loss: 0.7845\n",
      "Iteration 2000, Loss: 0.7789\n",
      "Iteration 2100, Loss: 0.7737\n",
      "Iteration 2200, Loss: 0.7687\n",
      "Iteration 2300, Loss: 0.7641\n",
      "Iteration 2400, Loss: 0.7597\n",
      "Iteration 2500, Loss: 0.7555\n",
      "Iteration 2600, Loss: 0.7516\n",
      "Iteration 2700, Loss: 0.7480\n",
      "Iteration 2800, Loss: 0.7445\n",
      "Iteration 2900, Loss: 0.7412\n",
      "Iteration 3000, Loss: 0.7380\n",
      "Iteration 3100, Loss: 0.7350\n",
      "Iteration 3200, Loss: 0.7321\n",
      "Iteration 3300, Loss: 0.7294\n",
      "Iteration 3400, Loss: 0.7267\n",
      "Iteration 3500, Loss: 0.7242\n",
      "Iteration 3600, Loss: 0.7218\n",
      "Iteration 3700, Loss: 0.7195\n",
      "Iteration 3800, Loss: 0.7173\n",
      "Iteration 3900, Loss: 0.7152\n",
      "Iteration 4000, Loss: 0.7131\n",
      "Iteration 4100, Loss: 0.7112\n",
      "Iteration 4200, Loss: 0.7093\n",
      "Iteration 4300, Loss: 0.7075\n",
      "Iteration 4400, Loss: 0.7057\n",
      "Iteration 4500, Loss: 0.7040\n",
      "Iteration 4600, Loss: 0.7023\n",
      "Iteration 4700, Loss: 0.7007\n",
      "Iteration 4800, Loss: 0.6992\n",
      "Iteration 4900, Loss: 0.6977\n",
      "176 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0529\n",
      "Iteration 200, Loss: 1.0170\n",
      "Iteration 300, Loss: 0.9871\n",
      "Iteration 400, Loss: 0.9618\n",
      "Iteration 500, Loss: 0.9400\n",
      "Iteration 600, Loss: 0.9210\n",
      "Iteration 700, Loss: 0.9041\n",
      "Iteration 800, Loss: 0.8891\n",
      "Iteration 900, Loss: 0.8757\n",
      "Iteration 1000, Loss: 0.8635\n",
      "Iteration 1100, Loss: 0.8525\n",
      "Iteration 1200, Loss: 0.8424\n",
      "Iteration 1300, Loss: 0.8331\n",
      "Iteration 1400, Loss: 0.8246\n",
      "Iteration 1500, Loss: 0.8168\n",
      "Iteration 1600, Loss: 0.8096\n",
      "Iteration 1700, Loss: 0.8028\n",
      "Iteration 1800, Loss: 0.7965\n",
      "Iteration 1900, Loss: 0.7907\n",
      "Iteration 2000, Loss: 0.7852\n",
      "Iteration 2100, Loss: 0.7801\n",
      "Iteration 2200, Loss: 0.7753\n",
      "Iteration 2300, Loss: 0.7708\n",
      "Iteration 2400, Loss: 0.7665\n",
      "Iteration 2500, Loss: 0.7625\n",
      "Iteration 2600, Loss: 0.7586\n",
      "Iteration 2700, Loss: 0.7550\n",
      "Iteration 2800, Loss: 0.7516\n",
      "Iteration 2900, Loss: 0.7483\n",
      "Iteration 3000, Loss: 0.7452\n",
      "Iteration 3100, Loss: 0.7423\n",
      "Iteration 3200, Loss: 0.7394\n",
      "Iteration 3300, Loss: 0.7367\n",
      "Iteration 3400, Loss: 0.7341\n",
      "Iteration 3500, Loss: 0.7317\n",
      "Iteration 3600, Loss: 0.7293\n",
      "Iteration 3700, Loss: 0.7270\n",
      "Iteration 3800, Loss: 0.7248\n",
      "Iteration 3900, Loss: 0.7227\n",
      "Iteration 4000, Loss: 0.7207\n",
      "Iteration 4100, Loss: 0.7187\n",
      "Iteration 4200, Loss: 0.7168\n",
      "Iteration 4300, Loss: 0.7150\n",
      "Iteration 4400, Loss: 0.7132\n",
      "Iteration 4500, Loss: 0.7115\n",
      "Iteration 4600, Loss: 0.7099\n",
      "Iteration 4700, Loss: 0.7083\n",
      "Iteration 4800, Loss: 0.7068\n",
      "Iteration 4900, Loss: 0.7053\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0541\n",
      "Iteration 200, Loss: 1.0184\n",
      "Iteration 300, Loss: 0.9882\n",
      "Iteration 400, Loss: 0.9626\n",
      "Iteration 500, Loss: 0.9404\n",
      "Iteration 600, Loss: 0.9210\n",
      "Iteration 700, Loss: 0.9037\n",
      "Iteration 800, Loss: 0.8882\n",
      "Iteration 900, Loss: 0.8743\n",
      "Iteration 1000, Loss: 0.8618\n",
      "Iteration 1100, Loss: 0.8505\n",
      "Iteration 1200, Loss: 0.8401\n",
      "Iteration 1300, Loss: 0.8306\n",
      "Iteration 1400, Loss: 0.8219\n",
      "Iteration 1500, Loss: 0.8139\n",
      "Iteration 1600, Loss: 0.8065\n",
      "Iteration 1700, Loss: 0.7996\n",
      "Iteration 1800, Loss: 0.7932\n",
      "Iteration 1900, Loss: 0.7872\n",
      "Iteration 2000, Loss: 0.7816\n",
      "Iteration 2100, Loss: 0.7764\n",
      "Iteration 2200, Loss: 0.7715\n",
      "Iteration 2300, Loss: 0.7669\n",
      "Iteration 2400, Loss: 0.7625\n",
      "Iteration 2500, Loss: 0.7584\n",
      "Iteration 2600, Loss: 0.7545\n",
      "Iteration 2700, Loss: 0.7508\n",
      "Iteration 2800, Loss: 0.7473\n",
      "Iteration 2900, Loss: 0.7440\n",
      "Iteration 3000, Loss: 0.7409\n",
      "Iteration 3100, Loss: 0.7379\n",
      "Iteration 3200, Loss: 0.7350\n",
      "Iteration 3300, Loss: 0.7323\n",
      "Iteration 3400, Loss: 0.7297\n",
      "Iteration 3500, Loss: 0.7272\n",
      "Iteration 3600, Loss: 0.7248\n",
      "Iteration 3700, Loss: 0.7224\n",
      "Iteration 3800, Loss: 0.7202\n",
      "Iteration 3900, Loss: 0.7181\n",
      "Iteration 4000, Loss: 0.7161\n",
      "Iteration 4100, Loss: 0.7141\n",
      "Iteration 4200, Loss: 0.7122\n",
      "Iteration 4300, Loss: 0.7103\n",
      "Iteration 4400, Loss: 0.7085\n",
      "Iteration 4500, Loss: 0.7068\n",
      "Iteration 4600, Loss: 0.7051\n",
      "Iteration 4700, Loss: 0.7035\n",
      "Iteration 4800, Loss: 0.7019\n",
      "Iteration 4900, Loss: 0.7004\n",
      "177 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0552\n",
      "Iteration 200, Loss: 1.0203\n",
      "Iteration 300, Loss: 0.9915\n",
      "Iteration 400, Loss: 0.9669\n",
      "Iteration 500, Loss: 0.9456\n",
      "Iteration 600, Loss: 0.9271\n",
      "Iteration 700, Loss: 0.9105\n",
      "Iteration 800, Loss: 0.8957\n",
      "Iteration 900, Loss: 0.8824\n",
      "Iteration 1000, Loss: 0.8703\n",
      "Iteration 1100, Loss: 0.8594\n",
      "Iteration 1200, Loss: 0.8493\n",
      "Iteration 1300, Loss: 0.8402\n",
      "Iteration 1400, Loss: 0.8317\n",
      "Iteration 1500, Loss: 0.8239\n",
      "Iteration 1600, Loss: 0.8166\n",
      "Iteration 1700, Loss: 0.8099\n",
      "Iteration 1800, Loss: 0.8036\n",
      "Iteration 1900, Loss: 0.7977\n",
      "Iteration 2000, Loss: 0.7922\n",
      "Iteration 2100, Loss: 0.7871\n",
      "Iteration 2200, Loss: 0.7822\n",
      "Iteration 2300, Loss: 0.7776\n",
      "Iteration 2400, Loss: 0.7733\n",
      "Iteration 2500, Loss: 0.7692\n",
      "Iteration 2600, Loss: 0.7654\n",
      "Iteration 2700, Loss: 0.7617\n",
      "Iteration 2800, Loss: 0.7582\n",
      "Iteration 2900, Loss: 0.7549\n",
      "Iteration 3000, Loss: 0.7518\n",
      "Iteration 3100, Loss: 0.7487\n",
      "Iteration 3200, Loss: 0.7459\n",
      "Iteration 3300, Loss: 0.7431\n",
      "Iteration 3400, Loss: 0.7405\n",
      "Iteration 3500, Loss: 0.7379\n",
      "Iteration 3600, Loss: 0.7355\n",
      "Iteration 3700, Loss: 0.7332\n",
      "Iteration 3800, Loss: 0.7309\n",
      "Iteration 3900, Loss: 0.7288\n",
      "Iteration 4000, Loss: 0.7267\n",
      "Iteration 4100, Loss: 0.7247\n",
      "Iteration 4200, Loss: 0.7228\n",
      "Iteration 4300, Loss: 0.7209\n",
      "Iteration 4400, Loss: 0.7191\n",
      "Iteration 4500, Loss: 0.7173\n",
      "Iteration 4600, Loss: 0.7156\n",
      "Iteration 4700, Loss: 0.7139\n",
      "Iteration 4800, Loss: 0.7123\n",
      "Iteration 4900, Loss: 0.7108\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0521\n",
      "Iteration 200, Loss: 1.0146\n",
      "Iteration 300, Loss: 0.9835\n",
      "Iteration 400, Loss: 0.9569\n",
      "Iteration 500, Loss: 0.9340\n",
      "Iteration 600, Loss: 0.9140\n",
      "Iteration 700, Loss: 0.8964\n",
      "Iteration 800, Loss: 0.8807\n",
      "Iteration 900, Loss: 0.8667\n",
      "Iteration 1000, Loss: 0.8540\n",
      "Iteration 1100, Loss: 0.8425\n",
      "Iteration 1200, Loss: 0.8321\n",
      "Iteration 1300, Loss: 0.8225\n",
      "Iteration 1400, Loss: 0.8137\n",
      "Iteration 1500, Loss: 0.8056\n",
      "Iteration 1600, Loss: 0.7982\n",
      "Iteration 1700, Loss: 0.7912\n",
      "Iteration 1800, Loss: 0.7848\n",
      "Iteration 1900, Loss: 0.7788\n",
      "Iteration 2000, Loss: 0.7733\n",
      "Iteration 2100, Loss: 0.7680\n",
      "Iteration 2200, Loss: 0.7631\n",
      "Iteration 2300, Loss: 0.7585\n",
      "Iteration 2400, Loss: 0.7542\n",
      "Iteration 2500, Loss: 0.7502\n",
      "Iteration 2600, Loss: 0.7463\n",
      "Iteration 2700, Loss: 0.7427\n",
      "Iteration 2800, Loss: 0.7392\n",
      "Iteration 2900, Loss: 0.7360\n",
      "Iteration 3000, Loss: 0.7329\n",
      "Iteration 3100, Loss: 0.7299\n",
      "Iteration 3200, Loss: 0.7271\n",
      "Iteration 3300, Loss: 0.7244\n",
      "Iteration 3400, Loss: 0.7218\n",
      "Iteration 3500, Loss: 0.7194\n",
      "Iteration 3600, Loss: 0.7170\n",
      "Iteration 3700, Loss: 0.7148\n",
      "Iteration 3800, Loss: 0.7126\n",
      "Iteration 3900, Loss: 0.7105\n",
      "Iteration 4000, Loss: 0.7085\n",
      "Iteration 4100, Loss: 0.7066\n",
      "Iteration 4200, Loss: 0.7047\n",
      "Iteration 4300, Loss: 0.7029\n",
      "Iteration 4400, Loss: 0.7012\n",
      "Iteration 4500, Loss: 0.6995\n",
      "Iteration 4600, Loss: 0.6979\n",
      "Iteration 4700, Loss: 0.6963\n",
      "Iteration 4800, Loss: 0.6948\n",
      "Iteration 4900, Loss: 0.6933\n",
      "178 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0535\n",
      "Iteration 200, Loss: 1.0170\n",
      "Iteration 300, Loss: 0.9861\n",
      "Iteration 400, Loss: 0.9596\n",
      "Iteration 500, Loss: 0.9366\n",
      "Iteration 600, Loss: 0.9164\n",
      "Iteration 700, Loss: 0.8985\n",
      "Iteration 800, Loss: 0.8824\n",
      "Iteration 900, Loss: 0.8679\n",
      "Iteration 1000, Loss: 0.8549\n",
      "Iteration 1100, Loss: 0.8430\n",
      "Iteration 1200, Loss: 0.8321\n",
      "Iteration 1300, Loss: 0.8222\n",
      "Iteration 1400, Loss: 0.8131\n",
      "Iteration 1500, Loss: 0.8047\n",
      "Iteration 1600, Loss: 0.7969\n",
      "Iteration 1700, Loss: 0.7898\n",
      "Iteration 1800, Loss: 0.7831\n",
      "Iteration 1900, Loss: 0.7769\n",
      "Iteration 2000, Loss: 0.7710\n",
      "Iteration 2100, Loss: 0.7656\n",
      "Iteration 2200, Loss: 0.7605\n",
      "Iteration 2300, Loss: 0.7557\n",
      "Iteration 2400, Loss: 0.7512\n",
      "Iteration 2500, Loss: 0.7469\n",
      "Iteration 2600, Loss: 0.7429\n",
      "Iteration 2700, Loss: 0.7391\n",
      "Iteration 2800, Loss: 0.7355\n",
      "Iteration 2900, Loss: 0.7321\n",
      "Iteration 3000, Loss: 0.7288\n",
      "Iteration 3100, Loss: 0.7257\n",
      "Iteration 3200, Loss: 0.7228\n",
      "Iteration 3300, Loss: 0.7200\n",
      "Iteration 3400, Loss: 0.7173\n",
      "Iteration 3500, Loss: 0.7147\n",
      "Iteration 3600, Loss: 0.7123\n",
      "Iteration 3700, Loss: 0.7099\n",
      "Iteration 3800, Loss: 0.7076\n",
      "Iteration 3900, Loss: 0.7054\n",
      "Iteration 4000, Loss: 0.7034\n",
      "Iteration 4100, Loss: 0.7013\n",
      "Iteration 4200, Loss: 0.6994\n",
      "Iteration 4300, Loss: 0.6975\n",
      "Iteration 4400, Loss: 0.6957\n",
      "Iteration 4500, Loss: 0.6939\n",
      "Iteration 4600, Loss: 0.6922\n",
      "Iteration 4700, Loss: 0.6905\n",
      "Iteration 4800, Loss: 0.6889\n",
      "Iteration 4900, Loss: 0.6873\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0532\n",
      "Iteration 200, Loss: 1.0174\n",
      "Iteration 300, Loss: 0.9881\n",
      "Iteration 400, Loss: 0.9633\n",
      "Iteration 500, Loss: 0.9419\n",
      "Iteration 600, Loss: 0.9232\n",
      "Iteration 700, Loss: 0.9069\n",
      "Iteration 800, Loss: 0.8923\n",
      "Iteration 900, Loss: 0.8792\n",
      "Iteration 1000, Loss: 0.8674\n",
      "Iteration 1100, Loss: 0.8568\n",
      "Iteration 1200, Loss: 0.8472\n",
      "Iteration 1300, Loss: 0.8383\n",
      "Iteration 1400, Loss: 0.8302\n",
      "Iteration 1500, Loss: 0.8227\n",
      "Iteration 1600, Loss: 0.8158\n",
      "Iteration 1700, Loss: 0.8094\n",
      "Iteration 1800, Loss: 0.8034\n",
      "Iteration 1900, Loss: 0.7978\n",
      "Iteration 2000, Loss: 0.7926\n",
      "Iteration 2100, Loss: 0.7878\n",
      "Iteration 2200, Loss: 0.7832\n",
      "Iteration 2300, Loss: 0.7789\n",
      "Iteration 2400, Loss: 0.7748\n",
      "Iteration 2500, Loss: 0.7709\n",
      "Iteration 2600, Loss: 0.7673\n",
      "Iteration 2700, Loss: 0.7638\n",
      "Iteration 2800, Loss: 0.7606\n",
      "Iteration 2900, Loss: 0.7574\n",
      "Iteration 3000, Loss: 0.7545\n",
      "Iteration 3100, Loss: 0.7517\n",
      "Iteration 3200, Loss: 0.7489\n",
      "Iteration 3300, Loss: 0.7464\n",
      "Iteration 3400, Loss: 0.7439\n",
      "Iteration 3500, Loss: 0.7415\n",
      "Iteration 3600, Loss: 0.7392\n",
      "Iteration 3700, Loss: 0.7370\n",
      "Iteration 3800, Loss: 0.7349\n",
      "Iteration 3900, Loss: 0.7329\n",
      "Iteration 4000, Loss: 0.7309\n",
      "Iteration 4100, Loss: 0.7290\n",
      "Iteration 4200, Loss: 0.7272\n",
      "Iteration 4300, Loss: 0.7255\n",
      "Iteration 4400, Loss: 0.7237\n",
      "Iteration 4500, Loss: 0.7221\n",
      "Iteration 4600, Loss: 0.7205\n",
      "Iteration 4700, Loss: 0.7189\n",
      "Iteration 4800, Loss: 0.7174\n",
      "Iteration 4900, Loss: 0.7160\n",
      "179 270\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0562\n",
      "Iteration 200, Loss: 1.0213\n",
      "Iteration 300, Loss: 0.9915\n",
      "Iteration 400, Loss: 0.9658\n",
      "Iteration 500, Loss: 0.9432\n",
      "Iteration 600, Loss: 0.9234\n",
      "Iteration 700, Loss: 0.9056\n",
      "Iteration 800, Loss: 0.8898\n",
      "Iteration 900, Loss: 0.8755\n",
      "Iteration 1000, Loss: 0.8625\n",
      "Iteration 1100, Loss: 0.8508\n",
      "Iteration 1200, Loss: 0.8401\n",
      "Iteration 1300, Loss: 0.8303\n",
      "Iteration 1400, Loss: 0.8213\n",
      "Iteration 1500, Loss: 0.8130\n",
      "Iteration 1600, Loss: 0.8053\n",
      "Iteration 1700, Loss: 0.7982\n",
      "Iteration 1800, Loss: 0.7915\n",
      "Iteration 1900, Loss: 0.7854\n",
      "Iteration 2000, Loss: 0.7796\n",
      "Iteration 2100, Loss: 0.7743\n",
      "Iteration 2200, Loss: 0.7693\n",
      "Iteration 2300, Loss: 0.7645\n",
      "Iteration 2400, Loss: 0.7601\n",
      "Iteration 2500, Loss: 0.7559\n",
      "Iteration 2600, Loss: 0.7520\n",
      "Iteration 2700, Loss: 0.7483\n",
      "Iteration 2800, Loss: 0.7447\n",
      "Iteration 2900, Loss: 0.7414\n",
      "Iteration 3000, Loss: 0.7382\n",
      "Iteration 3100, Loss: 0.7352\n",
      "Iteration 3200, Loss: 0.7323\n",
      "Iteration 3300, Loss: 0.7296\n",
      "Iteration 3400, Loss: 0.7270\n",
      "Iteration 3500, Loss: 0.7245\n",
      "Iteration 3600, Loss: 0.7221\n",
      "Iteration 3700, Loss: 0.7198\n",
      "Iteration 3800, Loss: 0.7176\n",
      "Iteration 3900, Loss: 0.7154\n",
      "Iteration 4000, Loss: 0.7134\n",
      "Iteration 4100, Loss: 0.7114\n",
      "Iteration 4200, Loss: 0.7096\n",
      "Iteration 4300, Loss: 0.7077\n",
      "Iteration 4400, Loss: 0.7060\n",
      "Iteration 4500, Loss: 0.7043\n",
      "Iteration 4600, Loss: 0.7026\n",
      "Iteration 4700, Loss: 0.7011\n",
      "Iteration 4800, Loss: 0.6995\n",
      "Iteration 4900, Loss: 0.6980\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0502\n",
      "Iteration 200, Loss: 1.0125\n",
      "Iteration 300, Loss: 0.9818\n",
      "Iteration 400, Loss: 0.9563\n",
      "Iteration 500, Loss: 0.9345\n",
      "Iteration 600, Loss: 0.9156\n",
      "Iteration 700, Loss: 0.8989\n",
      "Iteration 800, Loss: 0.8841\n",
      "Iteration 900, Loss: 0.8710\n",
      "Iteration 1000, Loss: 0.8592\n",
      "Iteration 1100, Loss: 0.8486\n",
      "Iteration 1200, Loss: 0.8388\n",
      "Iteration 1300, Loss: 0.8299\n",
      "Iteration 1400, Loss: 0.8216\n",
      "Iteration 1500, Loss: 0.8141\n",
      "Iteration 1600, Loss: 0.8072\n",
      "Iteration 1700, Loss: 0.8007\n",
      "Iteration 1800, Loss: 0.7946\n",
      "Iteration 1900, Loss: 0.7890\n",
      "Iteration 2000, Loss: 0.7837\n",
      "Iteration 2100, Loss: 0.7788\n",
      "Iteration 2200, Loss: 0.7741\n",
      "Iteration 2300, Loss: 0.7696\n",
      "Iteration 2400, Loss: 0.7655\n",
      "Iteration 2500, Loss: 0.7615\n",
      "Iteration 2600, Loss: 0.7577\n",
      "Iteration 2700, Loss: 0.7542\n",
      "Iteration 2800, Loss: 0.7508\n",
      "Iteration 2900, Loss: 0.7476\n",
      "Iteration 3000, Loss: 0.7445\n",
      "Iteration 3100, Loss: 0.7415\n",
      "Iteration 3200, Loss: 0.7387\n",
      "Iteration 3300, Loss: 0.7360\n",
      "Iteration 3400, Loss: 0.7334\n",
      "Iteration 3500, Loss: 0.7310\n",
      "Iteration 3600, Loss: 0.7286\n",
      "Iteration 3700, Loss: 0.7263\n",
      "Iteration 3800, Loss: 0.7241\n",
      "Iteration 3900, Loss: 0.7219\n",
      "Iteration 4000, Loss: 0.7199\n",
      "Iteration 4100, Loss: 0.7179\n",
      "Iteration 4200, Loss: 0.7159\n",
      "Iteration 4300, Loss: 0.7141\n",
      "Iteration 4400, Loss: 0.7123\n",
      "Iteration 4500, Loss: 0.7105\n",
      "Iteration 4600, Loss: 0.7089\n",
      "Iteration 4700, Loss: 0.7072\n",
      "Iteration 4800, Loss: 0.7056\n",
      "Iteration 4900, Loss: 0.7040\n",
      "180 270\n",
      "Iteration 0, Loss: 1.0257\n",
      "Iteration 100, Loss: 0.6404\n",
      "Iteration 200, Loss: 0.6119\n",
      "Iteration 300, Loss: 0.5966\n",
      "Iteration 400, Loss: 0.5861\n",
      "Iteration 500, Loss: 0.5781\n",
      "Iteration 600, Loss: 0.5717\n",
      "Iteration 700, Loss: 0.5664\n",
      "Iteration 800, Loss: 0.5619\n",
      "Iteration 900, Loss: 0.5580\n",
      "Iteration 1000, Loss: 0.5545\n",
      "Iteration 1100, Loss: 0.5514\n",
      "Iteration 1200, Loss: 0.5485\n",
      "Iteration 1300, Loss: 0.5459\n",
      "Iteration 1400, Loss: 0.5435\n",
      "Iteration 1500, Loss: 0.5413\n",
      "Iteration 1600, Loss: 0.5392\n",
      "Iteration 1700, Loss: 0.5373\n",
      "Iteration 1800, Loss: 0.5354\n",
      "Iteration 1900, Loss: 0.5337\n",
      "Iteration 2000, Loss: 0.5321\n",
      "Iteration 2100, Loss: 0.5305\n",
      "Iteration 2200, Loss: 0.5291\n",
      "Iteration 2300, Loss: 0.5277\n",
      "Iteration 2400, Loss: 0.5264\n",
      "Iteration 2500, Loss: 0.5251\n",
      "Iteration 2600, Loss: 0.5239\n",
      "Iteration 2700, Loss: 0.5228\n",
      "Iteration 2800, Loss: 0.5217\n",
      "Iteration 2900, Loss: 0.5206\n",
      "Iteration 3000, Loss: 0.5196\n",
      "Iteration 3100, Loss: 0.5186\n",
      "Iteration 3200, Loss: 0.5177\n",
      "Iteration 3300, Loss: 0.5168\n",
      "Iteration 3400, Loss: 0.5159\n",
      "Iteration 3500, Loss: 0.5151\n",
      "Iteration 3600, Loss: 0.5143\n",
      "Iteration 3700, Loss: 0.5135\n",
      "Iteration 3800, Loss: 0.5128\n",
      "Iteration 3900, Loss: 0.5121\n",
      "Iteration 4000, Loss: 0.5114\n",
      "Iteration 4100, Loss: 0.5107\n",
      "Iteration 4200, Loss: 0.5100\n",
      "Iteration 4300, Loss: 0.5094\n",
      "Iteration 4400, Loss: 0.5088\n",
      "Iteration 4500, Loss: 0.5082\n",
      "Iteration 4600, Loss: 0.5076\n",
      "Iteration 4700, Loss: 0.5071\n",
      "Iteration 4800, Loss: 0.5065\n",
      "Iteration 4900, Loss: 0.5060\n",
      "Iteration 5000, Loss: 0.5055\n",
      "Iteration 5100, Loss: 0.5050\n",
      "Iteration 5200, Loss: 0.5045\n",
      "Iteration 5300, Loss: 0.5040\n",
      "Iteration 5400, Loss: 0.5036\n",
      "Iteration 5500, Loss: 0.5031\n",
      "Iteration 5600, Loss: 0.5027\n",
      "Iteration 5700, Loss: 0.5023\n",
      "Iteration 5800, Loss: 0.5018\n",
      "Iteration 5900, Loss: 0.5014\n",
      "Iteration 0, Loss: 1.0152\n",
      "Iteration 100, Loss: 0.6073\n",
      "Iteration 200, Loss: 0.5757\n",
      "Iteration 300, Loss: 0.5580\n",
      "Iteration 400, Loss: 0.5459\n",
      "Iteration 500, Loss: 0.5370\n",
      "Iteration 600, Loss: 0.5299\n",
      "Iteration 700, Loss: 0.5241\n",
      "Iteration 800, Loss: 0.5193\n",
      "Iteration 900, Loss: 0.5151\n",
      "Iteration 1000, Loss: 0.5114\n",
      "Iteration 1100, Loss: 0.5082\n",
      "Iteration 1200, Loss: 0.5054\n",
      "Iteration 1300, Loss: 0.5028\n",
      "Iteration 1400, Loss: 0.5004\n",
      "Iteration 1500, Loss: 0.4982\n",
      "Iteration 1600, Loss: 0.4962\n",
      "Iteration 1700, Loss: 0.4944\n",
      "Iteration 1800, Loss: 0.4927\n",
      "Iteration 1900, Loss: 0.4911\n",
      "Iteration 2000, Loss: 0.4896\n",
      "Iteration 2100, Loss: 0.4882\n",
      "Iteration 2200, Loss: 0.4868\n",
      "Iteration 2300, Loss: 0.4856\n",
      "Iteration 2400, Loss: 0.4844\n",
      "Iteration 2500, Loss: 0.4832\n",
      "Iteration 2600, Loss: 0.4822\n",
      "Iteration 2700, Loss: 0.4811\n",
      "Iteration 2800, Loss: 0.4801\n",
      "Iteration 2900, Loss: 0.4792\n",
      "Iteration 3000, Loss: 0.4783\n",
      "Iteration 3100, Loss: 0.4775\n",
      "Iteration 3200, Loss: 0.4766\n",
      "Iteration 3300, Loss: 0.4758\n",
      "Iteration 3400, Loss: 0.4751\n",
      "Iteration 3500, Loss: 0.4743\n",
      "Iteration 3600, Loss: 0.4736\n",
      "Iteration 3700, Loss: 0.4729\n",
      "Iteration 3800, Loss: 0.4723\n",
      "Iteration 3900, Loss: 0.4716\n",
      "Iteration 4000, Loss: 0.4710\n",
      "Iteration 4100, Loss: 0.4704\n",
      "Iteration 4200, Loss: 0.4698\n",
      "Iteration 4300, Loss: 0.4693\n",
      "Iteration 4400, Loss: 0.4687\n",
      "Iteration 4500, Loss: 0.4682\n",
      "Iteration 4600, Loss: 0.4677\n",
      "Iteration 4700, Loss: 0.4672\n",
      "Iteration 4800, Loss: 0.4667\n",
      "Iteration 4900, Loss: 0.4662\n",
      "Iteration 5000, Loss: 0.4657\n",
      "Iteration 5100, Loss: 0.4653\n",
      "Iteration 5200, Loss: 0.4649\n",
      "Iteration 5300, Loss: 0.4644\n",
      "Iteration 5400, Loss: 0.4640\n",
      "Iteration 5500, Loss: 0.4636\n",
      "Iteration 5600, Loss: 0.4632\n",
      "Iteration 5700, Loss: 0.4628\n",
      "Iteration 5800, Loss: 0.4624\n",
      "Iteration 5900, Loss: 0.4621\n",
      "181 270\n",
      "Iteration 0, Loss: 1.0131\n",
      "Iteration 100, Loss: 0.6132\n",
      "Iteration 200, Loss: 0.5836\n",
      "Iteration 300, Loss: 0.5675\n",
      "Iteration 400, Loss: 0.5564\n",
      "Iteration 500, Loss: 0.5480\n",
      "Iteration 600, Loss: 0.5414\n",
      "Iteration 700, Loss: 0.5359\n",
      "Iteration 800, Loss: 0.5311\n",
      "Iteration 900, Loss: 0.5271\n",
      "Iteration 1000, Loss: 0.5236\n",
      "Iteration 1100, Loss: 0.5204\n",
      "Iteration 1200, Loss: 0.5176\n",
      "Iteration 1300, Loss: 0.5150\n",
      "Iteration 1400, Loss: 0.5127\n",
      "Iteration 1500, Loss: 0.5105\n",
      "Iteration 1600, Loss: 0.5086\n",
      "Iteration 1700, Loss: 0.5067\n",
      "Iteration 1800, Loss: 0.5050\n",
      "Iteration 1900, Loss: 0.5034\n",
      "Iteration 2000, Loss: 0.5019\n",
      "Iteration 2100, Loss: 0.5005\n",
      "Iteration 2200, Loss: 0.4992\n",
      "Iteration 2300, Loss: 0.4979\n",
      "Iteration 2400, Loss: 0.4967\n",
      "Iteration 2500, Loss: 0.4956\n",
      "Iteration 2600, Loss: 0.4945\n",
      "Iteration 2700, Loss: 0.4935\n",
      "Iteration 2800, Loss: 0.4925\n",
      "Iteration 2900, Loss: 0.4916\n",
      "Iteration 3000, Loss: 0.4907\n",
      "Iteration 3100, Loss: 0.4898\n",
      "Iteration 3200, Loss: 0.4890\n",
      "Iteration 3300, Loss: 0.4883\n",
      "Iteration 3400, Loss: 0.4875\n",
      "Iteration 3500, Loss: 0.4868\n",
      "Iteration 3600, Loss: 0.4861\n",
      "Iteration 3700, Loss: 0.4854\n",
      "Iteration 3800, Loss: 0.4848\n",
      "Iteration 3900, Loss: 0.4841\n",
      "Iteration 4000, Loss: 0.4835\n",
      "Iteration 4100, Loss: 0.4830\n",
      "Iteration 4200, Loss: 0.4824\n",
      "Iteration 4300, Loss: 0.4819\n",
      "Iteration 4400, Loss: 0.4813\n",
      "Iteration 4500, Loss: 0.4808\n",
      "Iteration 4600, Loss: 0.4803\n",
      "Iteration 4700, Loss: 0.4798\n",
      "Iteration 4800, Loss: 0.4794\n",
      "Iteration 4900, Loss: 0.4789\n",
      "Iteration 5000, Loss: 0.4784\n",
      "Iteration 5100, Loss: 0.4780\n",
      "Iteration 5200, Loss: 0.4776\n",
      "Iteration 5300, Loss: 0.4772\n",
      "Iteration 5400, Loss: 0.4768\n",
      "Iteration 5500, Loss: 0.4764\n",
      "Iteration 5600, Loss: 0.4760\n",
      "Iteration 5700, Loss: 0.4756\n",
      "Iteration 5800, Loss: 0.4753\n",
      "Iteration 5900, Loss: 0.4749\n",
      "Iteration 0, Loss: 1.0255\n",
      "Iteration 100, Loss: 0.6348\n",
      "Iteration 200, Loss: 0.6050\n",
      "Iteration 300, Loss: 0.5885\n",
      "Iteration 400, Loss: 0.5771\n",
      "Iteration 500, Loss: 0.5685\n",
      "Iteration 600, Loss: 0.5616\n",
      "Iteration 700, Loss: 0.5558\n",
      "Iteration 800, Loss: 0.5509\n",
      "Iteration 900, Loss: 0.5466\n",
      "Iteration 1000, Loss: 0.5429\n",
      "Iteration 1100, Loss: 0.5395\n",
      "Iteration 1200, Loss: 0.5365\n",
      "Iteration 1300, Loss: 0.5337\n",
      "Iteration 1400, Loss: 0.5312\n",
      "Iteration 1500, Loss: 0.5288\n",
      "Iteration 1600, Loss: 0.5267\n",
      "Iteration 1700, Loss: 0.5246\n",
      "Iteration 1800, Loss: 0.5228\n",
      "Iteration 1900, Loss: 0.5210\n",
      "Iteration 2000, Loss: 0.5193\n",
      "Iteration 2100, Loss: 0.5178\n",
      "Iteration 2200, Loss: 0.5163\n",
      "Iteration 2300, Loss: 0.5149\n",
      "Iteration 2400, Loss: 0.5136\n",
      "Iteration 2500, Loss: 0.5123\n",
      "Iteration 2600, Loss: 0.5111\n",
      "Iteration 2700, Loss: 0.5099\n",
      "Iteration 2800, Loss: 0.5088\n",
      "Iteration 2900, Loss: 0.5078\n",
      "Iteration 3000, Loss: 0.5067\n",
      "Iteration 3100, Loss: 0.5058\n",
      "Iteration 3200, Loss: 0.5048\n",
      "Iteration 3300, Loss: 0.5039\n",
      "Iteration 3400, Loss: 0.5030\n",
      "Iteration 3500, Loss: 0.5022\n",
      "Iteration 3600, Loss: 0.5014\n",
      "Iteration 3700, Loss: 0.5007\n",
      "Iteration 3800, Loss: 0.4999\n",
      "Iteration 3900, Loss: 0.4992\n",
      "Iteration 4000, Loss: 0.4985\n",
      "Iteration 4100, Loss: 0.4978\n",
      "Iteration 4200, Loss: 0.4971\n",
      "Iteration 4300, Loss: 0.4965\n",
      "Iteration 4400, Loss: 0.4959\n",
      "Iteration 4500, Loss: 0.4953\n",
      "Iteration 4600, Loss: 0.4947\n",
      "Iteration 4700, Loss: 0.4941\n",
      "Iteration 4800, Loss: 0.4936\n",
      "Iteration 4900, Loss: 0.4931\n",
      "Iteration 5000, Loss: 0.4926\n",
      "Iteration 5100, Loss: 0.4921\n",
      "Iteration 5200, Loss: 0.4916\n",
      "Iteration 5300, Loss: 0.4911\n",
      "Iteration 5400, Loss: 0.4906\n",
      "Iteration 5500, Loss: 0.4902\n",
      "Iteration 5600, Loss: 0.4897\n",
      "Iteration 5700, Loss: 0.4893\n",
      "Iteration 5800, Loss: 0.4889\n",
      "Iteration 5900, Loss: 0.4885\n",
      "182 270\n",
      "Iteration 0, Loss: 1.0252\n",
      "Iteration 100, Loss: 0.6389\n",
      "Iteration 200, Loss: 0.6083\n",
      "Iteration 300, Loss: 0.5917\n",
      "Iteration 400, Loss: 0.5804\n",
      "Iteration 500, Loss: 0.5720\n",
      "Iteration 600, Loss: 0.5651\n",
      "Iteration 700, Loss: 0.5596\n",
      "Iteration 800, Loss: 0.5548\n",
      "Iteration 900, Loss: 0.5507\n",
      "Iteration 1000, Loss: 0.5471\n",
      "Iteration 1100, Loss: 0.5439\n",
      "Iteration 1200, Loss: 0.5409\n",
      "Iteration 1300, Loss: 0.5383\n",
      "Iteration 1400, Loss: 0.5358\n",
      "Iteration 1500, Loss: 0.5336\n",
      "Iteration 1600, Loss: 0.5315\n",
      "Iteration 1700, Loss: 0.5296\n",
      "Iteration 1800, Loss: 0.5277\n",
      "Iteration 1900, Loss: 0.5260\n",
      "Iteration 2000, Loss: 0.5244\n",
      "Iteration 2100, Loss: 0.5229\n",
      "Iteration 2200, Loss: 0.5214\n",
      "Iteration 2300, Loss: 0.5201\n",
      "Iteration 2400, Loss: 0.5187\n",
      "Iteration 2500, Loss: 0.5175\n",
      "Iteration 2600, Loss: 0.5163\n",
      "Iteration 2700, Loss: 0.5152\n",
      "Iteration 2800, Loss: 0.5141\n",
      "Iteration 2900, Loss: 0.5131\n",
      "Iteration 3000, Loss: 0.5121\n",
      "Iteration 3100, Loss: 0.5111\n",
      "Iteration 3200, Loss: 0.5102\n",
      "Iteration 3300, Loss: 0.5093\n",
      "Iteration 3400, Loss: 0.5085\n",
      "Iteration 3500, Loss: 0.5077\n",
      "Iteration 3600, Loss: 0.5069\n",
      "Iteration 3700, Loss: 0.5061\n",
      "Iteration 3800, Loss: 0.5054\n",
      "Iteration 3900, Loss: 0.5047\n",
      "Iteration 4000, Loss: 0.5040\n",
      "Iteration 4100, Loss: 0.5034\n",
      "Iteration 4200, Loss: 0.5027\n",
      "Iteration 4300, Loss: 0.5021\n",
      "Iteration 4400, Loss: 0.5015\n",
      "Iteration 4500, Loss: 0.5009\n",
      "Iteration 4600, Loss: 0.5003\n",
      "Iteration 4700, Loss: 0.4998\n",
      "Iteration 4800, Loss: 0.4992\n",
      "Iteration 4900, Loss: 0.4987\n",
      "Iteration 5000, Loss: 0.4982\n",
      "Iteration 5100, Loss: 0.4977\n",
      "Iteration 5200, Loss: 0.4972\n",
      "Iteration 5300, Loss: 0.4967\n",
      "Iteration 5400, Loss: 0.4963\n",
      "Iteration 5500, Loss: 0.4958\n",
      "Iteration 5600, Loss: 0.4954\n",
      "Iteration 5700, Loss: 0.4950\n",
      "Iteration 5800, Loss: 0.4945\n",
      "Iteration 5900, Loss: 0.4941\n",
      "Iteration 0, Loss: 1.0158\n",
      "Iteration 100, Loss: 0.6118\n",
      "Iteration 200, Loss: 0.5820\n",
      "Iteration 300, Loss: 0.5659\n",
      "Iteration 400, Loss: 0.5549\n",
      "Iteration 500, Loss: 0.5467\n",
      "Iteration 600, Loss: 0.5403\n",
      "Iteration 700, Loss: 0.5349\n",
      "Iteration 800, Loss: 0.5304\n",
      "Iteration 900, Loss: 0.5265\n",
      "Iteration 1000, Loss: 0.5230\n",
      "Iteration 1100, Loss: 0.5200\n",
      "Iteration 1200, Loss: 0.5172\n",
      "Iteration 1300, Loss: 0.5147\n",
      "Iteration 1400, Loss: 0.5125\n",
      "Iteration 1500, Loss: 0.5104\n",
      "Iteration 1600, Loss: 0.5084\n",
      "Iteration 1700, Loss: 0.5066\n",
      "Iteration 1800, Loss: 0.5049\n",
      "Iteration 1900, Loss: 0.5033\n",
      "Iteration 2000, Loss: 0.5018\n",
      "Iteration 2100, Loss: 0.5004\n",
      "Iteration 2200, Loss: 0.4991\n",
      "Iteration 2300, Loss: 0.4978\n",
      "Iteration 2400, Loss: 0.4966\n",
      "Iteration 2500, Loss: 0.4954\n",
      "Iteration 2600, Loss: 0.4943\n",
      "Iteration 2700, Loss: 0.4933\n",
      "Iteration 2800, Loss: 0.4923\n",
      "Iteration 2900, Loss: 0.4913\n",
      "Iteration 3000, Loss: 0.4904\n",
      "Iteration 3100, Loss: 0.4895\n",
      "Iteration 3200, Loss: 0.4887\n",
      "Iteration 3300, Loss: 0.4878\n",
      "Iteration 3400, Loss: 0.4870\n",
      "Iteration 3500, Loss: 0.4863\n",
      "Iteration 3600, Loss: 0.4856\n",
      "Iteration 3700, Loss: 0.4848\n",
      "Iteration 3800, Loss: 0.4841\n",
      "Iteration 3900, Loss: 0.4835\n",
      "Iteration 4000, Loss: 0.4828\n",
      "Iteration 4100, Loss: 0.4822\n",
      "Iteration 4200, Loss: 0.4816\n",
      "Iteration 4300, Loss: 0.4810\n",
      "Iteration 4400, Loss: 0.4804\n",
      "Iteration 4500, Loss: 0.4799\n",
      "Iteration 4600, Loss: 0.4793\n",
      "Iteration 4700, Loss: 0.4788\n",
      "Iteration 4800, Loss: 0.4783\n",
      "Iteration 4900, Loss: 0.4778\n",
      "Iteration 5000, Loss: 0.4773\n",
      "Iteration 5100, Loss: 0.4769\n",
      "Iteration 5200, Loss: 0.4764\n",
      "Iteration 5300, Loss: 0.4760\n",
      "Iteration 5400, Loss: 0.4756\n",
      "Iteration 5500, Loss: 0.4751\n",
      "Iteration 5600, Loss: 0.4747\n",
      "Iteration 5700, Loss: 0.4743\n",
      "Iteration 5800, Loss: 0.4739\n",
      "Iteration 5900, Loss: 0.4736\n",
      "183 270\n",
      "Iteration 0, Loss: 1.0229\n",
      "Iteration 100, Loss: 0.6405\n",
      "Iteration 200, Loss: 0.6122\n",
      "Iteration 300, Loss: 0.5969\n",
      "Iteration 400, Loss: 0.5866\n",
      "Iteration 500, Loss: 0.5790\n",
      "Iteration 600, Loss: 0.5729\n",
      "Iteration 700, Loss: 0.5679\n",
      "Iteration 800, Loss: 0.5637\n",
      "Iteration 900, Loss: 0.5601\n",
      "Iteration 1000, Loss: 0.5569\n",
      "Iteration 1100, Loss: 0.5541\n",
      "Iteration 1200, Loss: 0.5515\n",
      "Iteration 1300, Loss: 0.5492\n",
      "Iteration 1400, Loss: 0.5471\n",
      "Iteration 1500, Loss: 0.5451\n",
      "Iteration 1600, Loss: 0.5433\n",
      "Iteration 1700, Loss: 0.5416\n",
      "Iteration 1800, Loss: 0.5400\n",
      "Iteration 1900, Loss: 0.5385\n",
      "Iteration 2000, Loss: 0.5371\n",
      "Iteration 2100, Loss: 0.5358\n",
      "Iteration 2200, Loss: 0.5346\n",
      "Iteration 2300, Loss: 0.5334\n",
      "Iteration 2400, Loss: 0.5322\n",
      "Iteration 2500, Loss: 0.5312\n",
      "Iteration 2600, Loss: 0.5301\n",
      "Iteration 2700, Loss: 0.5291\n",
      "Iteration 2800, Loss: 0.5281\n",
      "Iteration 2900, Loss: 0.5272\n",
      "Iteration 3000, Loss: 0.5264\n",
      "Iteration 3100, Loss: 0.5255\n",
      "Iteration 3200, Loss: 0.5247\n",
      "Iteration 3300, Loss: 0.5239\n",
      "Iteration 3400, Loss: 0.5232\n",
      "Iteration 3500, Loss: 0.5225\n",
      "Iteration 3600, Loss: 0.5218\n",
      "Iteration 3700, Loss: 0.5211\n",
      "Iteration 3800, Loss: 0.5205\n",
      "Iteration 3900, Loss: 0.5199\n",
      "Iteration 4000, Loss: 0.5192\n",
      "Iteration 4100, Loss: 0.5187\n",
      "Iteration 4200, Loss: 0.5181\n",
      "Iteration 4300, Loss: 0.5175\n",
      "Iteration 4400, Loss: 0.5170\n",
      "Iteration 4500, Loss: 0.5164\n",
      "Iteration 4600, Loss: 0.5159\n",
      "Iteration 4700, Loss: 0.5154\n",
      "Iteration 4800, Loss: 0.5150\n",
      "Iteration 4900, Loss: 0.5145\n",
      "Iteration 5000, Loss: 0.5140\n",
      "Iteration 5100, Loss: 0.5136\n",
      "Iteration 5200, Loss: 0.5132\n",
      "Iteration 5300, Loss: 0.5127\n",
      "Iteration 5400, Loss: 0.5123\n",
      "Iteration 5500, Loss: 0.5119\n",
      "Iteration 5600, Loss: 0.5115\n",
      "Iteration 5700, Loss: 0.5111\n",
      "Iteration 5800, Loss: 0.5108\n",
      "Iteration 5900, Loss: 0.5104\n",
      "Iteration 0, Loss: 1.0175\n",
      "Iteration 100, Loss: 0.6108\n",
      "Iteration 200, Loss: 0.5795\n",
      "Iteration 300, Loss: 0.5627\n",
      "Iteration 400, Loss: 0.5514\n",
      "Iteration 500, Loss: 0.5430\n",
      "Iteration 600, Loss: 0.5364\n",
      "Iteration 700, Loss: 0.5309\n",
      "Iteration 800, Loss: 0.5263\n",
      "Iteration 900, Loss: 0.5222\n",
      "Iteration 1000, Loss: 0.5187\n",
      "Iteration 1100, Loss: 0.5156\n",
      "Iteration 1200, Loss: 0.5127\n",
      "Iteration 1300, Loss: 0.5102\n",
      "Iteration 1400, Loss: 0.5078\n",
      "Iteration 1500, Loss: 0.5057\n",
      "Iteration 1600, Loss: 0.5037\n",
      "Iteration 1700, Loss: 0.5018\n",
      "Iteration 1800, Loss: 0.5000\n",
      "Iteration 1900, Loss: 0.4984\n",
      "Iteration 2000, Loss: 0.4968\n",
      "Iteration 2100, Loss: 0.4954\n",
      "Iteration 2200, Loss: 0.4940\n",
      "Iteration 2300, Loss: 0.4927\n",
      "Iteration 2400, Loss: 0.4915\n",
      "Iteration 2500, Loss: 0.4903\n",
      "Iteration 2600, Loss: 0.4892\n",
      "Iteration 2700, Loss: 0.4881\n",
      "Iteration 2800, Loss: 0.4871\n",
      "Iteration 2900, Loss: 0.4861\n",
      "Iteration 3000, Loss: 0.4851\n",
      "Iteration 3100, Loss: 0.4842\n",
      "Iteration 3200, Loss: 0.4834\n",
      "Iteration 3300, Loss: 0.4825\n",
      "Iteration 3400, Loss: 0.4817\n",
      "Iteration 3500, Loss: 0.4810\n",
      "Iteration 3600, Loss: 0.4802\n",
      "Iteration 3700, Loss: 0.4795\n",
      "Iteration 3800, Loss: 0.4789\n",
      "Iteration 3900, Loss: 0.4782\n",
      "Iteration 4000, Loss: 0.4775\n",
      "Iteration 4100, Loss: 0.4769\n",
      "Iteration 4200, Loss: 0.4763\n",
      "Iteration 4300, Loss: 0.4757\n",
      "Iteration 4400, Loss: 0.4752\n",
      "Iteration 4500, Loss: 0.4746\n",
      "Iteration 4600, Loss: 0.4741\n",
      "Iteration 4700, Loss: 0.4736\n",
      "Iteration 4800, Loss: 0.4731\n",
      "Iteration 4900, Loss: 0.4726\n",
      "Iteration 5000, Loss: 0.4721\n",
      "Iteration 5100, Loss: 0.4717\n",
      "Iteration 5200, Loss: 0.4712\n",
      "Iteration 5300, Loss: 0.4708\n",
      "Iteration 5400, Loss: 0.4704\n",
      "Iteration 5500, Loss: 0.4700\n",
      "Iteration 5600, Loss: 0.4696\n",
      "Iteration 5700, Loss: 0.4692\n",
      "Iteration 5800, Loss: 0.4688\n",
      "Iteration 5900, Loss: 0.4684\n",
      "184 270\n",
      "Iteration 0, Loss: 1.0185\n",
      "Iteration 100, Loss: 0.6219\n",
      "Iteration 200, Loss: 0.5942\n",
      "Iteration 300, Loss: 0.5786\n",
      "Iteration 400, Loss: 0.5680\n",
      "Iteration 500, Loss: 0.5599\n",
      "Iteration 600, Loss: 0.5535\n",
      "Iteration 700, Loss: 0.5481\n",
      "Iteration 800, Loss: 0.5436\n",
      "Iteration 900, Loss: 0.5397\n",
      "Iteration 1000, Loss: 0.5363\n",
      "Iteration 1100, Loss: 0.5333\n",
      "Iteration 1200, Loss: 0.5306\n",
      "Iteration 1300, Loss: 0.5281\n",
      "Iteration 1400, Loss: 0.5258\n",
      "Iteration 1500, Loss: 0.5237\n",
      "Iteration 1600, Loss: 0.5218\n",
      "Iteration 1700, Loss: 0.5199\n",
      "Iteration 1800, Loss: 0.5182\n",
      "Iteration 1900, Loss: 0.5167\n",
      "Iteration 2000, Loss: 0.5152\n",
      "Iteration 2100, Loss: 0.5138\n",
      "Iteration 2200, Loss: 0.5124\n",
      "Iteration 2300, Loss: 0.5112\n",
      "Iteration 2400, Loss: 0.5100\n",
      "Iteration 2500, Loss: 0.5089\n",
      "Iteration 2600, Loss: 0.5078\n",
      "Iteration 2700, Loss: 0.5067\n",
      "Iteration 2800, Loss: 0.5057\n",
      "Iteration 2900, Loss: 0.5048\n",
      "Iteration 3000, Loss: 0.5038\n",
      "Iteration 3100, Loss: 0.5030\n",
      "Iteration 3200, Loss: 0.5021\n",
      "Iteration 3300, Loss: 0.5013\n",
      "Iteration 3400, Loss: 0.5005\n",
      "Iteration 3500, Loss: 0.4998\n",
      "Iteration 3600, Loss: 0.4990\n",
      "Iteration 3700, Loss: 0.4983\n",
      "Iteration 3800, Loss: 0.4976\n",
      "Iteration 3900, Loss: 0.4970\n",
      "Iteration 4000, Loss: 0.4963\n",
      "Iteration 4100, Loss: 0.4957\n",
      "Iteration 4200, Loss: 0.4951\n",
      "Iteration 4300, Loss: 0.4945\n",
      "Iteration 4400, Loss: 0.4939\n",
      "Iteration 4500, Loss: 0.4934\n",
      "Iteration 4600, Loss: 0.4929\n",
      "Iteration 4700, Loss: 0.4923\n",
      "Iteration 4800, Loss: 0.4918\n",
      "Iteration 4900, Loss: 0.4914\n",
      "Iteration 5000, Loss: 0.4909\n",
      "Iteration 5100, Loss: 0.4904\n",
      "Iteration 5200, Loss: 0.4900\n",
      "Iteration 5300, Loss: 0.4895\n",
      "Iteration 5400, Loss: 0.4891\n",
      "Iteration 5500, Loss: 0.4887\n",
      "Iteration 5600, Loss: 0.4883\n",
      "Iteration 5700, Loss: 0.4879\n",
      "Iteration 5800, Loss: 0.4875\n",
      "Iteration 5900, Loss: 0.4871\n",
      "Iteration 0, Loss: 1.0238\n",
      "Iteration 100, Loss: 0.6283\n",
      "Iteration 200, Loss: 0.5963\n",
      "Iteration 300, Loss: 0.5795\n",
      "Iteration 400, Loss: 0.5684\n",
      "Iteration 500, Loss: 0.5603\n",
      "Iteration 600, Loss: 0.5540\n",
      "Iteration 700, Loss: 0.5488\n",
      "Iteration 800, Loss: 0.5446\n",
      "Iteration 900, Loss: 0.5410\n",
      "Iteration 1000, Loss: 0.5378\n",
      "Iteration 1100, Loss: 0.5350\n",
      "Iteration 1200, Loss: 0.5325\n",
      "Iteration 1300, Loss: 0.5303\n",
      "Iteration 1400, Loss: 0.5282\n",
      "Iteration 1500, Loss: 0.5263\n",
      "Iteration 1600, Loss: 0.5245\n",
      "Iteration 1700, Loss: 0.5229\n",
      "Iteration 1800, Loss: 0.5214\n",
      "Iteration 1900, Loss: 0.5200\n",
      "Iteration 2000, Loss: 0.5187\n",
      "Iteration 2100, Loss: 0.5174\n",
      "Iteration 2200, Loss: 0.5163\n",
      "Iteration 2300, Loss: 0.5151\n",
      "Iteration 2400, Loss: 0.5141\n",
      "Iteration 2500, Loss: 0.5131\n",
      "Iteration 2600, Loss: 0.5121\n",
      "Iteration 2700, Loss: 0.5112\n",
      "Iteration 2800, Loss: 0.5103\n",
      "Iteration 2900, Loss: 0.5094\n",
      "Iteration 3000, Loss: 0.5086\n",
      "Iteration 3100, Loss: 0.5078\n",
      "Iteration 3200, Loss: 0.5070\n",
      "Iteration 3300, Loss: 0.5063\n",
      "Iteration 3400, Loss: 0.5056\n",
      "Iteration 3500, Loss: 0.5049\n",
      "Iteration 3600, Loss: 0.5043\n",
      "Iteration 3700, Loss: 0.5036\n",
      "Iteration 3800, Loss: 0.5030\n",
      "Iteration 3900, Loss: 0.5025\n",
      "Iteration 4000, Loss: 0.5019\n",
      "Iteration 4100, Loss: 0.5013\n",
      "Iteration 4200, Loss: 0.5008\n",
      "Iteration 4300, Loss: 0.5002\n",
      "Iteration 4400, Loss: 0.4997\n",
      "Iteration 4500, Loss: 0.4992\n",
      "Iteration 4600, Loss: 0.4988\n",
      "Iteration 4700, Loss: 0.4983\n",
      "Iteration 4800, Loss: 0.4978\n",
      "Iteration 4900, Loss: 0.4974\n",
      "Iteration 5000, Loss: 0.4970\n",
      "Iteration 5100, Loss: 0.4965\n",
      "Iteration 5200, Loss: 0.4961\n",
      "Iteration 5300, Loss: 0.4957\n",
      "Iteration 5400, Loss: 0.4953\n",
      "Iteration 5500, Loss: 0.4950\n",
      "Iteration 5600, Loss: 0.4946\n",
      "Iteration 5700, Loss: 0.4942\n",
      "Iteration 5800, Loss: 0.4939\n",
      "Iteration 5900, Loss: 0.4935\n",
      "185 270\n",
      "Iteration 0, Loss: 1.0240\n",
      "Iteration 100, Loss: 0.6370\n",
      "Iteration 200, Loss: 0.6061\n",
      "Iteration 300, Loss: 0.5889\n",
      "Iteration 400, Loss: 0.5771\n",
      "Iteration 500, Loss: 0.5682\n",
      "Iteration 600, Loss: 0.5610\n",
      "Iteration 700, Loss: 0.5550\n",
      "Iteration 800, Loss: 0.5499\n",
      "Iteration 900, Loss: 0.5454\n",
      "Iteration 1000, Loss: 0.5415\n",
      "Iteration 1100, Loss: 0.5380\n",
      "Iteration 1200, Loss: 0.5348\n",
      "Iteration 1300, Loss: 0.5318\n",
      "Iteration 1400, Loss: 0.5291\n",
      "Iteration 1500, Loss: 0.5266\n",
      "Iteration 1600, Loss: 0.5243\n",
      "Iteration 1700, Loss: 0.5221\n",
      "Iteration 1800, Loss: 0.5201\n",
      "Iteration 1900, Loss: 0.5181\n",
      "Iteration 2000, Loss: 0.5163\n",
      "Iteration 2100, Loss: 0.5145\n",
      "Iteration 2200, Loss: 0.5129\n",
      "Iteration 2300, Loss: 0.5113\n",
      "Iteration 2400, Loss: 0.5098\n",
      "Iteration 2500, Loss: 0.5083\n",
      "Iteration 2600, Loss: 0.5069\n",
      "Iteration 2700, Loss: 0.5056\n",
      "Iteration 2800, Loss: 0.5043\n",
      "Iteration 2900, Loss: 0.5031\n",
      "Iteration 3000, Loss: 0.5019\n",
      "Iteration 3100, Loss: 0.5007\n",
      "Iteration 3200, Loss: 0.4996\n",
      "Iteration 3300, Loss: 0.4985\n",
      "Iteration 3400, Loss: 0.4975\n",
      "Iteration 3500, Loss: 0.4964\n",
      "Iteration 3600, Loss: 0.4955\n",
      "Iteration 3700, Loss: 0.4945\n",
      "Iteration 3800, Loss: 0.4935\n",
      "Iteration 3900, Loss: 0.4926\n",
      "Iteration 4000, Loss: 0.4917\n",
      "Iteration 4100, Loss: 0.4909\n",
      "Iteration 4200, Loss: 0.4900\n",
      "Iteration 4300, Loss: 0.4892\n",
      "Iteration 4400, Loss: 0.4884\n",
      "Iteration 4500, Loss: 0.4876\n",
      "Iteration 4600, Loss: 0.4869\n",
      "Iteration 4700, Loss: 0.4861\n",
      "Iteration 4800, Loss: 0.4854\n",
      "Iteration 4900, Loss: 0.4847\n",
      "Iteration 5000, Loss: 0.4840\n",
      "Iteration 5100, Loss: 0.4833\n",
      "Iteration 5200, Loss: 0.4826\n",
      "Iteration 5300, Loss: 0.4820\n",
      "Iteration 5400, Loss: 0.4813\n",
      "Iteration 5500, Loss: 0.4807\n",
      "Iteration 5600, Loss: 0.4801\n",
      "Iteration 5700, Loss: 0.4795\n",
      "Iteration 5800, Loss: 0.4788\n",
      "Iteration 5900, Loss: 0.4783\n",
      "Iteration 0, Loss: 1.0152\n",
      "Iteration 100, Loss: 0.6078\n",
      "Iteration 200, Loss: 0.5766\n",
      "Iteration 300, Loss: 0.5590\n",
      "Iteration 400, Loss: 0.5468\n",
      "Iteration 500, Loss: 0.5374\n",
      "Iteration 600, Loss: 0.5299\n",
      "Iteration 700, Loss: 0.5237\n",
      "Iteration 800, Loss: 0.5183\n",
      "Iteration 900, Loss: 0.5137\n",
      "Iteration 1000, Loss: 0.5096\n",
      "Iteration 1100, Loss: 0.5059\n",
      "Iteration 1200, Loss: 0.5026\n",
      "Iteration 1300, Loss: 0.4996\n",
      "Iteration 1400, Loss: 0.4968\n",
      "Iteration 1500, Loss: 0.4942\n",
      "Iteration 1600, Loss: 0.4919\n",
      "Iteration 1700, Loss: 0.4897\n",
      "Iteration 1800, Loss: 0.4876\n",
      "Iteration 1900, Loss: 0.4857\n",
      "Iteration 2000, Loss: 0.4838\n",
      "Iteration 2100, Loss: 0.4821\n",
      "Iteration 2200, Loss: 0.4805\n",
      "Iteration 2300, Loss: 0.4789\n",
      "Iteration 2400, Loss: 0.4774\n",
      "Iteration 2500, Loss: 0.4760\n",
      "Iteration 2600, Loss: 0.4747\n",
      "Iteration 2700, Loss: 0.4734\n",
      "Iteration 2800, Loss: 0.4721\n",
      "Iteration 2900, Loss: 0.4710\n",
      "Iteration 3000, Loss: 0.4698\n",
      "Iteration 3100, Loss: 0.4687\n",
      "Iteration 3200, Loss: 0.4676\n",
      "Iteration 3300, Loss: 0.4666\n",
      "Iteration 3400, Loss: 0.4656\n",
      "Iteration 3500, Loss: 0.4647\n",
      "Iteration 3600, Loss: 0.4637\n",
      "Iteration 3700, Loss: 0.4628\n",
      "Iteration 3800, Loss: 0.4619\n",
      "Iteration 3900, Loss: 0.4611\n",
      "Iteration 4000, Loss: 0.4603\n",
      "Iteration 4100, Loss: 0.4595\n",
      "Iteration 4200, Loss: 0.4587\n",
      "Iteration 4300, Loss: 0.4579\n",
      "Iteration 4400, Loss: 0.4572\n",
      "Iteration 4500, Loss: 0.4565\n",
      "Iteration 4600, Loss: 0.4558\n",
      "Iteration 4700, Loss: 0.4551\n",
      "Iteration 4800, Loss: 0.4544\n",
      "Iteration 4900, Loss: 0.4538\n",
      "Iteration 5000, Loss: 0.4531\n",
      "Iteration 5100, Loss: 0.4525\n",
      "Iteration 5200, Loss: 0.4519\n",
      "Iteration 5300, Loss: 0.4513\n",
      "Iteration 5400, Loss: 0.4507\n",
      "Iteration 5500, Loss: 0.4502\n",
      "Iteration 5600, Loss: 0.4496\n",
      "Iteration 5700, Loss: 0.4490\n",
      "Iteration 5800, Loss: 0.4485\n",
      "Iteration 5900, Loss: 0.4480\n",
      "186 270\n",
      "Iteration 0, Loss: 1.0199\n",
      "Iteration 100, Loss: 0.6073\n",
      "Iteration 200, Loss: 0.5749\n",
      "Iteration 300, Loss: 0.5569\n",
      "Iteration 400, Loss: 0.5445\n",
      "Iteration 500, Loss: 0.5353\n",
      "Iteration 600, Loss: 0.5279\n",
      "Iteration 700, Loss: 0.5217\n",
      "Iteration 800, Loss: 0.5165\n",
      "Iteration 900, Loss: 0.5120\n",
      "Iteration 1000, Loss: 0.5079\n",
      "Iteration 1100, Loss: 0.5043\n",
      "Iteration 1200, Loss: 0.5011\n",
      "Iteration 1300, Loss: 0.4981\n",
      "Iteration 1400, Loss: 0.4954\n",
      "Iteration 1500, Loss: 0.4929\n",
      "Iteration 1600, Loss: 0.4905\n",
      "Iteration 1700, Loss: 0.4883\n",
      "Iteration 1800, Loss: 0.4863\n",
      "Iteration 1900, Loss: 0.4844\n",
      "Iteration 2000, Loss: 0.4826\n",
      "Iteration 2100, Loss: 0.4808\n",
      "Iteration 2200, Loss: 0.4792\n",
      "Iteration 2300, Loss: 0.4776\n",
      "Iteration 2400, Loss: 0.4761\n",
      "Iteration 2500, Loss: 0.4747\n",
      "Iteration 2600, Loss: 0.4733\n",
      "Iteration 2700, Loss: 0.4720\n",
      "Iteration 2800, Loss: 0.4707\n",
      "Iteration 2900, Loss: 0.4695\n",
      "Iteration 3000, Loss: 0.4683\n",
      "Iteration 3100, Loss: 0.4672\n",
      "Iteration 3200, Loss: 0.4661\n",
      "Iteration 3300, Loss: 0.4650\n",
      "Iteration 3400, Loss: 0.4640\n",
      "Iteration 3500, Loss: 0.4630\n",
      "Iteration 3600, Loss: 0.4620\n",
      "Iteration 3700, Loss: 0.4611\n",
      "Iteration 3800, Loss: 0.4601\n",
      "Iteration 3900, Loss: 0.4593\n",
      "Iteration 4000, Loss: 0.4584\n",
      "Iteration 4100, Loss: 0.4576\n",
      "Iteration 4200, Loss: 0.4568\n",
      "Iteration 4300, Loss: 0.4560\n",
      "Iteration 4400, Loss: 0.4552\n",
      "Iteration 4500, Loss: 0.4544\n",
      "Iteration 4600, Loss: 0.4537\n",
      "Iteration 4700, Loss: 0.4529\n",
      "Iteration 4800, Loss: 0.4522\n",
      "Iteration 4900, Loss: 0.4515\n",
      "Iteration 5000, Loss: 0.4509\n",
      "Iteration 5100, Loss: 0.4502\n",
      "Iteration 5200, Loss: 0.4495\n",
      "Iteration 5300, Loss: 0.4489\n",
      "Iteration 5400, Loss: 0.4483\n",
      "Iteration 5500, Loss: 0.4477\n",
      "Iteration 5600, Loss: 0.4471\n",
      "Iteration 5700, Loss: 0.4465\n",
      "Iteration 5800, Loss: 0.4459\n",
      "Iteration 5900, Loss: 0.4453\n",
      "Iteration 0, Loss: 1.0191\n",
      "Iteration 100, Loss: 0.6362\n",
      "Iteration 200, Loss: 0.6063\n",
      "Iteration 300, Loss: 0.5891\n",
      "Iteration 400, Loss: 0.5770\n",
      "Iteration 500, Loss: 0.5676\n",
      "Iteration 600, Loss: 0.5601\n",
      "Iteration 700, Loss: 0.5536\n",
      "Iteration 800, Loss: 0.5481\n",
      "Iteration 900, Loss: 0.5433\n",
      "Iteration 1000, Loss: 0.5390\n",
      "Iteration 1100, Loss: 0.5351\n",
      "Iteration 1200, Loss: 0.5315\n",
      "Iteration 1300, Loss: 0.5282\n",
      "Iteration 1400, Loss: 0.5253\n",
      "Iteration 1500, Loss: 0.5224\n",
      "Iteration 1600, Loss: 0.5198\n",
      "Iteration 1700, Loss: 0.5174\n",
      "Iteration 1800, Loss: 0.5151\n",
      "Iteration 1900, Loss: 0.5129\n",
      "Iteration 2000, Loss: 0.5109\n",
      "Iteration 2100, Loss: 0.5089\n",
      "Iteration 2200, Loss: 0.5071\n",
      "Iteration 2300, Loss: 0.5054\n",
      "Iteration 2400, Loss: 0.5037\n",
      "Iteration 2500, Loss: 0.5021\n",
      "Iteration 2600, Loss: 0.5006\n",
      "Iteration 2700, Loss: 0.4991\n",
      "Iteration 2800, Loss: 0.4977\n",
      "Iteration 2900, Loss: 0.4964\n",
      "Iteration 3000, Loss: 0.4951\n",
      "Iteration 3100, Loss: 0.4938\n",
      "Iteration 3200, Loss: 0.4926\n",
      "Iteration 3300, Loss: 0.4915\n",
      "Iteration 3400, Loss: 0.4904\n",
      "Iteration 3500, Loss: 0.4893\n",
      "Iteration 3600, Loss: 0.4883\n",
      "Iteration 3700, Loss: 0.4873\n",
      "Iteration 3800, Loss: 0.4863\n",
      "Iteration 3900, Loss: 0.4854\n",
      "Iteration 4000, Loss: 0.4844\n",
      "Iteration 4100, Loss: 0.4836\n",
      "Iteration 4200, Loss: 0.4827\n",
      "Iteration 4300, Loss: 0.4819\n",
      "Iteration 4400, Loss: 0.4810\n",
      "Iteration 4500, Loss: 0.4803\n",
      "Iteration 4600, Loss: 0.4795\n",
      "Iteration 4700, Loss: 0.4787\n",
      "Iteration 4800, Loss: 0.4780\n",
      "Iteration 4900, Loss: 0.4773\n",
      "Iteration 5000, Loss: 0.4766\n",
      "Iteration 5100, Loss: 0.4759\n",
      "Iteration 5200, Loss: 0.4753\n",
      "Iteration 5300, Loss: 0.4746\n",
      "Iteration 5400, Loss: 0.4740\n",
      "Iteration 5500, Loss: 0.4734\n",
      "Iteration 5600, Loss: 0.4728\n",
      "Iteration 5700, Loss: 0.4722\n",
      "Iteration 5800, Loss: 0.4716\n",
      "Iteration 5900, Loss: 0.4711\n",
      "187 270\n",
      "Iteration 0, Loss: 1.0205\n",
      "Iteration 100, Loss: 0.6204\n",
      "Iteration 200, Loss: 0.5870\n",
      "Iteration 300, Loss: 0.5686\n",
      "Iteration 400, Loss: 0.5560\n",
      "Iteration 500, Loss: 0.5465\n",
      "Iteration 600, Loss: 0.5389\n",
      "Iteration 700, Loss: 0.5325\n",
      "Iteration 800, Loss: 0.5271\n",
      "Iteration 900, Loss: 0.5224\n",
      "Iteration 1000, Loss: 0.5182\n",
      "Iteration 1100, Loss: 0.5145\n",
      "Iteration 1200, Loss: 0.5111\n",
      "Iteration 1300, Loss: 0.5080\n",
      "Iteration 1400, Loss: 0.5052\n",
      "Iteration 1500, Loss: 0.5026\n",
      "Iteration 1600, Loss: 0.5002\n",
      "Iteration 1700, Loss: 0.4979\n",
      "Iteration 1800, Loss: 0.4958\n",
      "Iteration 1900, Loss: 0.4939\n",
      "Iteration 2000, Loss: 0.4920\n",
      "Iteration 2100, Loss: 0.4903\n",
      "Iteration 2200, Loss: 0.4886\n",
      "Iteration 2300, Loss: 0.4870\n",
      "Iteration 2400, Loss: 0.4855\n",
      "Iteration 2500, Loss: 0.4841\n",
      "Iteration 2600, Loss: 0.4827\n",
      "Iteration 2700, Loss: 0.4815\n",
      "Iteration 2800, Loss: 0.4802\n",
      "Iteration 2900, Loss: 0.4790\n",
      "Iteration 3000, Loss: 0.4779\n",
      "Iteration 3100, Loss: 0.4768\n",
      "Iteration 3200, Loss: 0.4757\n",
      "Iteration 3300, Loss: 0.4747\n",
      "Iteration 3400, Loss: 0.4737\n",
      "Iteration 3500, Loss: 0.4728\n",
      "Iteration 3600, Loss: 0.4718\n",
      "Iteration 3700, Loss: 0.4709\n",
      "Iteration 3800, Loss: 0.4701\n",
      "Iteration 3900, Loss: 0.4692\n",
      "Iteration 4000, Loss: 0.4684\n",
      "Iteration 4100, Loss: 0.4676\n",
      "Iteration 4200, Loss: 0.4669\n",
      "Iteration 4300, Loss: 0.4661\n",
      "Iteration 4400, Loss: 0.4654\n",
      "Iteration 4500, Loss: 0.4647\n",
      "Iteration 4600, Loss: 0.4640\n",
      "Iteration 4700, Loss: 0.4633\n",
      "Iteration 4800, Loss: 0.4627\n",
      "Iteration 4900, Loss: 0.4620\n",
      "Iteration 5000, Loss: 0.4614\n",
      "Iteration 5100, Loss: 0.4608\n",
      "Iteration 5200, Loss: 0.4602\n",
      "Iteration 5300, Loss: 0.4596\n",
      "Iteration 5400, Loss: 0.4591\n",
      "Iteration 5500, Loss: 0.4585\n",
      "Iteration 5600, Loss: 0.4580\n",
      "Iteration 5700, Loss: 0.4574\n",
      "Iteration 5800, Loss: 0.4569\n",
      "Iteration 5900, Loss: 0.4564\n",
      "Iteration 0, Loss: 1.0206\n",
      "Iteration 100, Loss: 0.6229\n",
      "Iteration 200, Loss: 0.5938\n",
      "Iteration 300, Loss: 0.5775\n",
      "Iteration 400, Loss: 0.5659\n",
      "Iteration 500, Loss: 0.5570\n",
      "Iteration 600, Loss: 0.5498\n",
      "Iteration 700, Loss: 0.5439\n",
      "Iteration 800, Loss: 0.5388\n",
      "Iteration 900, Loss: 0.5343\n",
      "Iteration 1000, Loss: 0.5304\n",
      "Iteration 1100, Loss: 0.5269\n",
      "Iteration 1200, Loss: 0.5237\n",
      "Iteration 1300, Loss: 0.5208\n",
      "Iteration 1400, Loss: 0.5182\n",
      "Iteration 1500, Loss: 0.5157\n",
      "Iteration 1600, Loss: 0.5134\n",
      "Iteration 1700, Loss: 0.5113\n",
      "Iteration 1800, Loss: 0.5093\n",
      "Iteration 1900, Loss: 0.5074\n",
      "Iteration 2000, Loss: 0.5056\n",
      "Iteration 2100, Loss: 0.5039\n",
      "Iteration 2200, Loss: 0.5023\n",
      "Iteration 2300, Loss: 0.5008\n",
      "Iteration 2400, Loss: 0.4993\n",
      "Iteration 2500, Loss: 0.4979\n",
      "Iteration 2600, Loss: 0.4966\n",
      "Iteration 2700, Loss: 0.4953\n",
      "Iteration 2800, Loss: 0.4941\n",
      "Iteration 2900, Loss: 0.4929\n",
      "Iteration 3000, Loss: 0.4918\n",
      "Iteration 3100, Loss: 0.4906\n",
      "Iteration 3200, Loss: 0.4896\n",
      "Iteration 3300, Loss: 0.4885\n",
      "Iteration 3400, Loss: 0.4875\n",
      "Iteration 3500, Loss: 0.4866\n",
      "Iteration 3600, Loss: 0.4856\n",
      "Iteration 3700, Loss: 0.4847\n",
      "Iteration 3800, Loss: 0.4839\n",
      "Iteration 3900, Loss: 0.4830\n",
      "Iteration 4000, Loss: 0.4822\n",
      "Iteration 4100, Loss: 0.4814\n",
      "Iteration 4200, Loss: 0.4806\n",
      "Iteration 4300, Loss: 0.4798\n",
      "Iteration 4400, Loss: 0.4791\n",
      "Iteration 4500, Loss: 0.4784\n",
      "Iteration 4600, Loss: 0.4776\n",
      "Iteration 4700, Loss: 0.4770\n",
      "Iteration 4800, Loss: 0.4763\n",
      "Iteration 4900, Loss: 0.4756\n",
      "Iteration 5000, Loss: 0.4750\n",
      "Iteration 5100, Loss: 0.4744\n",
      "Iteration 5200, Loss: 0.4737\n",
      "Iteration 5300, Loss: 0.4731\n",
      "Iteration 5400, Loss: 0.4725\n",
      "Iteration 5500, Loss: 0.4720\n",
      "Iteration 5600, Loss: 0.4714\n",
      "Iteration 5700, Loss: 0.4708\n",
      "Iteration 5800, Loss: 0.4703\n",
      "Iteration 5900, Loss: 0.4698\n",
      "188 270\n",
      "Iteration 0, Loss: 1.0188\n",
      "Iteration 100, Loss: 0.6337\n",
      "Iteration 200, Loss: 0.6008\n",
      "Iteration 300, Loss: 0.5824\n",
      "Iteration 400, Loss: 0.5699\n",
      "Iteration 500, Loss: 0.5604\n",
      "Iteration 600, Loss: 0.5529\n",
      "Iteration 700, Loss: 0.5466\n",
      "Iteration 800, Loss: 0.5412\n",
      "Iteration 900, Loss: 0.5366\n",
      "Iteration 1000, Loss: 0.5324\n",
      "Iteration 1100, Loss: 0.5287\n",
      "Iteration 1200, Loss: 0.5253\n",
      "Iteration 1300, Loss: 0.5223\n",
      "Iteration 1400, Loss: 0.5195\n",
      "Iteration 1500, Loss: 0.5169\n",
      "Iteration 1600, Loss: 0.5145\n",
      "Iteration 1700, Loss: 0.5122\n",
      "Iteration 1800, Loss: 0.5101\n",
      "Iteration 1900, Loss: 0.5081\n",
      "Iteration 2000, Loss: 0.5062\n",
      "Iteration 2100, Loss: 0.5044\n",
      "Iteration 2200, Loss: 0.5027\n",
      "Iteration 2300, Loss: 0.5011\n",
      "Iteration 2400, Loss: 0.4996\n",
      "Iteration 2500, Loss: 0.4981\n",
      "Iteration 2600, Loss: 0.4967\n",
      "Iteration 2700, Loss: 0.4954\n",
      "Iteration 2800, Loss: 0.4941\n",
      "Iteration 2900, Loss: 0.4929\n",
      "Iteration 3000, Loss: 0.4917\n",
      "Iteration 3100, Loss: 0.4905\n",
      "Iteration 3200, Loss: 0.4894\n",
      "Iteration 3300, Loss: 0.4883\n",
      "Iteration 3400, Loss: 0.4873\n",
      "Iteration 3500, Loss: 0.4863\n",
      "Iteration 3600, Loss: 0.4853\n",
      "Iteration 3700, Loss: 0.4844\n",
      "Iteration 3800, Loss: 0.4835\n",
      "Iteration 3900, Loss: 0.4826\n",
      "Iteration 4000, Loss: 0.4817\n",
      "Iteration 4100, Loss: 0.4809\n",
      "Iteration 4200, Loss: 0.4801\n",
      "Iteration 4300, Loss: 0.4793\n",
      "Iteration 4400, Loss: 0.4785\n",
      "Iteration 4500, Loss: 0.4777\n",
      "Iteration 4600, Loss: 0.4770\n",
      "Iteration 4700, Loss: 0.4763\n",
      "Iteration 4800, Loss: 0.4756\n",
      "Iteration 4900, Loss: 0.4749\n",
      "Iteration 5000, Loss: 0.4742\n",
      "Iteration 5100, Loss: 0.4735\n",
      "Iteration 5200, Loss: 0.4729\n",
      "Iteration 5300, Loss: 0.4723\n",
      "Iteration 5400, Loss: 0.4716\n",
      "Iteration 5500, Loss: 0.4710\n",
      "Iteration 5600, Loss: 0.4704\n",
      "Iteration 5700, Loss: 0.4698\n",
      "Iteration 5800, Loss: 0.4693\n",
      "Iteration 5900, Loss: 0.4687\n",
      "Iteration 0, Loss: 1.0209\n",
      "Iteration 100, Loss: 0.6067\n",
      "Iteration 200, Loss: 0.5762\n",
      "Iteration 300, Loss: 0.5598\n",
      "Iteration 400, Loss: 0.5484\n",
      "Iteration 500, Loss: 0.5399\n",
      "Iteration 600, Loss: 0.5330\n",
      "Iteration 700, Loss: 0.5274\n",
      "Iteration 800, Loss: 0.5225\n",
      "Iteration 900, Loss: 0.5183\n",
      "Iteration 1000, Loss: 0.5145\n",
      "Iteration 1100, Loss: 0.5111\n",
      "Iteration 1200, Loss: 0.5081\n",
      "Iteration 1300, Loss: 0.5053\n",
      "Iteration 1400, Loss: 0.5027\n",
      "Iteration 1500, Loss: 0.5004\n",
      "Iteration 1600, Loss: 0.4981\n",
      "Iteration 1700, Loss: 0.4961\n",
      "Iteration 1800, Loss: 0.4941\n",
      "Iteration 1900, Loss: 0.4923\n",
      "Iteration 2000, Loss: 0.4906\n",
      "Iteration 2100, Loss: 0.4890\n",
      "Iteration 2200, Loss: 0.4874\n",
      "Iteration 2300, Loss: 0.4859\n",
      "Iteration 2400, Loss: 0.4845\n",
      "Iteration 2500, Loss: 0.4831\n",
      "Iteration 2600, Loss: 0.4818\n",
      "Iteration 2700, Loss: 0.4806\n",
      "Iteration 2800, Loss: 0.4793\n",
      "Iteration 2900, Loss: 0.4782\n",
      "Iteration 3000, Loss: 0.4771\n",
      "Iteration 3100, Loss: 0.4760\n",
      "Iteration 3200, Loss: 0.4750\n",
      "Iteration 3300, Loss: 0.4740\n",
      "Iteration 3400, Loss: 0.4730\n",
      "Iteration 3500, Loss: 0.4720\n",
      "Iteration 3600, Loss: 0.4712\n",
      "Iteration 3700, Loss: 0.4703\n",
      "Iteration 3800, Loss: 0.4694\n",
      "Iteration 3900, Loss: 0.4686\n",
      "Iteration 4000, Loss: 0.4678\n",
      "Iteration 4100, Loss: 0.4670\n",
      "Iteration 4200, Loss: 0.4662\n",
      "Iteration 4300, Loss: 0.4655\n",
      "Iteration 4400, Loss: 0.4648\n",
      "Iteration 4500, Loss: 0.4640\n",
      "Iteration 4600, Loss: 0.4634\n",
      "Iteration 4700, Loss: 0.4627\n",
      "Iteration 4800, Loss: 0.4620\n",
      "Iteration 4900, Loss: 0.4614\n",
      "Iteration 5000, Loss: 0.4608\n",
      "Iteration 5100, Loss: 0.4602\n",
      "Iteration 5200, Loss: 0.4596\n",
      "Iteration 5300, Loss: 0.4590\n",
      "Iteration 5400, Loss: 0.4584\n",
      "Iteration 5500, Loss: 0.4579\n",
      "Iteration 5600, Loss: 0.4573\n",
      "Iteration 5700, Loss: 0.4568\n",
      "Iteration 5800, Loss: 0.4563\n",
      "Iteration 5900, Loss: 0.4557\n",
      "189 270\n",
      "Iteration 0, Loss: 1.0230\n",
      "Iteration 100, Loss: 0.6405\n",
      "Iteration 200, Loss: 0.6088\n",
      "Iteration 300, Loss: 0.5908\n",
      "Iteration 400, Loss: 0.5782\n",
      "Iteration 500, Loss: 0.5687\n",
      "Iteration 600, Loss: 0.5611\n",
      "Iteration 700, Loss: 0.5548\n",
      "Iteration 800, Loss: 0.5494\n",
      "Iteration 900, Loss: 0.5448\n",
      "Iteration 1000, Loss: 0.5407\n",
      "Iteration 1100, Loss: 0.5370\n",
      "Iteration 1200, Loss: 0.5337\n",
      "Iteration 1300, Loss: 0.5307\n",
      "Iteration 1400, Loss: 0.5280\n",
      "Iteration 1500, Loss: 0.5254\n",
      "Iteration 1600, Loss: 0.5230\n",
      "Iteration 1700, Loss: 0.5208\n",
      "Iteration 1800, Loss: 0.5187\n",
      "Iteration 1900, Loss: 0.5168\n",
      "Iteration 2000, Loss: 0.5149\n",
      "Iteration 2100, Loss: 0.5132\n",
      "Iteration 2200, Loss: 0.5115\n",
      "Iteration 2300, Loss: 0.5099\n",
      "Iteration 2400, Loss: 0.5084\n",
      "Iteration 2500, Loss: 0.5070\n",
      "Iteration 2600, Loss: 0.5055\n",
      "Iteration 2700, Loss: 0.5042\n",
      "Iteration 2800, Loss: 0.5029\n",
      "Iteration 2900, Loss: 0.5017\n",
      "Iteration 3000, Loss: 0.5005\n",
      "Iteration 3100, Loss: 0.4994\n",
      "Iteration 3200, Loss: 0.4983\n",
      "Iteration 3300, Loss: 0.4972\n",
      "Iteration 3400, Loss: 0.4962\n",
      "Iteration 3500, Loss: 0.4952\n",
      "Iteration 3600, Loss: 0.4942\n",
      "Iteration 3700, Loss: 0.4933\n",
      "Iteration 3800, Loss: 0.4923\n",
      "Iteration 3900, Loss: 0.4915\n",
      "Iteration 4000, Loss: 0.4906\n",
      "Iteration 4100, Loss: 0.4898\n",
      "Iteration 4200, Loss: 0.4889\n",
      "Iteration 4300, Loss: 0.4882\n",
      "Iteration 4400, Loss: 0.4874\n",
      "Iteration 4500, Loss: 0.4866\n",
      "Iteration 4600, Loss: 0.4859\n",
      "Iteration 4700, Loss: 0.4852\n",
      "Iteration 4800, Loss: 0.4845\n",
      "Iteration 4900, Loss: 0.4838\n",
      "Iteration 5000, Loss: 0.4831\n",
      "Iteration 5100, Loss: 0.4825\n",
      "Iteration 5200, Loss: 0.4818\n",
      "Iteration 5300, Loss: 0.4812\n",
      "Iteration 5400, Loss: 0.4806\n",
      "Iteration 5500, Loss: 0.4800\n",
      "Iteration 5600, Loss: 0.4794\n",
      "Iteration 5700, Loss: 0.4789\n",
      "Iteration 5800, Loss: 0.4783\n",
      "Iteration 5900, Loss: 0.4777\n",
      "Iteration 0, Loss: 1.0184\n",
      "Iteration 100, Loss: 0.6023\n",
      "Iteration 200, Loss: 0.5721\n",
      "Iteration 300, Loss: 0.5557\n",
      "Iteration 400, Loss: 0.5444\n",
      "Iteration 500, Loss: 0.5360\n",
      "Iteration 600, Loss: 0.5293\n",
      "Iteration 700, Loss: 0.5237\n",
      "Iteration 800, Loss: 0.5190\n",
      "Iteration 900, Loss: 0.5148\n",
      "Iteration 1000, Loss: 0.5111\n",
      "Iteration 1100, Loss: 0.5079\n",
      "Iteration 1200, Loss: 0.5049\n",
      "Iteration 1300, Loss: 0.5021\n",
      "Iteration 1400, Loss: 0.4996\n",
      "Iteration 1500, Loss: 0.4973\n",
      "Iteration 1600, Loss: 0.4951\n",
      "Iteration 1700, Loss: 0.4931\n",
      "Iteration 1800, Loss: 0.4912\n",
      "Iteration 1900, Loss: 0.4894\n",
      "Iteration 2000, Loss: 0.4877\n",
      "Iteration 2100, Loss: 0.4861\n",
      "Iteration 2200, Loss: 0.4846\n",
      "Iteration 2300, Loss: 0.4831\n",
      "Iteration 2400, Loss: 0.4817\n",
      "Iteration 2500, Loss: 0.4804\n",
      "Iteration 2600, Loss: 0.4791\n",
      "Iteration 2700, Loss: 0.4779\n",
      "Iteration 2800, Loss: 0.4768\n",
      "Iteration 2900, Loss: 0.4756\n",
      "Iteration 3000, Loss: 0.4746\n",
      "Iteration 3100, Loss: 0.4735\n",
      "Iteration 3200, Loss: 0.4725\n",
      "Iteration 3300, Loss: 0.4715\n",
      "Iteration 3400, Loss: 0.4706\n",
      "Iteration 3500, Loss: 0.4696\n",
      "Iteration 3600, Loss: 0.4688\n",
      "Iteration 3700, Loss: 0.4679\n",
      "Iteration 3800, Loss: 0.4671\n",
      "Iteration 3900, Loss: 0.4663\n",
      "Iteration 4000, Loss: 0.4655\n",
      "Iteration 4100, Loss: 0.4647\n",
      "Iteration 4200, Loss: 0.4640\n",
      "Iteration 4300, Loss: 0.4632\n",
      "Iteration 4400, Loss: 0.4625\n",
      "Iteration 4500, Loss: 0.4618\n",
      "Iteration 4600, Loss: 0.4611\n",
      "Iteration 4700, Loss: 0.4605\n",
      "Iteration 4800, Loss: 0.4598\n",
      "Iteration 4900, Loss: 0.4592\n",
      "Iteration 5000, Loss: 0.4586\n",
      "Iteration 5100, Loss: 0.4580\n",
      "Iteration 5200, Loss: 0.4574\n",
      "Iteration 5300, Loss: 0.4568\n",
      "Iteration 5400, Loss: 0.4563\n",
      "Iteration 5500, Loss: 0.4557\n",
      "Iteration 5600, Loss: 0.4551\n",
      "Iteration 5700, Loss: 0.4546\n",
      "Iteration 5800, Loss: 0.4541\n",
      "Iteration 5900, Loss: 0.4536\n",
      "190 270\n",
      "Iteration 0, Loss: 1.0210\n",
      "Iteration 100, Loss: 0.6264\n",
      "Iteration 200, Loss: 0.5949\n",
      "Iteration 300, Loss: 0.5771\n",
      "Iteration 400, Loss: 0.5649\n",
      "Iteration 500, Loss: 0.5557\n",
      "Iteration 600, Loss: 0.5484\n",
      "Iteration 700, Loss: 0.5424\n",
      "Iteration 800, Loss: 0.5372\n",
      "Iteration 900, Loss: 0.5327\n",
      "Iteration 1000, Loss: 0.5288\n",
      "Iteration 1100, Loss: 0.5252\n",
      "Iteration 1200, Loss: 0.5220\n",
      "Iteration 1300, Loss: 0.5190\n",
      "Iteration 1400, Loss: 0.5163\n",
      "Iteration 1500, Loss: 0.5138\n",
      "Iteration 1600, Loss: 0.5114\n",
      "Iteration 1700, Loss: 0.5092\n",
      "Iteration 1800, Loss: 0.5072\n",
      "Iteration 1900, Loss: 0.5052\n",
      "Iteration 2000, Loss: 0.5034\n",
      "Iteration 2100, Loss: 0.5016\n",
      "Iteration 2200, Loss: 0.4999\n",
      "Iteration 2300, Loss: 0.4984\n",
      "Iteration 2400, Loss: 0.4968\n",
      "Iteration 2500, Loss: 0.4954\n",
      "Iteration 2600, Loss: 0.4940\n",
      "Iteration 2700, Loss: 0.4927\n",
      "Iteration 2800, Loss: 0.4914\n",
      "Iteration 2900, Loss: 0.4901\n",
      "Iteration 3000, Loss: 0.4890\n",
      "Iteration 3100, Loss: 0.4878\n",
      "Iteration 3200, Loss: 0.4867\n",
      "Iteration 3300, Loss: 0.4856\n",
      "Iteration 3400, Loss: 0.4845\n",
      "Iteration 3500, Loss: 0.4835\n",
      "Iteration 3600, Loss: 0.4825\n",
      "Iteration 3700, Loss: 0.4816\n",
      "Iteration 3800, Loss: 0.4806\n",
      "Iteration 3900, Loss: 0.4797\n",
      "Iteration 4000, Loss: 0.4788\n",
      "Iteration 4100, Loss: 0.4779\n",
      "Iteration 4200, Loss: 0.4771\n",
      "Iteration 4300, Loss: 0.4763\n",
      "Iteration 4400, Loss: 0.4754\n",
      "Iteration 4500, Loss: 0.4747\n",
      "Iteration 4600, Loss: 0.4739\n",
      "Iteration 4700, Loss: 0.4731\n",
      "Iteration 4800, Loss: 0.4724\n",
      "Iteration 4900, Loss: 0.4717\n",
      "Iteration 5000, Loss: 0.4710\n",
      "Iteration 5100, Loss: 0.4703\n",
      "Iteration 5200, Loss: 0.4696\n",
      "Iteration 5300, Loss: 0.4689\n",
      "Iteration 5400, Loss: 0.4683\n",
      "Iteration 5500, Loss: 0.4676\n",
      "Iteration 5600, Loss: 0.4670\n",
      "Iteration 5700, Loss: 0.4664\n",
      "Iteration 5800, Loss: 0.4658\n",
      "Iteration 5900, Loss: 0.4652\n",
      "Iteration 0, Loss: 1.0197\n",
      "Iteration 100, Loss: 0.6191\n",
      "Iteration 200, Loss: 0.5872\n",
      "Iteration 300, Loss: 0.5698\n",
      "Iteration 400, Loss: 0.5578\n",
      "Iteration 500, Loss: 0.5486\n",
      "Iteration 600, Loss: 0.5411\n",
      "Iteration 700, Loss: 0.5348\n",
      "Iteration 800, Loss: 0.5294\n",
      "Iteration 900, Loss: 0.5246\n",
      "Iteration 1000, Loss: 0.5203\n",
      "Iteration 1100, Loss: 0.5165\n",
      "Iteration 1200, Loss: 0.5131\n",
      "Iteration 1300, Loss: 0.5099\n",
      "Iteration 1400, Loss: 0.5070\n",
      "Iteration 1500, Loss: 0.5042\n",
      "Iteration 1600, Loss: 0.5017\n",
      "Iteration 1700, Loss: 0.4993\n",
      "Iteration 1800, Loss: 0.4970\n",
      "Iteration 1900, Loss: 0.4949\n",
      "Iteration 2000, Loss: 0.4929\n",
      "Iteration 2100, Loss: 0.4910\n",
      "Iteration 2200, Loss: 0.4892\n",
      "Iteration 2300, Loss: 0.4875\n",
      "Iteration 2400, Loss: 0.4858\n",
      "Iteration 2500, Loss: 0.4843\n",
      "Iteration 2600, Loss: 0.4828\n",
      "Iteration 2700, Loss: 0.4813\n",
      "Iteration 2800, Loss: 0.4799\n",
      "Iteration 2900, Loss: 0.4786\n",
      "Iteration 3000, Loss: 0.4773\n",
      "Iteration 3100, Loss: 0.4761\n",
      "Iteration 3200, Loss: 0.4749\n",
      "Iteration 3300, Loss: 0.4737\n",
      "Iteration 3400, Loss: 0.4726\n",
      "Iteration 3500, Loss: 0.4715\n",
      "Iteration 3600, Loss: 0.4705\n",
      "Iteration 3700, Loss: 0.4695\n",
      "Iteration 3800, Loss: 0.4685\n",
      "Iteration 3900, Loss: 0.4676\n",
      "Iteration 4000, Loss: 0.4667\n",
      "Iteration 4100, Loss: 0.4658\n",
      "Iteration 4200, Loss: 0.4649\n",
      "Iteration 4300, Loss: 0.4641\n",
      "Iteration 4400, Loss: 0.4633\n",
      "Iteration 4500, Loss: 0.4625\n",
      "Iteration 4600, Loss: 0.4617\n",
      "Iteration 4700, Loss: 0.4610\n",
      "Iteration 4800, Loss: 0.4602\n",
      "Iteration 4900, Loss: 0.4595\n",
      "Iteration 5000, Loss: 0.4588\n",
      "Iteration 5100, Loss: 0.4581\n",
      "Iteration 5200, Loss: 0.4575\n",
      "Iteration 5300, Loss: 0.4568\n",
      "Iteration 5400, Loss: 0.4562\n",
      "Iteration 5500, Loss: 0.4556\n",
      "Iteration 5600, Loss: 0.4550\n",
      "Iteration 5700, Loss: 0.4544\n",
      "Iteration 5800, Loss: 0.4538\n",
      "Iteration 5900, Loss: 0.4533\n",
      "191 270\n",
      "Iteration 0, Loss: 1.0173\n",
      "Iteration 100, Loss: 0.6213\n",
      "Iteration 200, Loss: 0.5904\n",
      "Iteration 300, Loss: 0.5729\n",
      "Iteration 400, Loss: 0.5608\n",
      "Iteration 500, Loss: 0.5517\n",
      "Iteration 600, Loss: 0.5443\n",
      "Iteration 700, Loss: 0.5382\n",
      "Iteration 800, Loss: 0.5330\n",
      "Iteration 900, Loss: 0.5284\n",
      "Iteration 1000, Loss: 0.5244\n",
      "Iteration 1100, Loss: 0.5208\n",
      "Iteration 1200, Loss: 0.5175\n",
      "Iteration 1300, Loss: 0.5145\n",
      "Iteration 1400, Loss: 0.5117\n",
      "Iteration 1500, Loss: 0.5092\n",
      "Iteration 1600, Loss: 0.5068\n",
      "Iteration 1700, Loss: 0.5045\n",
      "Iteration 1800, Loss: 0.5024\n",
      "Iteration 1900, Loss: 0.5004\n",
      "Iteration 2000, Loss: 0.4985\n",
      "Iteration 2100, Loss: 0.4968\n",
      "Iteration 2200, Loss: 0.4951\n",
      "Iteration 2300, Loss: 0.4934\n",
      "Iteration 2400, Loss: 0.4919\n",
      "Iteration 2500, Loss: 0.4904\n",
      "Iteration 2600, Loss: 0.4890\n",
      "Iteration 2700, Loss: 0.4877\n",
      "Iteration 2800, Loss: 0.4864\n",
      "Iteration 2900, Loss: 0.4851\n",
      "Iteration 3000, Loss: 0.4839\n",
      "Iteration 3100, Loss: 0.4827\n",
      "Iteration 3200, Loss: 0.4816\n",
      "Iteration 3300, Loss: 0.4805\n",
      "Iteration 3400, Loss: 0.4795\n",
      "Iteration 3500, Loss: 0.4785\n",
      "Iteration 3600, Loss: 0.4775\n",
      "Iteration 3700, Loss: 0.4765\n",
      "Iteration 3800, Loss: 0.4756\n",
      "Iteration 3900, Loss: 0.4747\n",
      "Iteration 4000, Loss: 0.4738\n",
      "Iteration 4100, Loss: 0.4729\n",
      "Iteration 4200, Loss: 0.4721\n",
      "Iteration 4300, Loss: 0.4713\n",
      "Iteration 4400, Loss: 0.4705\n",
      "Iteration 4500, Loss: 0.4697\n",
      "Iteration 4600, Loss: 0.4690\n",
      "Iteration 4700, Loss: 0.4682\n",
      "Iteration 4800, Loss: 0.4675\n",
      "Iteration 4900, Loss: 0.4668\n",
      "Iteration 5000, Loss: 0.4661\n",
      "Iteration 5100, Loss: 0.4655\n",
      "Iteration 5200, Loss: 0.4648\n",
      "Iteration 5300, Loss: 0.4642\n",
      "Iteration 5400, Loss: 0.4635\n",
      "Iteration 5500, Loss: 0.4629\n",
      "Iteration 5600, Loss: 0.4623\n",
      "Iteration 5700, Loss: 0.4617\n",
      "Iteration 5800, Loss: 0.4611\n",
      "Iteration 5900, Loss: 0.4606\n",
      "Iteration 0, Loss: 1.0216\n",
      "Iteration 100, Loss: 0.6202\n",
      "Iteration 200, Loss: 0.5892\n",
      "Iteration 300, Loss: 0.5726\n",
      "Iteration 400, Loss: 0.5613\n",
      "Iteration 500, Loss: 0.5527\n",
      "Iteration 600, Loss: 0.5457\n",
      "Iteration 700, Loss: 0.5399\n",
      "Iteration 800, Loss: 0.5350\n",
      "Iteration 900, Loss: 0.5307\n",
      "Iteration 1000, Loss: 0.5269\n",
      "Iteration 1100, Loss: 0.5235\n",
      "Iteration 1200, Loss: 0.5204\n",
      "Iteration 1300, Loss: 0.5176\n",
      "Iteration 1400, Loss: 0.5150\n",
      "Iteration 1500, Loss: 0.5126\n",
      "Iteration 1600, Loss: 0.5104\n",
      "Iteration 1700, Loss: 0.5083\n",
      "Iteration 1800, Loss: 0.5063\n",
      "Iteration 1900, Loss: 0.5045\n",
      "Iteration 2000, Loss: 0.5028\n",
      "Iteration 2100, Loss: 0.5011\n",
      "Iteration 2200, Loss: 0.4996\n",
      "Iteration 2300, Loss: 0.4980\n",
      "Iteration 2400, Loss: 0.4966\n",
      "Iteration 2500, Loss: 0.4952\n",
      "Iteration 2600, Loss: 0.4939\n",
      "Iteration 2700, Loss: 0.4927\n",
      "Iteration 2800, Loss: 0.4915\n",
      "Iteration 2900, Loss: 0.4903\n",
      "Iteration 3000, Loss: 0.4892\n",
      "Iteration 3100, Loss: 0.4881\n",
      "Iteration 3200, Loss: 0.4870\n",
      "Iteration 3300, Loss: 0.4860\n",
      "Iteration 3400, Loss: 0.4850\n",
      "Iteration 3500, Loss: 0.4841\n",
      "Iteration 3600, Loss: 0.4831\n",
      "Iteration 3700, Loss: 0.4822\n",
      "Iteration 3800, Loss: 0.4813\n",
      "Iteration 3900, Loss: 0.4805\n",
      "Iteration 4000, Loss: 0.4796\n",
      "Iteration 4100, Loss: 0.4788\n",
      "Iteration 4200, Loss: 0.4781\n",
      "Iteration 4300, Loss: 0.4773\n",
      "Iteration 4400, Loss: 0.4765\n",
      "Iteration 4500, Loss: 0.4758\n",
      "Iteration 4600, Loss: 0.4751\n",
      "Iteration 4700, Loss: 0.4744\n",
      "Iteration 4800, Loss: 0.4737\n",
      "Iteration 4900, Loss: 0.4730\n",
      "Iteration 5000, Loss: 0.4723\n",
      "Iteration 5100, Loss: 0.4717\n",
      "Iteration 5200, Loss: 0.4711\n",
      "Iteration 5300, Loss: 0.4704\n",
      "Iteration 5400, Loss: 0.4698\n",
      "Iteration 5500, Loss: 0.4692\n",
      "Iteration 5600, Loss: 0.4687\n",
      "Iteration 5700, Loss: 0.4681\n",
      "Iteration 5800, Loss: 0.4675\n",
      "Iteration 5900, Loss: 0.4670\n",
      "192 270\n",
      "Iteration 0, Loss: 1.0162\n",
      "Iteration 100, Loss: 0.6171\n",
      "Iteration 200, Loss: 0.5848\n",
      "Iteration 300, Loss: 0.5674\n",
      "Iteration 400, Loss: 0.5558\n",
      "Iteration 500, Loss: 0.5472\n",
      "Iteration 600, Loss: 0.5404\n",
      "Iteration 700, Loss: 0.5348\n",
      "Iteration 800, Loss: 0.5300\n",
      "Iteration 900, Loss: 0.5259\n",
      "Iteration 1000, Loss: 0.5222\n",
      "Iteration 1100, Loss: 0.5189\n",
      "Iteration 1200, Loss: 0.5160\n",
      "Iteration 1300, Loss: 0.5132\n",
      "Iteration 1400, Loss: 0.5107\n",
      "Iteration 1500, Loss: 0.5084\n",
      "Iteration 1600, Loss: 0.5062\n",
      "Iteration 1700, Loss: 0.5041\n",
      "Iteration 1800, Loss: 0.5022\n",
      "Iteration 1900, Loss: 0.5004\n",
      "Iteration 2000, Loss: 0.4987\n",
      "Iteration 2100, Loss: 0.4971\n",
      "Iteration 2200, Loss: 0.4955\n",
      "Iteration 2300, Loss: 0.4940\n",
      "Iteration 2400, Loss: 0.4926\n",
      "Iteration 2500, Loss: 0.4913\n",
      "Iteration 2600, Loss: 0.4900\n",
      "Iteration 2700, Loss: 0.4887\n",
      "Iteration 2800, Loss: 0.4875\n",
      "Iteration 2900, Loss: 0.4864\n",
      "Iteration 3000, Loss: 0.4852\n",
      "Iteration 3100, Loss: 0.4842\n",
      "Iteration 3200, Loss: 0.4831\n",
      "Iteration 3300, Loss: 0.4821\n",
      "Iteration 3400, Loss: 0.4811\n",
      "Iteration 3500, Loss: 0.4802\n",
      "Iteration 3600, Loss: 0.4793\n",
      "Iteration 3700, Loss: 0.4784\n",
      "Iteration 3800, Loss: 0.4775\n",
      "Iteration 3900, Loss: 0.4767\n",
      "Iteration 4000, Loss: 0.4758\n",
      "Iteration 4100, Loss: 0.4750\n",
      "Iteration 4200, Loss: 0.4742\n",
      "Iteration 4300, Loss: 0.4735\n",
      "Iteration 4400, Loss: 0.4727\n",
      "Iteration 4500, Loss: 0.4720\n",
      "Iteration 4600, Loss: 0.4713\n",
      "Iteration 4700, Loss: 0.4706\n",
      "Iteration 4800, Loss: 0.4699\n",
      "Iteration 4900, Loss: 0.4693\n",
      "Iteration 5000, Loss: 0.4686\n",
      "Iteration 5100, Loss: 0.4680\n",
      "Iteration 5200, Loss: 0.4674\n",
      "Iteration 5300, Loss: 0.4668\n",
      "Iteration 5400, Loss: 0.4662\n",
      "Iteration 5500, Loss: 0.4656\n",
      "Iteration 5600, Loss: 0.4650\n",
      "Iteration 5700, Loss: 0.4645\n",
      "Iteration 5800, Loss: 0.4639\n",
      "Iteration 5900, Loss: 0.4634\n",
      "Iteration 0, Loss: 1.0219\n",
      "Iteration 100, Loss: 0.6260\n",
      "Iteration 200, Loss: 0.5958\n",
      "Iteration 300, Loss: 0.5784\n",
      "Iteration 400, Loss: 0.5662\n",
      "Iteration 500, Loss: 0.5568\n",
      "Iteration 600, Loss: 0.5491\n",
      "Iteration 700, Loss: 0.5428\n",
      "Iteration 800, Loss: 0.5374\n",
      "Iteration 900, Loss: 0.5327\n",
      "Iteration 1000, Loss: 0.5285\n",
      "Iteration 1100, Loss: 0.5247\n",
      "Iteration 1200, Loss: 0.5213\n",
      "Iteration 1300, Loss: 0.5181\n",
      "Iteration 1400, Loss: 0.5152\n",
      "Iteration 1500, Loss: 0.5126\n",
      "Iteration 1600, Loss: 0.5101\n",
      "Iteration 1700, Loss: 0.5078\n",
      "Iteration 1800, Loss: 0.5056\n",
      "Iteration 1900, Loss: 0.5035\n",
      "Iteration 2000, Loss: 0.5016\n",
      "Iteration 2100, Loss: 0.4997\n",
      "Iteration 2200, Loss: 0.4980\n",
      "Iteration 2300, Loss: 0.4963\n",
      "Iteration 2400, Loss: 0.4947\n",
      "Iteration 2500, Loss: 0.4931\n",
      "Iteration 2600, Loss: 0.4917\n",
      "Iteration 2700, Loss: 0.4903\n",
      "Iteration 2800, Loss: 0.4889\n",
      "Iteration 2900, Loss: 0.4876\n",
      "Iteration 3000, Loss: 0.4864\n",
      "Iteration 3100, Loss: 0.4852\n",
      "Iteration 3200, Loss: 0.4840\n",
      "Iteration 3300, Loss: 0.4829\n",
      "Iteration 3400, Loss: 0.4818\n",
      "Iteration 3500, Loss: 0.4808\n",
      "Iteration 3600, Loss: 0.4797\n",
      "Iteration 3700, Loss: 0.4787\n",
      "Iteration 3800, Loss: 0.4777\n",
      "Iteration 3900, Loss: 0.4768\n",
      "Iteration 4000, Loss: 0.4759\n",
      "Iteration 4100, Loss: 0.4750\n",
      "Iteration 4200, Loss: 0.4741\n",
      "Iteration 4300, Loss: 0.4733\n",
      "Iteration 4400, Loss: 0.4724\n",
      "Iteration 4500, Loss: 0.4717\n",
      "Iteration 4600, Loss: 0.4708\n",
      "Iteration 4700, Loss: 0.4701\n",
      "Iteration 4800, Loss: 0.4693\n",
      "Iteration 4900, Loss: 0.4686\n",
      "Iteration 5000, Loss: 0.4679\n",
      "Iteration 5100, Loss: 0.4672\n",
      "Iteration 5200, Loss: 0.4665\n",
      "Iteration 5300, Loss: 0.4658\n",
      "Iteration 5400, Loss: 0.4652\n",
      "Iteration 5500, Loss: 0.4645\n",
      "Iteration 5600, Loss: 0.4639\n",
      "Iteration 5700, Loss: 0.4633\n",
      "Iteration 5800, Loss: 0.4627\n",
      "Iteration 5900, Loss: 0.4621\n",
      "193 270\n",
      "Iteration 0, Loss: 1.0186\n",
      "Iteration 100, Loss: 0.6047\n",
      "Iteration 200, Loss: 0.5713\n",
      "Iteration 300, Loss: 0.5526\n",
      "Iteration 400, Loss: 0.5397\n",
      "Iteration 500, Loss: 0.5300\n",
      "Iteration 600, Loss: 0.5224\n",
      "Iteration 700, Loss: 0.5160\n",
      "Iteration 800, Loss: 0.5107\n",
      "Iteration 900, Loss: 0.5060\n",
      "Iteration 1000, Loss: 0.5019\n",
      "Iteration 1100, Loss: 0.4982\n",
      "Iteration 1200, Loss: 0.4949\n",
      "Iteration 1300, Loss: 0.4919\n",
      "Iteration 1400, Loss: 0.4891\n",
      "Iteration 1500, Loss: 0.4866\n",
      "Iteration 1600, Loss: 0.4842\n",
      "Iteration 1700, Loss: 0.4820\n",
      "Iteration 1800, Loss: 0.4799\n",
      "Iteration 1900, Loss: 0.4779\n",
      "Iteration 2000, Loss: 0.4761\n",
      "Iteration 2100, Loss: 0.4743\n",
      "Iteration 2200, Loss: 0.4726\n",
      "Iteration 2300, Loss: 0.4710\n",
      "Iteration 2400, Loss: 0.4695\n",
      "Iteration 2500, Loss: 0.4681\n",
      "Iteration 2600, Loss: 0.4666\n",
      "Iteration 2700, Loss: 0.4653\n",
      "Iteration 2800, Loss: 0.4640\n",
      "Iteration 2900, Loss: 0.4628\n",
      "Iteration 3000, Loss: 0.4616\n",
      "Iteration 3100, Loss: 0.4604\n",
      "Iteration 3200, Loss: 0.4593\n",
      "Iteration 3300, Loss: 0.4582\n",
      "Iteration 3400, Loss: 0.4572\n",
      "Iteration 3500, Loss: 0.4561\n",
      "Iteration 3600, Loss: 0.4551\n",
      "Iteration 3700, Loss: 0.4542\n",
      "Iteration 3800, Loss: 0.4533\n",
      "Iteration 3900, Loss: 0.4524\n",
      "Iteration 4000, Loss: 0.4515\n",
      "Iteration 4100, Loss: 0.4506\n",
      "Iteration 4200, Loss: 0.4498\n",
      "Iteration 4300, Loss: 0.4490\n",
      "Iteration 4400, Loss: 0.4482\n",
      "Iteration 4500, Loss: 0.4474\n",
      "Iteration 4600, Loss: 0.4467\n",
      "Iteration 4700, Loss: 0.4459\n",
      "Iteration 4800, Loss: 0.4452\n",
      "Iteration 4900, Loss: 0.4445\n",
      "Iteration 5000, Loss: 0.4438\n",
      "Iteration 5100, Loss: 0.4431\n",
      "Iteration 5200, Loss: 0.4425\n",
      "Iteration 5300, Loss: 0.4418\n",
      "Iteration 5400, Loss: 0.4412\n",
      "Iteration 5500, Loss: 0.4406\n",
      "Iteration 5600, Loss: 0.4400\n",
      "Iteration 5700, Loss: 0.4394\n",
      "Iteration 5800, Loss: 0.4388\n",
      "Iteration 5900, Loss: 0.4382\n",
      "Iteration 0, Loss: 1.0228\n",
      "Iteration 100, Loss: 0.6355\n",
      "Iteration 200, Loss: 0.6045\n",
      "Iteration 300, Loss: 0.5875\n",
      "Iteration 400, Loss: 0.5758\n",
      "Iteration 500, Loss: 0.5668\n",
      "Iteration 600, Loss: 0.5595\n",
      "Iteration 700, Loss: 0.5534\n",
      "Iteration 800, Loss: 0.5482\n",
      "Iteration 900, Loss: 0.5436\n",
      "Iteration 1000, Loss: 0.5395\n",
      "Iteration 1100, Loss: 0.5358\n",
      "Iteration 1200, Loss: 0.5324\n",
      "Iteration 1300, Loss: 0.5293\n",
      "Iteration 1400, Loss: 0.5264\n",
      "Iteration 1500, Loss: 0.5238\n",
      "Iteration 1600, Loss: 0.5213\n",
      "Iteration 1700, Loss: 0.5189\n",
      "Iteration 1800, Loss: 0.5167\n",
      "Iteration 1900, Loss: 0.5147\n",
      "Iteration 2000, Loss: 0.5127\n",
      "Iteration 2100, Loss: 0.5109\n",
      "Iteration 2200, Loss: 0.5091\n",
      "Iteration 2300, Loss: 0.5074\n",
      "Iteration 2400, Loss: 0.5058\n",
      "Iteration 2500, Loss: 0.5042\n",
      "Iteration 2600, Loss: 0.5028\n",
      "Iteration 2700, Loss: 0.5013\n",
      "Iteration 2800, Loss: 0.5000\n",
      "Iteration 2900, Loss: 0.4987\n",
      "Iteration 3000, Loss: 0.4974\n",
      "Iteration 3100, Loss: 0.4962\n",
      "Iteration 3200, Loss: 0.4950\n",
      "Iteration 3300, Loss: 0.4939\n",
      "Iteration 3400, Loss: 0.4928\n",
      "Iteration 3500, Loss: 0.4917\n",
      "Iteration 3600, Loss: 0.4907\n",
      "Iteration 3700, Loss: 0.4897\n",
      "Iteration 3800, Loss: 0.4887\n",
      "Iteration 3900, Loss: 0.4878\n",
      "Iteration 4000, Loss: 0.4869\n",
      "Iteration 4100, Loss: 0.4860\n",
      "Iteration 4200, Loss: 0.4851\n",
      "Iteration 4300, Loss: 0.4843\n",
      "Iteration 4400, Loss: 0.4835\n",
      "Iteration 4500, Loss: 0.4827\n",
      "Iteration 4600, Loss: 0.4819\n",
      "Iteration 4700, Loss: 0.4811\n",
      "Iteration 4800, Loss: 0.4804\n",
      "Iteration 4900, Loss: 0.4797\n",
      "Iteration 5000, Loss: 0.4790\n",
      "Iteration 5100, Loss: 0.4783\n",
      "Iteration 5200, Loss: 0.4776\n",
      "Iteration 5300, Loss: 0.4770\n",
      "Iteration 5400, Loss: 0.4763\n",
      "Iteration 5500, Loss: 0.4757\n",
      "Iteration 5600, Loss: 0.4751\n",
      "Iteration 5700, Loss: 0.4745\n",
      "Iteration 5800, Loss: 0.4739\n",
      "Iteration 5900, Loss: 0.4733\n",
      "194 270\n",
      "Iteration 0, Loss: 1.0189\n",
      "Iteration 100, Loss: 0.6213\n",
      "Iteration 200, Loss: 0.5884\n",
      "Iteration 300, Loss: 0.5698\n",
      "Iteration 400, Loss: 0.5569\n",
      "Iteration 500, Loss: 0.5470\n",
      "Iteration 600, Loss: 0.5391\n",
      "Iteration 700, Loss: 0.5325\n",
      "Iteration 800, Loss: 0.5268\n",
      "Iteration 900, Loss: 0.5219\n",
      "Iteration 1000, Loss: 0.5176\n",
      "Iteration 1100, Loss: 0.5137\n",
      "Iteration 1200, Loss: 0.5102\n",
      "Iteration 1300, Loss: 0.5070\n",
      "Iteration 1400, Loss: 0.5041\n",
      "Iteration 1500, Loss: 0.5014\n",
      "Iteration 1600, Loss: 0.4989\n",
      "Iteration 1700, Loss: 0.4965\n",
      "Iteration 1800, Loss: 0.4943\n",
      "Iteration 1900, Loss: 0.4922\n",
      "Iteration 2000, Loss: 0.4903\n",
      "Iteration 2100, Loss: 0.4884\n",
      "Iteration 2200, Loss: 0.4867\n",
      "Iteration 2300, Loss: 0.4850\n",
      "Iteration 2400, Loss: 0.4834\n",
      "Iteration 2500, Loss: 0.4819\n",
      "Iteration 2600, Loss: 0.4804\n",
      "Iteration 2700, Loss: 0.4790\n",
      "Iteration 2800, Loss: 0.4777\n",
      "Iteration 2900, Loss: 0.4764\n",
      "Iteration 3000, Loss: 0.4752\n",
      "Iteration 3100, Loss: 0.4740\n",
      "Iteration 3200, Loss: 0.4728\n",
      "Iteration 3300, Loss: 0.4717\n",
      "Iteration 3400, Loss: 0.4706\n",
      "Iteration 3500, Loss: 0.4696\n",
      "Iteration 3600, Loss: 0.4686\n",
      "Iteration 3700, Loss: 0.4676\n",
      "Iteration 3800, Loss: 0.4667\n",
      "Iteration 3900, Loss: 0.4658\n",
      "Iteration 4000, Loss: 0.4649\n",
      "Iteration 4100, Loss: 0.4640\n",
      "Iteration 4200, Loss: 0.4631\n",
      "Iteration 4300, Loss: 0.4623\n",
      "Iteration 4400, Loss: 0.4615\n",
      "Iteration 4500, Loss: 0.4607\n",
      "Iteration 4600, Loss: 0.4600\n",
      "Iteration 4700, Loss: 0.4592\n",
      "Iteration 4800, Loss: 0.4585\n",
      "Iteration 4900, Loss: 0.4578\n",
      "Iteration 5000, Loss: 0.4571\n",
      "Iteration 5100, Loss: 0.4564\n",
      "Iteration 5200, Loss: 0.4558\n",
      "Iteration 5300, Loss: 0.4551\n",
      "Iteration 5400, Loss: 0.4545\n",
      "Iteration 5500, Loss: 0.4538\n",
      "Iteration 5600, Loss: 0.4532\n",
      "Iteration 5700, Loss: 0.4526\n",
      "Iteration 5800, Loss: 0.4520\n",
      "Iteration 5900, Loss: 0.4515\n",
      "Iteration 0, Loss: 1.0213\n",
      "Iteration 100, Loss: 0.6219\n",
      "Iteration 200, Loss: 0.5922\n",
      "Iteration 300, Loss: 0.5760\n",
      "Iteration 400, Loss: 0.5648\n",
      "Iteration 500, Loss: 0.5563\n",
      "Iteration 600, Loss: 0.5493\n",
      "Iteration 700, Loss: 0.5436\n",
      "Iteration 800, Loss: 0.5386\n",
      "Iteration 900, Loss: 0.5342\n",
      "Iteration 1000, Loss: 0.5304\n",
      "Iteration 1100, Loss: 0.5269\n",
      "Iteration 1200, Loss: 0.5237\n",
      "Iteration 1300, Loss: 0.5208\n",
      "Iteration 1400, Loss: 0.5181\n",
      "Iteration 1500, Loss: 0.5157\n",
      "Iteration 1600, Loss: 0.5134\n",
      "Iteration 1700, Loss: 0.5112\n",
      "Iteration 1800, Loss: 0.5093\n",
      "Iteration 1900, Loss: 0.5074\n",
      "Iteration 2000, Loss: 0.5056\n",
      "Iteration 2100, Loss: 0.5038\n",
      "Iteration 2200, Loss: 0.5023\n",
      "Iteration 2300, Loss: 0.5007\n",
      "Iteration 2400, Loss: 0.4993\n",
      "Iteration 2500, Loss: 0.4979\n",
      "Iteration 2600, Loss: 0.4966\n",
      "Iteration 2700, Loss: 0.4953\n",
      "Iteration 2800, Loss: 0.4941\n",
      "Iteration 2900, Loss: 0.4929\n",
      "Iteration 3000, Loss: 0.4918\n",
      "Iteration 3100, Loss: 0.4907\n",
      "Iteration 3200, Loss: 0.4896\n",
      "Iteration 3300, Loss: 0.4886\n",
      "Iteration 3400, Loss: 0.4876\n",
      "Iteration 3500, Loss: 0.4867\n",
      "Iteration 3600, Loss: 0.4858\n",
      "Iteration 3700, Loss: 0.4849\n",
      "Iteration 3800, Loss: 0.4840\n",
      "Iteration 3900, Loss: 0.4832\n",
      "Iteration 4000, Loss: 0.4824\n",
      "Iteration 4100, Loss: 0.4816\n",
      "Iteration 4200, Loss: 0.4808\n",
      "Iteration 4300, Loss: 0.4801\n",
      "Iteration 4400, Loss: 0.4793\n",
      "Iteration 4500, Loss: 0.4786\n",
      "Iteration 4600, Loss: 0.4779\n",
      "Iteration 4700, Loss: 0.4772\n",
      "Iteration 4800, Loss: 0.4766\n",
      "Iteration 4900, Loss: 0.4759\n",
      "Iteration 5000, Loss: 0.4753\n",
      "Iteration 5100, Loss: 0.4747\n",
      "Iteration 5200, Loss: 0.4740\n",
      "Iteration 5300, Loss: 0.4735\n",
      "Iteration 5400, Loss: 0.4729\n",
      "Iteration 5500, Loss: 0.4723\n",
      "Iteration 5600, Loss: 0.4717\n",
      "Iteration 5700, Loss: 0.4712\n",
      "Iteration 5800, Loss: 0.4707\n",
      "Iteration 5900, Loss: 0.4701\n",
      "195 270\n",
      "Iteration 0, Loss: 1.0566\n",
      "Iteration 100, Loss: 0.6677\n",
      "Iteration 200, Loss: 0.6355\n",
      "Iteration 300, Loss: 0.6193\n",
      "Iteration 400, Loss: 0.6088\n",
      "Iteration 500, Loss: 0.6010\n",
      "Iteration 600, Loss: 0.5946\n",
      "Iteration 700, Loss: 0.5894\n",
      "Iteration 800, Loss: 0.5848\n",
      "Iteration 900, Loss: 0.5809\n",
      "Iteration 1000, Loss: 0.5774\n",
      "Iteration 1100, Loss: 0.5742\n",
      "Iteration 1200, Loss: 0.5714\n",
      "Iteration 1300, Loss: 0.5688\n",
      "Iteration 1400, Loss: 0.5664\n",
      "Iteration 1500, Loss: 0.5641\n",
      "Iteration 1600, Loss: 0.5621\n",
      "Iteration 1700, Loss: 0.5602\n",
      "Iteration 1800, Loss: 0.5583\n",
      "Iteration 1900, Loss: 0.5566\n",
      "Iteration 2000, Loss: 0.5550\n",
      "Iteration 2100, Loss: 0.5535\n",
      "Iteration 2200, Loss: 0.5520\n",
      "Iteration 2300, Loss: 0.5506\n",
      "Iteration 2400, Loss: 0.5493\n",
      "Iteration 2500, Loss: 0.5480\n",
      "Iteration 2600, Loss: 0.5468\n",
      "Iteration 2700, Loss: 0.5456\n",
      "Iteration 2800, Loss: 0.5445\n",
      "Iteration 2900, Loss: 0.5433\n",
      "Iteration 3000, Loss: 0.5423\n",
      "Iteration 3100, Loss: 0.5413\n",
      "Iteration 3200, Loss: 0.5403\n",
      "Iteration 3300, Loss: 0.5393\n",
      "Iteration 3400, Loss: 0.5384\n",
      "Iteration 3500, Loss: 0.5375\n",
      "Iteration 3600, Loss: 0.5367\n",
      "Iteration 3700, Loss: 0.5358\n",
      "Iteration 3800, Loss: 0.5349\n",
      "Iteration 3900, Loss: 0.5342\n",
      "Iteration 4000, Loss: 0.5334\n",
      "Iteration 4100, Loss: 0.5326\n",
      "Iteration 4200, Loss: 0.5319\n",
      "Iteration 4300, Loss: 0.5312\n",
      "Iteration 4400, Loss: 0.5305\n",
      "Iteration 4500, Loss: 0.5298\n",
      "Iteration 4600, Loss: 0.5291\n",
      "Iteration 4700, Loss: 0.5285\n",
      "Iteration 4800, Loss: 0.5279\n",
      "Iteration 4900, Loss: 0.5272\n",
      "Iteration 5000, Loss: 0.5266\n",
      "Iteration 5100, Loss: 0.5260\n",
      "Iteration 5200, Loss: 0.5254\n",
      "Iteration 5300, Loss: 0.5248\n",
      "Iteration 5400, Loss: 0.5243\n",
      "Iteration 5500, Loss: 0.5237\n",
      "Iteration 5600, Loss: 0.5232\n",
      "Iteration 5700, Loss: 0.5227\n",
      "Iteration 5800, Loss: 0.5222\n",
      "Iteration 5900, Loss: 0.5217\n",
      "Iteration 0, Loss: 1.0538\n",
      "Iteration 100, Loss: 0.6459\n",
      "Iteration 200, Loss: 0.6081\n",
      "Iteration 300, Loss: 0.5878\n",
      "Iteration 400, Loss: 0.5738\n",
      "Iteration 500, Loss: 0.5630\n",
      "Iteration 600, Loss: 0.5544\n",
      "Iteration 700, Loss: 0.5472\n",
      "Iteration 800, Loss: 0.5412\n",
      "Iteration 900, Loss: 0.5359\n",
      "Iteration 1000, Loss: 0.5313\n",
      "Iteration 1100, Loss: 0.5272\n",
      "Iteration 1200, Loss: 0.5235\n",
      "Iteration 1300, Loss: 0.5202\n",
      "Iteration 1400, Loss: 0.5171\n",
      "Iteration 1500, Loss: 0.5143\n",
      "Iteration 1600, Loss: 0.5117\n",
      "Iteration 1700, Loss: 0.5093\n",
      "Iteration 1800, Loss: 0.5071\n",
      "Iteration 1900, Loss: 0.5050\n",
      "Iteration 2000, Loss: 0.5030\n",
      "Iteration 2100, Loss: 0.5012\n",
      "Iteration 2200, Loss: 0.4995\n",
      "Iteration 2300, Loss: 0.4978\n",
      "Iteration 2400, Loss: 0.4963\n",
      "Iteration 2500, Loss: 0.4949\n",
      "Iteration 2600, Loss: 0.4934\n",
      "Iteration 2700, Loss: 0.4921\n",
      "Iteration 2800, Loss: 0.4908\n",
      "Iteration 2900, Loss: 0.4896\n",
      "Iteration 3000, Loss: 0.4884\n",
      "Iteration 3100, Loss: 0.4873\n",
      "Iteration 3200, Loss: 0.4862\n",
      "Iteration 3300, Loss: 0.4852\n",
      "Iteration 3400, Loss: 0.4842\n",
      "Iteration 3500, Loss: 0.4832\n",
      "Iteration 3600, Loss: 0.4823\n",
      "Iteration 3700, Loss: 0.4814\n",
      "Iteration 3800, Loss: 0.4805\n",
      "Iteration 3900, Loss: 0.4797\n",
      "Iteration 4000, Loss: 0.4789\n",
      "Iteration 4100, Loss: 0.4781\n",
      "Iteration 4200, Loss: 0.4774\n",
      "Iteration 4300, Loss: 0.4766\n",
      "Iteration 4400, Loss: 0.4759\n",
      "Iteration 4500, Loss: 0.4752\n",
      "Iteration 4600, Loss: 0.4745\n",
      "Iteration 4700, Loss: 0.4738\n",
      "Iteration 4800, Loss: 0.4732\n",
      "Iteration 4900, Loss: 0.4726\n",
      "Iteration 5000, Loss: 0.4720\n",
      "Iteration 5100, Loss: 0.4714\n",
      "Iteration 5200, Loss: 0.4708\n",
      "Iteration 5300, Loss: 0.4702\n",
      "Iteration 5400, Loss: 0.4696\n",
      "Iteration 5500, Loss: 0.4691\n",
      "Iteration 5600, Loss: 0.4686\n",
      "Iteration 5700, Loss: 0.4680\n",
      "Iteration 5800, Loss: 0.4675\n",
      "Iteration 5900, Loss: 0.4670\n",
      "196 270\n",
      "Iteration 0, Loss: 1.0549\n",
      "Iteration 100, Loss: 0.6665\n",
      "Iteration 200, Loss: 0.6293\n",
      "Iteration 300, Loss: 0.6103\n",
      "Iteration 400, Loss: 0.5975\n",
      "Iteration 500, Loss: 0.5878\n",
      "Iteration 600, Loss: 0.5802\n",
      "Iteration 700, Loss: 0.5738\n",
      "Iteration 800, Loss: 0.5684\n",
      "Iteration 900, Loss: 0.5637\n",
      "Iteration 1000, Loss: 0.5596\n",
      "Iteration 1100, Loss: 0.5559\n",
      "Iteration 1200, Loss: 0.5526\n",
      "Iteration 1300, Loss: 0.5496\n",
      "Iteration 1400, Loss: 0.5469\n",
      "Iteration 1500, Loss: 0.5444\n",
      "Iteration 1600, Loss: 0.5421\n",
      "Iteration 1700, Loss: 0.5398\n",
      "Iteration 1800, Loss: 0.5378\n",
      "Iteration 1900, Loss: 0.5360\n",
      "Iteration 2000, Loss: 0.5342\n",
      "Iteration 2100, Loss: 0.5325\n",
      "Iteration 2200, Loss: 0.5309\n",
      "Iteration 2300, Loss: 0.5294\n",
      "Iteration 2400, Loss: 0.5280\n",
      "Iteration 2500, Loss: 0.5266\n",
      "Iteration 2600, Loss: 0.5253\n",
      "Iteration 2700, Loss: 0.5240\n",
      "Iteration 2800, Loss: 0.5228\n",
      "Iteration 2900, Loss: 0.5217\n",
      "Iteration 3000, Loss: 0.5206\n",
      "Iteration 3100, Loss: 0.5195\n",
      "Iteration 3200, Loss: 0.5185\n",
      "Iteration 3300, Loss: 0.5175\n",
      "Iteration 3400, Loss: 0.5166\n",
      "Iteration 3500, Loss: 0.5157\n",
      "Iteration 3600, Loss: 0.5147\n",
      "Iteration 3700, Loss: 0.5139\n",
      "Iteration 3800, Loss: 0.5130\n",
      "Iteration 3900, Loss: 0.5123\n",
      "Iteration 4000, Loss: 0.5115\n",
      "Iteration 4100, Loss: 0.5107\n",
      "Iteration 4200, Loss: 0.5100\n",
      "Iteration 4300, Loss: 0.5093\n",
      "Iteration 4400, Loss: 0.5086\n",
      "Iteration 4500, Loss: 0.5079\n",
      "Iteration 4600, Loss: 0.5073\n",
      "Iteration 4700, Loss: 0.5066\n",
      "Iteration 4800, Loss: 0.5059\n",
      "Iteration 4900, Loss: 0.5053\n",
      "Iteration 5000, Loss: 0.5047\n",
      "Iteration 5100, Loss: 0.5041\n",
      "Iteration 5200, Loss: 0.5036\n",
      "Iteration 5300, Loss: 0.5030\n",
      "Iteration 5400, Loss: 0.5025\n",
      "Iteration 5500, Loss: 0.5019\n",
      "Iteration 5600, Loss: 0.5014\n",
      "Iteration 5700, Loss: 0.5009\n",
      "Iteration 5800, Loss: 0.5004\n",
      "Iteration 5900, Loss: 0.4999\n",
      "Iteration 0, Loss: 1.0509\n",
      "Iteration 100, Loss: 0.6484\n",
      "Iteration 200, Loss: 0.6153\n",
      "Iteration 300, Loss: 0.5979\n",
      "Iteration 400, Loss: 0.5859\n",
      "Iteration 500, Loss: 0.5768\n",
      "Iteration 600, Loss: 0.5694\n",
      "Iteration 700, Loss: 0.5632\n",
      "Iteration 800, Loss: 0.5580\n",
      "Iteration 900, Loss: 0.5534\n",
      "Iteration 1000, Loss: 0.5491\n",
      "Iteration 1100, Loss: 0.5454\n",
      "Iteration 1200, Loss: 0.5421\n",
      "Iteration 1300, Loss: 0.5391\n",
      "Iteration 1400, Loss: 0.5363\n",
      "Iteration 1500, Loss: 0.5337\n",
      "Iteration 1600, Loss: 0.5313\n",
      "Iteration 1700, Loss: 0.5292\n",
      "Iteration 1800, Loss: 0.5271\n",
      "Iteration 1900, Loss: 0.5252\n",
      "Iteration 2000, Loss: 0.5233\n",
      "Iteration 2100, Loss: 0.5216\n",
      "Iteration 2200, Loss: 0.5200\n",
      "Iteration 2300, Loss: 0.5185\n",
      "Iteration 2400, Loss: 0.5170\n",
      "Iteration 2500, Loss: 0.5157\n",
      "Iteration 2600, Loss: 0.5142\n",
      "Iteration 2700, Loss: 0.5130\n",
      "Iteration 2800, Loss: 0.5118\n",
      "Iteration 2900, Loss: 0.5106\n",
      "Iteration 3000, Loss: 0.5095\n",
      "Iteration 3100, Loss: 0.5084\n",
      "Iteration 3200, Loss: 0.5074\n",
      "Iteration 3300, Loss: 0.5064\n",
      "Iteration 3400, Loss: 0.5054\n",
      "Iteration 3500, Loss: 0.5045\n",
      "Iteration 3600, Loss: 0.5036\n",
      "Iteration 3700, Loss: 0.5027\n",
      "Iteration 3800, Loss: 0.5019\n",
      "Iteration 3900, Loss: 0.5010\n",
      "Iteration 4000, Loss: 0.5003\n",
      "Iteration 4100, Loss: 0.4995\n",
      "Iteration 4200, Loss: 0.4988\n",
      "Iteration 4300, Loss: 0.4980\n",
      "Iteration 4400, Loss: 0.4973\n",
      "Iteration 4500, Loss: 0.4966\n",
      "Iteration 4600, Loss: 0.4959\n",
      "Iteration 4700, Loss: 0.4953\n",
      "Iteration 4800, Loss: 0.4946\n",
      "Iteration 4900, Loss: 0.4940\n",
      "Iteration 5000, Loss: 0.4934\n",
      "Iteration 5100, Loss: 0.4928\n",
      "Iteration 5200, Loss: 0.4922\n",
      "Iteration 5300, Loss: 0.4916\n",
      "Iteration 5400, Loss: 0.4911\n",
      "Iteration 5500, Loss: 0.4905\n",
      "Iteration 5600, Loss: 0.4900\n",
      "Iteration 5700, Loss: 0.4894\n",
      "Iteration 5800, Loss: 0.4890\n",
      "Iteration 5900, Loss: 0.4885\n",
      "197 270\n",
      "Iteration 0, Loss: 1.0563\n",
      "Iteration 100, Loss: 0.6733\n",
      "Iteration 200, Loss: 0.6380\n",
      "Iteration 300, Loss: 0.6196\n",
      "Iteration 400, Loss: 0.6076\n",
      "Iteration 500, Loss: 0.5987\n",
      "Iteration 600, Loss: 0.5917\n",
      "Iteration 700, Loss: 0.5859\n",
      "Iteration 800, Loss: 0.5812\n",
      "Iteration 900, Loss: 0.5770\n",
      "Iteration 1000, Loss: 0.5734\n",
      "Iteration 1100, Loss: 0.5702\n",
      "Iteration 1200, Loss: 0.5674\n",
      "Iteration 1300, Loss: 0.5648\n",
      "Iteration 1400, Loss: 0.5624\n",
      "Iteration 1500, Loss: 0.5603\n",
      "Iteration 1600, Loss: 0.5583\n",
      "Iteration 1700, Loss: 0.5564\n",
      "Iteration 1800, Loss: 0.5546\n",
      "Iteration 1900, Loss: 0.5530\n",
      "Iteration 2000, Loss: 0.5515\n",
      "Iteration 2100, Loss: 0.5502\n",
      "Iteration 2200, Loss: 0.5486\n",
      "Iteration 2300, Loss: 0.5473\n",
      "Iteration 2400, Loss: 0.5461\n",
      "Iteration 2500, Loss: 0.5449\n",
      "Iteration 2600, Loss: 0.5438\n",
      "Iteration 2700, Loss: 0.5427\n",
      "Iteration 2800, Loss: 0.5417\n",
      "Iteration 2900, Loss: 0.5408\n",
      "Iteration 3000, Loss: 0.5398\n",
      "Iteration 3100, Loss: 0.5389\n",
      "Iteration 3200, Loss: 0.5380\n",
      "Iteration 3300, Loss: 0.5371\n",
      "Iteration 3400, Loss: 0.5363\n",
      "Iteration 3500, Loss: 0.5355\n",
      "Iteration 3600, Loss: 0.5348\n",
      "Iteration 3700, Loss: 0.5340\n",
      "Iteration 3800, Loss: 0.5333\n",
      "Iteration 3900, Loss: 0.5326\n",
      "Iteration 4000, Loss: 0.5318\n",
      "Iteration 4100, Loss: 0.5312\n",
      "Iteration 4200, Loss: 0.5305\n",
      "Iteration 4300, Loss: 0.5299\n",
      "Iteration 4400, Loss: 0.5292\n",
      "Iteration 4500, Loss: 0.5287\n",
      "Iteration 4600, Loss: 0.5281\n",
      "Iteration 4700, Loss: 0.5275\n",
      "Iteration 4800, Loss: 0.5270\n",
      "Iteration 4900, Loss: 0.5264\n",
      "Iteration 5000, Loss: 0.5259\n",
      "Iteration 5100, Loss: 0.5254\n",
      "Iteration 5200, Loss: 0.5249\n",
      "Iteration 5300, Loss: 0.5244\n",
      "Iteration 5400, Loss: 0.5239\n",
      "Iteration 5500, Loss: 0.5234\n",
      "Iteration 5600, Loss: 0.5230\n",
      "Iteration 5700, Loss: 0.5225\n",
      "Iteration 5800, Loss: 0.5221\n",
      "Iteration 5900, Loss: 0.5216\n",
      "Iteration 0, Loss: 1.0512\n",
      "Iteration 100, Loss: 0.6416\n",
      "Iteration 200, Loss: 0.6069\n",
      "Iteration 300, Loss: 0.5893\n",
      "Iteration 400, Loss: 0.5773\n",
      "Iteration 500, Loss: 0.5684\n",
      "Iteration 600, Loss: 0.5610\n",
      "Iteration 700, Loss: 0.5548\n",
      "Iteration 800, Loss: 0.5496\n",
      "Iteration 900, Loss: 0.5451\n",
      "Iteration 1000, Loss: 0.5411\n",
      "Iteration 1100, Loss: 0.5375\n",
      "Iteration 1200, Loss: 0.5343\n",
      "Iteration 1300, Loss: 0.5313\n",
      "Iteration 1400, Loss: 0.5286\n",
      "Iteration 1500, Loss: 0.5261\n",
      "Iteration 1600, Loss: 0.5237\n",
      "Iteration 1700, Loss: 0.5216\n",
      "Iteration 1800, Loss: 0.5196\n",
      "Iteration 1900, Loss: 0.5178\n",
      "Iteration 2000, Loss: 0.5159\n",
      "Iteration 2100, Loss: 0.5142\n",
      "Iteration 2200, Loss: 0.5127\n",
      "Iteration 2300, Loss: 0.5112\n",
      "Iteration 2400, Loss: 0.5097\n",
      "Iteration 2500, Loss: 0.5084\n",
      "Iteration 2600, Loss: 0.5070\n",
      "Iteration 2700, Loss: 0.5058\n",
      "Iteration 2800, Loss: 0.5046\n",
      "Iteration 2900, Loss: 0.5034\n",
      "Iteration 3000, Loss: 0.5024\n",
      "Iteration 3100, Loss: 0.5013\n",
      "Iteration 3200, Loss: 0.5003\n",
      "Iteration 3300, Loss: 0.4993\n",
      "Iteration 3400, Loss: 0.4984\n",
      "Iteration 3500, Loss: 0.4974\n",
      "Iteration 3600, Loss: 0.4966\n",
      "Iteration 3700, Loss: 0.4957\n",
      "Iteration 3800, Loss: 0.4949\n",
      "Iteration 3900, Loss: 0.4942\n",
      "Iteration 4000, Loss: 0.4933\n",
      "Iteration 4100, Loss: 0.4926\n",
      "Iteration 4200, Loss: 0.4919\n",
      "Iteration 4300, Loss: 0.4911\n",
      "Iteration 4400, Loss: 0.4904\n",
      "Iteration 4500, Loss: 0.4898\n",
      "Iteration 4600, Loss: 0.4891\n",
      "Iteration 4700, Loss: 0.4885\n",
      "Iteration 4800, Loss: 0.4879\n",
      "Iteration 4900, Loss: 0.4873\n",
      "Iteration 5000, Loss: 0.4866\n",
      "Iteration 5100, Loss: 0.4861\n",
      "Iteration 5200, Loss: 0.4855\n",
      "Iteration 5300, Loss: 0.4850\n",
      "Iteration 5400, Loss: 0.4845\n",
      "Iteration 5500, Loss: 0.4839\n",
      "Iteration 5600, Loss: 0.4834\n",
      "Iteration 5700, Loss: 0.4829\n",
      "Iteration 5800, Loss: 0.4824\n",
      "Iteration 5900, Loss: 0.4819\n",
      "198 270\n",
      "Iteration 0, Loss: 1.0521\n",
      "Iteration 100, Loss: 0.6528\n",
      "Iteration 200, Loss: 0.6191\n",
      "Iteration 300, Loss: 0.6019\n",
      "Iteration 400, Loss: 0.5902\n",
      "Iteration 500, Loss: 0.5817\n",
      "Iteration 600, Loss: 0.5746\n",
      "Iteration 700, Loss: 0.5688\n",
      "Iteration 800, Loss: 0.5640\n",
      "Iteration 900, Loss: 0.5597\n",
      "Iteration 1000, Loss: 0.5560\n",
      "Iteration 1100, Loss: 0.5527\n",
      "Iteration 1200, Loss: 0.5497\n",
      "Iteration 1300, Loss: 0.5470\n",
      "Iteration 1400, Loss: 0.5444\n",
      "Iteration 1500, Loss: 0.5421\n",
      "Iteration 1600, Loss: 0.5401\n",
      "Iteration 1700, Loss: 0.5381\n",
      "Iteration 1800, Loss: 0.5363\n",
      "Iteration 1900, Loss: 0.5346\n",
      "Iteration 2000, Loss: 0.5330\n",
      "Iteration 2100, Loss: 0.5315\n",
      "Iteration 2200, Loss: 0.5300\n",
      "Iteration 2300, Loss: 0.5287\n",
      "Iteration 2400, Loss: 0.5274\n",
      "Iteration 2500, Loss: 0.5261\n",
      "Iteration 2600, Loss: 0.5250\n",
      "Iteration 2700, Loss: 0.5239\n",
      "Iteration 2800, Loss: 0.5228\n",
      "Iteration 2900, Loss: 0.5218\n",
      "Iteration 3000, Loss: 0.5208\n",
      "Iteration 3100, Loss: 0.5198\n",
      "Iteration 3200, Loss: 0.5189\n",
      "Iteration 3300, Loss: 0.5180\n",
      "Iteration 3400, Loss: 0.5172\n",
      "Iteration 3500, Loss: 0.5164\n",
      "Iteration 3600, Loss: 0.5155\n",
      "Iteration 3700, Loss: 0.5148\n",
      "Iteration 3800, Loss: 0.5140\n",
      "Iteration 3900, Loss: 0.5134\n",
      "Iteration 4000, Loss: 0.5127\n",
      "Iteration 4100, Loss: 0.5120\n",
      "Iteration 4200, Loss: 0.5113\n",
      "Iteration 4300, Loss: 0.5107\n",
      "Iteration 4400, Loss: 0.5100\n",
      "Iteration 4500, Loss: 0.5094\n",
      "Iteration 4600, Loss: 0.5088\n",
      "Iteration 4700, Loss: 0.5083\n",
      "Iteration 4800, Loss: 0.5077\n",
      "Iteration 4900, Loss: 0.5072\n",
      "Iteration 5000, Loss: 0.5067\n",
      "Iteration 5100, Loss: 0.5062\n",
      "Iteration 5200, Loss: 0.5056\n",
      "Iteration 5300, Loss: 0.5051\n",
      "Iteration 5400, Loss: 0.5047\n",
      "Iteration 5500, Loss: 0.5042\n",
      "Iteration 5600, Loss: 0.5038\n",
      "Iteration 5700, Loss: 0.5033\n",
      "Iteration 5800, Loss: 0.5028\n",
      "Iteration 5900, Loss: 0.5024\n",
      "Iteration 0, Loss: 1.0523\n",
      "Iteration 100, Loss: 0.6612\n",
      "Iteration 200, Loss: 0.6256\n",
      "Iteration 300, Loss: 0.6072\n",
      "Iteration 400, Loss: 0.5950\n",
      "Iteration 500, Loss: 0.5859\n",
      "Iteration 600, Loss: 0.5787\n",
      "Iteration 700, Loss: 0.5729\n",
      "Iteration 800, Loss: 0.5678\n",
      "Iteration 900, Loss: 0.5634\n",
      "Iteration 1000, Loss: 0.5596\n",
      "Iteration 1100, Loss: 0.5562\n",
      "Iteration 1200, Loss: 0.5531\n",
      "Iteration 1300, Loss: 0.5503\n",
      "Iteration 1400, Loss: 0.5477\n",
      "Iteration 1500, Loss: 0.5453\n",
      "Iteration 1600, Loss: 0.5432\n",
      "Iteration 1700, Loss: 0.5411\n",
      "Iteration 1800, Loss: 0.5392\n",
      "Iteration 1900, Loss: 0.5375\n",
      "Iteration 2000, Loss: 0.5358\n",
      "Iteration 2100, Loss: 0.5342\n",
      "Iteration 2200, Loss: 0.5327\n",
      "Iteration 2300, Loss: 0.5313\n",
      "Iteration 2400, Loss: 0.5299\n",
      "Iteration 2500, Loss: 0.5286\n",
      "Iteration 2600, Loss: 0.5274\n",
      "Iteration 2700, Loss: 0.5262\n",
      "Iteration 2800, Loss: 0.5250\n",
      "Iteration 2900, Loss: 0.5239\n",
      "Iteration 3000, Loss: 0.5228\n",
      "Iteration 3100, Loss: 0.5218\n",
      "Iteration 3200, Loss: 0.5208\n",
      "Iteration 3300, Loss: 0.5198\n",
      "Iteration 3400, Loss: 0.5189\n",
      "Iteration 3500, Loss: 0.5180\n",
      "Iteration 3600, Loss: 0.5171\n",
      "Iteration 3700, Loss: 0.5163\n",
      "Iteration 3800, Loss: 0.5155\n",
      "Iteration 3900, Loss: 0.5147\n",
      "Iteration 4000, Loss: 0.5139\n",
      "Iteration 4100, Loss: 0.5132\n",
      "Iteration 4200, Loss: 0.5124\n",
      "Iteration 4300, Loss: 0.5117\n",
      "Iteration 4400, Loss: 0.5110\n",
      "Iteration 4500, Loss: 0.5104\n",
      "Iteration 4600, Loss: 0.5097\n",
      "Iteration 4700, Loss: 0.5091\n",
      "Iteration 4800, Loss: 0.5085\n",
      "Iteration 4900, Loss: 0.5079\n",
      "Iteration 5000, Loss: 0.5073\n",
      "Iteration 5100, Loss: 0.5067\n",
      "Iteration 5200, Loss: 0.5061\n",
      "Iteration 5300, Loss: 0.5056\n",
      "Iteration 5400, Loss: 0.5051\n",
      "Iteration 5500, Loss: 0.5045\n",
      "Iteration 5600, Loss: 0.5040\n",
      "Iteration 5700, Loss: 0.5035\n",
      "Iteration 5800, Loss: 0.5030\n",
      "Iteration 5900, Loss: 0.5025\n",
      "199 270\n",
      "Iteration 0, Loss: 1.0504\n",
      "Iteration 100, Loss: 0.6512\n",
      "Iteration 200, Loss: 0.6176\n",
      "Iteration 300, Loss: 0.6004\n",
      "Iteration 400, Loss: 0.5892\n",
      "Iteration 500, Loss: 0.5809\n",
      "Iteration 600, Loss: 0.5744\n",
      "Iteration 700, Loss: 0.5692\n",
      "Iteration 800, Loss: 0.5647\n",
      "Iteration 900, Loss: 0.5610\n",
      "Iteration 1000, Loss: 0.5576\n",
      "Iteration 1100, Loss: 0.5547\n",
      "Iteration 1200, Loss: 0.5521\n",
      "Iteration 1300, Loss: 0.5497\n",
      "Iteration 1400, Loss: 0.5475\n",
      "Iteration 1500, Loss: 0.5456\n",
      "Iteration 1600, Loss: 0.5437\n",
      "Iteration 1700, Loss: 0.5420\n",
      "Iteration 1800, Loss: 0.5405\n",
      "Iteration 1900, Loss: 0.5389\n",
      "Iteration 2000, Loss: 0.5374\n",
      "Iteration 2100, Loss: 0.5361\n",
      "Iteration 2200, Loss: 0.5349\n",
      "Iteration 2300, Loss: 0.5336\n",
      "Iteration 2400, Loss: 0.5324\n",
      "Iteration 2500, Loss: 0.5313\n",
      "Iteration 2600, Loss: 0.5303\n",
      "Iteration 2700, Loss: 0.5293\n",
      "Iteration 2800, Loss: 0.5283\n",
      "Iteration 2900, Loss: 0.5275\n",
      "Iteration 3000, Loss: 0.5266\n",
      "Iteration 3100, Loss: 0.5257\n",
      "Iteration 3200, Loss: 0.5249\n",
      "Iteration 3300, Loss: 0.5241\n",
      "Iteration 3400, Loss: 0.5233\n",
      "Iteration 3500, Loss: 0.5226\n",
      "Iteration 3600, Loss: 0.5218\n",
      "Iteration 3700, Loss: 0.5211\n",
      "Iteration 3800, Loss: 0.5204\n",
      "Iteration 3900, Loss: 0.5198\n",
      "Iteration 4000, Loss: 0.5191\n",
      "Iteration 4100, Loss: 0.5185\n",
      "Iteration 4200, Loss: 0.5178\n",
      "Iteration 4300, Loss: 0.5172\n",
      "Iteration 4400, Loss: 0.5167\n",
      "Iteration 4500, Loss: 0.5161\n",
      "Iteration 4600, Loss: 0.5155\n",
      "Iteration 4700, Loss: 0.5150\n",
      "Iteration 4800, Loss: 0.5145\n",
      "Iteration 4900, Loss: 0.5141\n",
      "Iteration 5000, Loss: 0.5135\n",
      "Iteration 5100, Loss: 0.5130\n",
      "Iteration 5200, Loss: 0.5125\n",
      "Iteration 5300, Loss: 0.5120\n",
      "Iteration 5400, Loss: 0.5115\n",
      "Iteration 5500, Loss: 0.5111\n",
      "Iteration 5600, Loss: 0.5107\n",
      "Iteration 5700, Loss: 0.5102\n",
      "Iteration 5800, Loss: 0.5098\n",
      "Iteration 5900, Loss: 0.5094\n",
      "Iteration 0, Loss: 1.0586\n",
      "Iteration 100, Loss: 0.6654\n",
      "Iteration 200, Loss: 0.6293\n",
      "Iteration 300, Loss: 0.6108\n",
      "Iteration 400, Loss: 0.5980\n",
      "Iteration 500, Loss: 0.5883\n",
      "Iteration 600, Loss: 0.5805\n",
      "Iteration 700, Loss: 0.5740\n",
      "Iteration 800, Loss: 0.5684\n",
      "Iteration 900, Loss: 0.5635\n",
      "Iteration 1000, Loss: 0.5593\n",
      "Iteration 1100, Loss: 0.5556\n",
      "Iteration 1200, Loss: 0.5521\n",
      "Iteration 1300, Loss: 0.5491\n",
      "Iteration 1400, Loss: 0.5462\n",
      "Iteration 1500, Loss: 0.5436\n",
      "Iteration 1600, Loss: 0.5412\n",
      "Iteration 1700, Loss: 0.5390\n",
      "Iteration 1800, Loss: 0.5369\n",
      "Iteration 1900, Loss: 0.5350\n",
      "Iteration 2000, Loss: 0.5332\n",
      "Iteration 2100, Loss: 0.5315\n",
      "Iteration 2200, Loss: 0.5299\n",
      "Iteration 2300, Loss: 0.5283\n",
      "Iteration 2400, Loss: 0.5268\n",
      "Iteration 2500, Loss: 0.5255\n",
      "Iteration 2600, Loss: 0.5242\n",
      "Iteration 2700, Loss: 0.5229\n",
      "Iteration 2800, Loss: 0.5217\n",
      "Iteration 2900, Loss: 0.5206\n",
      "Iteration 3000, Loss: 0.5195\n",
      "Iteration 3100, Loss: 0.5184\n",
      "Iteration 3200, Loss: 0.5174\n",
      "Iteration 3300, Loss: 0.5165\n",
      "Iteration 3400, Loss: 0.5155\n",
      "Iteration 3500, Loss: 0.5146\n",
      "Iteration 3600, Loss: 0.5137\n",
      "Iteration 3700, Loss: 0.5129\n",
      "Iteration 3800, Loss: 0.5121\n",
      "Iteration 3900, Loss: 0.5112\n",
      "Iteration 4000, Loss: 0.5105\n",
      "Iteration 4100, Loss: 0.5097\n",
      "Iteration 4200, Loss: 0.5089\n",
      "Iteration 4300, Loss: 0.5082\n",
      "Iteration 4400, Loss: 0.5076\n",
      "Iteration 4500, Loss: 0.5069\n",
      "Iteration 4600, Loss: 0.5062\n",
      "Iteration 4700, Loss: 0.5056\n",
      "Iteration 4800, Loss: 0.5050\n",
      "Iteration 4900, Loss: 0.5044\n",
      "Iteration 5000, Loss: 0.5038\n",
      "Iteration 5100, Loss: 0.5032\n",
      "Iteration 5200, Loss: 0.5026\n",
      "Iteration 5300, Loss: 0.5021\n",
      "Iteration 5400, Loss: 0.5015\n",
      "Iteration 5500, Loss: 0.5010\n",
      "Iteration 5600, Loss: 0.5005\n",
      "Iteration 5700, Loss: 0.5000\n",
      "Iteration 5800, Loss: 0.4995\n",
      "Iteration 5900, Loss: 0.4991\n",
      "200 270\n",
      "Iteration 0, Loss: 1.0539\n",
      "Iteration 100, Loss: 0.6622\n",
      "Iteration 200, Loss: 0.6279\n",
      "Iteration 300, Loss: 0.6098\n",
      "Iteration 400, Loss: 0.5970\n",
      "Iteration 500, Loss: 0.5874\n",
      "Iteration 600, Loss: 0.5797\n",
      "Iteration 700, Loss: 0.5732\n",
      "Iteration 800, Loss: 0.5676\n",
      "Iteration 900, Loss: 0.5628\n",
      "Iteration 1000, Loss: 0.5585\n",
      "Iteration 1100, Loss: 0.5547\n",
      "Iteration 1200, Loss: 0.5513\n",
      "Iteration 1300, Loss: 0.5481\n",
      "Iteration 1400, Loss: 0.5452\n",
      "Iteration 1500, Loss: 0.5425\n",
      "Iteration 1600, Loss: 0.5401\n",
      "Iteration 1700, Loss: 0.5377\n",
      "Iteration 1800, Loss: 0.5356\n",
      "Iteration 1900, Loss: 0.5335\n",
      "Iteration 2000, Loss: 0.5316\n",
      "Iteration 2100, Loss: 0.5298\n",
      "Iteration 2200, Loss: 0.5280\n",
      "Iteration 2300, Loss: 0.5264\n",
      "Iteration 2400, Loss: 0.5248\n",
      "Iteration 2500, Loss: 0.5234\n",
      "Iteration 2600, Loss: 0.5219\n",
      "Iteration 2700, Loss: 0.5205\n",
      "Iteration 2800, Loss: 0.5192\n",
      "Iteration 2900, Loss: 0.5179\n",
      "Iteration 3000, Loss: 0.5167\n",
      "Iteration 3100, Loss: 0.5155\n",
      "Iteration 3200, Loss: 0.5144\n",
      "Iteration 3300, Loss: 0.5133\n",
      "Iteration 3400, Loss: 0.5122\n",
      "Iteration 3500, Loss: 0.5111\n",
      "Iteration 3600, Loss: 0.5101\n",
      "Iteration 3700, Loss: 0.5092\n",
      "Iteration 3800, Loss: 0.5082\n",
      "Iteration 3900, Loss: 0.5073\n",
      "Iteration 4000, Loss: 0.5064\n",
      "Iteration 4100, Loss: 0.5056\n",
      "Iteration 4200, Loss: 0.5048\n",
      "Iteration 4300, Loss: 0.5039\n",
      "Iteration 4400, Loss: 0.5031\n",
      "Iteration 4500, Loss: 0.5023\n",
      "Iteration 4600, Loss: 0.5016\n",
      "Iteration 4700, Loss: 0.5008\n",
      "Iteration 4800, Loss: 0.5001\n",
      "Iteration 4900, Loss: 0.4994\n",
      "Iteration 5000, Loss: 0.4987\n",
      "Iteration 5100, Loss: 0.4980\n",
      "Iteration 5200, Loss: 0.4974\n",
      "Iteration 5300, Loss: 0.4967\n",
      "Iteration 5400, Loss: 0.4961\n",
      "Iteration 5500, Loss: 0.4954\n",
      "Iteration 5600, Loss: 0.4948\n",
      "Iteration 5700, Loss: 0.4942\n",
      "Iteration 5800, Loss: 0.4936\n",
      "Iteration 5900, Loss: 0.4930\n",
      "Iteration 0, Loss: 1.0519\n",
      "Iteration 100, Loss: 0.6523\n",
      "Iteration 200, Loss: 0.6162\n",
      "Iteration 300, Loss: 0.5975\n",
      "Iteration 400, Loss: 0.5847\n",
      "Iteration 500, Loss: 0.5751\n",
      "Iteration 600, Loss: 0.5673\n",
      "Iteration 700, Loss: 0.5608\n",
      "Iteration 800, Loss: 0.5553\n",
      "Iteration 900, Loss: 0.5504\n",
      "Iteration 1000, Loss: 0.5460\n",
      "Iteration 1100, Loss: 0.5421\n",
      "Iteration 1200, Loss: 0.5385\n",
      "Iteration 1300, Loss: 0.5352\n",
      "Iteration 1400, Loss: 0.5323\n",
      "Iteration 1500, Loss: 0.5294\n",
      "Iteration 1600, Loss: 0.5268\n",
      "Iteration 1700, Loss: 0.5243\n",
      "Iteration 1800, Loss: 0.5220\n",
      "Iteration 1900, Loss: 0.5199\n",
      "Iteration 2000, Loss: 0.5178\n",
      "Iteration 2100, Loss: 0.5159\n",
      "Iteration 2200, Loss: 0.5140\n",
      "Iteration 2300, Loss: 0.5122\n",
      "Iteration 2400, Loss: 0.5106\n",
      "Iteration 2500, Loss: 0.5089\n",
      "Iteration 2600, Loss: 0.5074\n",
      "Iteration 2700, Loss: 0.5059\n",
      "Iteration 2800, Loss: 0.5045\n",
      "Iteration 2900, Loss: 0.5031\n",
      "Iteration 3000, Loss: 0.5018\n",
      "Iteration 3100, Loss: 0.5005\n",
      "Iteration 3200, Loss: 0.4992\n",
      "Iteration 3300, Loss: 0.4980\n",
      "Iteration 3400, Loss: 0.4969\n",
      "Iteration 3500, Loss: 0.4958\n",
      "Iteration 3600, Loss: 0.4947\n",
      "Iteration 3700, Loss: 0.4936\n",
      "Iteration 3800, Loss: 0.4925\n",
      "Iteration 3900, Loss: 0.4915\n",
      "Iteration 4000, Loss: 0.4906\n",
      "Iteration 4100, Loss: 0.4896\n",
      "Iteration 4200, Loss: 0.4886\n",
      "Iteration 4300, Loss: 0.4877\n",
      "Iteration 4400, Loss: 0.4869\n",
      "Iteration 4500, Loss: 0.4860\n",
      "Iteration 4600, Loss: 0.4851\n",
      "Iteration 4700, Loss: 0.4843\n",
      "Iteration 4800, Loss: 0.4835\n",
      "Iteration 4900, Loss: 0.4827\n",
      "Iteration 5000, Loss: 0.4820\n",
      "Iteration 5100, Loss: 0.4812\n",
      "Iteration 5200, Loss: 0.4804\n",
      "Iteration 5300, Loss: 0.4797\n",
      "Iteration 5400, Loss: 0.4790\n",
      "Iteration 5500, Loss: 0.4783\n",
      "Iteration 5600, Loss: 0.4776\n",
      "Iteration 5700, Loss: 0.4769\n",
      "Iteration 5800, Loss: 0.4762\n",
      "Iteration 5900, Loss: 0.4756\n",
      "201 270\n",
      "Iteration 0, Loss: 1.0575\n",
      "Iteration 100, Loss: 0.6693\n",
      "Iteration 200, Loss: 0.6302\n",
      "Iteration 300, Loss: 0.6092\n",
      "Iteration 400, Loss: 0.5952\n",
      "Iteration 500, Loss: 0.5845\n",
      "Iteration 600, Loss: 0.5760\n",
      "Iteration 700, Loss: 0.5690\n",
      "Iteration 800, Loss: 0.5630\n",
      "Iteration 900, Loss: 0.5578\n",
      "Iteration 1000, Loss: 0.5533\n",
      "Iteration 1100, Loss: 0.5492\n",
      "Iteration 1200, Loss: 0.5456\n",
      "Iteration 1300, Loss: 0.5423\n",
      "Iteration 1400, Loss: 0.5392\n",
      "Iteration 1500, Loss: 0.5364\n",
      "Iteration 1600, Loss: 0.5338\n",
      "Iteration 1700, Loss: 0.5314\n",
      "Iteration 1800, Loss: 0.5291\n",
      "Iteration 1900, Loss: 0.5270\n",
      "Iteration 2000, Loss: 0.5250\n",
      "Iteration 2100, Loss: 0.5231\n",
      "Iteration 2200, Loss: 0.5213\n",
      "Iteration 2300, Loss: 0.5196\n",
      "Iteration 2400, Loss: 0.5179\n",
      "Iteration 2500, Loss: 0.5164\n",
      "Iteration 2600, Loss: 0.5149\n",
      "Iteration 2700, Loss: 0.5134\n",
      "Iteration 2800, Loss: 0.5120\n",
      "Iteration 2900, Loss: 0.5107\n",
      "Iteration 3000, Loss: 0.5094\n",
      "Iteration 3100, Loss: 0.5082\n",
      "Iteration 3200, Loss: 0.5070\n",
      "Iteration 3300, Loss: 0.5059\n",
      "Iteration 3400, Loss: 0.5047\n",
      "Iteration 3500, Loss: 0.5037\n",
      "Iteration 3600, Loss: 0.5026\n",
      "Iteration 3700, Loss: 0.5016\n",
      "Iteration 3800, Loss: 0.5006\n",
      "Iteration 3900, Loss: 0.4997\n",
      "Iteration 4000, Loss: 0.4987\n",
      "Iteration 4100, Loss: 0.4978\n",
      "Iteration 4200, Loss: 0.4970\n",
      "Iteration 4300, Loss: 0.4961\n",
      "Iteration 4400, Loss: 0.4953\n",
      "Iteration 4500, Loss: 0.4944\n",
      "Iteration 4600, Loss: 0.4936\n",
      "Iteration 4700, Loss: 0.4929\n",
      "Iteration 4800, Loss: 0.4921\n",
      "Iteration 4900, Loss: 0.4913\n",
      "Iteration 5000, Loss: 0.4906\n",
      "Iteration 5100, Loss: 0.4899\n",
      "Iteration 5200, Loss: 0.4892\n",
      "Iteration 5300, Loss: 0.4885\n",
      "Iteration 5400, Loss: 0.4879\n",
      "Iteration 5500, Loss: 0.4872\n",
      "Iteration 5600, Loss: 0.4866\n",
      "Iteration 5700, Loss: 0.4859\n",
      "Iteration 5800, Loss: 0.4853\n",
      "Iteration 5900, Loss: 0.4847\n",
      "Iteration 0, Loss: 1.0503\n",
      "Iteration 100, Loss: 0.6421\n",
      "Iteration 200, Loss: 0.6101\n",
      "Iteration 300, Loss: 0.5927\n",
      "Iteration 400, Loss: 0.5807\n",
      "Iteration 500, Loss: 0.5715\n",
      "Iteration 600, Loss: 0.5638\n",
      "Iteration 700, Loss: 0.5574\n",
      "Iteration 800, Loss: 0.5520\n",
      "Iteration 900, Loss: 0.5472\n",
      "Iteration 1000, Loss: 0.5430\n",
      "Iteration 1100, Loss: 0.5392\n",
      "Iteration 1200, Loss: 0.5357\n",
      "Iteration 1300, Loss: 0.5326\n",
      "Iteration 1400, Loss: 0.5297\n",
      "Iteration 1500, Loss: 0.5270\n",
      "Iteration 1600, Loss: 0.5246\n",
      "Iteration 1700, Loss: 0.5223\n",
      "Iteration 1800, Loss: 0.5201\n",
      "Iteration 1900, Loss: 0.5181\n",
      "Iteration 2000, Loss: 0.5162\n",
      "Iteration 2100, Loss: 0.5144\n",
      "Iteration 2200, Loss: 0.5126\n",
      "Iteration 2300, Loss: 0.5110\n",
      "Iteration 2400, Loss: 0.5094\n",
      "Iteration 2500, Loss: 0.5079\n",
      "Iteration 2600, Loss: 0.5064\n",
      "Iteration 2700, Loss: 0.5050\n",
      "Iteration 2800, Loss: 0.5037\n",
      "Iteration 2900, Loss: 0.5024\n",
      "Iteration 3000, Loss: 0.5011\n",
      "Iteration 3100, Loss: 0.5000\n",
      "Iteration 3200, Loss: 0.4988\n",
      "Iteration 3300, Loss: 0.4977\n",
      "Iteration 3400, Loss: 0.4966\n",
      "Iteration 3500, Loss: 0.4956\n",
      "Iteration 3600, Loss: 0.4946\n",
      "Iteration 3700, Loss: 0.4936\n",
      "Iteration 3800, Loss: 0.4927\n",
      "Iteration 3900, Loss: 0.4917\n",
      "Iteration 4000, Loss: 0.4908\n",
      "Iteration 4100, Loss: 0.4900\n",
      "Iteration 4200, Loss: 0.4891\n",
      "Iteration 4300, Loss: 0.4882\n",
      "Iteration 4400, Loss: 0.4874\n",
      "Iteration 4500, Loss: 0.4866\n",
      "Iteration 4600, Loss: 0.4858\n",
      "Iteration 4700, Loss: 0.4851\n",
      "Iteration 4800, Loss: 0.4843\n",
      "Iteration 4900, Loss: 0.4836\n",
      "Iteration 5000, Loss: 0.4829\n",
      "Iteration 5100, Loss: 0.4822\n",
      "Iteration 5200, Loss: 0.4815\n",
      "Iteration 5300, Loss: 0.4809\n",
      "Iteration 5400, Loss: 0.4802\n",
      "Iteration 5500, Loss: 0.4796\n",
      "Iteration 5600, Loss: 0.4789\n",
      "Iteration 5700, Loss: 0.4783\n",
      "Iteration 5800, Loss: 0.4777\n",
      "Iteration 5900, Loss: 0.4771\n",
      "202 270\n",
      "Iteration 0, Loss: 1.0505\n",
      "Iteration 100, Loss: 0.6498\n",
      "Iteration 200, Loss: 0.6161\n",
      "Iteration 300, Loss: 0.5992\n",
      "Iteration 400, Loss: 0.5874\n",
      "Iteration 500, Loss: 0.5785\n",
      "Iteration 600, Loss: 0.5713\n",
      "Iteration 700, Loss: 0.5652\n",
      "Iteration 800, Loss: 0.5599\n",
      "Iteration 900, Loss: 0.5553\n",
      "Iteration 1000, Loss: 0.5513\n",
      "Iteration 1100, Loss: 0.5476\n",
      "Iteration 1200, Loss: 0.5443\n",
      "Iteration 1300, Loss: 0.5412\n",
      "Iteration 1400, Loss: 0.5385\n",
      "Iteration 1500, Loss: 0.5359\n",
      "Iteration 1600, Loss: 0.5335\n",
      "Iteration 1700, Loss: 0.5313\n",
      "Iteration 1800, Loss: 0.5292\n",
      "Iteration 1900, Loss: 0.5272\n",
      "Iteration 2000, Loss: 0.5253\n",
      "Iteration 2100, Loss: 0.5236\n",
      "Iteration 2200, Loss: 0.5219\n",
      "Iteration 2300, Loss: 0.5203\n",
      "Iteration 2400, Loss: 0.5187\n",
      "Iteration 2500, Loss: 0.5173\n",
      "Iteration 2600, Loss: 0.5159\n",
      "Iteration 2700, Loss: 0.5145\n",
      "Iteration 2800, Loss: 0.5132\n",
      "Iteration 2900, Loss: 0.5120\n",
      "Iteration 3000, Loss: 0.5108\n",
      "Iteration 3100, Loss: 0.5096\n",
      "Iteration 3200, Loss: 0.5086\n",
      "Iteration 3300, Loss: 0.5074\n",
      "Iteration 3400, Loss: 0.5064\n",
      "Iteration 3500, Loss: 0.5054\n",
      "Iteration 3600, Loss: 0.5044\n",
      "Iteration 3700, Loss: 0.5035\n",
      "Iteration 3800, Loss: 0.5025\n",
      "Iteration 3900, Loss: 0.5016\n",
      "Iteration 4000, Loss: 0.5007\n",
      "Iteration 4100, Loss: 0.4999\n",
      "Iteration 4200, Loss: 0.4990\n",
      "Iteration 4300, Loss: 0.4982\n",
      "Iteration 4400, Loss: 0.4974\n",
      "Iteration 4500, Loss: 0.4966\n",
      "Iteration 4600, Loss: 0.4959\n",
      "Iteration 4700, Loss: 0.4951\n",
      "Iteration 4800, Loss: 0.4944\n",
      "Iteration 4900, Loss: 0.4937\n",
      "Iteration 5000, Loss: 0.4930\n",
      "Iteration 5100, Loss: 0.4923\n",
      "Iteration 5200, Loss: 0.4916\n",
      "Iteration 5300, Loss: 0.4910\n",
      "Iteration 5400, Loss: 0.4903\n",
      "Iteration 5500, Loss: 0.4898\n",
      "Iteration 5600, Loss: 0.4891\n",
      "Iteration 5700, Loss: 0.4885\n",
      "Iteration 5800, Loss: 0.4879\n",
      "Iteration 5900, Loss: 0.4873\n",
      "Iteration 0, Loss: 1.0529\n",
      "Iteration 100, Loss: 0.6628\n",
      "Iteration 200, Loss: 0.6262\n",
      "Iteration 300, Loss: 0.6065\n",
      "Iteration 400, Loss: 0.5932\n",
      "Iteration 500, Loss: 0.5834\n",
      "Iteration 600, Loss: 0.5754\n",
      "Iteration 700, Loss: 0.5688\n",
      "Iteration 800, Loss: 0.5632\n",
      "Iteration 900, Loss: 0.5584\n",
      "Iteration 1000, Loss: 0.5540\n",
      "Iteration 1100, Loss: 0.5503\n",
      "Iteration 1200, Loss: 0.5467\n",
      "Iteration 1300, Loss: 0.5435\n",
      "Iteration 1400, Loss: 0.5406\n",
      "Iteration 1500, Loss: 0.5379\n",
      "Iteration 1600, Loss: 0.5354\n",
      "Iteration 1700, Loss: 0.5331\n",
      "Iteration 1800, Loss: 0.5309\n",
      "Iteration 1900, Loss: 0.5288\n",
      "Iteration 2000, Loss: 0.5269\n",
      "Iteration 2100, Loss: 0.5251\n",
      "Iteration 2200, Loss: 0.5233\n",
      "Iteration 2300, Loss: 0.5217\n",
      "Iteration 2400, Loss: 0.5201\n",
      "Iteration 2500, Loss: 0.5186\n",
      "Iteration 2600, Loss: 0.5171\n",
      "Iteration 2700, Loss: 0.5158\n",
      "Iteration 2800, Loss: 0.5145\n",
      "Iteration 2900, Loss: 0.5133\n",
      "Iteration 3000, Loss: 0.5120\n",
      "Iteration 3100, Loss: 0.5108\n",
      "Iteration 3200, Loss: 0.5097\n",
      "Iteration 3300, Loss: 0.5087\n",
      "Iteration 3400, Loss: 0.5076\n",
      "Iteration 3500, Loss: 0.5066\n",
      "Iteration 3600, Loss: 0.5056\n",
      "Iteration 3700, Loss: 0.5046\n",
      "Iteration 3800, Loss: 0.5037\n",
      "Iteration 3900, Loss: 0.5028\n",
      "Iteration 4000, Loss: 0.5019\n",
      "Iteration 4100, Loss: 0.5010\n",
      "Iteration 4200, Loss: 0.5002\n",
      "Iteration 4300, Loss: 0.4994\n",
      "Iteration 4400, Loss: 0.4986\n",
      "Iteration 4500, Loss: 0.4979\n",
      "Iteration 4600, Loss: 0.4971\n",
      "Iteration 4700, Loss: 0.4963\n",
      "Iteration 4800, Loss: 0.4956\n",
      "Iteration 4900, Loss: 0.4949\n",
      "Iteration 5000, Loss: 0.4942\n",
      "Iteration 5100, Loss: 0.4936\n",
      "Iteration 5200, Loss: 0.4929\n",
      "Iteration 5300, Loss: 0.4922\n",
      "Iteration 5400, Loss: 0.4916\n",
      "Iteration 5500, Loss: 0.4910\n",
      "Iteration 5600, Loss: 0.4904\n",
      "Iteration 5700, Loss: 0.4898\n",
      "Iteration 5800, Loss: 0.4892\n",
      "Iteration 5900, Loss: 0.4887\n",
      "203 270\n",
      "Iteration 0, Loss: 1.0492\n",
      "Iteration 100, Loss: 0.6398\n",
      "Iteration 200, Loss: 0.6072\n",
      "Iteration 300, Loss: 0.5901\n",
      "Iteration 400, Loss: 0.5779\n",
      "Iteration 500, Loss: 0.5686\n",
      "Iteration 600, Loss: 0.5608\n",
      "Iteration 700, Loss: 0.5543\n",
      "Iteration 800, Loss: 0.5488\n",
      "Iteration 900, Loss: 0.5439\n",
      "Iteration 1000, Loss: 0.5397\n",
      "Iteration 1100, Loss: 0.5358\n",
      "Iteration 1200, Loss: 0.5323\n",
      "Iteration 1300, Loss: 0.5291\n",
      "Iteration 1400, Loss: 0.5262\n",
      "Iteration 1500, Loss: 0.5234\n",
      "Iteration 1600, Loss: 0.5209\n",
      "Iteration 1700, Loss: 0.5185\n",
      "Iteration 1800, Loss: 0.5163\n",
      "Iteration 1900, Loss: 0.5143\n",
      "Iteration 2000, Loss: 0.5123\n",
      "Iteration 2100, Loss: 0.5104\n",
      "Iteration 2200, Loss: 0.5087\n",
      "Iteration 2300, Loss: 0.5071\n",
      "Iteration 2400, Loss: 0.5054\n",
      "Iteration 2500, Loss: 0.5039\n",
      "Iteration 2600, Loss: 0.5024\n",
      "Iteration 2700, Loss: 0.5010\n",
      "Iteration 2800, Loss: 0.4997\n",
      "Iteration 2900, Loss: 0.4984\n",
      "Iteration 3000, Loss: 0.4972\n",
      "Iteration 3100, Loss: 0.4959\n",
      "Iteration 3200, Loss: 0.4948\n",
      "Iteration 3300, Loss: 0.4937\n",
      "Iteration 3400, Loss: 0.4926\n",
      "Iteration 3500, Loss: 0.4915\n",
      "Iteration 3600, Loss: 0.4905\n",
      "Iteration 3700, Loss: 0.4895\n",
      "Iteration 3800, Loss: 0.4885\n",
      "Iteration 3900, Loss: 0.4876\n",
      "Iteration 4000, Loss: 0.4867\n",
      "Iteration 4100, Loss: 0.4858\n",
      "Iteration 4200, Loss: 0.4849\n",
      "Iteration 4300, Loss: 0.4841\n",
      "Iteration 4400, Loss: 0.4833\n",
      "Iteration 4500, Loss: 0.4825\n",
      "Iteration 4600, Loss: 0.4816\n",
      "Iteration 4700, Loss: 0.4809\n",
      "Iteration 4800, Loss: 0.4802\n",
      "Iteration 4900, Loss: 0.4794\n",
      "Iteration 5000, Loss: 0.4787\n",
      "Iteration 5100, Loss: 0.4780\n",
      "Iteration 5200, Loss: 0.4773\n",
      "Iteration 5300, Loss: 0.4766\n",
      "Iteration 5400, Loss: 0.4759\n",
      "Iteration 5500, Loss: 0.4753\n",
      "Iteration 5600, Loss: 0.4747\n",
      "Iteration 5700, Loss: 0.4740\n",
      "Iteration 5800, Loss: 0.4734\n",
      "Iteration 5900, Loss: 0.4728\n",
      "Iteration 0, Loss: 1.0567\n",
      "Iteration 100, Loss: 0.6731\n",
      "Iteration 200, Loss: 0.6361\n",
      "Iteration 300, Loss: 0.6170\n",
      "Iteration 400, Loss: 0.6043\n",
      "Iteration 500, Loss: 0.5947\n",
      "Iteration 600, Loss: 0.5871\n",
      "Iteration 700, Loss: 0.5808\n",
      "Iteration 800, Loss: 0.5754\n",
      "Iteration 900, Loss: 0.5708\n",
      "Iteration 1000, Loss: 0.5667\n",
      "Iteration 1100, Loss: 0.5631\n",
      "Iteration 1200, Loss: 0.5598\n",
      "Iteration 1300, Loss: 0.5569\n",
      "Iteration 1400, Loss: 0.5539\n",
      "Iteration 1500, Loss: 0.5513\n",
      "Iteration 1600, Loss: 0.5489\n",
      "Iteration 1700, Loss: 0.5467\n",
      "Iteration 1800, Loss: 0.5446\n",
      "Iteration 1900, Loss: 0.5426\n",
      "Iteration 2000, Loss: 0.5407\n",
      "Iteration 2100, Loss: 0.5389\n",
      "Iteration 2200, Loss: 0.5372\n",
      "Iteration 2300, Loss: 0.5356\n",
      "Iteration 2400, Loss: 0.5340\n",
      "Iteration 2500, Loss: 0.5325\n",
      "Iteration 2600, Loss: 0.5311\n",
      "Iteration 2700, Loss: 0.5298\n",
      "Iteration 2800, Loss: 0.5285\n",
      "Iteration 2900, Loss: 0.5272\n",
      "Iteration 3000, Loss: 0.5260\n",
      "Iteration 3100, Loss: 0.5248\n",
      "Iteration 3200, Loss: 0.5236\n",
      "Iteration 3300, Loss: 0.5225\n",
      "Iteration 3400, Loss: 0.5215\n",
      "Iteration 3500, Loss: 0.5204\n",
      "Iteration 3600, Loss: 0.5195\n",
      "Iteration 3700, Loss: 0.5185\n",
      "Iteration 3800, Loss: 0.5176\n",
      "Iteration 3900, Loss: 0.5167\n",
      "Iteration 4000, Loss: 0.5158\n",
      "Iteration 4100, Loss: 0.5149\n",
      "Iteration 4200, Loss: 0.5140\n",
      "Iteration 4300, Loss: 0.5132\n",
      "Iteration 4400, Loss: 0.5124\n",
      "Iteration 4500, Loss: 0.5116\n",
      "Iteration 4600, Loss: 0.5108\n",
      "Iteration 4700, Loss: 0.5101\n",
      "Iteration 4800, Loss: 0.5094\n",
      "Iteration 4900, Loss: 0.5087\n",
      "Iteration 5000, Loss: 0.5080\n",
      "Iteration 5100, Loss: 0.5073\n",
      "Iteration 5200, Loss: 0.5066\n",
      "Iteration 5300, Loss: 0.5060\n",
      "Iteration 5400, Loss: 0.5053\n",
      "Iteration 5500, Loss: 0.5047\n",
      "Iteration 5600, Loss: 0.5041\n",
      "Iteration 5700, Loss: 0.5035\n",
      "Iteration 5800, Loss: 0.5029\n",
      "Iteration 5900, Loss: 0.5023\n",
      "204 270\n",
      "Iteration 0, Loss: 1.0590\n",
      "Iteration 100, Loss: 0.6684\n",
      "Iteration 200, Loss: 0.6357\n",
      "Iteration 300, Loss: 0.6186\n",
      "Iteration 400, Loss: 0.6067\n",
      "Iteration 500, Loss: 0.5976\n",
      "Iteration 600, Loss: 0.5901\n",
      "Iteration 700, Loss: 0.5839\n",
      "Iteration 800, Loss: 0.5785\n",
      "Iteration 900, Loss: 0.5738\n",
      "Iteration 1000, Loss: 0.5696\n",
      "Iteration 1100, Loss: 0.5659\n",
      "Iteration 1200, Loss: 0.5625\n",
      "Iteration 1300, Loss: 0.5594\n",
      "Iteration 1400, Loss: 0.5566\n",
      "Iteration 1500, Loss: 0.5540\n",
      "Iteration 1600, Loss: 0.5515\n",
      "Iteration 1700, Loss: 0.5493\n",
      "Iteration 1800, Loss: 0.5471\n",
      "Iteration 1900, Loss: 0.5451\n",
      "Iteration 2000, Loss: 0.5431\n",
      "Iteration 2100, Loss: 0.5414\n",
      "Iteration 2200, Loss: 0.5396\n",
      "Iteration 2300, Loss: 0.5380\n",
      "Iteration 2400, Loss: 0.5364\n",
      "Iteration 2500, Loss: 0.5349\n",
      "Iteration 2600, Loss: 0.5335\n",
      "Iteration 2700, Loss: 0.5322\n",
      "Iteration 2800, Loss: 0.5308\n",
      "Iteration 2900, Loss: 0.5295\n",
      "Iteration 3000, Loss: 0.5283\n",
      "Iteration 3100, Loss: 0.5271\n",
      "Iteration 3200, Loss: 0.5260\n",
      "Iteration 3300, Loss: 0.5249\n",
      "Iteration 3400, Loss: 0.5239\n",
      "Iteration 3500, Loss: 0.5229\n",
      "Iteration 3600, Loss: 0.5218\n",
      "Iteration 3700, Loss: 0.5209\n",
      "Iteration 3800, Loss: 0.5200\n",
      "Iteration 3900, Loss: 0.5190\n",
      "Iteration 4000, Loss: 0.5182\n",
      "Iteration 4100, Loss: 0.5173\n",
      "Iteration 4200, Loss: 0.5165\n",
      "Iteration 4300, Loss: 0.5157\n",
      "Iteration 4400, Loss: 0.5148\n",
      "Iteration 4500, Loss: 0.5141\n",
      "Iteration 4600, Loss: 0.5133\n",
      "Iteration 4700, Loss: 0.5126\n",
      "Iteration 4800, Loss: 0.5119\n",
      "Iteration 4900, Loss: 0.5112\n",
      "Iteration 5000, Loss: 0.5105\n",
      "Iteration 5100, Loss: 0.5098\n",
      "Iteration 5200, Loss: 0.5091\n",
      "Iteration 5300, Loss: 0.5085\n",
      "Iteration 5400, Loss: 0.5079\n",
      "Iteration 5500, Loss: 0.5072\n",
      "Iteration 5600, Loss: 0.5066\n",
      "Iteration 5700, Loss: 0.5061\n",
      "Iteration 5800, Loss: 0.5055\n",
      "Iteration 5900, Loss: 0.5049\n",
      "Iteration 0, Loss: 1.0483\n",
      "Iteration 100, Loss: 0.6414\n",
      "Iteration 200, Loss: 0.6013\n",
      "Iteration 300, Loss: 0.5802\n",
      "Iteration 400, Loss: 0.5659\n",
      "Iteration 500, Loss: 0.5553\n",
      "Iteration 600, Loss: 0.5470\n",
      "Iteration 700, Loss: 0.5401\n",
      "Iteration 800, Loss: 0.5344\n",
      "Iteration 900, Loss: 0.5293\n",
      "Iteration 1000, Loss: 0.5249\n",
      "Iteration 1100, Loss: 0.5210\n",
      "Iteration 1200, Loss: 0.5175\n",
      "Iteration 1300, Loss: 0.5142\n",
      "Iteration 1400, Loss: 0.5113\n",
      "Iteration 1500, Loss: 0.5086\n",
      "Iteration 1600, Loss: 0.5060\n",
      "Iteration 1700, Loss: 0.5037\n",
      "Iteration 1800, Loss: 0.5015\n",
      "Iteration 1900, Loss: 0.4995\n",
      "Iteration 2000, Loss: 0.4975\n",
      "Iteration 2100, Loss: 0.4957\n",
      "Iteration 2200, Loss: 0.4940\n",
      "Iteration 2300, Loss: 0.4923\n",
      "Iteration 2400, Loss: 0.4908\n",
      "Iteration 2500, Loss: 0.4892\n",
      "Iteration 2600, Loss: 0.4878\n",
      "Iteration 2700, Loss: 0.4864\n",
      "Iteration 2800, Loss: 0.4851\n",
      "Iteration 2900, Loss: 0.4838\n",
      "Iteration 3000, Loss: 0.4826\n",
      "Iteration 3100, Loss: 0.4815\n",
      "Iteration 3200, Loss: 0.4803\n",
      "Iteration 3300, Loss: 0.4792\n",
      "Iteration 3400, Loss: 0.4782\n",
      "Iteration 3500, Loss: 0.4771\n",
      "Iteration 3600, Loss: 0.4762\n",
      "Iteration 3700, Loss: 0.4752\n",
      "Iteration 3800, Loss: 0.4743\n",
      "Iteration 3900, Loss: 0.4733\n",
      "Iteration 4000, Loss: 0.4724\n",
      "Iteration 4100, Loss: 0.4716\n",
      "Iteration 4200, Loss: 0.4707\n",
      "Iteration 4300, Loss: 0.4699\n",
      "Iteration 4400, Loss: 0.4691\n",
      "Iteration 4500, Loss: 0.4684\n",
      "Iteration 4600, Loss: 0.4676\n",
      "Iteration 4700, Loss: 0.4669\n",
      "Iteration 4800, Loss: 0.4661\n",
      "Iteration 4900, Loss: 0.4654\n",
      "Iteration 5000, Loss: 0.4648\n",
      "Iteration 5100, Loss: 0.4641\n",
      "Iteration 5200, Loss: 0.4635\n",
      "Iteration 5300, Loss: 0.4628\n",
      "Iteration 5400, Loss: 0.4621\n",
      "Iteration 5500, Loss: 0.4615\n",
      "Iteration 5600, Loss: 0.4609\n",
      "Iteration 5700, Loss: 0.4604\n",
      "Iteration 5800, Loss: 0.4598\n",
      "Iteration 5900, Loss: 0.4592\n",
      "205 270\n",
      "Iteration 0, Loss: 1.0552\n",
      "Iteration 100, Loss: 0.6633\n",
      "Iteration 200, Loss: 0.6275\n",
      "Iteration 300, Loss: 0.6084\n",
      "Iteration 400, Loss: 0.5956\n",
      "Iteration 500, Loss: 0.5859\n",
      "Iteration 600, Loss: 0.5780\n",
      "Iteration 700, Loss: 0.5713\n",
      "Iteration 800, Loss: 0.5656\n",
      "Iteration 900, Loss: 0.5607\n",
      "Iteration 1000, Loss: 0.5564\n",
      "Iteration 1100, Loss: 0.5525\n",
      "Iteration 1200, Loss: 0.5490\n",
      "Iteration 1300, Loss: 0.5458\n",
      "Iteration 1400, Loss: 0.5428\n",
      "Iteration 1500, Loss: 0.5401\n",
      "Iteration 1600, Loss: 0.5375\n",
      "Iteration 1700, Loss: 0.5351\n",
      "Iteration 1800, Loss: 0.5329\n",
      "Iteration 1900, Loss: 0.5307\n",
      "Iteration 2000, Loss: 0.5287\n",
      "Iteration 2100, Loss: 0.5268\n",
      "Iteration 2200, Loss: 0.5250\n",
      "Iteration 2300, Loss: 0.5233\n",
      "Iteration 2400, Loss: 0.5216\n",
      "Iteration 2500, Loss: 0.5200\n",
      "Iteration 2600, Loss: 0.5185\n",
      "Iteration 2700, Loss: 0.5170\n",
      "Iteration 2800, Loss: 0.5156\n",
      "Iteration 2900, Loss: 0.5142\n",
      "Iteration 3000, Loss: 0.5129\n",
      "Iteration 3100, Loss: 0.5116\n",
      "Iteration 3200, Loss: 0.5105\n",
      "Iteration 3300, Loss: 0.5092\n",
      "Iteration 3400, Loss: 0.5080\n",
      "Iteration 3500, Loss: 0.5069\n",
      "Iteration 3600, Loss: 0.5059\n",
      "Iteration 3700, Loss: 0.5048\n",
      "Iteration 3800, Loss: 0.5037\n",
      "Iteration 3900, Loss: 0.5027\n",
      "Iteration 4000, Loss: 0.5017\n",
      "Iteration 4100, Loss: 0.5007\n",
      "Iteration 4200, Loss: 0.4998\n",
      "Iteration 4300, Loss: 0.4989\n",
      "Iteration 4400, Loss: 0.4980\n",
      "Iteration 4500, Loss: 0.4971\n",
      "Iteration 4600, Loss: 0.4963\n",
      "Iteration 4700, Loss: 0.4955\n",
      "Iteration 4800, Loss: 0.4946\n",
      "Iteration 4900, Loss: 0.4938\n",
      "Iteration 5000, Loss: 0.4930\n",
      "Iteration 5100, Loss: 0.4922\n",
      "Iteration 5200, Loss: 0.4915\n",
      "Iteration 5300, Loss: 0.4907\n",
      "Iteration 5400, Loss: 0.4900\n",
      "Iteration 5500, Loss: 0.4893\n",
      "Iteration 5600, Loss: 0.4886\n",
      "Iteration 5700, Loss: 0.4879\n",
      "Iteration 5800, Loss: 0.4872\n",
      "Iteration 5900, Loss: 0.4866\n",
      "Iteration 0, Loss: 1.0523\n",
      "Iteration 100, Loss: 0.6487\n",
      "Iteration 200, Loss: 0.6133\n",
      "Iteration 300, Loss: 0.5952\n",
      "Iteration 400, Loss: 0.5830\n",
      "Iteration 500, Loss: 0.5737\n",
      "Iteration 600, Loss: 0.5664\n",
      "Iteration 700, Loss: 0.5603\n",
      "Iteration 800, Loss: 0.5550\n",
      "Iteration 900, Loss: 0.5504\n",
      "Iteration 1000, Loss: 0.5464\n",
      "Iteration 1100, Loss: 0.5426\n",
      "Iteration 1200, Loss: 0.5393\n",
      "Iteration 1300, Loss: 0.5363\n",
      "Iteration 1400, Loss: 0.5335\n",
      "Iteration 1500, Loss: 0.5310\n",
      "Iteration 1600, Loss: 0.5285\n",
      "Iteration 1700, Loss: 0.5262\n",
      "Iteration 1800, Loss: 0.5241\n",
      "Iteration 1900, Loss: 0.5220\n",
      "Iteration 2000, Loss: 0.5202\n",
      "Iteration 2100, Loss: 0.5184\n",
      "Iteration 2200, Loss: 0.5167\n",
      "Iteration 2300, Loss: 0.5150\n",
      "Iteration 2400, Loss: 0.5135\n",
      "Iteration 2500, Loss: 0.5120\n",
      "Iteration 2600, Loss: 0.5105\n",
      "Iteration 2700, Loss: 0.5092\n",
      "Iteration 2800, Loss: 0.5078\n",
      "Iteration 2900, Loss: 0.5066\n",
      "Iteration 3000, Loss: 0.5054\n",
      "Iteration 3100, Loss: 0.5042\n",
      "Iteration 3200, Loss: 0.5030\n",
      "Iteration 3300, Loss: 0.5019\n",
      "Iteration 3400, Loss: 0.5008\n",
      "Iteration 3500, Loss: 0.4998\n",
      "Iteration 3600, Loss: 0.4988\n",
      "Iteration 3700, Loss: 0.4978\n",
      "Iteration 3800, Loss: 0.4968\n",
      "Iteration 3900, Loss: 0.4959\n",
      "Iteration 4000, Loss: 0.4951\n",
      "Iteration 4100, Loss: 0.4941\n",
      "Iteration 4200, Loss: 0.4933\n",
      "Iteration 4300, Loss: 0.4925\n",
      "Iteration 4400, Loss: 0.4917\n",
      "Iteration 4500, Loss: 0.4909\n",
      "Iteration 4600, Loss: 0.4901\n",
      "Iteration 4700, Loss: 0.4893\n",
      "Iteration 4800, Loss: 0.4886\n",
      "Iteration 4900, Loss: 0.4879\n",
      "Iteration 5000, Loss: 0.4872\n",
      "Iteration 5100, Loss: 0.4865\n",
      "Iteration 5200, Loss: 0.4858\n",
      "Iteration 5300, Loss: 0.4852\n",
      "Iteration 5400, Loss: 0.4845\n",
      "Iteration 5500, Loss: 0.4839\n",
      "Iteration 5600, Loss: 0.4833\n",
      "Iteration 5700, Loss: 0.4827\n",
      "Iteration 5800, Loss: 0.4822\n",
      "Iteration 5900, Loss: 0.4815\n",
      "206 270\n",
      "Iteration 0, Loss: 1.0545\n",
      "Iteration 100, Loss: 0.6556\n",
      "Iteration 200, Loss: 0.6173\n",
      "Iteration 300, Loss: 0.5977\n",
      "Iteration 400, Loss: 0.5845\n",
      "Iteration 500, Loss: 0.5744\n",
      "Iteration 600, Loss: 0.5663\n",
      "Iteration 700, Loss: 0.5596\n",
      "Iteration 800, Loss: 0.5538\n",
      "Iteration 900, Loss: 0.5488\n",
      "Iteration 1000, Loss: 0.5444\n",
      "Iteration 1100, Loss: 0.5405\n",
      "Iteration 1200, Loss: 0.5370\n",
      "Iteration 1300, Loss: 0.5338\n",
      "Iteration 1400, Loss: 0.5308\n",
      "Iteration 1500, Loss: 0.5281\n",
      "Iteration 1600, Loss: 0.5255\n",
      "Iteration 1700, Loss: 0.5231\n",
      "Iteration 1800, Loss: 0.5209\n",
      "Iteration 1900, Loss: 0.5188\n",
      "Iteration 2000, Loss: 0.5168\n",
      "Iteration 2100, Loss: 0.5149\n",
      "Iteration 2200, Loss: 0.5131\n",
      "Iteration 2300, Loss: 0.5115\n",
      "Iteration 2400, Loss: 0.5098\n",
      "Iteration 2500, Loss: 0.5083\n",
      "Iteration 2600, Loss: 0.5068\n",
      "Iteration 2700, Loss: 0.5054\n",
      "Iteration 2800, Loss: 0.5039\n",
      "Iteration 2900, Loss: 0.5026\n",
      "Iteration 3000, Loss: 0.5013\n",
      "Iteration 3100, Loss: 0.5001\n",
      "Iteration 3200, Loss: 0.4989\n",
      "Iteration 3300, Loss: 0.4977\n",
      "Iteration 3400, Loss: 0.4966\n",
      "Iteration 3500, Loss: 0.4955\n",
      "Iteration 3600, Loss: 0.4945\n",
      "Iteration 3700, Loss: 0.4934\n",
      "Iteration 3800, Loss: 0.4924\n",
      "Iteration 3900, Loss: 0.4915\n",
      "Iteration 4000, Loss: 0.4905\n",
      "Iteration 4100, Loss: 0.4896\n",
      "Iteration 4200, Loss: 0.4887\n",
      "Iteration 4300, Loss: 0.4878\n",
      "Iteration 4400, Loss: 0.4870\n",
      "Iteration 4500, Loss: 0.4862\n",
      "Iteration 4600, Loss: 0.4853\n",
      "Iteration 4700, Loss: 0.4846\n",
      "Iteration 4800, Loss: 0.4838\n",
      "Iteration 4900, Loss: 0.4830\n",
      "Iteration 5000, Loss: 0.4823\n",
      "Iteration 5100, Loss: 0.4816\n",
      "Iteration 5200, Loss: 0.4809\n",
      "Iteration 5300, Loss: 0.4801\n",
      "Iteration 5400, Loss: 0.4795\n",
      "Iteration 5500, Loss: 0.4788\n",
      "Iteration 5600, Loss: 0.4782\n",
      "Iteration 5700, Loss: 0.4775\n",
      "Iteration 5800, Loss: 0.4768\n",
      "Iteration 5900, Loss: 0.4762\n",
      "Iteration 0, Loss: 1.0510\n",
      "Iteration 100, Loss: 0.6546\n",
      "Iteration 200, Loss: 0.6221\n",
      "Iteration 300, Loss: 0.6046\n",
      "Iteration 400, Loss: 0.5926\n",
      "Iteration 500, Loss: 0.5832\n",
      "Iteration 600, Loss: 0.5756\n",
      "Iteration 700, Loss: 0.5694\n",
      "Iteration 800, Loss: 0.5639\n",
      "Iteration 900, Loss: 0.5591\n",
      "Iteration 1000, Loss: 0.5548\n",
      "Iteration 1100, Loss: 0.5510\n",
      "Iteration 1200, Loss: 0.5474\n",
      "Iteration 1300, Loss: 0.5442\n",
      "Iteration 1400, Loss: 0.5412\n",
      "Iteration 1500, Loss: 0.5385\n",
      "Iteration 1600, Loss: 0.5359\n",
      "Iteration 1700, Loss: 0.5335\n",
      "Iteration 1800, Loss: 0.5313\n",
      "Iteration 1900, Loss: 0.5291\n",
      "Iteration 2000, Loss: 0.5271\n",
      "Iteration 2100, Loss: 0.5252\n",
      "Iteration 2200, Loss: 0.5234\n",
      "Iteration 2300, Loss: 0.5217\n",
      "Iteration 2400, Loss: 0.5200\n",
      "Iteration 2500, Loss: 0.5185\n",
      "Iteration 2600, Loss: 0.5170\n",
      "Iteration 2700, Loss: 0.5155\n",
      "Iteration 2800, Loss: 0.5141\n",
      "Iteration 2900, Loss: 0.5128\n",
      "Iteration 3000, Loss: 0.5115\n",
      "Iteration 3100, Loss: 0.5102\n",
      "Iteration 3200, Loss: 0.5090\n",
      "Iteration 3300, Loss: 0.5079\n",
      "Iteration 3400, Loss: 0.5067\n",
      "Iteration 3500, Loss: 0.5057\n",
      "Iteration 3600, Loss: 0.5046\n",
      "Iteration 3700, Loss: 0.5037\n",
      "Iteration 3800, Loss: 0.5026\n",
      "Iteration 3900, Loss: 0.5017\n",
      "Iteration 4000, Loss: 0.5007\n",
      "Iteration 4100, Loss: 0.4998\n",
      "Iteration 4200, Loss: 0.4990\n",
      "Iteration 4300, Loss: 0.4981\n",
      "Iteration 4400, Loss: 0.4973\n",
      "Iteration 4500, Loss: 0.4965\n",
      "Iteration 4600, Loss: 0.4957\n",
      "Iteration 4700, Loss: 0.4949\n",
      "Iteration 4800, Loss: 0.4942\n",
      "Iteration 4900, Loss: 0.4935\n",
      "Iteration 5000, Loss: 0.4927\n",
      "Iteration 5100, Loss: 0.4920\n",
      "Iteration 5200, Loss: 0.4914\n",
      "Iteration 5300, Loss: 0.4907\n",
      "Iteration 5400, Loss: 0.4900\n",
      "Iteration 5500, Loss: 0.4894\n",
      "Iteration 5600, Loss: 0.4888\n",
      "Iteration 5700, Loss: 0.4881\n",
      "Iteration 5800, Loss: 0.4875\n",
      "Iteration 5900, Loss: 0.4870\n",
      "207 270\n",
      "Iteration 0, Loss: 1.0552\n",
      "Iteration 100, Loss: 0.6598\n",
      "Iteration 200, Loss: 0.6217\n",
      "Iteration 300, Loss: 0.6019\n",
      "Iteration 400, Loss: 0.5888\n",
      "Iteration 500, Loss: 0.5792\n",
      "Iteration 600, Loss: 0.5715\n",
      "Iteration 700, Loss: 0.5652\n",
      "Iteration 800, Loss: 0.5600\n",
      "Iteration 900, Loss: 0.5555\n",
      "Iteration 1000, Loss: 0.5515\n",
      "Iteration 1100, Loss: 0.5481\n",
      "Iteration 1200, Loss: 0.5449\n",
      "Iteration 1300, Loss: 0.5420\n",
      "Iteration 1400, Loss: 0.5393\n",
      "Iteration 1500, Loss: 0.5369\n",
      "Iteration 1600, Loss: 0.5347\n",
      "Iteration 1700, Loss: 0.5326\n",
      "Iteration 1800, Loss: 0.5306\n",
      "Iteration 1900, Loss: 0.5288\n",
      "Iteration 2000, Loss: 0.5271\n",
      "Iteration 2100, Loss: 0.5255\n",
      "Iteration 2200, Loss: 0.5238\n",
      "Iteration 2300, Loss: 0.5224\n",
      "Iteration 2400, Loss: 0.5210\n",
      "Iteration 2500, Loss: 0.5196\n",
      "Iteration 2600, Loss: 0.5183\n",
      "Iteration 2700, Loss: 0.5171\n",
      "Iteration 2800, Loss: 0.5159\n",
      "Iteration 2900, Loss: 0.5147\n",
      "Iteration 3000, Loss: 0.5136\n",
      "Iteration 3100, Loss: 0.5125\n",
      "Iteration 3200, Loss: 0.5115\n",
      "Iteration 3300, Loss: 0.5105\n",
      "Iteration 3400, Loss: 0.5095\n",
      "Iteration 3500, Loss: 0.5086\n",
      "Iteration 3600, Loss: 0.5078\n",
      "Iteration 3700, Loss: 0.5068\n",
      "Iteration 3800, Loss: 0.5059\n",
      "Iteration 3900, Loss: 0.5051\n",
      "Iteration 4000, Loss: 0.5043\n",
      "Iteration 4100, Loss: 0.5035\n",
      "Iteration 4200, Loss: 0.5027\n",
      "Iteration 4300, Loss: 0.5019\n",
      "Iteration 4400, Loss: 0.5012\n",
      "Iteration 4500, Loss: 0.5004\n",
      "Iteration 4600, Loss: 0.4997\n",
      "Iteration 4700, Loss: 0.4990\n",
      "Iteration 4800, Loss: 0.4984\n",
      "Iteration 4900, Loss: 0.4977\n",
      "Iteration 5000, Loss: 0.4970\n",
      "Iteration 5100, Loss: 0.4964\n",
      "Iteration 5200, Loss: 0.4958\n",
      "Iteration 5300, Loss: 0.4951\n",
      "Iteration 5400, Loss: 0.4945\n",
      "Iteration 5500, Loss: 0.4940\n",
      "Iteration 5600, Loss: 0.4934\n",
      "Iteration 5700, Loss: 0.4928\n",
      "Iteration 5800, Loss: 0.4923\n",
      "Iteration 5900, Loss: 0.4917\n",
      "Iteration 0, Loss: 1.0522\n",
      "Iteration 100, Loss: 0.6526\n",
      "Iteration 200, Loss: 0.6207\n",
      "Iteration 300, Loss: 0.6034\n",
      "Iteration 400, Loss: 0.5912\n",
      "Iteration 500, Loss: 0.5817\n",
      "Iteration 600, Loss: 0.5738\n",
      "Iteration 700, Loss: 0.5671\n",
      "Iteration 800, Loss: 0.5613\n",
      "Iteration 900, Loss: 0.5562\n",
      "Iteration 1000, Loss: 0.5518\n",
      "Iteration 1100, Loss: 0.5477\n",
      "Iteration 1200, Loss: 0.5440\n",
      "Iteration 1300, Loss: 0.5406\n",
      "Iteration 1400, Loss: 0.5375\n",
      "Iteration 1500, Loss: 0.5346\n",
      "Iteration 1600, Loss: 0.5320\n",
      "Iteration 1700, Loss: 0.5294\n",
      "Iteration 1800, Loss: 0.5271\n",
      "Iteration 1900, Loss: 0.5249\n",
      "Iteration 2000, Loss: 0.5228\n",
      "Iteration 2100, Loss: 0.5208\n",
      "Iteration 2200, Loss: 0.5190\n",
      "Iteration 2300, Loss: 0.5172\n",
      "Iteration 2400, Loss: 0.5155\n",
      "Iteration 2500, Loss: 0.5139\n",
      "Iteration 2600, Loss: 0.5123\n",
      "Iteration 2700, Loss: 0.5108\n",
      "Iteration 2800, Loss: 0.5094\n",
      "Iteration 2900, Loss: 0.5080\n",
      "Iteration 3000, Loss: 0.5067\n",
      "Iteration 3100, Loss: 0.5054\n",
      "Iteration 3200, Loss: 0.5041\n",
      "Iteration 3300, Loss: 0.5029\n",
      "Iteration 3400, Loss: 0.5018\n",
      "Iteration 3500, Loss: 0.5006\n",
      "Iteration 3600, Loss: 0.4995\n",
      "Iteration 3700, Loss: 0.4985\n",
      "Iteration 3800, Loss: 0.4975\n",
      "Iteration 3900, Loss: 0.4964\n",
      "Iteration 4000, Loss: 0.4954\n",
      "Iteration 4100, Loss: 0.4945\n",
      "Iteration 4200, Loss: 0.4936\n",
      "Iteration 4300, Loss: 0.4926\n",
      "Iteration 4400, Loss: 0.4918\n",
      "Iteration 4500, Loss: 0.4909\n",
      "Iteration 4600, Loss: 0.4900\n",
      "Iteration 4700, Loss: 0.4892\n",
      "Iteration 4800, Loss: 0.4884\n",
      "Iteration 4900, Loss: 0.4876\n",
      "Iteration 5000, Loss: 0.4868\n",
      "Iteration 5100, Loss: 0.4861\n",
      "Iteration 5200, Loss: 0.4854\n",
      "Iteration 5300, Loss: 0.4846\n",
      "Iteration 5400, Loss: 0.4839\n",
      "Iteration 5500, Loss: 0.4832\n",
      "Iteration 5600, Loss: 0.4825\n",
      "Iteration 5700, Loss: 0.4819\n",
      "Iteration 5800, Loss: 0.4812\n",
      "Iteration 5900, Loss: 0.4806\n",
      "208 270\n",
      "Iteration 0, Loss: 1.0540\n",
      "Iteration 100, Loss: 0.6392\n",
      "Iteration 200, Loss: 0.6059\n",
      "Iteration 300, Loss: 0.5884\n",
      "Iteration 400, Loss: 0.5765\n",
      "Iteration 500, Loss: 0.5673\n",
      "Iteration 600, Loss: 0.5597\n",
      "Iteration 700, Loss: 0.5534\n",
      "Iteration 800, Loss: 0.5479\n",
      "Iteration 900, Loss: 0.5432\n",
      "Iteration 1000, Loss: 0.5389\n",
      "Iteration 1100, Loss: 0.5350\n",
      "Iteration 1200, Loss: 0.5315\n",
      "Iteration 1300, Loss: 0.5283\n",
      "Iteration 1400, Loss: 0.5254\n",
      "Iteration 1500, Loss: 0.5227\n",
      "Iteration 1600, Loss: 0.5201\n",
      "Iteration 1700, Loss: 0.5177\n",
      "Iteration 1800, Loss: 0.5155\n",
      "Iteration 1900, Loss: 0.5134\n",
      "Iteration 2000, Loss: 0.5114\n",
      "Iteration 2100, Loss: 0.5095\n",
      "Iteration 2200, Loss: 0.5077\n",
      "Iteration 2300, Loss: 0.5061\n",
      "Iteration 2400, Loss: 0.5044\n",
      "Iteration 2500, Loss: 0.5029\n",
      "Iteration 2600, Loss: 0.5014\n",
      "Iteration 2700, Loss: 0.5000\n",
      "Iteration 2800, Loss: 0.4987\n",
      "Iteration 2900, Loss: 0.4973\n",
      "Iteration 3000, Loss: 0.4960\n",
      "Iteration 3100, Loss: 0.4948\n",
      "Iteration 3200, Loss: 0.4937\n",
      "Iteration 3300, Loss: 0.4925\n",
      "Iteration 3400, Loss: 0.4914\n",
      "Iteration 3500, Loss: 0.4904\n",
      "Iteration 3600, Loss: 0.4894\n",
      "Iteration 3700, Loss: 0.4883\n",
      "Iteration 3800, Loss: 0.4874\n",
      "Iteration 3900, Loss: 0.4865\n",
      "Iteration 4000, Loss: 0.4855\n",
      "Iteration 4100, Loss: 0.4846\n",
      "Iteration 4200, Loss: 0.4837\n",
      "Iteration 4300, Loss: 0.4829\n",
      "Iteration 4400, Loss: 0.4820\n",
      "Iteration 4500, Loss: 0.4813\n",
      "Iteration 4600, Loss: 0.4805\n",
      "Iteration 4700, Loss: 0.4797\n",
      "Iteration 4800, Loss: 0.4789\n",
      "Iteration 4900, Loss: 0.4782\n",
      "Iteration 5000, Loss: 0.4775\n",
      "Iteration 5100, Loss: 0.4768\n",
      "Iteration 5200, Loss: 0.4761\n",
      "Iteration 5300, Loss: 0.4754\n",
      "Iteration 5400, Loss: 0.4748\n",
      "Iteration 5500, Loss: 0.4741\n",
      "Iteration 5600, Loss: 0.4735\n",
      "Iteration 5700, Loss: 0.4729\n",
      "Iteration 5800, Loss: 0.4723\n",
      "Iteration 5900, Loss: 0.4717\n",
      "Iteration 0, Loss: 1.0542\n",
      "Iteration 100, Loss: 0.6712\n",
      "Iteration 200, Loss: 0.6343\n",
      "Iteration 300, Loss: 0.6149\n",
      "Iteration 400, Loss: 0.6016\n",
      "Iteration 500, Loss: 0.5915\n",
      "Iteration 600, Loss: 0.5835\n",
      "Iteration 700, Loss: 0.5768\n",
      "Iteration 800, Loss: 0.5711\n",
      "Iteration 900, Loss: 0.5661\n",
      "Iteration 1000, Loss: 0.5617\n",
      "Iteration 1100, Loss: 0.5578\n",
      "Iteration 1200, Loss: 0.5542\n",
      "Iteration 1300, Loss: 0.5509\n",
      "Iteration 1400, Loss: 0.5480\n",
      "Iteration 1500, Loss: 0.5452\n",
      "Iteration 1600, Loss: 0.5427\n",
      "Iteration 1700, Loss: 0.5402\n",
      "Iteration 1800, Loss: 0.5380\n",
      "Iteration 1900, Loss: 0.5359\n",
      "Iteration 2000, Loss: 0.5339\n",
      "Iteration 2100, Loss: 0.5320\n",
      "Iteration 2200, Loss: 0.5302\n",
      "Iteration 2300, Loss: 0.5285\n",
      "Iteration 2400, Loss: 0.5269\n",
      "Iteration 2500, Loss: 0.5254\n",
      "Iteration 2600, Loss: 0.5239\n",
      "Iteration 2700, Loss: 0.5224\n",
      "Iteration 2800, Loss: 0.5211\n",
      "Iteration 2900, Loss: 0.5198\n",
      "Iteration 3000, Loss: 0.5186\n",
      "Iteration 3100, Loss: 0.5173\n",
      "Iteration 3200, Loss: 0.5161\n",
      "Iteration 3300, Loss: 0.5150\n",
      "Iteration 3400, Loss: 0.5138\n",
      "Iteration 3500, Loss: 0.5128\n",
      "Iteration 3600, Loss: 0.5117\n",
      "Iteration 3700, Loss: 0.5107\n",
      "Iteration 3800, Loss: 0.5097\n",
      "Iteration 3900, Loss: 0.5088\n",
      "Iteration 4000, Loss: 0.5078\n",
      "Iteration 4100, Loss: 0.5069\n",
      "Iteration 4200, Loss: 0.5060\n",
      "Iteration 4300, Loss: 0.5052\n",
      "Iteration 4400, Loss: 0.5043\n",
      "Iteration 4500, Loss: 0.5035\n",
      "Iteration 4600, Loss: 0.5027\n",
      "Iteration 4700, Loss: 0.5019\n",
      "Iteration 4800, Loss: 0.5012\n",
      "Iteration 4900, Loss: 0.5004\n",
      "Iteration 5000, Loss: 0.4997\n",
      "Iteration 5100, Loss: 0.4989\n",
      "Iteration 5200, Loss: 0.4982\n",
      "Iteration 5300, Loss: 0.4976\n",
      "Iteration 5400, Loss: 0.4969\n",
      "Iteration 5500, Loss: 0.4962\n",
      "Iteration 5600, Loss: 0.4956\n",
      "Iteration 5700, Loss: 0.4949\n",
      "Iteration 5800, Loss: 0.4943\n",
      "Iteration 5900, Loss: 0.4937\n",
      "209 270\n",
      "Iteration 0, Loss: 1.0588\n",
      "Iteration 100, Loss: 0.6627\n",
      "Iteration 200, Loss: 0.6245\n",
      "Iteration 300, Loss: 0.6041\n",
      "Iteration 400, Loss: 0.5903\n",
      "Iteration 500, Loss: 0.5797\n",
      "Iteration 600, Loss: 0.5714\n",
      "Iteration 700, Loss: 0.5644\n",
      "Iteration 800, Loss: 0.5585\n",
      "Iteration 900, Loss: 0.5533\n",
      "Iteration 1000, Loss: 0.5488\n",
      "Iteration 1100, Loss: 0.5446\n",
      "Iteration 1200, Loss: 0.5409\n",
      "Iteration 1300, Loss: 0.5376\n",
      "Iteration 1400, Loss: 0.5345\n",
      "Iteration 1500, Loss: 0.5316\n",
      "Iteration 1600, Loss: 0.5289\n",
      "Iteration 1700, Loss: 0.5264\n",
      "Iteration 1800, Loss: 0.5241\n",
      "Iteration 1900, Loss: 0.5218\n",
      "Iteration 2000, Loss: 0.5198\n",
      "Iteration 2100, Loss: 0.5178\n",
      "Iteration 2200, Loss: 0.5159\n",
      "Iteration 2300, Loss: 0.5141\n",
      "Iteration 2400, Loss: 0.5124\n",
      "Iteration 2500, Loss: 0.5108\n",
      "Iteration 2600, Loss: 0.5093\n",
      "Iteration 2700, Loss: 0.5078\n",
      "Iteration 2800, Loss: 0.5064\n",
      "Iteration 2900, Loss: 0.5050\n",
      "Iteration 3000, Loss: 0.5037\n",
      "Iteration 3100, Loss: 0.5024\n",
      "Iteration 3200, Loss: 0.5012\n",
      "Iteration 3300, Loss: 0.5000\n",
      "Iteration 3400, Loss: 0.4988\n",
      "Iteration 3500, Loss: 0.4977\n",
      "Iteration 3600, Loss: 0.4966\n",
      "Iteration 3700, Loss: 0.4956\n",
      "Iteration 3800, Loss: 0.4945\n",
      "Iteration 3900, Loss: 0.4936\n",
      "Iteration 4000, Loss: 0.4926\n",
      "Iteration 4100, Loss: 0.4917\n",
      "Iteration 4200, Loss: 0.4907\n",
      "Iteration 4300, Loss: 0.4899\n",
      "Iteration 4400, Loss: 0.4890\n",
      "Iteration 4500, Loss: 0.4882\n",
      "Iteration 4600, Loss: 0.4873\n",
      "Iteration 4700, Loss: 0.4865\n",
      "Iteration 4800, Loss: 0.4858\n",
      "Iteration 4900, Loss: 0.4850\n",
      "Iteration 5000, Loss: 0.4842\n",
      "Iteration 5100, Loss: 0.4835\n",
      "Iteration 5200, Loss: 0.4828\n",
      "Iteration 5300, Loss: 0.4821\n",
      "Iteration 5400, Loss: 0.4814\n",
      "Iteration 5500, Loss: 0.4807\n",
      "Iteration 5600, Loss: 0.4801\n",
      "Iteration 5700, Loss: 0.4794\n",
      "Iteration 5800, Loss: 0.4788\n",
      "Iteration 5900, Loss: 0.4782\n",
      "Iteration 0, Loss: 1.0504\n",
      "Iteration 100, Loss: 0.6487\n",
      "Iteration 200, Loss: 0.6146\n",
      "Iteration 300, Loss: 0.5968\n",
      "Iteration 400, Loss: 0.5847\n",
      "Iteration 500, Loss: 0.5754\n",
      "Iteration 600, Loss: 0.5680\n",
      "Iteration 700, Loss: 0.5618\n",
      "Iteration 800, Loss: 0.5564\n",
      "Iteration 900, Loss: 0.5518\n",
      "Iteration 1000, Loss: 0.5477\n",
      "Iteration 1100, Loss: 0.5441\n",
      "Iteration 1200, Loss: 0.5407\n",
      "Iteration 1300, Loss: 0.5376\n",
      "Iteration 1400, Loss: 0.5348\n",
      "Iteration 1500, Loss: 0.5323\n",
      "Iteration 1600, Loss: 0.5298\n",
      "Iteration 1700, Loss: 0.5276\n",
      "Iteration 1800, Loss: 0.5255\n",
      "Iteration 1900, Loss: 0.5235\n",
      "Iteration 2000, Loss: 0.5216\n",
      "Iteration 2100, Loss: 0.5198\n",
      "Iteration 2200, Loss: 0.5181\n",
      "Iteration 2300, Loss: 0.5165\n",
      "Iteration 2400, Loss: 0.5150\n",
      "Iteration 2500, Loss: 0.5135\n",
      "Iteration 2600, Loss: 0.5121\n",
      "Iteration 2700, Loss: 0.5107\n",
      "Iteration 2800, Loss: 0.5094\n",
      "Iteration 2900, Loss: 0.5081\n",
      "Iteration 3000, Loss: 0.5069\n",
      "Iteration 3100, Loss: 0.5058\n",
      "Iteration 3200, Loss: 0.5047\n",
      "Iteration 3300, Loss: 0.5035\n",
      "Iteration 3400, Loss: 0.5025\n",
      "Iteration 3500, Loss: 0.5014\n",
      "Iteration 3600, Loss: 0.5004\n",
      "Iteration 3700, Loss: 0.4995\n",
      "Iteration 3800, Loss: 0.4985\n",
      "Iteration 3900, Loss: 0.4976\n",
      "Iteration 4000, Loss: 0.4967\n",
      "Iteration 4100, Loss: 0.4958\n",
      "Iteration 4200, Loss: 0.4950\n",
      "Iteration 4300, Loss: 0.4941\n",
      "Iteration 4400, Loss: 0.4933\n",
      "Iteration 4500, Loss: 0.4925\n",
      "Iteration 4600, Loss: 0.4917\n",
      "Iteration 4700, Loss: 0.4909\n",
      "Iteration 4800, Loss: 0.4902\n",
      "Iteration 4900, Loss: 0.4895\n",
      "Iteration 5000, Loss: 0.4888\n",
      "Iteration 5100, Loss: 0.4881\n",
      "Iteration 5200, Loss: 0.4874\n",
      "Iteration 5300, Loss: 0.4867\n",
      "Iteration 5400, Loss: 0.4860\n",
      "Iteration 5500, Loss: 0.4854\n",
      "Iteration 5600, Loss: 0.4848\n",
      "Iteration 5700, Loss: 0.4841\n",
      "Iteration 5800, Loss: 0.4835\n",
      "Iteration 5900, Loss: 0.4829\n",
      "210 270\n",
      "Iteration 0, Loss: 1.0882\n",
      "Iteration 100, Loss: 0.7717\n",
      "Iteration 200, Loss: 0.7061\n",
      "Iteration 300, Loss: 0.6756\n",
      "Iteration 400, Loss: 0.6569\n",
      "Iteration 500, Loss: 0.6436\n",
      "Iteration 600, Loss: 0.6334\n",
      "Iteration 700, Loss: 0.6252\n",
      "Iteration 800, Loss: 0.6184\n",
      "Iteration 900, Loss: 0.6125\n",
      "Iteration 1000, Loss: 0.6074\n",
      "Iteration 1100, Loss: 0.6030\n",
      "Iteration 1200, Loss: 0.5990\n",
      "Iteration 1300, Loss: 0.5953\n",
      "Iteration 1400, Loss: 0.5920\n",
      "Iteration 1500, Loss: 0.5889\n",
      "Iteration 1600, Loss: 0.5861\n",
      "Iteration 1700, Loss: 0.5834\n",
      "Iteration 1800, Loss: 0.5809\n",
      "Iteration 1900, Loss: 0.5786\n",
      "Iteration 2000, Loss: 0.5764\n",
      "Iteration 2100, Loss: 0.5743\n",
      "Iteration 2200, Loss: 0.5723\n",
      "Iteration 2300, Loss: 0.5705\n",
      "Iteration 2400, Loss: 0.5687\n",
      "Iteration 2500, Loss: 0.5670\n",
      "Iteration 2600, Loss: 0.5653\n",
      "Iteration 2700, Loss: 0.5638\n",
      "Iteration 2800, Loss: 0.5623\n",
      "Iteration 2900, Loss: 0.5608\n",
      "Iteration 3000, Loss: 0.5595\n",
      "Iteration 3100, Loss: 0.5581\n",
      "Iteration 3200, Loss: 0.5568\n",
      "Iteration 3300, Loss: 0.5556\n",
      "Iteration 3400, Loss: 0.5544\n",
      "Iteration 3500, Loss: 0.5532\n",
      "Iteration 3600, Loss: 0.5521\n",
      "Iteration 3700, Loss: 0.5510\n",
      "Iteration 3800, Loss: 0.5500\n",
      "Iteration 3900, Loss: 0.5489\n",
      "Iteration 4000, Loss: 0.5479\n",
      "Iteration 4100, Loss: 0.5470\n",
      "Iteration 4200, Loss: 0.5461\n",
      "Iteration 4300, Loss: 0.5451\n",
      "Iteration 4400, Loss: 0.5443\n",
      "Iteration 4500, Loss: 0.5434\n",
      "Iteration 4600, Loss: 0.5425\n",
      "Iteration 4700, Loss: 0.5417\n",
      "Iteration 4800, Loss: 0.5409\n",
      "Iteration 4900, Loss: 0.5402\n",
      "Iteration 5000, Loss: 0.5394\n",
      "Iteration 5100, Loss: 0.5387\n",
      "Iteration 5200, Loss: 0.5379\n",
      "Iteration 5300, Loss: 0.5372\n",
      "Iteration 5400, Loss: 0.5365\n",
      "Iteration 5500, Loss: 0.5358\n",
      "Iteration 5600, Loss: 0.5352\n",
      "Iteration 5700, Loss: 0.5345\n",
      "Iteration 5800, Loss: 0.5339\n",
      "Iteration 5900, Loss: 0.5333\n",
      "Iteration 0, Loss: 1.0903\n",
      "Iteration 100, Loss: 0.8018\n",
      "Iteration 200, Loss: 0.7376\n",
      "Iteration 300, Loss: 0.7083\n",
      "Iteration 400, Loss: 0.6902\n",
      "Iteration 500, Loss: 0.6773\n",
      "Iteration 600, Loss: 0.6674\n",
      "Iteration 700, Loss: 0.6594\n",
      "Iteration 800, Loss: 0.6527\n",
      "Iteration 900, Loss: 0.6470\n",
      "Iteration 1000, Loss: 0.6421\n",
      "Iteration 1100, Loss: 0.6377\n",
      "Iteration 1200, Loss: 0.6337\n",
      "Iteration 1300, Loss: 0.6301\n",
      "Iteration 1400, Loss: 0.6269\n",
      "Iteration 1500, Loss: 0.6238\n",
      "Iteration 1600, Loss: 0.6210\n",
      "Iteration 1700, Loss: 0.6184\n",
      "Iteration 1800, Loss: 0.6160\n",
      "Iteration 1900, Loss: 0.6137\n",
      "Iteration 2000, Loss: 0.6115\n",
      "Iteration 2100, Loss: 0.6095\n",
      "Iteration 2200, Loss: 0.6075\n",
      "Iteration 2300, Loss: 0.6057\n",
      "Iteration 2400, Loss: 0.6040\n",
      "Iteration 2500, Loss: 0.6023\n",
      "Iteration 2600, Loss: 0.6007\n",
      "Iteration 2700, Loss: 0.5992\n",
      "Iteration 2800, Loss: 0.5977\n",
      "Iteration 2900, Loss: 0.5963\n",
      "Iteration 3000, Loss: 0.5950\n",
      "Iteration 3100, Loss: 0.5937\n",
      "Iteration 3200, Loss: 0.5924\n",
      "Iteration 3300, Loss: 0.5912\n",
      "Iteration 3400, Loss: 0.5900\n",
      "Iteration 3500, Loss: 0.5889\n",
      "Iteration 3600, Loss: 0.5878\n",
      "Iteration 3700, Loss: 0.5868\n",
      "Iteration 3800, Loss: 0.5857\n",
      "Iteration 3900, Loss: 0.5847\n",
      "Iteration 4000, Loss: 0.5838\n",
      "Iteration 4100, Loss: 0.5828\n",
      "Iteration 4200, Loss: 0.5819\n",
      "Iteration 4300, Loss: 0.5810\n",
      "Iteration 4400, Loss: 0.5802\n",
      "Iteration 4500, Loss: 0.5793\n",
      "Iteration 4600, Loss: 0.5785\n",
      "Iteration 4700, Loss: 0.5777\n",
      "Iteration 4800, Loss: 0.5769\n",
      "Iteration 4900, Loss: 0.5761\n",
      "Iteration 5000, Loss: 0.5754\n",
      "Iteration 5100, Loss: 0.5747\n",
      "Iteration 5200, Loss: 0.5740\n",
      "Iteration 5300, Loss: 0.5733\n",
      "Iteration 5400, Loss: 0.5726\n",
      "Iteration 5500, Loss: 0.5719\n",
      "Iteration 5600, Loss: 0.5712\n",
      "Iteration 5700, Loss: 0.5706\n",
      "Iteration 5800, Loss: 0.5700\n",
      "Iteration 5900, Loss: 0.5693\n",
      "211 270\n",
      "Iteration 0, Loss: 1.0890\n",
      "Iteration 100, Loss: 0.7860\n",
      "Iteration 200, Loss: 0.7209\n",
      "Iteration 300, Loss: 0.6908\n",
      "Iteration 400, Loss: 0.6723\n",
      "Iteration 500, Loss: 0.6591\n",
      "Iteration 600, Loss: 0.6491\n",
      "Iteration 700, Loss: 0.6409\n",
      "Iteration 800, Loss: 0.6341\n",
      "Iteration 900, Loss: 0.6283\n",
      "Iteration 1000, Loss: 0.6232\n",
      "Iteration 1100, Loss: 0.6186\n",
      "Iteration 1200, Loss: 0.6146\n",
      "Iteration 1300, Loss: 0.6109\n",
      "Iteration 1400, Loss: 0.6075\n",
      "Iteration 1500, Loss: 0.6044\n",
      "Iteration 1600, Loss: 0.6014\n",
      "Iteration 1700, Loss: 0.5987\n",
      "Iteration 1800, Loss: 0.5962\n",
      "Iteration 1900, Loss: 0.5938\n",
      "Iteration 2000, Loss: 0.5915\n",
      "Iteration 2100, Loss: 0.5894\n",
      "Iteration 2200, Loss: 0.5874\n",
      "Iteration 2300, Loss: 0.5854\n",
      "Iteration 2400, Loss: 0.5836\n",
      "Iteration 2500, Loss: 0.5818\n",
      "Iteration 2600, Loss: 0.5801\n",
      "Iteration 2700, Loss: 0.5785\n",
      "Iteration 2800, Loss: 0.5769\n",
      "Iteration 2900, Loss: 0.5754\n",
      "Iteration 3000, Loss: 0.5740\n",
      "Iteration 3100, Loss: 0.5726\n",
      "Iteration 3200, Loss: 0.5713\n",
      "Iteration 3300, Loss: 0.5700\n",
      "Iteration 3400, Loss: 0.5688\n",
      "Iteration 3500, Loss: 0.5676\n",
      "Iteration 3600, Loss: 0.5664\n",
      "Iteration 3700, Loss: 0.5652\n",
      "Iteration 3800, Loss: 0.5641\n",
      "Iteration 3900, Loss: 0.5631\n",
      "Iteration 4000, Loss: 0.5620\n",
      "Iteration 4100, Loss: 0.5610\n",
      "Iteration 4200, Loss: 0.5601\n",
      "Iteration 4300, Loss: 0.5591\n",
      "Iteration 4400, Loss: 0.5582\n",
      "Iteration 4500, Loss: 0.5573\n",
      "Iteration 4600, Loss: 0.5564\n",
      "Iteration 4700, Loss: 0.5555\n",
      "Iteration 4800, Loss: 0.5547\n",
      "Iteration 4900, Loss: 0.5539\n",
      "Iteration 5000, Loss: 0.5531\n",
      "Iteration 5100, Loss: 0.5523\n",
      "Iteration 5200, Loss: 0.5515\n",
      "Iteration 5300, Loss: 0.5508\n",
      "Iteration 5400, Loss: 0.5500\n",
      "Iteration 5500, Loss: 0.5493\n",
      "Iteration 5600, Loss: 0.5486\n",
      "Iteration 5700, Loss: 0.5479\n",
      "Iteration 5800, Loss: 0.5472\n",
      "Iteration 5900, Loss: 0.5466\n",
      "Iteration 0, Loss: 1.0894\n",
      "Iteration 100, Loss: 0.7893\n",
      "Iteration 200, Loss: 0.7242\n",
      "Iteration 300, Loss: 0.6945\n",
      "Iteration 400, Loss: 0.6765\n",
      "Iteration 500, Loss: 0.6638\n",
      "Iteration 600, Loss: 0.6540\n",
      "Iteration 700, Loss: 0.6462\n",
      "Iteration 800, Loss: 0.6397\n",
      "Iteration 900, Loss: 0.6342\n",
      "Iteration 1000, Loss: 0.6293\n",
      "Iteration 1100, Loss: 0.6251\n",
      "Iteration 1200, Loss: 0.6212\n",
      "Iteration 1300, Loss: 0.6177\n",
      "Iteration 1400, Loss: 0.6146\n",
      "Iteration 1500, Loss: 0.6117\n",
      "Iteration 1600, Loss: 0.6090\n",
      "Iteration 1700, Loss: 0.6064\n",
      "Iteration 1800, Loss: 0.6041\n",
      "Iteration 1900, Loss: 0.6019\n",
      "Iteration 2000, Loss: 0.5998\n",
      "Iteration 2100, Loss: 0.5978\n",
      "Iteration 2200, Loss: 0.5959\n",
      "Iteration 2300, Loss: 0.5942\n",
      "Iteration 2400, Loss: 0.5925\n",
      "Iteration 2500, Loss: 0.5908\n",
      "Iteration 2600, Loss: 0.5893\n",
      "Iteration 2700, Loss: 0.5878\n",
      "Iteration 2800, Loss: 0.5864\n",
      "Iteration 2900, Loss: 0.5850\n",
      "Iteration 3000, Loss: 0.5837\n",
      "Iteration 3100, Loss: 0.5824\n",
      "Iteration 3200, Loss: 0.5812\n",
      "Iteration 3300, Loss: 0.5800\n",
      "Iteration 3400, Loss: 0.5789\n",
      "Iteration 3500, Loss: 0.5778\n",
      "Iteration 3600, Loss: 0.5767\n",
      "Iteration 3700, Loss: 0.5756\n",
      "Iteration 3800, Loss: 0.5746\n",
      "Iteration 3900, Loss: 0.5737\n",
      "Iteration 4000, Loss: 0.5727\n",
      "Iteration 4100, Loss: 0.5718\n",
      "Iteration 4200, Loss: 0.5709\n",
      "Iteration 4300, Loss: 0.5700\n",
      "Iteration 4400, Loss: 0.5692\n",
      "Iteration 4500, Loss: 0.5684\n",
      "Iteration 4600, Loss: 0.5675\n",
      "Iteration 4700, Loss: 0.5667\n",
      "Iteration 4800, Loss: 0.5660\n",
      "Iteration 4900, Loss: 0.5652\n",
      "Iteration 5000, Loss: 0.5645\n",
      "Iteration 5100, Loss: 0.5638\n",
      "Iteration 5200, Loss: 0.5631\n",
      "Iteration 5300, Loss: 0.5624\n",
      "Iteration 5400, Loss: 0.5617\n",
      "Iteration 5500, Loss: 0.5610\n",
      "Iteration 5600, Loss: 0.5604\n",
      "Iteration 5700, Loss: 0.5597\n",
      "Iteration 5800, Loss: 0.5591\n",
      "Iteration 5900, Loss: 0.5585\n",
      "212 270\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7936\n",
      "Iteration 200, Loss: 0.7282\n",
      "Iteration 300, Loss: 0.6973\n",
      "Iteration 400, Loss: 0.6781\n",
      "Iteration 500, Loss: 0.6645\n",
      "Iteration 600, Loss: 0.6540\n",
      "Iteration 700, Loss: 0.6456\n",
      "Iteration 800, Loss: 0.6387\n",
      "Iteration 900, Loss: 0.6327\n",
      "Iteration 1000, Loss: 0.6276\n",
      "Iteration 1100, Loss: 0.6231\n",
      "Iteration 1200, Loss: 0.6190\n",
      "Iteration 1300, Loss: 0.6153\n",
      "Iteration 1400, Loss: 0.6119\n",
      "Iteration 1500, Loss: 0.6088\n",
      "Iteration 1600, Loss: 0.6059\n",
      "Iteration 1700, Loss: 0.6033\n",
      "Iteration 1800, Loss: 0.6008\n",
      "Iteration 1900, Loss: 0.5984\n",
      "Iteration 2000, Loss: 0.5962\n",
      "Iteration 2100, Loss: 0.5942\n",
      "Iteration 2200, Loss: 0.5922\n",
      "Iteration 2300, Loss: 0.5904\n",
      "Iteration 2400, Loss: 0.5886\n",
      "Iteration 2500, Loss: 0.5869\n",
      "Iteration 2600, Loss: 0.5853\n",
      "Iteration 2700, Loss: 0.5838\n",
      "Iteration 2800, Loss: 0.5823\n",
      "Iteration 2900, Loss: 0.5809\n",
      "Iteration 3000, Loss: 0.5795\n",
      "Iteration 3100, Loss: 0.5782\n",
      "Iteration 3200, Loss: 0.5770\n",
      "Iteration 3300, Loss: 0.5758\n",
      "Iteration 3400, Loss: 0.5746\n",
      "Iteration 3500, Loss: 0.5735\n",
      "Iteration 3600, Loss: 0.5724\n",
      "Iteration 3700, Loss: 0.5713\n",
      "Iteration 3800, Loss: 0.5703\n",
      "Iteration 3900, Loss: 0.5693\n",
      "Iteration 4000, Loss: 0.5684\n",
      "Iteration 4100, Loss: 0.5674\n",
      "Iteration 4200, Loss: 0.5665\n",
      "Iteration 4300, Loss: 0.5657\n",
      "Iteration 4400, Loss: 0.5648\n",
      "Iteration 4500, Loss: 0.5640\n",
      "Iteration 4600, Loss: 0.5632\n",
      "Iteration 4700, Loss: 0.5624\n",
      "Iteration 4800, Loss: 0.5616\n",
      "Iteration 4900, Loss: 0.5608\n",
      "Iteration 5000, Loss: 0.5601\n",
      "Iteration 5100, Loss: 0.5594\n",
      "Iteration 5200, Loss: 0.5587\n",
      "Iteration 5300, Loss: 0.5580\n",
      "Iteration 5400, Loss: 0.5573\n",
      "Iteration 5500, Loss: 0.5567\n",
      "Iteration 5600, Loss: 0.5560\n",
      "Iteration 5700, Loss: 0.5554\n",
      "Iteration 5800, Loss: 0.5548\n",
      "Iteration 5900, Loss: 0.5542\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7816\n",
      "Iteration 200, Loss: 0.7159\n",
      "Iteration 300, Loss: 0.6867\n",
      "Iteration 400, Loss: 0.6689\n",
      "Iteration 500, Loss: 0.6563\n",
      "Iteration 600, Loss: 0.6467\n",
      "Iteration 700, Loss: 0.6389\n",
      "Iteration 800, Loss: 0.6325\n",
      "Iteration 900, Loss: 0.6269\n",
      "Iteration 1000, Loss: 0.6221\n",
      "Iteration 1100, Loss: 0.6178\n",
      "Iteration 1200, Loss: 0.6140\n",
      "Iteration 1300, Loss: 0.6106\n",
      "Iteration 1400, Loss: 0.6074\n",
      "Iteration 1500, Loss: 0.6045\n",
      "Iteration 1600, Loss: 0.6018\n",
      "Iteration 1700, Loss: 0.5992\n",
      "Iteration 1800, Loss: 0.5968\n",
      "Iteration 1900, Loss: 0.5946\n",
      "Iteration 2000, Loss: 0.5925\n",
      "Iteration 2100, Loss: 0.5905\n",
      "Iteration 2200, Loss: 0.5886\n",
      "Iteration 2300, Loss: 0.5868\n",
      "Iteration 2400, Loss: 0.5851\n",
      "Iteration 2500, Loss: 0.5834\n",
      "Iteration 2600, Loss: 0.5819\n",
      "Iteration 2700, Loss: 0.5803\n",
      "Iteration 2800, Loss: 0.5789\n",
      "Iteration 2900, Loss: 0.5775\n",
      "Iteration 3000, Loss: 0.5762\n",
      "Iteration 3100, Loss: 0.5749\n",
      "Iteration 3200, Loss: 0.5736\n",
      "Iteration 3300, Loss: 0.5724\n",
      "Iteration 3400, Loss: 0.5712\n",
      "Iteration 3500, Loss: 0.5701\n",
      "Iteration 3600, Loss: 0.5690\n",
      "Iteration 3700, Loss: 0.5680\n",
      "Iteration 3800, Loss: 0.5669\n",
      "Iteration 3900, Loss: 0.5660\n",
      "Iteration 4000, Loss: 0.5650\n",
      "Iteration 4100, Loss: 0.5640\n",
      "Iteration 4200, Loss: 0.5631\n",
      "Iteration 4300, Loss: 0.5622\n",
      "Iteration 4400, Loss: 0.5614\n",
      "Iteration 4500, Loss: 0.5605\n",
      "Iteration 4600, Loss: 0.5597\n",
      "Iteration 4700, Loss: 0.5589\n",
      "Iteration 4800, Loss: 0.5581\n",
      "Iteration 4900, Loss: 0.5574\n",
      "Iteration 5000, Loss: 0.5566\n",
      "Iteration 5100, Loss: 0.5559\n",
      "Iteration 5200, Loss: 0.5552\n",
      "Iteration 5300, Loss: 0.5545\n",
      "Iteration 5400, Loss: 0.5538\n",
      "Iteration 5500, Loss: 0.5531\n",
      "Iteration 5600, Loss: 0.5524\n",
      "Iteration 5700, Loss: 0.5518\n",
      "Iteration 5800, Loss: 0.5512\n",
      "Iteration 5900, Loss: 0.5505\n",
      "213 270\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7820\n",
      "Iteration 200, Loss: 0.7118\n",
      "Iteration 300, Loss: 0.6779\n",
      "Iteration 400, Loss: 0.6567\n",
      "Iteration 500, Loss: 0.6415\n",
      "Iteration 600, Loss: 0.6300\n",
      "Iteration 700, Loss: 0.6208\n",
      "Iteration 800, Loss: 0.6131\n",
      "Iteration 900, Loss: 0.6066\n",
      "Iteration 1000, Loss: 0.6010\n",
      "Iteration 1100, Loss: 0.5960\n",
      "Iteration 1200, Loss: 0.5916\n",
      "Iteration 1300, Loss: 0.5876\n",
      "Iteration 1400, Loss: 0.5840\n",
      "Iteration 1500, Loss: 0.5807\n",
      "Iteration 1600, Loss: 0.5776\n",
      "Iteration 1700, Loss: 0.5748\n",
      "Iteration 1800, Loss: 0.5722\n",
      "Iteration 1900, Loss: 0.5697\n",
      "Iteration 2000, Loss: 0.5674\n",
      "Iteration 2100, Loss: 0.5652\n",
      "Iteration 2200, Loss: 0.5631\n",
      "Iteration 2300, Loss: 0.5612\n",
      "Iteration 2400, Loss: 0.5593\n",
      "Iteration 2500, Loss: 0.5576\n",
      "Iteration 2600, Loss: 0.5559\n",
      "Iteration 2700, Loss: 0.5543\n",
      "Iteration 2800, Loss: 0.5528\n",
      "Iteration 2900, Loss: 0.5513\n",
      "Iteration 3000, Loss: 0.5499\n",
      "Iteration 3100, Loss: 0.5486\n",
      "Iteration 3200, Loss: 0.5473\n",
      "Iteration 3300, Loss: 0.5460\n",
      "Iteration 3400, Loss: 0.5448\n",
      "Iteration 3500, Loss: 0.5437\n",
      "Iteration 3600, Loss: 0.5426\n",
      "Iteration 3700, Loss: 0.5415\n",
      "Iteration 3800, Loss: 0.5405\n",
      "Iteration 3900, Loss: 0.5394\n",
      "Iteration 4000, Loss: 0.5385\n",
      "Iteration 4100, Loss: 0.5375\n",
      "Iteration 4200, Loss: 0.5366\n",
      "Iteration 4300, Loss: 0.5357\n",
      "Iteration 4400, Loss: 0.5349\n",
      "Iteration 4500, Loss: 0.5340\n",
      "Iteration 4600, Loss: 0.5332\n",
      "Iteration 4700, Loss: 0.5324\n",
      "Iteration 4800, Loss: 0.5317\n",
      "Iteration 4900, Loss: 0.5309\n",
      "Iteration 5000, Loss: 0.5302\n",
      "Iteration 5100, Loss: 0.5295\n",
      "Iteration 5200, Loss: 0.5288\n",
      "Iteration 5300, Loss: 0.5281\n",
      "Iteration 5400, Loss: 0.5274\n",
      "Iteration 5500, Loss: 0.5268\n",
      "Iteration 5600, Loss: 0.5262\n",
      "Iteration 5700, Loss: 0.5255\n",
      "Iteration 5800, Loss: 0.5249\n",
      "Iteration 5900, Loss: 0.5243\n",
      "Iteration 0, Loss: 1.0894\n",
      "Iteration 100, Loss: 0.7915\n",
      "Iteration 200, Loss: 0.7304\n",
      "Iteration 300, Loss: 0.7039\n",
      "Iteration 400, Loss: 0.6879\n",
      "Iteration 500, Loss: 0.6766\n",
      "Iteration 600, Loss: 0.6680\n",
      "Iteration 700, Loss: 0.6610\n",
      "Iteration 800, Loss: 0.6551\n",
      "Iteration 900, Loss: 0.6501\n",
      "Iteration 1000, Loss: 0.6456\n",
      "Iteration 1100, Loss: 0.6417\n",
      "Iteration 1200, Loss: 0.6381\n",
      "Iteration 1300, Loss: 0.6349\n",
      "Iteration 1400, Loss: 0.6319\n",
      "Iteration 1500, Loss: 0.6292\n",
      "Iteration 1600, Loss: 0.6266\n",
      "Iteration 1700, Loss: 0.6242\n",
      "Iteration 1800, Loss: 0.6219\n",
      "Iteration 1900, Loss: 0.6198\n",
      "Iteration 2000, Loss: 0.6177\n",
      "Iteration 2100, Loss: 0.6158\n",
      "Iteration 2200, Loss: 0.6140\n",
      "Iteration 2300, Loss: 0.6122\n",
      "Iteration 2400, Loss: 0.6105\n",
      "Iteration 2500, Loss: 0.6089\n",
      "Iteration 2600, Loss: 0.6074\n",
      "Iteration 2700, Loss: 0.6059\n",
      "Iteration 2800, Loss: 0.6045\n",
      "Iteration 2900, Loss: 0.6031\n",
      "Iteration 3000, Loss: 0.6018\n",
      "Iteration 3100, Loss: 0.6005\n",
      "Iteration 3200, Loss: 0.5992\n",
      "Iteration 3300, Loss: 0.5980\n",
      "Iteration 3400, Loss: 0.5968\n",
      "Iteration 3500, Loss: 0.5957\n",
      "Iteration 3600, Loss: 0.5946\n",
      "Iteration 3700, Loss: 0.5935\n",
      "Iteration 3800, Loss: 0.5925\n",
      "Iteration 3900, Loss: 0.5915\n",
      "Iteration 4000, Loss: 0.5905\n",
      "Iteration 4100, Loss: 0.5895\n",
      "Iteration 4200, Loss: 0.5886\n",
      "Iteration 4300, Loss: 0.5876\n",
      "Iteration 4400, Loss: 0.5868\n",
      "Iteration 4500, Loss: 0.5859\n",
      "Iteration 4600, Loss: 0.5850\n",
      "Iteration 4700, Loss: 0.5842\n",
      "Iteration 4800, Loss: 0.5834\n",
      "Iteration 4900, Loss: 0.5826\n",
      "Iteration 5000, Loss: 0.5818\n",
      "Iteration 5100, Loss: 0.5810\n",
      "Iteration 5200, Loss: 0.5803\n",
      "Iteration 5300, Loss: 0.5795\n",
      "Iteration 5400, Loss: 0.5788\n",
      "Iteration 5500, Loss: 0.5781\n",
      "Iteration 5600, Loss: 0.5774\n",
      "Iteration 5700, Loss: 0.5767\n",
      "Iteration 5800, Loss: 0.5761\n",
      "Iteration 5900, Loss: 0.5754\n",
      "214 270\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7879\n",
      "Iteration 200, Loss: 0.7232\n",
      "Iteration 300, Loss: 0.6930\n",
      "Iteration 400, Loss: 0.6743\n",
      "Iteration 500, Loss: 0.6612\n",
      "Iteration 600, Loss: 0.6512\n",
      "Iteration 700, Loss: 0.6433\n",
      "Iteration 800, Loss: 0.6367\n",
      "Iteration 900, Loss: 0.6311\n",
      "Iteration 1000, Loss: 0.6263\n",
      "Iteration 1100, Loss: 0.6220\n",
      "Iteration 1200, Loss: 0.6182\n",
      "Iteration 1300, Loss: 0.6148\n",
      "Iteration 1400, Loss: 0.6117\n",
      "Iteration 1500, Loss: 0.6088\n",
      "Iteration 1600, Loss: 0.6062\n",
      "Iteration 1700, Loss: 0.6037\n",
      "Iteration 1800, Loss: 0.6014\n",
      "Iteration 1900, Loss: 0.5992\n",
      "Iteration 2000, Loss: 0.5972\n",
      "Iteration 2100, Loss: 0.5953\n",
      "Iteration 2200, Loss: 0.5934\n",
      "Iteration 2300, Loss: 0.5917\n",
      "Iteration 2400, Loss: 0.5901\n",
      "Iteration 2500, Loss: 0.5885\n",
      "Iteration 2600, Loss: 0.5870\n",
      "Iteration 2700, Loss: 0.5856\n",
      "Iteration 2800, Loss: 0.5842\n",
      "Iteration 2900, Loss: 0.5829\n",
      "Iteration 3000, Loss: 0.5816\n",
      "Iteration 3100, Loss: 0.5804\n",
      "Iteration 3200, Loss: 0.5792\n",
      "Iteration 3300, Loss: 0.5781\n",
      "Iteration 3400, Loss: 0.5770\n",
      "Iteration 3500, Loss: 0.5760\n",
      "Iteration 3600, Loss: 0.5749\n",
      "Iteration 3700, Loss: 0.5739\n",
      "Iteration 3800, Loss: 0.5730\n",
      "Iteration 3900, Loss: 0.5720\n",
      "Iteration 4000, Loss: 0.5711\n",
      "Iteration 4100, Loss: 0.5702\n",
      "Iteration 4200, Loss: 0.5694\n",
      "Iteration 4300, Loss: 0.5685\n",
      "Iteration 4400, Loss: 0.5677\n",
      "Iteration 4500, Loss: 0.5669\n",
      "Iteration 4600, Loss: 0.5662\n",
      "Iteration 4700, Loss: 0.5654\n",
      "Iteration 4800, Loss: 0.5647\n",
      "Iteration 4900, Loss: 0.5640\n",
      "Iteration 5000, Loss: 0.5633\n",
      "Iteration 5100, Loss: 0.5626\n",
      "Iteration 5200, Loss: 0.5619\n",
      "Iteration 5300, Loss: 0.5613\n",
      "Iteration 5400, Loss: 0.5606\n",
      "Iteration 5500, Loss: 0.5600\n",
      "Iteration 5600, Loss: 0.5594\n",
      "Iteration 5700, Loss: 0.5588\n",
      "Iteration 5800, Loss: 0.5582\n",
      "Iteration 5900, Loss: 0.5576\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7874\n",
      "Iteration 200, Loss: 0.7222\n",
      "Iteration 300, Loss: 0.6925\n",
      "Iteration 400, Loss: 0.6743\n",
      "Iteration 500, Loss: 0.6613\n",
      "Iteration 600, Loss: 0.6512\n",
      "Iteration 700, Loss: 0.6431\n",
      "Iteration 800, Loss: 0.6363\n",
      "Iteration 900, Loss: 0.6305\n",
      "Iteration 1000, Loss: 0.6254\n",
      "Iteration 1100, Loss: 0.6209\n",
      "Iteration 1200, Loss: 0.6168\n",
      "Iteration 1300, Loss: 0.6131\n",
      "Iteration 1400, Loss: 0.6098\n",
      "Iteration 1500, Loss: 0.6066\n",
      "Iteration 1600, Loss: 0.6037\n",
      "Iteration 1700, Loss: 0.6011\n",
      "Iteration 1800, Loss: 0.5985\n",
      "Iteration 1900, Loss: 0.5961\n",
      "Iteration 2000, Loss: 0.5939\n",
      "Iteration 2100, Loss: 0.5918\n",
      "Iteration 2200, Loss: 0.5897\n",
      "Iteration 2300, Loss: 0.5878\n",
      "Iteration 2400, Loss: 0.5860\n",
      "Iteration 2500, Loss: 0.5842\n",
      "Iteration 2600, Loss: 0.5825\n",
      "Iteration 2700, Loss: 0.5809\n",
      "Iteration 2800, Loss: 0.5794\n",
      "Iteration 2900, Loss: 0.5779\n",
      "Iteration 3000, Loss: 0.5764\n",
      "Iteration 3100, Loss: 0.5751\n",
      "Iteration 3200, Loss: 0.5737\n",
      "Iteration 3300, Loss: 0.5724\n",
      "Iteration 3400, Loss: 0.5712\n",
      "Iteration 3500, Loss: 0.5700\n",
      "Iteration 3600, Loss: 0.5688\n",
      "Iteration 3700, Loss: 0.5677\n",
      "Iteration 3800, Loss: 0.5666\n",
      "Iteration 3900, Loss: 0.5655\n",
      "Iteration 4000, Loss: 0.5645\n",
      "Iteration 4100, Loss: 0.5635\n",
      "Iteration 4200, Loss: 0.5625\n",
      "Iteration 4300, Loss: 0.5616\n",
      "Iteration 4400, Loss: 0.5606\n",
      "Iteration 4500, Loss: 0.5597\n",
      "Iteration 4600, Loss: 0.5589\n",
      "Iteration 4700, Loss: 0.5580\n",
      "Iteration 4800, Loss: 0.5572\n",
      "Iteration 4900, Loss: 0.5564\n",
      "Iteration 5000, Loss: 0.5556\n",
      "Iteration 5100, Loss: 0.5548\n",
      "Iteration 5200, Loss: 0.5540\n",
      "Iteration 5300, Loss: 0.5533\n",
      "Iteration 5400, Loss: 0.5526\n",
      "Iteration 5500, Loss: 0.5519\n",
      "Iteration 5600, Loss: 0.5512\n",
      "Iteration 5700, Loss: 0.5505\n",
      "Iteration 5800, Loss: 0.5498\n",
      "Iteration 5900, Loss: 0.5492\n",
      "215 270\n",
      "Iteration 0, Loss: 1.0894\n",
      "Iteration 100, Loss: 0.7844\n",
      "Iteration 200, Loss: 0.7197\n",
      "Iteration 300, Loss: 0.6911\n",
      "Iteration 400, Loss: 0.6736\n",
      "Iteration 500, Loss: 0.6612\n",
      "Iteration 600, Loss: 0.6515\n",
      "Iteration 700, Loss: 0.6437\n",
      "Iteration 800, Loss: 0.6371\n",
      "Iteration 900, Loss: 0.6315\n",
      "Iteration 1000, Loss: 0.6265\n",
      "Iteration 1100, Loss: 0.6221\n",
      "Iteration 1200, Loss: 0.6182\n",
      "Iteration 1300, Loss: 0.6146\n",
      "Iteration 1400, Loss: 0.6113\n",
      "Iteration 1500, Loss: 0.6083\n",
      "Iteration 1600, Loss: 0.6055\n",
      "Iteration 1700, Loss: 0.6028\n",
      "Iteration 1800, Loss: 0.6004\n",
      "Iteration 1900, Loss: 0.5981\n",
      "Iteration 2000, Loss: 0.5959\n",
      "Iteration 2100, Loss: 0.5938\n",
      "Iteration 2200, Loss: 0.5918\n",
      "Iteration 2300, Loss: 0.5899\n",
      "Iteration 2400, Loss: 0.5881\n",
      "Iteration 2500, Loss: 0.5864\n",
      "Iteration 2600, Loss: 0.5848\n",
      "Iteration 2700, Loss: 0.5832\n",
      "Iteration 2800, Loss: 0.5817\n",
      "Iteration 2900, Loss: 0.5802\n",
      "Iteration 3000, Loss: 0.5788\n",
      "Iteration 3100, Loss: 0.5774\n",
      "Iteration 3200, Loss: 0.5761\n",
      "Iteration 3300, Loss: 0.5749\n",
      "Iteration 3400, Loss: 0.5736\n",
      "Iteration 3500, Loss: 0.5725\n",
      "Iteration 3600, Loss: 0.5713\n",
      "Iteration 3700, Loss: 0.5702\n",
      "Iteration 3800, Loss: 0.5691\n",
      "Iteration 3900, Loss: 0.5680\n",
      "Iteration 4000, Loss: 0.5670\n",
      "Iteration 4100, Loss: 0.5660\n",
      "Iteration 4200, Loss: 0.5650\n",
      "Iteration 4300, Loss: 0.5641\n",
      "Iteration 4400, Loss: 0.5631\n",
      "Iteration 4500, Loss: 0.5622\n",
      "Iteration 4600, Loss: 0.5614\n",
      "Iteration 4700, Loss: 0.5605\n",
      "Iteration 4800, Loss: 0.5596\n",
      "Iteration 4900, Loss: 0.5588\n",
      "Iteration 5000, Loss: 0.5580\n",
      "Iteration 5100, Loss: 0.5572\n",
      "Iteration 5200, Loss: 0.5564\n",
      "Iteration 5300, Loss: 0.5557\n",
      "Iteration 5400, Loss: 0.5549\n",
      "Iteration 5500, Loss: 0.5542\n",
      "Iteration 5600, Loss: 0.5535\n",
      "Iteration 5700, Loss: 0.5528\n",
      "Iteration 5800, Loss: 0.5521\n",
      "Iteration 5900, Loss: 0.5514\n",
      "Iteration 0, Loss: 1.0888\n",
      "Iteration 100, Loss: 0.7878\n",
      "Iteration 200, Loss: 0.7224\n",
      "Iteration 300, Loss: 0.6910\n",
      "Iteration 400, Loss: 0.6714\n",
      "Iteration 500, Loss: 0.6574\n",
      "Iteration 600, Loss: 0.6467\n",
      "Iteration 700, Loss: 0.6381\n",
      "Iteration 800, Loss: 0.6310\n",
      "Iteration 900, Loss: 0.6249\n",
      "Iteration 1000, Loss: 0.6197\n",
      "Iteration 1100, Loss: 0.6150\n",
      "Iteration 1200, Loss: 0.6108\n",
      "Iteration 1300, Loss: 0.6070\n",
      "Iteration 1400, Loss: 0.6035\n",
      "Iteration 1500, Loss: 0.6003\n",
      "Iteration 1600, Loss: 0.5974\n",
      "Iteration 1700, Loss: 0.5946\n",
      "Iteration 1800, Loss: 0.5920\n",
      "Iteration 1900, Loss: 0.5895\n",
      "Iteration 2000, Loss: 0.5872\n",
      "Iteration 2100, Loss: 0.5850\n",
      "Iteration 2200, Loss: 0.5829\n",
      "Iteration 2300, Loss: 0.5809\n",
      "Iteration 2400, Loss: 0.5790\n",
      "Iteration 2500, Loss: 0.5772\n",
      "Iteration 2600, Loss: 0.5755\n",
      "Iteration 2700, Loss: 0.5738\n",
      "Iteration 2800, Loss: 0.5722\n",
      "Iteration 2900, Loss: 0.5707\n",
      "Iteration 3000, Loss: 0.5692\n",
      "Iteration 3100, Loss: 0.5677\n",
      "Iteration 3200, Loss: 0.5663\n",
      "Iteration 3300, Loss: 0.5650\n",
      "Iteration 3400, Loss: 0.5637\n",
      "Iteration 3500, Loss: 0.5624\n",
      "Iteration 3600, Loss: 0.5612\n",
      "Iteration 3700, Loss: 0.5600\n",
      "Iteration 3800, Loss: 0.5589\n",
      "Iteration 3900, Loss: 0.5578\n",
      "Iteration 4000, Loss: 0.5567\n",
      "Iteration 4100, Loss: 0.5556\n",
      "Iteration 4200, Loss: 0.5546\n",
      "Iteration 4300, Loss: 0.5536\n",
      "Iteration 4400, Loss: 0.5526\n",
      "Iteration 4500, Loss: 0.5517\n",
      "Iteration 4600, Loss: 0.5508\n",
      "Iteration 4700, Loss: 0.5499\n",
      "Iteration 4800, Loss: 0.5490\n",
      "Iteration 4900, Loss: 0.5481\n",
      "Iteration 5000, Loss: 0.5473\n",
      "Iteration 5100, Loss: 0.5464\n",
      "Iteration 5200, Loss: 0.5456\n",
      "Iteration 5300, Loss: 0.5448\n",
      "Iteration 5400, Loss: 0.5441\n",
      "Iteration 5500, Loss: 0.5433\n",
      "Iteration 5600, Loss: 0.5426\n",
      "Iteration 5700, Loss: 0.5418\n",
      "Iteration 5800, Loss: 0.5411\n",
      "Iteration 5900, Loss: 0.5404\n",
      "216 270\n",
      "Iteration 0, Loss: 1.0898\n",
      "Iteration 100, Loss: 0.7850\n",
      "Iteration 200, Loss: 0.7193\n",
      "Iteration 300, Loss: 0.6895\n",
      "Iteration 400, Loss: 0.6710\n",
      "Iteration 500, Loss: 0.6580\n",
      "Iteration 600, Loss: 0.6480\n",
      "Iteration 700, Loss: 0.6400\n",
      "Iteration 800, Loss: 0.6333\n",
      "Iteration 900, Loss: 0.6276\n",
      "Iteration 1000, Loss: 0.6227\n",
      "Iteration 1100, Loss: 0.6183\n",
      "Iteration 1200, Loss: 0.6144\n",
      "Iteration 1300, Loss: 0.6108\n",
      "Iteration 1400, Loss: 0.6075\n",
      "Iteration 1500, Loss: 0.6045\n",
      "Iteration 1600, Loss: 0.6017\n",
      "Iteration 1700, Loss: 0.5991\n",
      "Iteration 1800, Loss: 0.5966\n",
      "Iteration 1900, Loss: 0.5943\n",
      "Iteration 2000, Loss: 0.5921\n",
      "Iteration 2100, Loss: 0.5901\n",
      "Iteration 2200, Loss: 0.5881\n",
      "Iteration 2300, Loss: 0.5862\n",
      "Iteration 2400, Loss: 0.5845\n",
      "Iteration 2500, Loss: 0.5828\n",
      "Iteration 2600, Loss: 0.5811\n",
      "Iteration 2700, Loss: 0.5796\n",
      "Iteration 2800, Loss: 0.5781\n",
      "Iteration 2900, Loss: 0.5766\n",
      "Iteration 3000, Loss: 0.5752\n",
      "Iteration 3100, Loss: 0.5739\n",
      "Iteration 3200, Loss: 0.5726\n",
      "Iteration 3300, Loss: 0.5713\n",
      "Iteration 3400, Loss: 0.5701\n",
      "Iteration 3500, Loss: 0.5689\n",
      "Iteration 3600, Loss: 0.5678\n",
      "Iteration 3700, Loss: 0.5667\n",
      "Iteration 3800, Loss: 0.5656\n",
      "Iteration 3900, Loss: 0.5646\n",
      "Iteration 4000, Loss: 0.5636\n",
      "Iteration 4100, Loss: 0.5626\n",
      "Iteration 4200, Loss: 0.5616\n",
      "Iteration 4300, Loss: 0.5607\n",
      "Iteration 4400, Loss: 0.5598\n",
      "Iteration 4500, Loss: 0.5589\n",
      "Iteration 4600, Loss: 0.5580\n",
      "Iteration 4700, Loss: 0.5571\n",
      "Iteration 4800, Loss: 0.5563\n",
      "Iteration 4900, Loss: 0.5555\n",
      "Iteration 5000, Loss: 0.5547\n",
      "Iteration 5100, Loss: 0.5539\n",
      "Iteration 5200, Loss: 0.5532\n",
      "Iteration 5300, Loss: 0.5524\n",
      "Iteration 5400, Loss: 0.5517\n",
      "Iteration 5500, Loss: 0.5510\n",
      "Iteration 5600, Loss: 0.5503\n",
      "Iteration 5700, Loss: 0.5496\n",
      "Iteration 5800, Loss: 0.5489\n",
      "Iteration 5900, Loss: 0.5483\n",
      "Iteration 0, Loss: 1.0890\n",
      "Iteration 100, Loss: 0.7876\n",
      "Iteration 200, Loss: 0.7225\n",
      "Iteration 300, Loss: 0.6924\n",
      "Iteration 400, Loss: 0.6735\n",
      "Iteration 500, Loss: 0.6600\n",
      "Iteration 600, Loss: 0.6497\n",
      "Iteration 700, Loss: 0.6412\n",
      "Iteration 800, Loss: 0.6342\n",
      "Iteration 900, Loss: 0.6282\n",
      "Iteration 1000, Loss: 0.6229\n",
      "Iteration 1100, Loss: 0.6183\n",
      "Iteration 1200, Loss: 0.6141\n",
      "Iteration 1300, Loss: 0.6103\n",
      "Iteration 1400, Loss: 0.6069\n",
      "Iteration 1500, Loss: 0.6037\n",
      "Iteration 1600, Loss: 0.6007\n",
      "Iteration 1700, Loss: 0.5980\n",
      "Iteration 1800, Loss: 0.5954\n",
      "Iteration 1900, Loss: 0.5929\n",
      "Iteration 2000, Loss: 0.5906\n",
      "Iteration 2100, Loss: 0.5885\n",
      "Iteration 2200, Loss: 0.5864\n",
      "Iteration 2300, Loss: 0.5844\n",
      "Iteration 2400, Loss: 0.5825\n",
      "Iteration 2500, Loss: 0.5807\n",
      "Iteration 2600, Loss: 0.5790\n",
      "Iteration 2700, Loss: 0.5773\n",
      "Iteration 2800, Loss: 0.5757\n",
      "Iteration 2900, Loss: 0.5742\n",
      "Iteration 3000, Loss: 0.5727\n",
      "Iteration 3100, Loss: 0.5713\n",
      "Iteration 3200, Loss: 0.5699\n",
      "Iteration 3300, Loss: 0.5686\n",
      "Iteration 3400, Loss: 0.5673\n",
      "Iteration 3500, Loss: 0.5660\n",
      "Iteration 3600, Loss: 0.5648\n",
      "Iteration 3700, Loss: 0.5636\n",
      "Iteration 3800, Loss: 0.5625\n",
      "Iteration 3900, Loss: 0.5613\n",
      "Iteration 4000, Loss: 0.5603\n",
      "Iteration 4100, Loss: 0.5592\n",
      "Iteration 4200, Loss: 0.5582\n",
      "Iteration 4300, Loss: 0.5572\n",
      "Iteration 4400, Loss: 0.5562\n",
      "Iteration 4500, Loss: 0.5552\n",
      "Iteration 4600, Loss: 0.5543\n",
      "Iteration 4700, Loss: 0.5534\n",
      "Iteration 4800, Loss: 0.5525\n",
      "Iteration 4900, Loss: 0.5516\n",
      "Iteration 5000, Loss: 0.5507\n",
      "Iteration 5100, Loss: 0.5499\n",
      "Iteration 5200, Loss: 0.5491\n",
      "Iteration 5300, Loss: 0.5483\n",
      "Iteration 5400, Loss: 0.5475\n",
      "Iteration 5500, Loss: 0.5467\n",
      "Iteration 5600, Loss: 0.5460\n",
      "Iteration 5700, Loss: 0.5452\n",
      "Iteration 5800, Loss: 0.5445\n",
      "Iteration 5900, Loss: 0.5438\n",
      "217 270\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7827\n",
      "Iteration 200, Loss: 0.7181\n",
      "Iteration 300, Loss: 0.6892\n",
      "Iteration 400, Loss: 0.6716\n",
      "Iteration 500, Loss: 0.6590\n",
      "Iteration 600, Loss: 0.6494\n",
      "Iteration 700, Loss: 0.6417\n",
      "Iteration 800, Loss: 0.6353\n",
      "Iteration 900, Loss: 0.6299\n",
      "Iteration 1000, Loss: 0.6252\n",
      "Iteration 1100, Loss: 0.6210\n",
      "Iteration 1200, Loss: 0.6173\n",
      "Iteration 1300, Loss: 0.6139\n",
      "Iteration 1400, Loss: 0.6108\n",
      "Iteration 1500, Loss: 0.6079\n",
      "Iteration 1600, Loss: 0.6053\n",
      "Iteration 1700, Loss: 0.6028\n",
      "Iteration 1800, Loss: 0.6005\n",
      "Iteration 1900, Loss: 0.5983\n",
      "Iteration 2000, Loss: 0.5963\n",
      "Iteration 2100, Loss: 0.5943\n",
      "Iteration 2200, Loss: 0.5925\n",
      "Iteration 2300, Loss: 0.5907\n",
      "Iteration 2400, Loss: 0.5890\n",
      "Iteration 2500, Loss: 0.5874\n",
      "Iteration 2600, Loss: 0.5858\n",
      "Iteration 2700, Loss: 0.5844\n",
      "Iteration 2800, Loss: 0.5829\n",
      "Iteration 2900, Loss: 0.5815\n",
      "Iteration 3000, Loss: 0.5802\n",
      "Iteration 3100, Loss: 0.5789\n",
      "Iteration 3200, Loss: 0.5777\n",
      "Iteration 3300, Loss: 0.5765\n",
      "Iteration 3400, Loss: 0.5754\n",
      "Iteration 3500, Loss: 0.5742\n",
      "Iteration 3600, Loss: 0.5731\n",
      "Iteration 3700, Loss: 0.5721\n",
      "Iteration 3800, Loss: 0.5711\n",
      "Iteration 3900, Loss: 0.5701\n",
      "Iteration 4000, Loss: 0.5691\n",
      "Iteration 4100, Loss: 0.5682\n",
      "Iteration 4200, Loss: 0.5672\n",
      "Iteration 4300, Loss: 0.5663\n",
      "Iteration 4400, Loss: 0.5655\n",
      "Iteration 4500, Loss: 0.5646\n",
      "Iteration 4600, Loss: 0.5638\n",
      "Iteration 4700, Loss: 0.5630\n",
      "Iteration 4800, Loss: 0.5622\n",
      "Iteration 4900, Loss: 0.5614\n",
      "Iteration 5000, Loss: 0.5606\n",
      "Iteration 5100, Loss: 0.5599\n",
      "Iteration 5200, Loss: 0.5592\n",
      "Iteration 5300, Loss: 0.5584\n",
      "Iteration 5400, Loss: 0.5577\n",
      "Iteration 5500, Loss: 0.5571\n",
      "Iteration 5600, Loss: 0.5564\n",
      "Iteration 5700, Loss: 0.5557\n",
      "Iteration 5800, Loss: 0.5551\n",
      "Iteration 5900, Loss: 0.5544\n",
      "Iteration 0, Loss: 1.0890\n",
      "Iteration 100, Loss: 0.7887\n",
      "Iteration 200, Loss: 0.7228\n",
      "Iteration 300, Loss: 0.6924\n",
      "Iteration 400, Loss: 0.6737\n",
      "Iteration 500, Loss: 0.6604\n",
      "Iteration 600, Loss: 0.6503\n",
      "Iteration 700, Loss: 0.6420\n",
      "Iteration 800, Loss: 0.6352\n",
      "Iteration 900, Loss: 0.6293\n",
      "Iteration 1000, Loss: 0.6242\n",
      "Iteration 1100, Loss: 0.6197\n",
      "Iteration 1200, Loss: 0.6156\n",
      "Iteration 1300, Loss: 0.6119\n",
      "Iteration 1400, Loss: 0.6084\n",
      "Iteration 1500, Loss: 0.6053\n",
      "Iteration 1600, Loss: 0.6023\n",
      "Iteration 1700, Loss: 0.5996\n",
      "Iteration 1800, Loss: 0.5970\n",
      "Iteration 1900, Loss: 0.5946\n",
      "Iteration 2000, Loss: 0.5923\n",
      "Iteration 2100, Loss: 0.5902\n",
      "Iteration 2200, Loss: 0.5881\n",
      "Iteration 2300, Loss: 0.5861\n",
      "Iteration 2400, Loss: 0.5843\n",
      "Iteration 2500, Loss: 0.5825\n",
      "Iteration 2600, Loss: 0.5808\n",
      "Iteration 2700, Loss: 0.5791\n",
      "Iteration 2800, Loss: 0.5775\n",
      "Iteration 2900, Loss: 0.5760\n",
      "Iteration 3000, Loss: 0.5745\n",
      "Iteration 3100, Loss: 0.5731\n",
      "Iteration 3200, Loss: 0.5717\n",
      "Iteration 3300, Loss: 0.5704\n",
      "Iteration 3400, Loss: 0.5691\n",
      "Iteration 3500, Loss: 0.5679\n",
      "Iteration 3600, Loss: 0.5667\n",
      "Iteration 3700, Loss: 0.5655\n",
      "Iteration 3800, Loss: 0.5644\n",
      "Iteration 3900, Loss: 0.5633\n",
      "Iteration 4000, Loss: 0.5622\n",
      "Iteration 4100, Loss: 0.5612\n",
      "Iteration 4200, Loss: 0.5602\n",
      "Iteration 4300, Loss: 0.5592\n",
      "Iteration 4400, Loss: 0.5582\n",
      "Iteration 4500, Loss: 0.5573\n",
      "Iteration 4600, Loss: 0.5564\n",
      "Iteration 4700, Loss: 0.5555\n",
      "Iteration 4800, Loss: 0.5546\n",
      "Iteration 4900, Loss: 0.5538\n",
      "Iteration 5000, Loss: 0.5529\n",
      "Iteration 5100, Loss: 0.5521\n",
      "Iteration 5200, Loss: 0.5513\n",
      "Iteration 5300, Loss: 0.5506\n",
      "Iteration 5400, Loss: 0.5498\n",
      "Iteration 5500, Loss: 0.5491\n",
      "Iteration 5600, Loss: 0.5483\n",
      "Iteration 5700, Loss: 0.5476\n",
      "Iteration 5800, Loss: 0.5469\n",
      "Iteration 5900, Loss: 0.5462\n",
      "218 270\n",
      "Iteration 0, Loss: 1.0895\n",
      "Iteration 100, Loss: 0.7857\n",
      "Iteration 200, Loss: 0.7210\n",
      "Iteration 300, Loss: 0.6911\n",
      "Iteration 400, Loss: 0.6725\n",
      "Iteration 500, Loss: 0.6591\n",
      "Iteration 600, Loss: 0.6488\n",
      "Iteration 700, Loss: 0.6405\n",
      "Iteration 800, Loss: 0.6335\n",
      "Iteration 900, Loss: 0.6276\n",
      "Iteration 1000, Loss: 0.6224\n",
      "Iteration 1100, Loss: 0.6178\n",
      "Iteration 1200, Loss: 0.6137\n",
      "Iteration 1300, Loss: 0.6100\n",
      "Iteration 1400, Loss: 0.6065\n",
      "Iteration 1500, Loss: 0.6034\n",
      "Iteration 1600, Loss: 0.6004\n",
      "Iteration 1700, Loss: 0.5977\n",
      "Iteration 1800, Loss: 0.5951\n",
      "Iteration 1900, Loss: 0.5927\n",
      "Iteration 2000, Loss: 0.5904\n",
      "Iteration 2100, Loss: 0.5883\n",
      "Iteration 2200, Loss: 0.5862\n",
      "Iteration 2300, Loss: 0.5843\n",
      "Iteration 2400, Loss: 0.5824\n",
      "Iteration 2500, Loss: 0.5806\n",
      "Iteration 2600, Loss: 0.5789\n",
      "Iteration 2700, Loss: 0.5773\n",
      "Iteration 2800, Loss: 0.5757\n",
      "Iteration 2900, Loss: 0.5742\n",
      "Iteration 3000, Loss: 0.5728\n",
      "Iteration 3100, Loss: 0.5713\n",
      "Iteration 3200, Loss: 0.5700\n",
      "Iteration 3300, Loss: 0.5687\n",
      "Iteration 3400, Loss: 0.5674\n",
      "Iteration 3500, Loss: 0.5662\n",
      "Iteration 3600, Loss: 0.5650\n",
      "Iteration 3700, Loss: 0.5638\n",
      "Iteration 3800, Loss: 0.5627\n",
      "Iteration 3900, Loss: 0.5616\n",
      "Iteration 4000, Loss: 0.5605\n",
      "Iteration 4100, Loss: 0.5595\n",
      "Iteration 4200, Loss: 0.5585\n",
      "Iteration 4300, Loss: 0.5575\n",
      "Iteration 4400, Loss: 0.5565\n",
      "Iteration 4500, Loss: 0.5556\n",
      "Iteration 4600, Loss: 0.5547\n",
      "Iteration 4700, Loss: 0.5538\n",
      "Iteration 4800, Loss: 0.5529\n",
      "Iteration 4900, Loss: 0.5520\n",
      "Iteration 5000, Loss: 0.5512\n",
      "Iteration 5100, Loss: 0.5504\n",
      "Iteration 5200, Loss: 0.5496\n",
      "Iteration 5300, Loss: 0.5488\n",
      "Iteration 5400, Loss: 0.5480\n",
      "Iteration 5500, Loss: 0.5472\n",
      "Iteration 5600, Loss: 0.5465\n",
      "Iteration 5700, Loss: 0.5458\n",
      "Iteration 5800, Loss: 0.5450\n",
      "Iteration 5900, Loss: 0.5443\n",
      "Iteration 0, Loss: 1.0889\n",
      "Iteration 100, Loss: 0.7854\n",
      "Iteration 200, Loss: 0.7202\n",
      "Iteration 300, Loss: 0.6905\n",
      "Iteration 400, Loss: 0.6721\n",
      "Iteration 500, Loss: 0.6590\n",
      "Iteration 600, Loss: 0.6489\n",
      "Iteration 700, Loss: 0.6409\n",
      "Iteration 800, Loss: 0.6342\n",
      "Iteration 900, Loss: 0.6285\n",
      "Iteration 1000, Loss: 0.6235\n",
      "Iteration 1100, Loss: 0.6192\n",
      "Iteration 1200, Loss: 0.6153\n",
      "Iteration 1300, Loss: 0.6117\n",
      "Iteration 1400, Loss: 0.6085\n",
      "Iteration 1500, Loss: 0.6055\n",
      "Iteration 1600, Loss: 0.6027\n",
      "Iteration 1700, Loss: 0.6001\n",
      "Iteration 1800, Loss: 0.5977\n",
      "Iteration 1900, Loss: 0.5954\n",
      "Iteration 2000, Loss: 0.5932\n",
      "Iteration 2100, Loss: 0.5912\n",
      "Iteration 2200, Loss: 0.5893\n",
      "Iteration 2300, Loss: 0.5874\n",
      "Iteration 2400, Loss: 0.5857\n",
      "Iteration 2500, Loss: 0.5840\n",
      "Iteration 2600, Loss: 0.5823\n",
      "Iteration 2700, Loss: 0.5808\n",
      "Iteration 2800, Loss: 0.5793\n",
      "Iteration 2900, Loss: 0.5779\n",
      "Iteration 3000, Loss: 0.5765\n",
      "Iteration 3100, Loss: 0.5752\n",
      "Iteration 3200, Loss: 0.5739\n",
      "Iteration 3300, Loss: 0.5727\n",
      "Iteration 3400, Loss: 0.5715\n",
      "Iteration 3500, Loss: 0.5703\n",
      "Iteration 3600, Loss: 0.5692\n",
      "Iteration 3700, Loss: 0.5681\n",
      "Iteration 3800, Loss: 0.5670\n",
      "Iteration 3900, Loss: 0.5660\n",
      "Iteration 4000, Loss: 0.5650\n",
      "Iteration 4100, Loss: 0.5640\n",
      "Iteration 4200, Loss: 0.5631\n",
      "Iteration 4300, Loss: 0.5622\n",
      "Iteration 4400, Loss: 0.5613\n",
      "Iteration 4500, Loss: 0.5604\n",
      "Iteration 4600, Loss: 0.5595\n",
      "Iteration 4700, Loss: 0.5587\n",
      "Iteration 4800, Loss: 0.5579\n",
      "Iteration 4900, Loss: 0.5571\n",
      "Iteration 5000, Loss: 0.5563\n",
      "Iteration 5100, Loss: 0.5555\n",
      "Iteration 5200, Loss: 0.5548\n",
      "Iteration 5300, Loss: 0.5541\n",
      "Iteration 5400, Loss: 0.5534\n",
      "Iteration 5500, Loss: 0.5527\n",
      "Iteration 5600, Loss: 0.5520\n",
      "Iteration 5700, Loss: 0.5513\n",
      "Iteration 5800, Loss: 0.5506\n",
      "Iteration 5900, Loss: 0.5500\n",
      "219 270\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7772\n",
      "Iteration 200, Loss: 0.7097\n",
      "Iteration 300, Loss: 0.6788\n",
      "Iteration 400, Loss: 0.6596\n",
      "Iteration 500, Loss: 0.6458\n",
      "Iteration 600, Loss: 0.6353\n",
      "Iteration 700, Loss: 0.6268\n",
      "Iteration 800, Loss: 0.6198\n",
      "Iteration 900, Loss: 0.6138\n",
      "Iteration 1000, Loss: 0.6086\n",
      "Iteration 1100, Loss: 0.6040\n",
      "Iteration 1200, Loss: 0.5999\n",
      "Iteration 1300, Loss: 0.5962\n",
      "Iteration 1400, Loss: 0.5928\n",
      "Iteration 1500, Loss: 0.5896\n",
      "Iteration 1600, Loss: 0.5867\n",
      "Iteration 1700, Loss: 0.5840\n",
      "Iteration 1800, Loss: 0.5814\n",
      "Iteration 1900, Loss: 0.5790\n",
      "Iteration 2000, Loss: 0.5768\n",
      "Iteration 2100, Loss: 0.5746\n",
      "Iteration 2200, Loss: 0.5726\n",
      "Iteration 2300, Loss: 0.5707\n",
      "Iteration 2400, Loss: 0.5688\n",
      "Iteration 2500, Loss: 0.5670\n",
      "Iteration 2600, Loss: 0.5653\n",
      "Iteration 2700, Loss: 0.5637\n",
      "Iteration 2800, Loss: 0.5622\n",
      "Iteration 2900, Loss: 0.5607\n",
      "Iteration 3000, Loss: 0.5592\n",
      "Iteration 3100, Loss: 0.5578\n",
      "Iteration 3200, Loss: 0.5564\n",
      "Iteration 3300, Loss: 0.5551\n",
      "Iteration 3400, Loss: 0.5538\n",
      "Iteration 3500, Loss: 0.5526\n",
      "Iteration 3600, Loss: 0.5514\n",
      "Iteration 3700, Loss: 0.5502\n",
      "Iteration 3800, Loss: 0.5491\n",
      "Iteration 3900, Loss: 0.5480\n",
      "Iteration 4000, Loss: 0.5469\n",
      "Iteration 4100, Loss: 0.5459\n",
      "Iteration 4200, Loss: 0.5449\n",
      "Iteration 4300, Loss: 0.5439\n",
      "Iteration 4400, Loss: 0.5429\n",
      "Iteration 4500, Loss: 0.5420\n",
      "Iteration 4600, Loss: 0.5411\n",
      "Iteration 4700, Loss: 0.5402\n",
      "Iteration 4800, Loss: 0.5393\n",
      "Iteration 4900, Loss: 0.5384\n",
      "Iteration 5000, Loss: 0.5376\n",
      "Iteration 5100, Loss: 0.5368\n",
      "Iteration 5200, Loss: 0.5359\n",
      "Iteration 5300, Loss: 0.5352\n",
      "Iteration 5400, Loss: 0.5344\n",
      "Iteration 5500, Loss: 0.5336\n",
      "Iteration 5600, Loss: 0.5329\n",
      "Iteration 5700, Loss: 0.5321\n",
      "Iteration 5800, Loss: 0.5314\n",
      "Iteration 5900, Loss: 0.5307\n",
      "Iteration 0, Loss: 1.0890\n",
      "Iteration 100, Loss: 0.7939\n",
      "Iteration 200, Loss: 0.7305\n",
      "Iteration 300, Loss: 0.7011\n",
      "Iteration 400, Loss: 0.6831\n",
      "Iteration 500, Loss: 0.6703\n",
      "Iteration 600, Loss: 0.6604\n",
      "Iteration 700, Loss: 0.6525\n",
      "Iteration 800, Loss: 0.6459\n",
      "Iteration 900, Loss: 0.6402\n",
      "Iteration 1000, Loss: 0.6352\n",
      "Iteration 1100, Loss: 0.6307\n",
      "Iteration 1200, Loss: 0.6266\n",
      "Iteration 1300, Loss: 0.6230\n",
      "Iteration 1400, Loss: 0.6196\n",
      "Iteration 1500, Loss: 0.6165\n",
      "Iteration 1600, Loss: 0.6136\n",
      "Iteration 1700, Loss: 0.6109\n",
      "Iteration 1800, Loss: 0.6083\n",
      "Iteration 1900, Loss: 0.6059\n",
      "Iteration 2000, Loss: 0.6036\n",
      "Iteration 2100, Loss: 0.6014\n",
      "Iteration 2200, Loss: 0.5994\n",
      "Iteration 2300, Loss: 0.5974\n",
      "Iteration 2400, Loss: 0.5955\n",
      "Iteration 2500, Loss: 0.5937\n",
      "Iteration 2600, Loss: 0.5920\n",
      "Iteration 2700, Loss: 0.5904\n",
      "Iteration 2800, Loss: 0.5888\n",
      "Iteration 2900, Loss: 0.5872\n",
      "Iteration 3000, Loss: 0.5858\n",
      "Iteration 3100, Loss: 0.5844\n",
      "Iteration 3200, Loss: 0.5830\n",
      "Iteration 3300, Loss: 0.5817\n",
      "Iteration 3400, Loss: 0.5804\n",
      "Iteration 3500, Loss: 0.5791\n",
      "Iteration 3600, Loss: 0.5779\n",
      "Iteration 3700, Loss: 0.5767\n",
      "Iteration 3800, Loss: 0.5756\n",
      "Iteration 3900, Loss: 0.5745\n",
      "Iteration 4000, Loss: 0.5734\n",
      "Iteration 4100, Loss: 0.5724\n",
      "Iteration 4200, Loss: 0.5714\n",
      "Iteration 4300, Loss: 0.5704\n",
      "Iteration 4400, Loss: 0.5694\n",
      "Iteration 4500, Loss: 0.5685\n",
      "Iteration 4600, Loss: 0.5676\n",
      "Iteration 4700, Loss: 0.5667\n",
      "Iteration 4800, Loss: 0.5658\n",
      "Iteration 4900, Loss: 0.5649\n",
      "Iteration 5000, Loss: 0.5641\n",
      "Iteration 5100, Loss: 0.5633\n",
      "Iteration 5200, Loss: 0.5625\n",
      "Iteration 5300, Loss: 0.5617\n",
      "Iteration 5400, Loss: 0.5609\n",
      "Iteration 5500, Loss: 0.5602\n",
      "Iteration 5600, Loss: 0.5594\n",
      "Iteration 5700, Loss: 0.5587\n",
      "Iteration 5800, Loss: 0.5580\n",
      "Iteration 5900, Loss: 0.5573\n",
      "220 270\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7872\n",
      "Iteration 200, Loss: 0.7245\n",
      "Iteration 300, Loss: 0.6960\n",
      "Iteration 400, Loss: 0.6783\n",
      "Iteration 500, Loss: 0.6657\n",
      "Iteration 600, Loss: 0.6560\n",
      "Iteration 700, Loss: 0.6482\n",
      "Iteration 800, Loss: 0.6418\n",
      "Iteration 900, Loss: 0.6362\n",
      "Iteration 1000, Loss: 0.6313\n",
      "Iteration 1100, Loss: 0.6270\n",
      "Iteration 1200, Loss: 0.6231\n",
      "Iteration 1300, Loss: 0.6195\n",
      "Iteration 1400, Loss: 0.6163\n",
      "Iteration 1500, Loss: 0.6133\n",
      "Iteration 1600, Loss: 0.6104\n",
      "Iteration 1700, Loss: 0.6078\n",
      "Iteration 1800, Loss: 0.6053\n",
      "Iteration 1900, Loss: 0.6030\n",
      "Iteration 2000, Loss: 0.6008\n",
      "Iteration 2100, Loss: 0.5987\n",
      "Iteration 2200, Loss: 0.5967\n",
      "Iteration 2300, Loss: 0.5948\n",
      "Iteration 2400, Loss: 0.5929\n",
      "Iteration 2500, Loss: 0.5912\n",
      "Iteration 2600, Loss: 0.5895\n",
      "Iteration 2700, Loss: 0.5879\n",
      "Iteration 2800, Loss: 0.5863\n",
      "Iteration 2900, Loss: 0.5849\n",
      "Iteration 3000, Loss: 0.5834\n",
      "Iteration 3100, Loss: 0.5820\n",
      "Iteration 3200, Loss: 0.5807\n",
      "Iteration 3300, Loss: 0.5793\n",
      "Iteration 3400, Loss: 0.5781\n",
      "Iteration 3500, Loss: 0.5769\n",
      "Iteration 3600, Loss: 0.5757\n",
      "Iteration 3700, Loss: 0.5745\n",
      "Iteration 3800, Loss: 0.5734\n",
      "Iteration 3900, Loss: 0.5723\n",
      "Iteration 4000, Loss: 0.5712\n",
      "Iteration 4100, Loss: 0.5702\n",
      "Iteration 4200, Loss: 0.5692\n",
      "Iteration 4300, Loss: 0.5682\n",
      "Iteration 4400, Loss: 0.5672\n",
      "Iteration 4500, Loss: 0.5663\n",
      "Iteration 4600, Loss: 0.5654\n",
      "Iteration 4700, Loss: 0.5645\n",
      "Iteration 4800, Loss: 0.5636\n",
      "Iteration 4900, Loss: 0.5627\n",
      "Iteration 5000, Loss: 0.5619\n",
      "Iteration 5100, Loss: 0.5611\n",
      "Iteration 5200, Loss: 0.5603\n",
      "Iteration 5300, Loss: 0.5595\n",
      "Iteration 5400, Loss: 0.5587\n",
      "Iteration 5500, Loss: 0.5579\n",
      "Iteration 5600, Loss: 0.5572\n",
      "Iteration 5700, Loss: 0.5565\n",
      "Iteration 5800, Loss: 0.5557\n",
      "Iteration 5900, Loss: 0.5550\n",
      "Iteration 0, Loss: 1.0895\n",
      "Iteration 100, Loss: 0.7849\n",
      "Iteration 200, Loss: 0.7172\n",
      "Iteration 300, Loss: 0.6863\n",
      "Iteration 400, Loss: 0.6674\n",
      "Iteration 500, Loss: 0.6539\n",
      "Iteration 600, Loss: 0.6436\n",
      "Iteration 700, Loss: 0.6352\n",
      "Iteration 800, Loss: 0.6283\n",
      "Iteration 900, Loss: 0.6223\n",
      "Iteration 1000, Loss: 0.6170\n",
      "Iteration 1100, Loss: 0.6124\n",
      "Iteration 1200, Loss: 0.6082\n",
      "Iteration 1300, Loss: 0.6044\n",
      "Iteration 1400, Loss: 0.6009\n",
      "Iteration 1500, Loss: 0.5977\n",
      "Iteration 1600, Loss: 0.5947\n",
      "Iteration 1700, Loss: 0.5919\n",
      "Iteration 1800, Loss: 0.5893\n",
      "Iteration 1900, Loss: 0.5869\n",
      "Iteration 2000, Loss: 0.5846\n",
      "Iteration 2100, Loss: 0.5824\n",
      "Iteration 2200, Loss: 0.5803\n",
      "Iteration 2300, Loss: 0.5783\n",
      "Iteration 2400, Loss: 0.5764\n",
      "Iteration 2500, Loss: 0.5746\n",
      "Iteration 2600, Loss: 0.5729\n",
      "Iteration 2700, Loss: 0.5712\n",
      "Iteration 2800, Loss: 0.5696\n",
      "Iteration 2900, Loss: 0.5681\n",
      "Iteration 3000, Loss: 0.5666\n",
      "Iteration 3100, Loss: 0.5652\n",
      "Iteration 3200, Loss: 0.5639\n",
      "Iteration 3300, Loss: 0.5625\n",
      "Iteration 3400, Loss: 0.5613\n",
      "Iteration 3500, Loss: 0.5600\n",
      "Iteration 3600, Loss: 0.5588\n",
      "Iteration 3700, Loss: 0.5577\n",
      "Iteration 3800, Loss: 0.5566\n",
      "Iteration 3900, Loss: 0.5555\n",
      "Iteration 4000, Loss: 0.5544\n",
      "Iteration 4100, Loss: 0.5534\n",
      "Iteration 4200, Loss: 0.5524\n",
      "Iteration 4300, Loss: 0.5514\n",
      "Iteration 4400, Loss: 0.5504\n",
      "Iteration 4500, Loss: 0.5495\n",
      "Iteration 4600, Loss: 0.5486\n",
      "Iteration 4700, Loss: 0.5477\n",
      "Iteration 4800, Loss: 0.5468\n",
      "Iteration 4900, Loss: 0.5460\n",
      "Iteration 5000, Loss: 0.5452\n",
      "Iteration 5100, Loss: 0.5444\n",
      "Iteration 5200, Loss: 0.5436\n",
      "Iteration 5300, Loss: 0.5428\n",
      "Iteration 5400, Loss: 0.5420\n",
      "Iteration 5500, Loss: 0.5413\n",
      "Iteration 5600, Loss: 0.5406\n",
      "Iteration 5700, Loss: 0.5398\n",
      "Iteration 5800, Loss: 0.5391\n",
      "Iteration 5900, Loss: 0.5385\n",
      "221 270\n",
      "Iteration 0, Loss: 1.0899\n",
      "Iteration 100, Loss: 0.7998\n",
      "Iteration 200, Loss: 0.7358\n",
      "Iteration 300, Loss: 0.7066\n",
      "Iteration 400, Loss: 0.6886\n",
      "Iteration 500, Loss: 0.6756\n",
      "Iteration 600, Loss: 0.6656\n",
      "Iteration 700, Loss: 0.6575\n",
      "Iteration 800, Loss: 0.6507\n",
      "Iteration 900, Loss: 0.6449\n",
      "Iteration 1000, Loss: 0.6398\n",
      "Iteration 1100, Loss: 0.6353\n",
      "Iteration 1200, Loss: 0.6312\n",
      "Iteration 1300, Loss: 0.6276\n",
      "Iteration 1400, Loss: 0.6242\n",
      "Iteration 1500, Loss: 0.6211\n",
      "Iteration 1600, Loss: 0.6182\n",
      "Iteration 1700, Loss: 0.6155\n",
      "Iteration 1800, Loss: 0.6130\n",
      "Iteration 1900, Loss: 0.6106\n",
      "Iteration 2000, Loss: 0.6084\n",
      "Iteration 2100, Loss: 0.6063\n",
      "Iteration 2200, Loss: 0.6043\n",
      "Iteration 2300, Loss: 0.6024\n",
      "Iteration 2400, Loss: 0.6005\n",
      "Iteration 2500, Loss: 0.5988\n",
      "Iteration 2600, Loss: 0.5971\n",
      "Iteration 2700, Loss: 0.5955\n",
      "Iteration 2800, Loss: 0.5940\n",
      "Iteration 2900, Loss: 0.5925\n",
      "Iteration 3000, Loss: 0.5911\n",
      "Iteration 3100, Loss: 0.5897\n",
      "Iteration 3200, Loss: 0.5883\n",
      "Iteration 3300, Loss: 0.5871\n",
      "Iteration 3400, Loss: 0.5858\n",
      "Iteration 3500, Loss: 0.5846\n",
      "Iteration 3600, Loss: 0.5834\n",
      "Iteration 3700, Loss: 0.5823\n",
      "Iteration 3800, Loss: 0.5812\n",
      "Iteration 3900, Loss: 0.5801\n",
      "Iteration 4000, Loss: 0.5791\n",
      "Iteration 4100, Loss: 0.5781\n",
      "Iteration 4200, Loss: 0.5771\n",
      "Iteration 4300, Loss: 0.5761\n",
      "Iteration 4400, Loss: 0.5752\n",
      "Iteration 4500, Loss: 0.5743\n",
      "Iteration 4600, Loss: 0.5734\n",
      "Iteration 4700, Loss: 0.5725\n",
      "Iteration 4800, Loss: 0.5716\n",
      "Iteration 4900, Loss: 0.5708\n",
      "Iteration 5000, Loss: 0.5700\n",
      "Iteration 5100, Loss: 0.5691\n",
      "Iteration 5200, Loss: 0.5684\n",
      "Iteration 5300, Loss: 0.5676\n",
      "Iteration 5400, Loss: 0.5668\n",
      "Iteration 5500, Loss: 0.5661\n",
      "Iteration 5600, Loss: 0.5654\n",
      "Iteration 5700, Loss: 0.5646\n",
      "Iteration 5800, Loss: 0.5639\n",
      "Iteration 5900, Loss: 0.5633\n",
      "Iteration 0, Loss: 1.0886\n",
      "Iteration 100, Loss: 0.7714\n",
      "Iteration 200, Loss: 0.7053\n",
      "Iteration 300, Loss: 0.6746\n",
      "Iteration 400, Loss: 0.6556\n",
      "Iteration 500, Loss: 0.6422\n",
      "Iteration 600, Loss: 0.6319\n",
      "Iteration 700, Loss: 0.6236\n",
      "Iteration 800, Loss: 0.6167\n",
      "Iteration 900, Loss: 0.6108\n",
      "Iteration 1000, Loss: 0.6056\n",
      "Iteration 1100, Loss: 0.6011\n",
      "Iteration 1200, Loss: 0.5971\n",
      "Iteration 1300, Loss: 0.5934\n",
      "Iteration 1400, Loss: 0.5900\n",
      "Iteration 1500, Loss: 0.5869\n",
      "Iteration 1600, Loss: 0.5840\n",
      "Iteration 1700, Loss: 0.5814\n",
      "Iteration 1800, Loss: 0.5788\n",
      "Iteration 1900, Loss: 0.5765\n",
      "Iteration 2000, Loss: 0.5742\n",
      "Iteration 2100, Loss: 0.5721\n",
      "Iteration 2200, Loss: 0.5701\n",
      "Iteration 2300, Loss: 0.5682\n",
      "Iteration 2400, Loss: 0.5664\n",
      "Iteration 2500, Loss: 0.5646\n",
      "Iteration 2600, Loss: 0.5629\n",
      "Iteration 2700, Loss: 0.5613\n",
      "Iteration 2800, Loss: 0.5597\n",
      "Iteration 2900, Loss: 0.5582\n",
      "Iteration 3000, Loss: 0.5568\n",
      "Iteration 3100, Loss: 0.5554\n",
      "Iteration 3200, Loss: 0.5540\n",
      "Iteration 3300, Loss: 0.5527\n",
      "Iteration 3400, Loss: 0.5515\n",
      "Iteration 3500, Loss: 0.5502\n",
      "Iteration 3600, Loss: 0.5491\n",
      "Iteration 3700, Loss: 0.5479\n",
      "Iteration 3800, Loss: 0.5468\n",
      "Iteration 3900, Loss: 0.5457\n",
      "Iteration 4000, Loss: 0.5446\n",
      "Iteration 4100, Loss: 0.5436\n",
      "Iteration 4200, Loss: 0.5426\n",
      "Iteration 4300, Loss: 0.5416\n",
      "Iteration 4400, Loss: 0.5406\n",
      "Iteration 4500, Loss: 0.5397\n",
      "Iteration 4600, Loss: 0.5388\n",
      "Iteration 4700, Loss: 0.5379\n",
      "Iteration 4800, Loss: 0.5370\n",
      "Iteration 4900, Loss: 0.5362\n",
      "Iteration 5000, Loss: 0.5353\n",
      "Iteration 5100, Loss: 0.5345\n",
      "Iteration 5200, Loss: 0.5337\n",
      "Iteration 5300, Loss: 0.5329\n",
      "Iteration 5400, Loss: 0.5322\n",
      "Iteration 5500, Loss: 0.5314\n",
      "Iteration 5600, Loss: 0.5307\n",
      "Iteration 5700, Loss: 0.5299\n",
      "Iteration 5800, Loss: 0.5292\n",
      "Iteration 5900, Loss: 0.5285\n",
      "222 270\n",
      "Iteration 0, Loss: 1.0897\n",
      "Iteration 100, Loss: 0.7786\n",
      "Iteration 200, Loss: 0.7103\n",
      "Iteration 300, Loss: 0.6804\n",
      "Iteration 400, Loss: 0.6624\n",
      "Iteration 500, Loss: 0.6498\n",
      "Iteration 600, Loss: 0.6401\n",
      "Iteration 700, Loss: 0.6323\n",
      "Iteration 800, Loss: 0.6257\n",
      "Iteration 900, Loss: 0.6201\n",
      "Iteration 1000, Loss: 0.6152\n",
      "Iteration 1100, Loss: 0.6108\n",
      "Iteration 1200, Loss: 0.6069\n",
      "Iteration 1300, Loss: 0.6033\n",
      "Iteration 1400, Loss: 0.6000\n",
      "Iteration 1500, Loss: 0.5970\n",
      "Iteration 1600, Loss: 0.5942\n",
      "Iteration 1700, Loss: 0.5915\n",
      "Iteration 1800, Loss: 0.5891\n",
      "Iteration 1900, Loss: 0.5867\n",
      "Iteration 2000, Loss: 0.5845\n",
      "Iteration 2100, Loss: 0.5824\n",
      "Iteration 2200, Loss: 0.5805\n",
      "Iteration 2300, Loss: 0.5785\n",
      "Iteration 2400, Loss: 0.5767\n",
      "Iteration 2500, Loss: 0.5750\n",
      "Iteration 2600, Loss: 0.5733\n",
      "Iteration 2700, Loss: 0.5717\n",
      "Iteration 2800, Loss: 0.5702\n",
      "Iteration 2900, Loss: 0.5687\n",
      "Iteration 3000, Loss: 0.5673\n",
      "Iteration 3100, Loss: 0.5659\n",
      "Iteration 3200, Loss: 0.5645\n",
      "Iteration 3300, Loss: 0.5633\n",
      "Iteration 3400, Loss: 0.5620\n",
      "Iteration 3500, Loss: 0.5608\n",
      "Iteration 3600, Loss: 0.5596\n",
      "Iteration 3700, Loss: 0.5585\n",
      "Iteration 3800, Loss: 0.5574\n",
      "Iteration 3900, Loss: 0.5563\n",
      "Iteration 4000, Loss: 0.5552\n",
      "Iteration 4100, Loss: 0.5542\n",
      "Iteration 4200, Loss: 0.5532\n",
      "Iteration 4300, Loss: 0.5523\n",
      "Iteration 4400, Loss: 0.5513\n",
      "Iteration 4500, Loss: 0.5504\n",
      "Iteration 4600, Loss: 0.5495\n",
      "Iteration 4700, Loss: 0.5486\n",
      "Iteration 4800, Loss: 0.5478\n",
      "Iteration 4900, Loss: 0.5469\n",
      "Iteration 5000, Loss: 0.5461\n",
      "Iteration 5100, Loss: 0.5453\n",
      "Iteration 5200, Loss: 0.5445\n",
      "Iteration 5300, Loss: 0.5437\n",
      "Iteration 5400, Loss: 0.5430\n",
      "Iteration 5500, Loss: 0.5422\n",
      "Iteration 5600, Loss: 0.5415\n",
      "Iteration 5700, Loss: 0.5408\n",
      "Iteration 5800, Loss: 0.5401\n",
      "Iteration 5900, Loss: 0.5394\n",
      "Iteration 0, Loss: 1.0884\n",
      "Iteration 100, Loss: 0.7919\n",
      "Iteration 200, Loss: 0.7293\n",
      "Iteration 300, Loss: 0.6993\n",
      "Iteration 400, Loss: 0.6803\n",
      "Iteration 500, Loss: 0.6668\n",
      "Iteration 600, Loss: 0.6564\n",
      "Iteration 700, Loss: 0.6480\n",
      "Iteration 800, Loss: 0.6410\n",
      "Iteration 900, Loss: 0.6351\n",
      "Iteration 1000, Loss: 0.6299\n",
      "Iteration 1100, Loss: 0.6253\n",
      "Iteration 1200, Loss: 0.6212\n",
      "Iteration 1300, Loss: 0.6175\n",
      "Iteration 1400, Loss: 0.6141\n",
      "Iteration 1500, Loss: 0.6110\n",
      "Iteration 1600, Loss: 0.6081\n",
      "Iteration 1700, Loss: 0.6054\n",
      "Iteration 1800, Loss: 0.6028\n",
      "Iteration 1900, Loss: 0.6005\n",
      "Iteration 2000, Loss: 0.5982\n",
      "Iteration 2100, Loss: 0.5961\n",
      "Iteration 2200, Loss: 0.5940\n",
      "Iteration 2300, Loss: 0.5921\n",
      "Iteration 2400, Loss: 0.5903\n",
      "Iteration 2500, Loss: 0.5885\n",
      "Iteration 2600, Loss: 0.5868\n",
      "Iteration 2700, Loss: 0.5852\n",
      "Iteration 2800, Loss: 0.5837\n",
      "Iteration 2900, Loss: 0.5822\n",
      "Iteration 3000, Loss: 0.5807\n",
      "Iteration 3100, Loss: 0.5794\n",
      "Iteration 3200, Loss: 0.5780\n",
      "Iteration 3300, Loss: 0.5767\n",
      "Iteration 3400, Loss: 0.5755\n",
      "Iteration 3500, Loss: 0.5743\n",
      "Iteration 3600, Loss: 0.5731\n",
      "Iteration 3700, Loss: 0.5719\n",
      "Iteration 3800, Loss: 0.5708\n",
      "Iteration 3900, Loss: 0.5698\n",
      "Iteration 4000, Loss: 0.5687\n",
      "Iteration 4100, Loss: 0.5677\n",
      "Iteration 4200, Loss: 0.5667\n",
      "Iteration 4300, Loss: 0.5657\n",
      "Iteration 4400, Loss: 0.5648\n",
      "Iteration 4500, Loss: 0.5639\n",
      "Iteration 4600, Loss: 0.5630\n",
      "Iteration 4700, Loss: 0.5621\n",
      "Iteration 4800, Loss: 0.5613\n",
      "Iteration 4900, Loss: 0.5604\n",
      "Iteration 5000, Loss: 0.5596\n",
      "Iteration 5100, Loss: 0.5588\n",
      "Iteration 5200, Loss: 0.5580\n",
      "Iteration 5300, Loss: 0.5572\n",
      "Iteration 5400, Loss: 0.5565\n",
      "Iteration 5500, Loss: 0.5557\n",
      "Iteration 5600, Loss: 0.5550\n",
      "Iteration 5700, Loss: 0.5543\n",
      "Iteration 5800, Loss: 0.5536\n",
      "Iteration 5900, Loss: 0.5529\n",
      "223 270\n",
      "Iteration 0, Loss: 1.0894\n",
      "Iteration 100, Loss: 0.7870\n",
      "Iteration 200, Loss: 0.7228\n",
      "Iteration 300, Loss: 0.6941\n",
      "Iteration 400, Loss: 0.6767\n",
      "Iteration 500, Loss: 0.6645\n",
      "Iteration 600, Loss: 0.6553\n",
      "Iteration 700, Loss: 0.6479\n",
      "Iteration 800, Loss: 0.6418\n",
      "Iteration 900, Loss: 0.6366\n",
      "Iteration 1000, Loss: 0.6320\n",
      "Iteration 1100, Loss: 0.6280\n",
      "Iteration 1200, Loss: 0.6244\n",
      "Iteration 1300, Loss: 0.6211\n",
      "Iteration 1400, Loss: 0.6181\n",
      "Iteration 1500, Loss: 0.6153\n",
      "Iteration 1600, Loss: 0.6127\n",
      "Iteration 1700, Loss: 0.6103\n",
      "Iteration 1800, Loss: 0.6080\n",
      "Iteration 1900, Loss: 0.6059\n",
      "Iteration 2000, Loss: 0.6038\n",
      "Iteration 2100, Loss: 0.6019\n",
      "Iteration 2200, Loss: 0.6000\n",
      "Iteration 2300, Loss: 0.5982\n",
      "Iteration 2400, Loss: 0.5965\n",
      "Iteration 2500, Loss: 0.5949\n",
      "Iteration 2600, Loss: 0.5933\n",
      "Iteration 2700, Loss: 0.5918\n",
      "Iteration 2800, Loss: 0.5903\n",
      "Iteration 2900, Loss: 0.5889\n",
      "Iteration 3000, Loss: 0.5875\n",
      "Iteration 3100, Loss: 0.5862\n",
      "Iteration 3200, Loss: 0.5849\n",
      "Iteration 3300, Loss: 0.5837\n",
      "Iteration 3400, Loss: 0.5824\n",
      "Iteration 3500, Loss: 0.5813\n",
      "Iteration 3600, Loss: 0.5801\n",
      "Iteration 3700, Loss: 0.5790\n",
      "Iteration 3800, Loss: 0.5779\n",
      "Iteration 3900, Loss: 0.5769\n",
      "Iteration 4000, Loss: 0.5759\n",
      "Iteration 4100, Loss: 0.5749\n",
      "Iteration 4200, Loss: 0.5739\n",
      "Iteration 4300, Loss: 0.5729\n",
      "Iteration 4400, Loss: 0.5720\n",
      "Iteration 4500, Loss: 0.5711\n",
      "Iteration 4600, Loss: 0.5702\n",
      "Iteration 4700, Loss: 0.5693\n",
      "Iteration 4800, Loss: 0.5685\n",
      "Iteration 4900, Loss: 0.5676\n",
      "Iteration 5000, Loss: 0.5668\n",
      "Iteration 5100, Loss: 0.5660\n",
      "Iteration 5200, Loss: 0.5652\n",
      "Iteration 5300, Loss: 0.5645\n",
      "Iteration 5400, Loss: 0.5637\n",
      "Iteration 5500, Loss: 0.5630\n",
      "Iteration 5600, Loss: 0.5622\n",
      "Iteration 5700, Loss: 0.5615\n",
      "Iteration 5800, Loss: 0.5608\n",
      "Iteration 5900, Loss: 0.5601\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7852\n",
      "Iteration 200, Loss: 0.7182\n",
      "Iteration 300, Loss: 0.6864\n",
      "Iteration 400, Loss: 0.6663\n",
      "Iteration 500, Loss: 0.6519\n",
      "Iteration 600, Loss: 0.6407\n",
      "Iteration 700, Loss: 0.6317\n",
      "Iteration 800, Loss: 0.6241\n",
      "Iteration 900, Loss: 0.6177\n",
      "Iteration 1000, Loss: 0.6121\n",
      "Iteration 1100, Loss: 0.6072\n",
      "Iteration 1200, Loss: 0.6028\n",
      "Iteration 1300, Loss: 0.5989\n",
      "Iteration 1400, Loss: 0.5952\n",
      "Iteration 1500, Loss: 0.5919\n",
      "Iteration 1600, Loss: 0.5888\n",
      "Iteration 1700, Loss: 0.5859\n",
      "Iteration 1800, Loss: 0.5833\n",
      "Iteration 1900, Loss: 0.5807\n",
      "Iteration 2000, Loss: 0.5784\n",
      "Iteration 2100, Loss: 0.5761\n",
      "Iteration 2200, Loss: 0.5740\n",
      "Iteration 2300, Loss: 0.5720\n",
      "Iteration 2400, Loss: 0.5701\n",
      "Iteration 2500, Loss: 0.5683\n",
      "Iteration 2600, Loss: 0.5665\n",
      "Iteration 2700, Loss: 0.5648\n",
      "Iteration 2800, Loss: 0.5632\n",
      "Iteration 2900, Loss: 0.5617\n",
      "Iteration 3000, Loss: 0.5602\n",
      "Iteration 3100, Loss: 0.5588\n",
      "Iteration 3200, Loss: 0.5574\n",
      "Iteration 3300, Loss: 0.5561\n",
      "Iteration 3400, Loss: 0.5548\n",
      "Iteration 3500, Loss: 0.5535\n",
      "Iteration 3600, Loss: 0.5523\n",
      "Iteration 3700, Loss: 0.5511\n",
      "Iteration 3800, Loss: 0.5500\n",
      "Iteration 3900, Loss: 0.5489\n",
      "Iteration 4000, Loss: 0.5478\n",
      "Iteration 4100, Loss: 0.5468\n",
      "Iteration 4200, Loss: 0.5458\n",
      "Iteration 4300, Loss: 0.5448\n",
      "Iteration 4400, Loss: 0.5438\n",
      "Iteration 4500, Loss: 0.5429\n",
      "Iteration 4600, Loss: 0.5420\n",
      "Iteration 4700, Loss: 0.5411\n",
      "Iteration 4800, Loss: 0.5402\n",
      "Iteration 4900, Loss: 0.5393\n",
      "Iteration 5000, Loss: 0.5385\n",
      "Iteration 5100, Loss: 0.5377\n",
      "Iteration 5200, Loss: 0.5369\n",
      "Iteration 5300, Loss: 0.5361\n",
      "Iteration 5400, Loss: 0.5353\n",
      "Iteration 5500, Loss: 0.5346\n",
      "Iteration 5600, Loss: 0.5339\n",
      "Iteration 5700, Loss: 0.5331\n",
      "Iteration 5800, Loss: 0.5324\n",
      "Iteration 5900, Loss: 0.5317\n",
      "224 270\n",
      "Iteration 0, Loss: 1.0890\n",
      "Iteration 100, Loss: 0.7834\n",
      "Iteration 200, Loss: 0.7178\n",
      "Iteration 300, Loss: 0.6873\n",
      "Iteration 400, Loss: 0.6681\n",
      "Iteration 500, Loss: 0.6544\n",
      "Iteration 600, Loss: 0.6438\n",
      "Iteration 700, Loss: 0.6353\n",
      "Iteration 800, Loss: 0.6282\n",
      "Iteration 900, Loss: 0.6222\n",
      "Iteration 1000, Loss: 0.6169\n",
      "Iteration 1100, Loss: 0.6122\n",
      "Iteration 1200, Loss: 0.6080\n",
      "Iteration 1300, Loss: 0.6042\n",
      "Iteration 1400, Loss: 0.6008\n",
      "Iteration 1500, Loss: 0.5976\n",
      "Iteration 1600, Loss: 0.5946\n",
      "Iteration 1700, Loss: 0.5919\n",
      "Iteration 1800, Loss: 0.5893\n",
      "Iteration 1900, Loss: 0.5869\n",
      "Iteration 2000, Loss: 0.5846\n",
      "Iteration 2100, Loss: 0.5824\n",
      "Iteration 2200, Loss: 0.5804\n",
      "Iteration 2300, Loss: 0.5784\n",
      "Iteration 2400, Loss: 0.5766\n",
      "Iteration 2500, Loss: 0.5748\n",
      "Iteration 2600, Loss: 0.5731\n",
      "Iteration 2700, Loss: 0.5714\n",
      "Iteration 2800, Loss: 0.5699\n",
      "Iteration 2900, Loss: 0.5684\n",
      "Iteration 3000, Loss: 0.5669\n",
      "Iteration 3100, Loss: 0.5655\n",
      "Iteration 3200, Loss: 0.5642\n",
      "Iteration 3300, Loss: 0.5629\n",
      "Iteration 3400, Loss: 0.5616\n",
      "Iteration 3500, Loss: 0.5604\n",
      "Iteration 3600, Loss: 0.5592\n",
      "Iteration 3700, Loss: 0.5581\n",
      "Iteration 3800, Loss: 0.5570\n",
      "Iteration 3900, Loss: 0.5559\n",
      "Iteration 4000, Loss: 0.5549\n",
      "Iteration 4100, Loss: 0.5538\n",
      "Iteration 4200, Loss: 0.5529\n",
      "Iteration 4300, Loss: 0.5519\n",
      "Iteration 4400, Loss: 0.5509\n",
      "Iteration 4500, Loss: 0.5500\n",
      "Iteration 4600, Loss: 0.5491\n",
      "Iteration 4700, Loss: 0.5483\n",
      "Iteration 4800, Loss: 0.5474\n",
      "Iteration 4900, Loss: 0.5466\n",
      "Iteration 5000, Loss: 0.5457\n",
      "Iteration 5100, Loss: 0.5449\n",
      "Iteration 5200, Loss: 0.5442\n",
      "Iteration 5300, Loss: 0.5434\n",
      "Iteration 5400, Loss: 0.5426\n",
      "Iteration 5500, Loss: 0.5419\n",
      "Iteration 5600, Loss: 0.5412\n",
      "Iteration 5700, Loss: 0.5405\n",
      "Iteration 5800, Loss: 0.5398\n",
      "Iteration 5900, Loss: 0.5391\n",
      "Iteration 0, Loss: 1.0892\n",
      "Iteration 100, Loss: 0.7894\n",
      "Iteration 200, Loss: 0.7241\n",
      "Iteration 300, Loss: 0.6943\n",
      "Iteration 400, Loss: 0.6762\n",
      "Iteration 500, Loss: 0.6633\n",
      "Iteration 600, Loss: 0.6534\n",
      "Iteration 700, Loss: 0.6455\n",
      "Iteration 800, Loss: 0.6388\n",
      "Iteration 900, Loss: 0.6331\n",
      "Iteration 1000, Loss: 0.6282\n",
      "Iteration 1100, Loss: 0.6237\n",
      "Iteration 1200, Loss: 0.6197\n",
      "Iteration 1300, Loss: 0.6161\n",
      "Iteration 1400, Loss: 0.6127\n",
      "Iteration 1500, Loss: 0.6096\n",
      "Iteration 1600, Loss: 0.6067\n",
      "Iteration 1700, Loss: 0.6040\n",
      "Iteration 1800, Loss: 0.6014\n",
      "Iteration 1900, Loss: 0.5990\n",
      "Iteration 2000, Loss: 0.5967\n",
      "Iteration 2100, Loss: 0.5945\n",
      "Iteration 2200, Loss: 0.5925\n",
      "Iteration 2300, Loss: 0.5905\n",
      "Iteration 2400, Loss: 0.5886\n",
      "Iteration 2500, Loss: 0.5868\n",
      "Iteration 2600, Loss: 0.5850\n",
      "Iteration 2700, Loss: 0.5834\n",
      "Iteration 2800, Loss: 0.5818\n",
      "Iteration 2900, Loss: 0.5802\n",
      "Iteration 3000, Loss: 0.5787\n",
      "Iteration 3100, Loss: 0.5773\n",
      "Iteration 3200, Loss: 0.5759\n",
      "Iteration 3300, Loss: 0.5745\n",
      "Iteration 3400, Loss: 0.5732\n",
      "Iteration 3500, Loss: 0.5719\n",
      "Iteration 3600, Loss: 0.5707\n",
      "Iteration 3700, Loss: 0.5695\n",
      "Iteration 3800, Loss: 0.5683\n",
      "Iteration 3900, Loss: 0.5672\n",
      "Iteration 4000, Loss: 0.5661\n",
      "Iteration 4100, Loss: 0.5650\n",
      "Iteration 4200, Loss: 0.5639\n",
      "Iteration 4300, Loss: 0.5629\n",
      "Iteration 4400, Loss: 0.5619\n",
      "Iteration 4500, Loss: 0.5609\n",
      "Iteration 4600, Loss: 0.5600\n",
      "Iteration 4700, Loss: 0.5591\n",
      "Iteration 4800, Loss: 0.5582\n",
      "Iteration 4900, Loss: 0.5573\n",
      "Iteration 5000, Loss: 0.5564\n",
      "Iteration 5100, Loss: 0.5555\n",
      "Iteration 5200, Loss: 0.5547\n",
      "Iteration 5300, Loss: 0.5539\n",
      "Iteration 5400, Loss: 0.5531\n",
      "Iteration 5500, Loss: 0.5523\n",
      "Iteration 5600, Loss: 0.5515\n",
      "Iteration 5700, Loss: 0.5508\n",
      "Iteration 5800, Loss: 0.5500\n",
      "Iteration 5900, Loss: 0.5493\n",
      "225 270\n",
      "Iteration 0, Loss: 1.0933\n",
      "Iteration 100, Loss: 0.8612\n",
      "Iteration 200, Loss: 0.7860\n",
      "Iteration 300, Loss: 0.7473\n",
      "Iteration 400, Loss: 0.7236\n",
      "Iteration 500, Loss: 0.7072\n",
      "Iteration 600, Loss: 0.6948\n",
      "Iteration 700, Loss: 0.6849\n",
      "Iteration 800, Loss: 0.6767\n",
      "Iteration 900, Loss: 0.6697\n",
      "Iteration 1000, Loss: 0.6635\n",
      "Iteration 1100, Loss: 0.6581\n",
      "Iteration 1200, Loss: 0.6532\n",
      "Iteration 1300, Loss: 0.6488\n",
      "Iteration 1400, Loss: 0.6448\n",
      "Iteration 1500, Loss: 0.6412\n",
      "Iteration 1600, Loss: 0.6378\n",
      "Iteration 1700, Loss: 0.6346\n",
      "Iteration 1800, Loss: 0.6317\n",
      "Iteration 1900, Loss: 0.6289\n",
      "Iteration 2000, Loss: 0.6263\n",
      "Iteration 2100, Loss: 0.6238\n",
      "Iteration 2200, Loss: 0.6215\n",
      "Iteration 2300, Loss: 0.6193\n",
      "Iteration 2400, Loss: 0.6172\n",
      "Iteration 2500, Loss: 0.6152\n",
      "Iteration 2600, Loss: 0.6133\n",
      "Iteration 2700, Loss: 0.6114\n",
      "Iteration 2800, Loss: 0.6096\n",
      "Iteration 2900, Loss: 0.6080\n",
      "Iteration 3000, Loss: 0.6064\n",
      "Iteration 3100, Loss: 0.6048\n",
      "Iteration 3200, Loss: 0.6032\n",
      "Iteration 3300, Loss: 0.6018\n",
      "Iteration 3400, Loss: 0.6004\n",
      "Iteration 3500, Loss: 0.5990\n",
      "Iteration 3600, Loss: 0.5977\n",
      "Iteration 3700, Loss: 0.5964\n",
      "Iteration 3800, Loss: 0.5951\n",
      "Iteration 3900, Loss: 0.5939\n",
      "Iteration 4000, Loss: 0.5927\n",
      "Iteration 4100, Loss: 0.5915\n",
      "Iteration 4200, Loss: 0.5904\n",
      "Iteration 4300, Loss: 0.5893\n",
      "Iteration 4400, Loss: 0.5882\n",
      "Iteration 4500, Loss: 0.5872\n",
      "Iteration 4600, Loss: 0.5862\n",
      "Iteration 4700, Loss: 0.5852\n",
      "Iteration 4800, Loss: 0.5842\n",
      "Iteration 4900, Loss: 0.5833\n",
      "Iteration 5000, Loss: 0.5823\n",
      "Iteration 5100, Loss: 0.5814\n",
      "Iteration 5200, Loss: 0.5805\n",
      "Iteration 5300, Loss: 0.5797\n",
      "Iteration 5400, Loss: 0.5788\n",
      "Iteration 5500, Loss: 0.5780\n",
      "Iteration 5600, Loss: 0.5771\n",
      "Iteration 5700, Loss: 0.5763\n",
      "Iteration 5800, Loss: 0.5755\n",
      "Iteration 5900, Loss: 0.5748\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8605\n",
      "Iteration 200, Loss: 0.7794\n",
      "Iteration 300, Loss: 0.7380\n",
      "Iteration 400, Loss: 0.7125\n",
      "Iteration 500, Loss: 0.6949\n",
      "Iteration 600, Loss: 0.6816\n",
      "Iteration 700, Loss: 0.6713\n",
      "Iteration 800, Loss: 0.6628\n",
      "Iteration 900, Loss: 0.6556\n",
      "Iteration 1000, Loss: 0.6494\n",
      "Iteration 1100, Loss: 0.6441\n",
      "Iteration 1200, Loss: 0.6394\n",
      "Iteration 1300, Loss: 0.6352\n",
      "Iteration 1400, Loss: 0.6314\n",
      "Iteration 1500, Loss: 0.6280\n",
      "Iteration 1600, Loss: 0.6249\n",
      "Iteration 1700, Loss: 0.6220\n",
      "Iteration 1800, Loss: 0.6194\n",
      "Iteration 1900, Loss: 0.6170\n",
      "Iteration 2000, Loss: 0.6147\n",
      "Iteration 2100, Loss: 0.6126\n",
      "Iteration 2200, Loss: 0.6106\n",
      "Iteration 2300, Loss: 0.6087\n",
      "Iteration 2400, Loss: 0.6069\n",
      "Iteration 2500, Loss: 0.6052\n",
      "Iteration 2600, Loss: 0.6036\n",
      "Iteration 2700, Loss: 0.6021\n",
      "Iteration 2800, Loss: 0.6006\n",
      "Iteration 2900, Loss: 0.5992\n",
      "Iteration 3000, Loss: 0.5979\n",
      "Iteration 3100, Loss: 0.5966\n",
      "Iteration 3200, Loss: 0.5953\n",
      "Iteration 3300, Loss: 0.5942\n",
      "Iteration 3400, Loss: 0.5930\n",
      "Iteration 3500, Loss: 0.5919\n",
      "Iteration 3600, Loss: 0.5908\n",
      "Iteration 3700, Loss: 0.5898\n",
      "Iteration 3800, Loss: 0.5888\n",
      "Iteration 3900, Loss: 0.5879\n",
      "Iteration 4000, Loss: 0.5869\n",
      "Iteration 4100, Loss: 0.5860\n",
      "Iteration 4200, Loss: 0.5851\n",
      "Iteration 4300, Loss: 0.5843\n",
      "Iteration 4400, Loss: 0.5834\n",
      "Iteration 4500, Loss: 0.5826\n",
      "Iteration 4600, Loss: 0.5818\n",
      "Iteration 4700, Loss: 0.5810\n",
      "Iteration 4800, Loss: 0.5803\n",
      "Iteration 4900, Loss: 0.5795\n",
      "Iteration 5000, Loss: 0.5788\n",
      "Iteration 5100, Loss: 0.5781\n",
      "Iteration 5200, Loss: 0.5774\n",
      "Iteration 5300, Loss: 0.5767\n",
      "Iteration 5400, Loss: 0.5761\n",
      "Iteration 5500, Loss: 0.5754\n",
      "Iteration 5600, Loss: 0.5748\n",
      "Iteration 5700, Loss: 0.5742\n",
      "Iteration 5800, Loss: 0.5736\n",
      "Iteration 5900, Loss: 0.5730\n",
      "226 270\n",
      "Iteration 0, Loss: 1.0935\n",
      "Iteration 100, Loss: 0.8456\n",
      "Iteration 200, Loss: 0.7678\n",
      "Iteration 300, Loss: 0.7286\n",
      "Iteration 400, Loss: 0.7053\n",
      "Iteration 500, Loss: 0.6892\n",
      "Iteration 600, Loss: 0.6773\n",
      "Iteration 700, Loss: 0.6678\n",
      "Iteration 800, Loss: 0.6601\n",
      "Iteration 900, Loss: 0.6536\n",
      "Iteration 1000, Loss: 0.6480\n",
      "Iteration 1100, Loss: 0.6431\n",
      "Iteration 1200, Loss: 0.6387\n",
      "Iteration 1300, Loss: 0.6347\n",
      "Iteration 1400, Loss: 0.6312\n",
      "Iteration 1500, Loss: 0.6279\n",
      "Iteration 1600, Loss: 0.6250\n",
      "Iteration 1700, Loss: 0.6222\n",
      "Iteration 1800, Loss: 0.6196\n",
      "Iteration 1900, Loss: 0.6172\n",
      "Iteration 2000, Loss: 0.6149\n",
      "Iteration 2100, Loss: 0.6127\n",
      "Iteration 2200, Loss: 0.6107\n",
      "Iteration 2300, Loss: 0.6088\n",
      "Iteration 2400, Loss: 0.6070\n",
      "Iteration 2500, Loss: 0.6052\n",
      "Iteration 2600, Loss: 0.6036\n",
      "Iteration 2700, Loss: 0.6020\n",
      "Iteration 2800, Loss: 0.6004\n",
      "Iteration 2900, Loss: 0.5989\n",
      "Iteration 3000, Loss: 0.5975\n",
      "Iteration 3100, Loss: 0.5961\n",
      "Iteration 3200, Loss: 0.5948\n",
      "Iteration 3300, Loss: 0.5935\n",
      "Iteration 3400, Loss: 0.5923\n",
      "Iteration 3500, Loss: 0.5911\n",
      "Iteration 3600, Loss: 0.5900\n",
      "Iteration 3700, Loss: 0.5888\n",
      "Iteration 3800, Loss: 0.5877\n",
      "Iteration 3900, Loss: 0.5867\n",
      "Iteration 4000, Loss: 0.5856\n",
      "Iteration 4100, Loss: 0.5846\n",
      "Iteration 4200, Loss: 0.5836\n",
      "Iteration 4300, Loss: 0.5827\n",
      "Iteration 4400, Loss: 0.5817\n",
      "Iteration 4500, Loss: 0.5808\n",
      "Iteration 4600, Loss: 0.5799\n",
      "Iteration 4700, Loss: 0.5791\n",
      "Iteration 4800, Loss: 0.5782\n",
      "Iteration 4900, Loss: 0.5774\n",
      "Iteration 5000, Loss: 0.5766\n",
      "Iteration 5100, Loss: 0.5758\n",
      "Iteration 5200, Loss: 0.5750\n",
      "Iteration 5300, Loss: 0.5742\n",
      "Iteration 5400, Loss: 0.5735\n",
      "Iteration 5500, Loss: 0.5728\n",
      "Iteration 5600, Loss: 0.5720\n",
      "Iteration 5700, Loss: 0.5713\n",
      "Iteration 5800, Loss: 0.5707\n",
      "Iteration 5900, Loss: 0.5700\n",
      "Iteration 0, Loss: 1.0942\n",
      "Iteration 100, Loss: 0.8766\n",
      "Iteration 200, Loss: 0.7982\n",
      "Iteration 300, Loss: 0.7572\n",
      "Iteration 400, Loss: 0.7321\n",
      "Iteration 500, Loss: 0.7146\n",
      "Iteration 600, Loss: 0.7015\n",
      "Iteration 700, Loss: 0.6911\n",
      "Iteration 800, Loss: 0.6826\n",
      "Iteration 900, Loss: 0.6754\n",
      "Iteration 1000, Loss: 0.6692\n",
      "Iteration 1100, Loss: 0.6638\n",
      "Iteration 1200, Loss: 0.6589\n",
      "Iteration 1300, Loss: 0.6546\n",
      "Iteration 1400, Loss: 0.6506\n",
      "Iteration 1500, Loss: 0.6470\n",
      "Iteration 1600, Loss: 0.6437\n",
      "Iteration 1700, Loss: 0.6407\n",
      "Iteration 1800, Loss: 0.6379\n",
      "Iteration 1900, Loss: 0.6353\n",
      "Iteration 2000, Loss: 0.6328\n",
      "Iteration 2100, Loss: 0.6305\n",
      "Iteration 2200, Loss: 0.6283\n",
      "Iteration 2300, Loss: 0.6262\n",
      "Iteration 2400, Loss: 0.6243\n",
      "Iteration 2500, Loss: 0.6224\n",
      "Iteration 2600, Loss: 0.6207\n",
      "Iteration 2700, Loss: 0.6190\n",
      "Iteration 2800, Loss: 0.6173\n",
      "Iteration 2900, Loss: 0.6158\n",
      "Iteration 3000, Loss: 0.6143\n",
      "Iteration 3100, Loss: 0.6128\n",
      "Iteration 3200, Loss: 0.6114\n",
      "Iteration 3300, Loss: 0.6101\n",
      "Iteration 3400, Loss: 0.6088\n",
      "Iteration 3500, Loss: 0.6075\n",
      "Iteration 3600, Loss: 0.6063\n",
      "Iteration 3700, Loss: 0.6052\n",
      "Iteration 3800, Loss: 0.6041\n",
      "Iteration 3900, Loss: 0.6030\n",
      "Iteration 4000, Loss: 0.6019\n",
      "Iteration 4100, Loss: 0.6009\n",
      "Iteration 4200, Loss: 0.5998\n",
      "Iteration 4300, Loss: 0.5989\n",
      "Iteration 4400, Loss: 0.5979\n",
      "Iteration 4500, Loss: 0.5970\n",
      "Iteration 4600, Loss: 0.5961\n",
      "Iteration 4700, Loss: 0.5952\n",
      "Iteration 4800, Loss: 0.5943\n",
      "Iteration 4900, Loss: 0.5934\n",
      "Iteration 5000, Loss: 0.5926\n",
      "Iteration 5100, Loss: 0.5918\n",
      "Iteration 5200, Loss: 0.5910\n",
      "Iteration 5300, Loss: 0.5902\n",
      "Iteration 5400, Loss: 0.5895\n",
      "Iteration 5500, Loss: 0.5887\n",
      "Iteration 5600, Loss: 0.5880\n",
      "Iteration 5700, Loss: 0.5873\n",
      "Iteration 5800, Loss: 0.5866\n",
      "Iteration 5900, Loss: 0.5859\n",
      "227 270\n",
      "Iteration 0, Loss: 1.0941\n",
      "Iteration 100, Loss: 0.8695\n",
      "Iteration 200, Loss: 0.7889\n",
      "Iteration 300, Loss: 0.7474\n",
      "Iteration 400, Loss: 0.7219\n",
      "Iteration 500, Loss: 0.7042\n",
      "Iteration 600, Loss: 0.6909\n",
      "Iteration 700, Loss: 0.6803\n",
      "Iteration 800, Loss: 0.6715\n",
      "Iteration 900, Loss: 0.6640\n",
      "Iteration 1000, Loss: 0.6576\n",
      "Iteration 1100, Loss: 0.6519\n",
      "Iteration 1200, Loss: 0.6469\n",
      "Iteration 1300, Loss: 0.6425\n",
      "Iteration 1400, Loss: 0.6384\n",
      "Iteration 1500, Loss: 0.6346\n",
      "Iteration 1600, Loss: 0.6313\n",
      "Iteration 1700, Loss: 0.6281\n",
      "Iteration 1800, Loss: 0.6252\n",
      "Iteration 1900, Loss: 0.6225\n",
      "Iteration 2000, Loss: 0.6199\n",
      "Iteration 2100, Loss: 0.6175\n",
      "Iteration 2200, Loss: 0.6153\n",
      "Iteration 2300, Loss: 0.6131\n",
      "Iteration 2400, Loss: 0.6111\n",
      "Iteration 2500, Loss: 0.6092\n",
      "Iteration 2600, Loss: 0.6073\n",
      "Iteration 2700, Loss: 0.6056\n",
      "Iteration 2800, Loss: 0.6039\n",
      "Iteration 2900, Loss: 0.6023\n",
      "Iteration 3000, Loss: 0.6007\n",
      "Iteration 3100, Loss: 0.5992\n",
      "Iteration 3200, Loss: 0.5978\n",
      "Iteration 3300, Loss: 0.5964\n",
      "Iteration 3400, Loss: 0.5951\n",
      "Iteration 3500, Loss: 0.5938\n",
      "Iteration 3600, Loss: 0.5926\n",
      "Iteration 3700, Loss: 0.5914\n",
      "Iteration 3800, Loss: 0.5902\n",
      "Iteration 3900, Loss: 0.5891\n",
      "Iteration 4000, Loss: 0.5879\n",
      "Iteration 4100, Loss: 0.5869\n",
      "Iteration 4200, Loss: 0.5858\n",
      "Iteration 4300, Loss: 0.5848\n",
      "Iteration 4400, Loss: 0.5838\n",
      "Iteration 4500, Loss: 0.5828\n",
      "Iteration 4600, Loss: 0.5819\n",
      "Iteration 4700, Loss: 0.5810\n",
      "Iteration 4800, Loss: 0.5801\n",
      "Iteration 4900, Loss: 0.5792\n",
      "Iteration 5000, Loss: 0.5784\n",
      "Iteration 5100, Loss: 0.5775\n",
      "Iteration 5200, Loss: 0.5767\n",
      "Iteration 5300, Loss: 0.5759\n",
      "Iteration 5400, Loss: 0.5751\n",
      "Iteration 5500, Loss: 0.5744\n",
      "Iteration 5600, Loss: 0.5736\n",
      "Iteration 5700, Loss: 0.5729\n",
      "Iteration 5800, Loss: 0.5722\n",
      "Iteration 5900, Loss: 0.5715\n",
      "Iteration 0, Loss: 1.0932\n",
      "Iteration 100, Loss: 0.8539\n",
      "Iteration 200, Loss: 0.7766\n",
      "Iteration 300, Loss: 0.7381\n",
      "Iteration 400, Loss: 0.7147\n",
      "Iteration 500, Loss: 0.6988\n",
      "Iteration 600, Loss: 0.6868\n",
      "Iteration 700, Loss: 0.6774\n",
      "Iteration 800, Loss: 0.6696\n",
      "Iteration 900, Loss: 0.6631\n",
      "Iteration 1000, Loss: 0.6575\n",
      "Iteration 1100, Loss: 0.6525\n",
      "Iteration 1200, Loss: 0.6482\n",
      "Iteration 1300, Loss: 0.6443\n",
      "Iteration 1400, Loss: 0.6407\n",
      "Iteration 1500, Loss: 0.6375\n",
      "Iteration 1600, Loss: 0.6345\n",
      "Iteration 1700, Loss: 0.6318\n",
      "Iteration 1800, Loss: 0.6292\n",
      "Iteration 1900, Loss: 0.6269\n",
      "Iteration 2000, Loss: 0.6246\n",
      "Iteration 2100, Loss: 0.6225\n",
      "Iteration 2200, Loss: 0.6206\n",
      "Iteration 2300, Loss: 0.6187\n",
      "Iteration 2400, Loss: 0.6169\n",
      "Iteration 2500, Loss: 0.6152\n",
      "Iteration 2600, Loss: 0.6136\n",
      "Iteration 2700, Loss: 0.6120\n",
      "Iteration 2800, Loss: 0.6106\n",
      "Iteration 2900, Loss: 0.6091\n",
      "Iteration 3000, Loss: 0.6077\n",
      "Iteration 3100, Loss: 0.6064\n",
      "Iteration 3200, Loss: 0.6051\n",
      "Iteration 3300, Loss: 0.6039\n",
      "Iteration 3400, Loss: 0.6028\n",
      "Iteration 3500, Loss: 0.6016\n",
      "Iteration 3600, Loss: 0.6005\n",
      "Iteration 3700, Loss: 0.5994\n",
      "Iteration 3800, Loss: 0.5983\n",
      "Iteration 3900, Loss: 0.5973\n",
      "Iteration 4000, Loss: 0.5963\n",
      "Iteration 4100, Loss: 0.5954\n",
      "Iteration 4200, Loss: 0.5944\n",
      "Iteration 4300, Loss: 0.5935\n",
      "Iteration 4400, Loss: 0.5926\n",
      "Iteration 4500, Loss: 0.5917\n",
      "Iteration 4600, Loss: 0.5909\n",
      "Iteration 4700, Loss: 0.5901\n",
      "Iteration 4800, Loss: 0.5893\n",
      "Iteration 4900, Loss: 0.5885\n",
      "Iteration 5000, Loss: 0.5877\n",
      "Iteration 5100, Loss: 0.5869\n",
      "Iteration 5200, Loss: 0.5862\n",
      "Iteration 5300, Loss: 0.5855\n",
      "Iteration 5400, Loss: 0.5848\n",
      "Iteration 5500, Loss: 0.5841\n",
      "Iteration 5600, Loss: 0.5834\n",
      "Iteration 5700, Loss: 0.5827\n",
      "Iteration 5800, Loss: 0.5821\n",
      "Iteration 5900, Loss: 0.5814\n",
      "228 270\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8603\n",
      "Iteration 200, Loss: 0.7847\n",
      "Iteration 300, Loss: 0.7463\n",
      "Iteration 400, Loss: 0.7231\n",
      "Iteration 500, Loss: 0.7072\n",
      "Iteration 600, Loss: 0.6954\n",
      "Iteration 700, Loss: 0.6862\n",
      "Iteration 800, Loss: 0.6786\n",
      "Iteration 900, Loss: 0.6721\n",
      "Iteration 1000, Loss: 0.6665\n",
      "Iteration 1100, Loss: 0.6616\n",
      "Iteration 1200, Loss: 0.6572\n",
      "Iteration 1300, Loss: 0.6532\n",
      "Iteration 1400, Loss: 0.6496\n",
      "Iteration 1500, Loss: 0.6463\n",
      "Iteration 1600, Loss: 0.6433\n",
      "Iteration 1700, Loss: 0.6405\n",
      "Iteration 1800, Loss: 0.6378\n",
      "Iteration 1900, Loss: 0.6354\n",
      "Iteration 2000, Loss: 0.6331\n",
      "Iteration 2100, Loss: 0.6309\n",
      "Iteration 2200, Loss: 0.6288\n",
      "Iteration 2300, Loss: 0.6269\n",
      "Iteration 2400, Loss: 0.6250\n",
      "Iteration 2500, Loss: 0.6233\n",
      "Iteration 2600, Loss: 0.6216\n",
      "Iteration 2700, Loss: 0.6199\n",
      "Iteration 2800, Loss: 0.6184\n",
      "Iteration 2900, Loss: 0.6169\n",
      "Iteration 3000, Loss: 0.6154\n",
      "Iteration 3100, Loss: 0.6140\n",
      "Iteration 3200, Loss: 0.6126\n",
      "Iteration 3300, Loss: 0.6113\n",
      "Iteration 3400, Loss: 0.6101\n",
      "Iteration 3500, Loss: 0.6089\n",
      "Iteration 3600, Loss: 0.6077\n",
      "Iteration 3700, Loss: 0.6065\n",
      "Iteration 3800, Loss: 0.6054\n",
      "Iteration 3900, Loss: 0.6043\n",
      "Iteration 4000, Loss: 0.6033\n",
      "Iteration 4100, Loss: 0.6023\n",
      "Iteration 4200, Loss: 0.6013\n",
      "Iteration 4300, Loss: 0.6003\n",
      "Iteration 4400, Loss: 0.5993\n",
      "Iteration 4500, Loss: 0.5984\n",
      "Iteration 4600, Loss: 0.5975\n",
      "Iteration 4700, Loss: 0.5966\n",
      "Iteration 4800, Loss: 0.5958\n",
      "Iteration 4900, Loss: 0.5949\n",
      "Iteration 5000, Loss: 0.5941\n",
      "Iteration 5100, Loss: 0.5933\n",
      "Iteration 5200, Loss: 0.5925\n",
      "Iteration 5300, Loss: 0.5917\n",
      "Iteration 5400, Loss: 0.5910\n",
      "Iteration 5500, Loss: 0.5902\n",
      "Iteration 5600, Loss: 0.5895\n",
      "Iteration 5700, Loss: 0.5888\n",
      "Iteration 5800, Loss: 0.5881\n",
      "Iteration 5900, Loss: 0.5874\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8629\n",
      "Iteration 200, Loss: 0.7816\n",
      "Iteration 300, Loss: 0.7398\n",
      "Iteration 400, Loss: 0.7139\n",
      "Iteration 500, Loss: 0.6961\n",
      "Iteration 600, Loss: 0.6825\n",
      "Iteration 700, Loss: 0.6717\n",
      "Iteration 800, Loss: 0.6629\n",
      "Iteration 900, Loss: 0.6554\n",
      "Iteration 1000, Loss: 0.6489\n",
      "Iteration 1100, Loss: 0.6434\n",
      "Iteration 1200, Loss: 0.6384\n",
      "Iteration 1300, Loss: 0.6339\n",
      "Iteration 1400, Loss: 0.6300\n",
      "Iteration 1500, Loss: 0.6264\n",
      "Iteration 1600, Loss: 0.6231\n",
      "Iteration 1700, Loss: 0.6200\n",
      "Iteration 1800, Loss: 0.6173\n",
      "Iteration 1900, Loss: 0.6147\n",
      "Iteration 2000, Loss: 0.6123\n",
      "Iteration 2100, Loss: 0.6100\n",
      "Iteration 2200, Loss: 0.6079\n",
      "Iteration 2300, Loss: 0.6059\n",
      "Iteration 2400, Loss: 0.6040\n",
      "Iteration 2500, Loss: 0.6022\n",
      "Iteration 2600, Loss: 0.6006\n",
      "Iteration 2700, Loss: 0.5989\n",
      "Iteration 2800, Loss: 0.5974\n",
      "Iteration 2900, Loss: 0.5959\n",
      "Iteration 3000, Loss: 0.5945\n",
      "Iteration 3100, Loss: 0.5932\n",
      "Iteration 3200, Loss: 0.5919\n",
      "Iteration 3300, Loss: 0.5906\n",
      "Iteration 3400, Loss: 0.5894\n",
      "Iteration 3500, Loss: 0.5883\n",
      "Iteration 3600, Loss: 0.5872\n",
      "Iteration 3700, Loss: 0.5861\n",
      "Iteration 3800, Loss: 0.5851\n",
      "Iteration 3900, Loss: 0.5840\n",
      "Iteration 4000, Loss: 0.5830\n",
      "Iteration 4100, Loss: 0.5821\n",
      "Iteration 4200, Loss: 0.5812\n",
      "Iteration 4300, Loss: 0.5803\n",
      "Iteration 4400, Loss: 0.5794\n",
      "Iteration 4500, Loss: 0.5785\n",
      "Iteration 4600, Loss: 0.5777\n",
      "Iteration 4700, Loss: 0.5769\n",
      "Iteration 4800, Loss: 0.5761\n",
      "Iteration 4900, Loss: 0.5753\n",
      "Iteration 5000, Loss: 0.5746\n",
      "Iteration 5100, Loss: 0.5738\n",
      "Iteration 5200, Loss: 0.5731\n",
      "Iteration 5300, Loss: 0.5724\n",
      "Iteration 5400, Loss: 0.5717\n",
      "Iteration 5500, Loss: 0.5710\n",
      "Iteration 5600, Loss: 0.5704\n",
      "Iteration 5700, Loss: 0.5697\n",
      "Iteration 5800, Loss: 0.5691\n",
      "Iteration 5900, Loss: 0.5685\n",
      "229 270\n",
      "Iteration 0, Loss: 1.0935\n",
      "Iteration 100, Loss: 0.8574\n",
      "Iteration 200, Loss: 0.7814\n",
      "Iteration 300, Loss: 0.7417\n",
      "Iteration 400, Loss: 0.7168\n",
      "Iteration 500, Loss: 0.6991\n",
      "Iteration 600, Loss: 0.6856\n",
      "Iteration 700, Loss: 0.6748\n",
      "Iteration 800, Loss: 0.6659\n",
      "Iteration 900, Loss: 0.6583\n",
      "Iteration 1000, Loss: 0.6519\n",
      "Iteration 1100, Loss: 0.6461\n",
      "Iteration 1200, Loss: 0.6410\n",
      "Iteration 1300, Loss: 0.6365\n",
      "Iteration 1400, Loss: 0.6323\n",
      "Iteration 1500, Loss: 0.6286\n",
      "Iteration 1600, Loss: 0.6251\n",
      "Iteration 1700, Loss: 0.6219\n",
      "Iteration 1800, Loss: 0.6190\n",
      "Iteration 1900, Loss: 0.6162\n",
      "Iteration 2000, Loss: 0.6136\n",
      "Iteration 2100, Loss: 0.6112\n",
      "Iteration 2200, Loss: 0.6089\n",
      "Iteration 2300, Loss: 0.6067\n",
      "Iteration 2400, Loss: 0.6047\n",
      "Iteration 2500, Loss: 0.6028\n",
      "Iteration 2600, Loss: 0.6009\n",
      "Iteration 2700, Loss: 0.5991\n",
      "Iteration 2800, Loss: 0.5974\n",
      "Iteration 2900, Loss: 0.5958\n",
      "Iteration 3000, Loss: 0.5943\n",
      "Iteration 3100, Loss: 0.5928\n",
      "Iteration 3200, Loss: 0.5913\n",
      "Iteration 3300, Loss: 0.5899\n",
      "Iteration 3400, Loss: 0.5886\n",
      "Iteration 3500, Loss: 0.5873\n",
      "Iteration 3600, Loss: 0.5861\n",
      "Iteration 3700, Loss: 0.5848\n",
      "Iteration 3800, Loss: 0.5837\n",
      "Iteration 3900, Loss: 0.5825\n",
      "Iteration 4000, Loss: 0.5814\n",
      "Iteration 4100, Loss: 0.5804\n",
      "Iteration 4200, Loss: 0.5793\n",
      "Iteration 4300, Loss: 0.5783\n",
      "Iteration 4400, Loss: 0.5773\n",
      "Iteration 4500, Loss: 0.5764\n",
      "Iteration 4600, Loss: 0.5754\n",
      "Iteration 4700, Loss: 0.5745\n",
      "Iteration 4800, Loss: 0.5736\n",
      "Iteration 4900, Loss: 0.5727\n",
      "Iteration 5000, Loss: 0.5719\n",
      "Iteration 5100, Loss: 0.5711\n",
      "Iteration 5200, Loss: 0.5703\n",
      "Iteration 5300, Loss: 0.5695\n",
      "Iteration 5400, Loss: 0.5687\n",
      "Iteration 5500, Loss: 0.5680\n",
      "Iteration 5600, Loss: 0.5672\n",
      "Iteration 5700, Loss: 0.5665\n",
      "Iteration 5800, Loss: 0.5658\n",
      "Iteration 5900, Loss: 0.5651\n",
      "Iteration 0, Loss: 1.0942\n",
      "Iteration 100, Loss: 0.8648\n",
      "Iteration 200, Loss: 0.7840\n",
      "Iteration 300, Loss: 0.7441\n",
      "Iteration 400, Loss: 0.7205\n",
      "Iteration 500, Loss: 0.7047\n",
      "Iteration 600, Loss: 0.6930\n",
      "Iteration 700, Loss: 0.6838\n",
      "Iteration 800, Loss: 0.6764\n",
      "Iteration 900, Loss: 0.6701\n",
      "Iteration 1000, Loss: 0.6648\n",
      "Iteration 1100, Loss: 0.6601\n",
      "Iteration 1200, Loss: 0.6560\n",
      "Iteration 1300, Loss: 0.6523\n",
      "Iteration 1400, Loss: 0.6490\n",
      "Iteration 1500, Loss: 0.6460\n",
      "Iteration 1600, Loss: 0.6432\n",
      "Iteration 1700, Loss: 0.6406\n",
      "Iteration 1800, Loss: 0.6382\n",
      "Iteration 1900, Loss: 0.6360\n",
      "Iteration 2000, Loss: 0.6339\n",
      "Iteration 2100, Loss: 0.6319\n",
      "Iteration 2200, Loss: 0.6300\n",
      "Iteration 2300, Loss: 0.6282\n",
      "Iteration 2400, Loss: 0.6265\n",
      "Iteration 2500, Loss: 0.6249\n",
      "Iteration 2600, Loss: 0.6234\n",
      "Iteration 2700, Loss: 0.6219\n",
      "Iteration 2800, Loss: 0.6204\n",
      "Iteration 2900, Loss: 0.6190\n",
      "Iteration 3000, Loss: 0.6177\n",
      "Iteration 3100, Loss: 0.6165\n",
      "Iteration 3200, Loss: 0.6152\n",
      "Iteration 3300, Loss: 0.6140\n",
      "Iteration 3400, Loss: 0.6129\n",
      "Iteration 3500, Loss: 0.6118\n",
      "Iteration 3600, Loss: 0.6107\n",
      "Iteration 3700, Loss: 0.6096\n",
      "Iteration 3800, Loss: 0.6086\n",
      "Iteration 3900, Loss: 0.6076\n",
      "Iteration 4000, Loss: 0.6066\n",
      "Iteration 4100, Loss: 0.6057\n",
      "Iteration 4200, Loss: 0.6048\n",
      "Iteration 4300, Loss: 0.6039\n",
      "Iteration 4400, Loss: 0.6030\n",
      "Iteration 4500, Loss: 0.6021\n",
      "Iteration 4600, Loss: 0.6013\n",
      "Iteration 4700, Loss: 0.6005\n",
      "Iteration 4800, Loss: 0.5997\n",
      "Iteration 4900, Loss: 0.5989\n",
      "Iteration 5000, Loss: 0.5981\n",
      "Iteration 5100, Loss: 0.5974\n",
      "Iteration 5200, Loss: 0.5967\n",
      "Iteration 5300, Loss: 0.5959\n",
      "Iteration 5400, Loss: 0.5953\n",
      "Iteration 5500, Loss: 0.5946\n",
      "Iteration 5600, Loss: 0.5939\n",
      "Iteration 5700, Loss: 0.5932\n",
      "Iteration 5800, Loss: 0.5926\n",
      "Iteration 5900, Loss: 0.5920\n",
      "230 270\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8702\n",
      "Iteration 200, Loss: 0.7912\n",
      "Iteration 300, Loss: 0.7507\n",
      "Iteration 400, Loss: 0.7258\n",
      "Iteration 500, Loss: 0.7085\n",
      "Iteration 600, Loss: 0.6956\n",
      "Iteration 700, Loss: 0.6854\n",
      "Iteration 800, Loss: 0.6768\n",
      "Iteration 900, Loss: 0.6695\n",
      "Iteration 1000, Loss: 0.6632\n",
      "Iteration 1100, Loss: 0.6577\n",
      "Iteration 1200, Loss: 0.6527\n",
      "Iteration 1300, Loss: 0.6483\n",
      "Iteration 1400, Loss: 0.6442\n",
      "Iteration 1500, Loss: 0.6405\n",
      "Iteration 1600, Loss: 0.6371\n",
      "Iteration 1700, Loss: 0.6339\n",
      "Iteration 1800, Loss: 0.6310\n",
      "Iteration 1900, Loss: 0.6282\n",
      "Iteration 2000, Loss: 0.6256\n",
      "Iteration 2100, Loss: 0.6232\n",
      "Iteration 2200, Loss: 0.6209\n",
      "Iteration 2300, Loss: 0.6187\n",
      "Iteration 2400, Loss: 0.6166\n",
      "Iteration 2500, Loss: 0.6146\n",
      "Iteration 2600, Loss: 0.6127\n",
      "Iteration 2700, Loss: 0.6109\n",
      "Iteration 2800, Loss: 0.6091\n",
      "Iteration 2900, Loss: 0.6074\n",
      "Iteration 3000, Loss: 0.6058\n",
      "Iteration 3100, Loss: 0.6042\n",
      "Iteration 3200, Loss: 0.6027\n",
      "Iteration 3300, Loss: 0.6013\n",
      "Iteration 3400, Loss: 0.5999\n",
      "Iteration 3500, Loss: 0.5985\n",
      "Iteration 3600, Loss: 0.5971\n",
      "Iteration 3700, Loss: 0.5958\n",
      "Iteration 3800, Loss: 0.5946\n",
      "Iteration 3900, Loss: 0.5934\n",
      "Iteration 4000, Loss: 0.5922\n",
      "Iteration 4100, Loss: 0.5910\n",
      "Iteration 4200, Loss: 0.5899\n",
      "Iteration 4300, Loss: 0.5888\n",
      "Iteration 4400, Loss: 0.5877\n",
      "Iteration 4500, Loss: 0.5866\n",
      "Iteration 4600, Loss: 0.5856\n",
      "Iteration 4700, Loss: 0.5846\n",
      "Iteration 4800, Loss: 0.5836\n",
      "Iteration 4900, Loss: 0.5826\n",
      "Iteration 5000, Loss: 0.5817\n",
      "Iteration 5100, Loss: 0.5807\n",
      "Iteration 5200, Loss: 0.5798\n",
      "Iteration 5300, Loss: 0.5789\n",
      "Iteration 5400, Loss: 0.5781\n",
      "Iteration 5500, Loss: 0.5772\n",
      "Iteration 5600, Loss: 0.5764\n",
      "Iteration 5700, Loss: 0.5755\n",
      "Iteration 5800, Loss: 0.5747\n",
      "Iteration 5900, Loss: 0.5739\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8515\n",
      "Iteration 200, Loss: 0.7740\n",
      "Iteration 300, Loss: 0.7344\n",
      "Iteration 400, Loss: 0.7102\n",
      "Iteration 500, Loss: 0.6935\n",
      "Iteration 600, Loss: 0.6811\n",
      "Iteration 700, Loss: 0.6713\n",
      "Iteration 800, Loss: 0.6632\n",
      "Iteration 900, Loss: 0.6564\n",
      "Iteration 1000, Loss: 0.6506\n",
      "Iteration 1100, Loss: 0.6456\n",
      "Iteration 1200, Loss: 0.6411\n",
      "Iteration 1300, Loss: 0.6370\n",
      "Iteration 1400, Loss: 0.6334\n",
      "Iteration 1500, Loss: 0.6301\n",
      "Iteration 1600, Loss: 0.6271\n",
      "Iteration 1700, Loss: 0.6243\n",
      "Iteration 1800, Loss: 0.6217\n",
      "Iteration 1900, Loss: 0.6193\n",
      "Iteration 2000, Loss: 0.6170\n",
      "Iteration 2100, Loss: 0.6149\n",
      "Iteration 2200, Loss: 0.6129\n",
      "Iteration 2300, Loss: 0.6110\n",
      "Iteration 2400, Loss: 0.6091\n",
      "Iteration 2500, Loss: 0.6074\n",
      "Iteration 2600, Loss: 0.6058\n",
      "Iteration 2700, Loss: 0.6042\n",
      "Iteration 2800, Loss: 0.6027\n",
      "Iteration 2900, Loss: 0.6012\n",
      "Iteration 3000, Loss: 0.5998\n",
      "Iteration 3100, Loss: 0.5984\n",
      "Iteration 3200, Loss: 0.5971\n",
      "Iteration 3300, Loss: 0.5959\n",
      "Iteration 3400, Loss: 0.5947\n",
      "Iteration 3500, Loss: 0.5935\n",
      "Iteration 3600, Loss: 0.5923\n",
      "Iteration 3700, Loss: 0.5912\n",
      "Iteration 3800, Loss: 0.5902\n",
      "Iteration 3900, Loss: 0.5891\n",
      "Iteration 4000, Loss: 0.5881\n",
      "Iteration 4100, Loss: 0.5871\n",
      "Iteration 4200, Loss: 0.5862\n",
      "Iteration 4300, Loss: 0.5853\n",
      "Iteration 4400, Loss: 0.5844\n",
      "Iteration 4500, Loss: 0.5835\n",
      "Iteration 4600, Loss: 0.5826\n",
      "Iteration 4700, Loss: 0.5818\n",
      "Iteration 4800, Loss: 0.5809\n",
      "Iteration 4900, Loss: 0.5801\n",
      "Iteration 5000, Loss: 0.5794\n",
      "Iteration 5100, Loss: 0.5786\n",
      "Iteration 5200, Loss: 0.5778\n",
      "Iteration 5300, Loss: 0.5771\n",
      "Iteration 5400, Loss: 0.5764\n",
      "Iteration 5500, Loss: 0.5757\n",
      "Iteration 5600, Loss: 0.5750\n",
      "Iteration 5700, Loss: 0.5743\n",
      "Iteration 5800, Loss: 0.5737\n",
      "Iteration 5900, Loss: 0.5730\n",
      "231 270\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8621\n",
      "Iteration 200, Loss: 0.7854\n",
      "Iteration 300, Loss: 0.7464\n",
      "Iteration 400, Loss: 0.7223\n",
      "Iteration 500, Loss: 0.7057\n",
      "Iteration 600, Loss: 0.6933\n",
      "Iteration 700, Loss: 0.6834\n",
      "Iteration 800, Loss: 0.6753\n",
      "Iteration 900, Loss: 0.6684\n",
      "Iteration 1000, Loss: 0.6625\n",
      "Iteration 1100, Loss: 0.6573\n",
      "Iteration 1200, Loss: 0.6527\n",
      "Iteration 1300, Loss: 0.6485\n",
      "Iteration 1400, Loss: 0.6447\n",
      "Iteration 1500, Loss: 0.6413\n",
      "Iteration 1600, Loss: 0.6381\n",
      "Iteration 1700, Loss: 0.6351\n",
      "Iteration 1800, Loss: 0.6324\n",
      "Iteration 1900, Loss: 0.6299\n",
      "Iteration 2000, Loss: 0.6275\n",
      "Iteration 2100, Loss: 0.6252\n",
      "Iteration 2200, Loss: 0.6230\n",
      "Iteration 2300, Loss: 0.6209\n",
      "Iteration 2400, Loss: 0.6190\n",
      "Iteration 2500, Loss: 0.6171\n",
      "Iteration 2600, Loss: 0.6153\n",
      "Iteration 2700, Loss: 0.6137\n",
      "Iteration 2800, Loss: 0.6120\n",
      "Iteration 2900, Loss: 0.6105\n",
      "Iteration 3000, Loss: 0.6089\n",
      "Iteration 3100, Loss: 0.6075\n",
      "Iteration 3200, Loss: 0.6061\n",
      "Iteration 3300, Loss: 0.6047\n",
      "Iteration 3400, Loss: 0.6034\n",
      "Iteration 3500, Loss: 0.6022\n",
      "Iteration 3600, Loss: 0.6009\n",
      "Iteration 3700, Loss: 0.5997\n",
      "Iteration 3800, Loss: 0.5986\n",
      "Iteration 3900, Loss: 0.5975\n",
      "Iteration 4000, Loss: 0.5964\n",
      "Iteration 4100, Loss: 0.5953\n",
      "Iteration 4200, Loss: 0.5943\n",
      "Iteration 4300, Loss: 0.5933\n",
      "Iteration 4400, Loss: 0.5923\n",
      "Iteration 4500, Loss: 0.5913\n",
      "Iteration 4600, Loss: 0.5904\n",
      "Iteration 4700, Loss: 0.5895\n",
      "Iteration 4800, Loss: 0.5886\n",
      "Iteration 4900, Loss: 0.5877\n",
      "Iteration 5000, Loss: 0.5868\n",
      "Iteration 5100, Loss: 0.5860\n",
      "Iteration 5200, Loss: 0.5852\n",
      "Iteration 5300, Loss: 0.5844\n",
      "Iteration 5400, Loss: 0.5836\n",
      "Iteration 5500, Loss: 0.5828\n",
      "Iteration 5600, Loss: 0.5821\n",
      "Iteration 5700, Loss: 0.5813\n",
      "Iteration 5800, Loss: 0.5806\n",
      "Iteration 5900, Loss: 0.5799\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8596\n",
      "Iteration 200, Loss: 0.7789\n",
      "Iteration 300, Loss: 0.7379\n",
      "Iteration 400, Loss: 0.7125\n",
      "Iteration 500, Loss: 0.6951\n",
      "Iteration 600, Loss: 0.6822\n",
      "Iteration 700, Loss: 0.6719\n",
      "Iteration 800, Loss: 0.6635\n",
      "Iteration 900, Loss: 0.6564\n",
      "Iteration 1000, Loss: 0.6502\n",
      "Iteration 1100, Loss: 0.6449\n",
      "Iteration 1200, Loss: 0.6401\n",
      "Iteration 1300, Loss: 0.6358\n",
      "Iteration 1400, Loss: 0.6319\n",
      "Iteration 1500, Loss: 0.6284\n",
      "Iteration 1600, Loss: 0.6251\n",
      "Iteration 1700, Loss: 0.6221\n",
      "Iteration 1800, Loss: 0.6193\n",
      "Iteration 1900, Loss: 0.6166\n",
      "Iteration 2000, Loss: 0.6142\n",
      "Iteration 2100, Loss: 0.6118\n",
      "Iteration 2200, Loss: 0.6096\n",
      "Iteration 2300, Loss: 0.6075\n",
      "Iteration 2400, Loss: 0.6055\n",
      "Iteration 2500, Loss: 0.6036\n",
      "Iteration 2600, Loss: 0.6018\n",
      "Iteration 2700, Loss: 0.6000\n",
      "Iteration 2800, Loss: 0.5983\n",
      "Iteration 2900, Loss: 0.5967\n",
      "Iteration 3000, Loss: 0.5951\n",
      "Iteration 3100, Loss: 0.5936\n",
      "Iteration 3200, Loss: 0.5921\n",
      "Iteration 3300, Loss: 0.5907\n",
      "Iteration 3400, Loss: 0.5893\n",
      "Iteration 3500, Loss: 0.5880\n",
      "Iteration 3600, Loss: 0.5867\n",
      "Iteration 3700, Loss: 0.5855\n",
      "Iteration 3800, Loss: 0.5843\n",
      "Iteration 3900, Loss: 0.5831\n",
      "Iteration 4000, Loss: 0.5819\n",
      "Iteration 4100, Loss: 0.5808\n",
      "Iteration 4200, Loss: 0.5797\n",
      "Iteration 4300, Loss: 0.5787\n",
      "Iteration 4400, Loss: 0.5776\n",
      "Iteration 4500, Loss: 0.5766\n",
      "Iteration 4600, Loss: 0.5756\n",
      "Iteration 4700, Loss: 0.5746\n",
      "Iteration 4800, Loss: 0.5737\n",
      "Iteration 4900, Loss: 0.5728\n",
      "Iteration 5000, Loss: 0.5719\n",
      "Iteration 5100, Loss: 0.5710\n",
      "Iteration 5200, Loss: 0.5701\n",
      "Iteration 5300, Loss: 0.5692\n",
      "Iteration 5400, Loss: 0.5684\n",
      "Iteration 5500, Loss: 0.5676\n",
      "Iteration 5600, Loss: 0.5668\n",
      "Iteration 5700, Loss: 0.5660\n",
      "Iteration 5800, Loss: 0.5652\n",
      "Iteration 5900, Loss: 0.5644\n",
      "232 270\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8679\n",
      "Iteration 200, Loss: 0.7892\n",
      "Iteration 300, Loss: 0.7490\n",
      "Iteration 400, Loss: 0.7246\n",
      "Iteration 500, Loss: 0.7077\n",
      "Iteration 600, Loss: 0.6951\n",
      "Iteration 700, Loss: 0.6851\n",
      "Iteration 800, Loss: 0.6769\n",
      "Iteration 900, Loss: 0.6699\n",
      "Iteration 1000, Loss: 0.6638\n",
      "Iteration 1100, Loss: 0.6585\n",
      "Iteration 1200, Loss: 0.6538\n",
      "Iteration 1300, Loss: 0.6496\n",
      "Iteration 1400, Loss: 0.6456\n",
      "Iteration 1500, Loss: 0.6421\n",
      "Iteration 1600, Loss: 0.6388\n",
      "Iteration 1700, Loss: 0.6358\n",
      "Iteration 1800, Loss: 0.6330\n",
      "Iteration 1900, Loss: 0.6303\n",
      "Iteration 2000, Loss: 0.6278\n",
      "Iteration 2100, Loss: 0.6255\n",
      "Iteration 2200, Loss: 0.6232\n",
      "Iteration 2300, Loss: 0.6211\n",
      "Iteration 2400, Loss: 0.6190\n",
      "Iteration 2500, Loss: 0.6171\n",
      "Iteration 2600, Loss: 0.6153\n",
      "Iteration 2700, Loss: 0.6135\n",
      "Iteration 2800, Loss: 0.6118\n",
      "Iteration 2900, Loss: 0.6101\n",
      "Iteration 3000, Loss: 0.6085\n",
      "Iteration 3100, Loss: 0.6070\n",
      "Iteration 3200, Loss: 0.6055\n",
      "Iteration 3300, Loss: 0.6041\n",
      "Iteration 3400, Loss: 0.6027\n",
      "Iteration 3500, Loss: 0.6013\n",
      "Iteration 3600, Loss: 0.6000\n",
      "Iteration 3700, Loss: 0.5987\n",
      "Iteration 3800, Loss: 0.5975\n",
      "Iteration 3900, Loss: 0.5963\n",
      "Iteration 4000, Loss: 0.5951\n",
      "Iteration 4100, Loss: 0.5939\n",
      "Iteration 4200, Loss: 0.5928\n",
      "Iteration 4300, Loss: 0.5917\n",
      "Iteration 4400, Loss: 0.5906\n",
      "Iteration 4500, Loss: 0.5896\n",
      "Iteration 4600, Loss: 0.5886\n",
      "Iteration 4700, Loss: 0.5876\n",
      "Iteration 4800, Loss: 0.5866\n",
      "Iteration 4900, Loss: 0.5856\n",
      "Iteration 5000, Loss: 0.5847\n",
      "Iteration 5100, Loss: 0.5838\n",
      "Iteration 5200, Loss: 0.5829\n",
      "Iteration 5300, Loss: 0.5820\n",
      "Iteration 5400, Loss: 0.5811\n",
      "Iteration 5500, Loss: 0.5803\n",
      "Iteration 5600, Loss: 0.5795\n",
      "Iteration 5700, Loss: 0.5786\n",
      "Iteration 5800, Loss: 0.5778\n",
      "Iteration 5900, Loss: 0.5770\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8533\n",
      "Iteration 200, Loss: 0.7747\n",
      "Iteration 300, Loss: 0.7345\n",
      "Iteration 400, Loss: 0.7097\n",
      "Iteration 500, Loss: 0.6927\n",
      "Iteration 600, Loss: 0.6798\n",
      "Iteration 700, Loss: 0.6696\n",
      "Iteration 800, Loss: 0.6612\n",
      "Iteration 900, Loss: 0.6541\n",
      "Iteration 1000, Loss: 0.6479\n",
      "Iteration 1100, Loss: 0.6426\n",
      "Iteration 1200, Loss: 0.6378\n",
      "Iteration 1300, Loss: 0.6336\n",
      "Iteration 1400, Loss: 0.6297\n",
      "Iteration 1500, Loss: 0.6261\n",
      "Iteration 1600, Loss: 0.6228\n",
      "Iteration 1700, Loss: 0.6198\n",
      "Iteration 1800, Loss: 0.6170\n",
      "Iteration 1900, Loss: 0.6144\n",
      "Iteration 2000, Loss: 0.6119\n",
      "Iteration 2100, Loss: 0.6096\n",
      "Iteration 2200, Loss: 0.6074\n",
      "Iteration 2300, Loss: 0.6053\n",
      "Iteration 2400, Loss: 0.6034\n",
      "Iteration 2500, Loss: 0.6015\n",
      "Iteration 2600, Loss: 0.5996\n",
      "Iteration 2700, Loss: 0.5979\n",
      "Iteration 2800, Loss: 0.5963\n",
      "Iteration 2900, Loss: 0.5947\n",
      "Iteration 3000, Loss: 0.5932\n",
      "Iteration 3100, Loss: 0.5917\n",
      "Iteration 3200, Loss: 0.5903\n",
      "Iteration 3300, Loss: 0.5889\n",
      "Iteration 3400, Loss: 0.5876\n",
      "Iteration 3500, Loss: 0.5863\n",
      "Iteration 3600, Loss: 0.5851\n",
      "Iteration 3700, Loss: 0.5838\n",
      "Iteration 3800, Loss: 0.5827\n",
      "Iteration 3900, Loss: 0.5815\n",
      "Iteration 4000, Loss: 0.5804\n",
      "Iteration 4100, Loss: 0.5793\n",
      "Iteration 4200, Loss: 0.5783\n",
      "Iteration 4300, Loss: 0.5773\n",
      "Iteration 4400, Loss: 0.5763\n",
      "Iteration 4500, Loss: 0.5753\n",
      "Iteration 4600, Loss: 0.5743\n",
      "Iteration 4700, Loss: 0.5734\n",
      "Iteration 4800, Loss: 0.5725\n",
      "Iteration 4900, Loss: 0.5716\n",
      "Iteration 5000, Loss: 0.5708\n",
      "Iteration 5100, Loss: 0.5699\n",
      "Iteration 5200, Loss: 0.5691\n",
      "Iteration 5300, Loss: 0.5683\n",
      "Iteration 5400, Loss: 0.5675\n",
      "Iteration 5500, Loss: 0.5667\n",
      "Iteration 5600, Loss: 0.5659\n",
      "Iteration 5700, Loss: 0.5652\n",
      "Iteration 5800, Loss: 0.5644\n",
      "Iteration 5900, Loss: 0.5637\n",
      "233 270\n",
      "Iteration 0, Loss: 1.0933\n",
      "Iteration 100, Loss: 0.8597\n",
      "Iteration 200, Loss: 0.7819\n",
      "Iteration 300, Loss: 0.7413\n",
      "Iteration 400, Loss: 0.7159\n",
      "Iteration 500, Loss: 0.6983\n",
      "Iteration 600, Loss: 0.6849\n",
      "Iteration 700, Loss: 0.6742\n",
      "Iteration 800, Loss: 0.6653\n",
      "Iteration 900, Loss: 0.6578\n",
      "Iteration 1000, Loss: 0.6514\n",
      "Iteration 1100, Loss: 0.6457\n",
      "Iteration 1200, Loss: 0.6407\n",
      "Iteration 1300, Loss: 0.6363\n",
      "Iteration 1400, Loss: 0.6322\n",
      "Iteration 1500, Loss: 0.6285\n",
      "Iteration 1600, Loss: 0.6251\n",
      "Iteration 1700, Loss: 0.6220\n",
      "Iteration 1800, Loss: 0.6192\n",
      "Iteration 1900, Loss: 0.6165\n",
      "Iteration 2000, Loss: 0.6139\n",
      "Iteration 2100, Loss: 0.6116\n",
      "Iteration 2200, Loss: 0.6094\n",
      "Iteration 2300, Loss: 0.6073\n",
      "Iteration 2400, Loss: 0.6053\n",
      "Iteration 2500, Loss: 0.6034\n",
      "Iteration 2600, Loss: 0.6017\n",
      "Iteration 2700, Loss: 0.5999\n",
      "Iteration 2800, Loss: 0.5983\n",
      "Iteration 2900, Loss: 0.5967\n",
      "Iteration 3000, Loss: 0.5952\n",
      "Iteration 3100, Loss: 0.5938\n",
      "Iteration 3200, Loss: 0.5924\n",
      "Iteration 3300, Loss: 0.5910\n",
      "Iteration 3400, Loss: 0.5897\n",
      "Iteration 3500, Loss: 0.5885\n",
      "Iteration 3600, Loss: 0.5872\n",
      "Iteration 3700, Loss: 0.5861\n",
      "Iteration 3800, Loss: 0.5849\n",
      "Iteration 3900, Loss: 0.5838\n",
      "Iteration 4000, Loss: 0.5827\n",
      "Iteration 4100, Loss: 0.5817\n",
      "Iteration 4200, Loss: 0.5806\n",
      "Iteration 4300, Loss: 0.5796\n",
      "Iteration 4400, Loss: 0.5787\n",
      "Iteration 4500, Loss: 0.5777\n",
      "Iteration 4600, Loss: 0.5768\n",
      "Iteration 4700, Loss: 0.5759\n",
      "Iteration 4800, Loss: 0.5750\n",
      "Iteration 4900, Loss: 0.5741\n",
      "Iteration 5000, Loss: 0.5733\n",
      "Iteration 5100, Loss: 0.5725\n",
      "Iteration 5200, Loss: 0.5716\n",
      "Iteration 5300, Loss: 0.5708\n",
      "Iteration 5400, Loss: 0.5701\n",
      "Iteration 5500, Loss: 0.5693\n",
      "Iteration 5600, Loss: 0.5685\n",
      "Iteration 5700, Loss: 0.5678\n",
      "Iteration 5800, Loss: 0.5671\n",
      "Iteration 5900, Loss: 0.5664\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8623\n",
      "Iteration 200, Loss: 0.7823\n",
      "Iteration 300, Loss: 0.7423\n",
      "Iteration 400, Loss: 0.7183\n",
      "Iteration 500, Loss: 0.7022\n",
      "Iteration 600, Loss: 0.6902\n",
      "Iteration 700, Loss: 0.6808\n",
      "Iteration 800, Loss: 0.6731\n",
      "Iteration 900, Loss: 0.6666\n",
      "Iteration 1000, Loss: 0.6610\n",
      "Iteration 1100, Loss: 0.6560\n",
      "Iteration 1200, Loss: 0.6516\n",
      "Iteration 1300, Loss: 0.6476\n",
      "Iteration 1400, Loss: 0.6439\n",
      "Iteration 1500, Loss: 0.6405\n",
      "Iteration 1600, Loss: 0.6374\n",
      "Iteration 1700, Loss: 0.6345\n",
      "Iteration 1800, Loss: 0.6318\n",
      "Iteration 1900, Loss: 0.6292\n",
      "Iteration 2000, Loss: 0.6269\n",
      "Iteration 2100, Loss: 0.6246\n",
      "Iteration 2200, Loss: 0.6225\n",
      "Iteration 2300, Loss: 0.6204\n",
      "Iteration 2400, Loss: 0.6185\n",
      "Iteration 2500, Loss: 0.6167\n",
      "Iteration 2600, Loss: 0.6149\n",
      "Iteration 2700, Loss: 0.6132\n",
      "Iteration 2800, Loss: 0.6116\n",
      "Iteration 2900, Loss: 0.6100\n",
      "Iteration 3000, Loss: 0.6085\n",
      "Iteration 3100, Loss: 0.6071\n",
      "Iteration 3200, Loss: 0.6056\n",
      "Iteration 3300, Loss: 0.6043\n",
      "Iteration 3400, Loss: 0.6029\n",
      "Iteration 3500, Loss: 0.6017\n",
      "Iteration 3600, Loss: 0.6004\n",
      "Iteration 3700, Loss: 0.5992\n",
      "Iteration 3800, Loss: 0.5981\n",
      "Iteration 3900, Loss: 0.5969\n",
      "Iteration 4000, Loss: 0.5958\n",
      "Iteration 4100, Loss: 0.5948\n",
      "Iteration 4200, Loss: 0.5937\n",
      "Iteration 4300, Loss: 0.5927\n",
      "Iteration 4400, Loss: 0.5917\n",
      "Iteration 4500, Loss: 0.5907\n",
      "Iteration 4600, Loss: 0.5898\n",
      "Iteration 4700, Loss: 0.5889\n",
      "Iteration 4800, Loss: 0.5879\n",
      "Iteration 4900, Loss: 0.5871\n",
      "Iteration 5000, Loss: 0.5862\n",
      "Iteration 5100, Loss: 0.5854\n",
      "Iteration 5200, Loss: 0.5845\n",
      "Iteration 5300, Loss: 0.5837\n",
      "Iteration 5400, Loss: 0.5829\n",
      "Iteration 5500, Loss: 0.5822\n",
      "Iteration 5600, Loss: 0.5814\n",
      "Iteration 5700, Loss: 0.5806\n",
      "Iteration 5800, Loss: 0.5799\n",
      "Iteration 5900, Loss: 0.5792\n",
      "234 270\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8715\n",
      "Iteration 200, Loss: 0.7969\n",
      "Iteration 300, Loss: 0.7588\n",
      "Iteration 400, Loss: 0.7357\n",
      "Iteration 500, Loss: 0.7197\n",
      "Iteration 600, Loss: 0.7077\n",
      "Iteration 700, Loss: 0.6982\n",
      "Iteration 800, Loss: 0.6903\n",
      "Iteration 900, Loss: 0.6837\n",
      "Iteration 1000, Loss: 0.6779\n",
      "Iteration 1100, Loss: 0.6729\n",
      "Iteration 1200, Loss: 0.6684\n",
      "Iteration 1300, Loss: 0.6643\n",
      "Iteration 1400, Loss: 0.6606\n",
      "Iteration 1500, Loss: 0.6572\n",
      "Iteration 1600, Loss: 0.6541\n",
      "Iteration 1700, Loss: 0.6512\n",
      "Iteration 1800, Loss: 0.6486\n",
      "Iteration 1900, Loss: 0.6460\n",
      "Iteration 2000, Loss: 0.6437\n",
      "Iteration 2100, Loss: 0.6414\n",
      "Iteration 2200, Loss: 0.6393\n",
      "Iteration 2300, Loss: 0.6374\n",
      "Iteration 2400, Loss: 0.6355\n",
      "Iteration 2500, Loss: 0.6337\n",
      "Iteration 2600, Loss: 0.6320\n",
      "Iteration 2700, Loss: 0.6303\n",
      "Iteration 2800, Loss: 0.6287\n",
      "Iteration 2900, Loss: 0.6272\n",
      "Iteration 3000, Loss: 0.6258\n",
      "Iteration 3100, Loss: 0.6244\n",
      "Iteration 3200, Loss: 0.6230\n",
      "Iteration 3300, Loss: 0.6217\n",
      "Iteration 3400, Loss: 0.6204\n",
      "Iteration 3500, Loss: 0.6192\n",
      "Iteration 3600, Loss: 0.6180\n",
      "Iteration 3700, Loss: 0.6169\n",
      "Iteration 3800, Loss: 0.6158\n",
      "Iteration 3900, Loss: 0.6147\n",
      "Iteration 4000, Loss: 0.6136\n",
      "Iteration 4100, Loss: 0.6126\n",
      "Iteration 4200, Loss: 0.6116\n",
      "Iteration 4300, Loss: 0.6107\n",
      "Iteration 4400, Loss: 0.6097\n",
      "Iteration 4500, Loss: 0.6088\n",
      "Iteration 4600, Loss: 0.6079\n",
      "Iteration 4700, Loss: 0.6070\n",
      "Iteration 4800, Loss: 0.6062\n",
      "Iteration 4900, Loss: 0.6053\n",
      "Iteration 5000, Loss: 0.6045\n",
      "Iteration 5100, Loss: 0.6037\n",
      "Iteration 5200, Loss: 0.6029\n",
      "Iteration 5300, Loss: 0.6021\n",
      "Iteration 5400, Loss: 0.6014\n",
      "Iteration 5500, Loss: 0.6006\n",
      "Iteration 5600, Loss: 0.5999\n",
      "Iteration 5700, Loss: 0.5992\n",
      "Iteration 5800, Loss: 0.5985\n",
      "Iteration 5900, Loss: 0.5978\n",
      "Iteration 0, Loss: 1.0935\n",
      "Iteration 100, Loss: 0.8506\n",
      "Iteration 200, Loss: 0.7677\n",
      "Iteration 300, Loss: 0.7257\n",
      "Iteration 400, Loss: 0.6998\n",
      "Iteration 500, Loss: 0.6821\n",
      "Iteration 600, Loss: 0.6687\n",
      "Iteration 700, Loss: 0.6581\n",
      "Iteration 800, Loss: 0.6495\n",
      "Iteration 900, Loss: 0.6421\n",
      "Iteration 1000, Loss: 0.6357\n",
      "Iteration 1100, Loss: 0.6302\n",
      "Iteration 1200, Loss: 0.6253\n",
      "Iteration 1300, Loss: 0.6209\n",
      "Iteration 1400, Loss: 0.6169\n",
      "Iteration 1500, Loss: 0.6131\n",
      "Iteration 1600, Loss: 0.6098\n",
      "Iteration 1700, Loss: 0.6067\n",
      "Iteration 1800, Loss: 0.6038\n",
      "Iteration 1900, Loss: 0.6011\n",
      "Iteration 2000, Loss: 0.5985\n",
      "Iteration 2100, Loss: 0.5962\n",
      "Iteration 2200, Loss: 0.5939\n",
      "Iteration 2300, Loss: 0.5917\n",
      "Iteration 2400, Loss: 0.5897\n",
      "Iteration 2500, Loss: 0.5878\n",
      "Iteration 2600, Loss: 0.5859\n",
      "Iteration 2700, Loss: 0.5841\n",
      "Iteration 2800, Loss: 0.5824\n",
      "Iteration 2900, Loss: 0.5807\n",
      "Iteration 3000, Loss: 0.5791\n",
      "Iteration 3100, Loss: 0.5776\n",
      "Iteration 3200, Loss: 0.5761\n",
      "Iteration 3300, Loss: 0.5747\n",
      "Iteration 3400, Loss: 0.5733\n",
      "Iteration 3500, Loss: 0.5720\n",
      "Iteration 3600, Loss: 0.5707\n",
      "Iteration 3700, Loss: 0.5694\n",
      "Iteration 3800, Loss: 0.5682\n",
      "Iteration 3900, Loss: 0.5670\n",
      "Iteration 4000, Loss: 0.5659\n",
      "Iteration 4100, Loss: 0.5647\n",
      "Iteration 4200, Loss: 0.5636\n",
      "Iteration 4300, Loss: 0.5626\n",
      "Iteration 4400, Loss: 0.5615\n",
      "Iteration 4500, Loss: 0.5605\n",
      "Iteration 4600, Loss: 0.5595\n",
      "Iteration 4700, Loss: 0.5585\n",
      "Iteration 4800, Loss: 0.5576\n",
      "Iteration 4900, Loss: 0.5566\n",
      "Iteration 5000, Loss: 0.5557\n",
      "Iteration 5100, Loss: 0.5548\n",
      "Iteration 5200, Loss: 0.5540\n",
      "Iteration 5300, Loss: 0.5531\n",
      "Iteration 5400, Loss: 0.5523\n",
      "Iteration 5500, Loss: 0.5514\n",
      "Iteration 5600, Loss: 0.5506\n",
      "Iteration 5700, Loss: 0.5498\n",
      "Iteration 5800, Loss: 0.5491\n",
      "Iteration 5900, Loss: 0.5483\n",
      "235 270\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8660\n",
      "Iteration 200, Loss: 0.7906\n",
      "Iteration 300, Loss: 0.7522\n",
      "Iteration 400, Loss: 0.7289\n",
      "Iteration 500, Loss: 0.7125\n",
      "Iteration 600, Loss: 0.7004\n",
      "Iteration 700, Loss: 0.6906\n",
      "Iteration 800, Loss: 0.6827\n",
      "Iteration 900, Loss: 0.6760\n",
      "Iteration 1000, Loss: 0.6702\n",
      "Iteration 1100, Loss: 0.6651\n",
      "Iteration 1200, Loss: 0.6606\n",
      "Iteration 1300, Loss: 0.6565\n",
      "Iteration 1400, Loss: 0.6528\n",
      "Iteration 1500, Loss: 0.6494\n",
      "Iteration 1600, Loss: 0.6463\n",
      "Iteration 1700, Loss: 0.6434\n",
      "Iteration 1800, Loss: 0.6407\n",
      "Iteration 1900, Loss: 0.6382\n",
      "Iteration 2000, Loss: 0.6358\n",
      "Iteration 2100, Loss: 0.6336\n",
      "Iteration 2200, Loss: 0.6315\n",
      "Iteration 2300, Loss: 0.6295\n",
      "Iteration 2400, Loss: 0.6276\n",
      "Iteration 2500, Loss: 0.6258\n",
      "Iteration 2600, Loss: 0.6240\n",
      "Iteration 2700, Loss: 0.6224\n",
      "Iteration 2800, Loss: 0.6208\n",
      "Iteration 2900, Loss: 0.6192\n",
      "Iteration 3000, Loss: 0.6177\n",
      "Iteration 3100, Loss: 0.6163\n",
      "Iteration 3200, Loss: 0.6149\n",
      "Iteration 3300, Loss: 0.6136\n",
      "Iteration 3400, Loss: 0.6123\n",
      "Iteration 3500, Loss: 0.6110\n",
      "Iteration 3600, Loss: 0.6098\n",
      "Iteration 3700, Loss: 0.6086\n",
      "Iteration 3800, Loss: 0.6075\n",
      "Iteration 3900, Loss: 0.6063\n",
      "Iteration 4000, Loss: 0.6053\n",
      "Iteration 4100, Loss: 0.6042\n",
      "Iteration 4200, Loss: 0.6032\n",
      "Iteration 4300, Loss: 0.6022\n",
      "Iteration 4400, Loss: 0.6012\n",
      "Iteration 4500, Loss: 0.6002\n",
      "Iteration 4600, Loss: 0.5993\n",
      "Iteration 4700, Loss: 0.5984\n",
      "Iteration 4800, Loss: 0.5975\n",
      "Iteration 4900, Loss: 0.5966\n",
      "Iteration 5000, Loss: 0.5957\n",
      "Iteration 5100, Loss: 0.5949\n",
      "Iteration 5200, Loss: 0.5941\n",
      "Iteration 5300, Loss: 0.5933\n",
      "Iteration 5400, Loss: 0.5925\n",
      "Iteration 5500, Loss: 0.5917\n",
      "Iteration 5600, Loss: 0.5909\n",
      "Iteration 5700, Loss: 0.5902\n",
      "Iteration 5800, Loss: 0.5894\n",
      "Iteration 5900, Loss: 0.5887\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8556\n",
      "Iteration 200, Loss: 0.7739\n",
      "Iteration 300, Loss: 0.7317\n",
      "Iteration 400, Loss: 0.7058\n",
      "Iteration 500, Loss: 0.6880\n",
      "Iteration 600, Loss: 0.6749\n",
      "Iteration 700, Loss: 0.6645\n",
      "Iteration 800, Loss: 0.6559\n",
      "Iteration 900, Loss: 0.6487\n",
      "Iteration 1000, Loss: 0.6425\n",
      "Iteration 1100, Loss: 0.6370\n",
      "Iteration 1200, Loss: 0.6321\n",
      "Iteration 1300, Loss: 0.6276\n",
      "Iteration 1400, Loss: 0.6236\n",
      "Iteration 1500, Loss: 0.6199\n",
      "Iteration 1600, Loss: 0.6166\n",
      "Iteration 1700, Loss: 0.6134\n",
      "Iteration 1800, Loss: 0.6105\n",
      "Iteration 1900, Loss: 0.6078\n",
      "Iteration 2000, Loss: 0.6052\n",
      "Iteration 2100, Loss: 0.6028\n",
      "Iteration 2200, Loss: 0.6005\n",
      "Iteration 2300, Loss: 0.5983\n",
      "Iteration 2400, Loss: 0.5963\n",
      "Iteration 2500, Loss: 0.5943\n",
      "Iteration 2600, Loss: 0.5924\n",
      "Iteration 2700, Loss: 0.5906\n",
      "Iteration 2800, Loss: 0.5889\n",
      "Iteration 2900, Loss: 0.5872\n",
      "Iteration 3000, Loss: 0.5857\n",
      "Iteration 3100, Loss: 0.5841\n",
      "Iteration 3200, Loss: 0.5826\n",
      "Iteration 3300, Loss: 0.5812\n",
      "Iteration 3400, Loss: 0.5798\n",
      "Iteration 3500, Loss: 0.5785\n",
      "Iteration 3600, Loss: 0.5771\n",
      "Iteration 3700, Loss: 0.5759\n",
      "Iteration 3800, Loss: 0.5747\n",
      "Iteration 3900, Loss: 0.5735\n",
      "Iteration 4000, Loss: 0.5723\n",
      "Iteration 4100, Loss: 0.5712\n",
      "Iteration 4200, Loss: 0.5701\n",
      "Iteration 4300, Loss: 0.5690\n",
      "Iteration 4400, Loss: 0.5680\n",
      "Iteration 4500, Loss: 0.5670\n",
      "Iteration 4600, Loss: 0.5660\n",
      "Iteration 4700, Loss: 0.5650\n",
      "Iteration 4800, Loss: 0.5641\n",
      "Iteration 4900, Loss: 0.5631\n",
      "Iteration 5000, Loss: 0.5622\n",
      "Iteration 5100, Loss: 0.5613\n",
      "Iteration 5200, Loss: 0.5605\n",
      "Iteration 5300, Loss: 0.5596\n",
      "Iteration 5400, Loss: 0.5588\n",
      "Iteration 5500, Loss: 0.5580\n",
      "Iteration 5600, Loss: 0.5572\n",
      "Iteration 5700, Loss: 0.5564\n",
      "Iteration 5800, Loss: 0.5556\n",
      "Iteration 5900, Loss: 0.5549\n",
      "236 270\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8543\n",
      "Iteration 200, Loss: 0.7760\n",
      "Iteration 300, Loss: 0.7363\n",
      "Iteration 400, Loss: 0.7118\n",
      "Iteration 500, Loss: 0.6949\n",
      "Iteration 600, Loss: 0.6823\n",
      "Iteration 700, Loss: 0.6723\n",
      "Iteration 800, Loss: 0.6638\n",
      "Iteration 900, Loss: 0.6567\n",
      "Iteration 1000, Loss: 0.6504\n",
      "Iteration 1100, Loss: 0.6449\n",
      "Iteration 1200, Loss: 0.6399\n",
      "Iteration 1300, Loss: 0.6354\n",
      "Iteration 1400, Loss: 0.6314\n",
      "Iteration 1500, Loss: 0.6276\n",
      "Iteration 1600, Loss: 0.6242\n",
      "Iteration 1700, Loss: 0.6210\n",
      "Iteration 1800, Loss: 0.6180\n",
      "Iteration 1900, Loss: 0.6153\n",
      "Iteration 2000, Loss: 0.6126\n",
      "Iteration 2100, Loss: 0.6101\n",
      "Iteration 2200, Loss: 0.6078\n",
      "Iteration 2300, Loss: 0.6056\n",
      "Iteration 2400, Loss: 0.6035\n",
      "Iteration 2500, Loss: 0.6014\n",
      "Iteration 2600, Loss: 0.5995\n",
      "Iteration 2700, Loss: 0.5977\n",
      "Iteration 2800, Loss: 0.5959\n",
      "Iteration 2900, Loss: 0.5942\n",
      "Iteration 3000, Loss: 0.5926\n",
      "Iteration 3100, Loss: 0.5910\n",
      "Iteration 3200, Loss: 0.5895\n",
      "Iteration 3300, Loss: 0.5880\n",
      "Iteration 3400, Loss: 0.5866\n",
      "Iteration 3500, Loss: 0.5852\n",
      "Iteration 3600, Loss: 0.5839\n",
      "Iteration 3700, Loss: 0.5826\n",
      "Iteration 3800, Loss: 0.5814\n",
      "Iteration 3900, Loss: 0.5801\n",
      "Iteration 4000, Loss: 0.5790\n",
      "Iteration 4100, Loss: 0.5778\n",
      "Iteration 4200, Loss: 0.5767\n",
      "Iteration 4300, Loss: 0.5756\n",
      "Iteration 4400, Loss: 0.5746\n",
      "Iteration 4500, Loss: 0.5735\n",
      "Iteration 4600, Loss: 0.5725\n",
      "Iteration 4700, Loss: 0.5716\n",
      "Iteration 4800, Loss: 0.5706\n",
      "Iteration 4900, Loss: 0.5697\n",
      "Iteration 5000, Loss: 0.5688\n",
      "Iteration 5100, Loss: 0.5679\n",
      "Iteration 5200, Loss: 0.5670\n",
      "Iteration 5300, Loss: 0.5661\n",
      "Iteration 5400, Loss: 0.5653\n",
      "Iteration 5500, Loss: 0.5645\n",
      "Iteration 5600, Loss: 0.5637\n",
      "Iteration 5700, Loss: 0.5629\n",
      "Iteration 5800, Loss: 0.5622\n",
      "Iteration 5900, Loss: 0.5614\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8666\n",
      "Iteration 200, Loss: 0.7866\n",
      "Iteration 300, Loss: 0.7460\n",
      "Iteration 400, Loss: 0.7211\n",
      "Iteration 500, Loss: 0.7040\n",
      "Iteration 600, Loss: 0.6912\n",
      "Iteration 700, Loss: 0.6810\n",
      "Iteration 800, Loss: 0.6727\n",
      "Iteration 900, Loss: 0.6656\n",
      "Iteration 1000, Loss: 0.6595\n",
      "Iteration 1100, Loss: 0.6542\n",
      "Iteration 1200, Loss: 0.6495\n",
      "Iteration 1300, Loss: 0.6453\n",
      "Iteration 1400, Loss: 0.6415\n",
      "Iteration 1500, Loss: 0.6380\n",
      "Iteration 1600, Loss: 0.6348\n",
      "Iteration 1700, Loss: 0.6318\n",
      "Iteration 1800, Loss: 0.6291\n",
      "Iteration 1900, Loss: 0.6265\n",
      "Iteration 2000, Loss: 0.6241\n",
      "Iteration 2100, Loss: 0.6219\n",
      "Iteration 2200, Loss: 0.6197\n",
      "Iteration 2300, Loss: 0.6177\n",
      "Iteration 2400, Loss: 0.6158\n",
      "Iteration 2500, Loss: 0.6140\n",
      "Iteration 2600, Loss: 0.6123\n",
      "Iteration 2700, Loss: 0.6107\n",
      "Iteration 2800, Loss: 0.6091\n",
      "Iteration 2900, Loss: 0.6076\n",
      "Iteration 3000, Loss: 0.6061\n",
      "Iteration 3100, Loss: 0.6047\n",
      "Iteration 3200, Loss: 0.6034\n",
      "Iteration 3300, Loss: 0.6021\n",
      "Iteration 3400, Loss: 0.6008\n",
      "Iteration 3500, Loss: 0.5996\n",
      "Iteration 3600, Loss: 0.5984\n",
      "Iteration 3700, Loss: 0.5973\n",
      "Iteration 3800, Loss: 0.5961\n",
      "Iteration 3900, Loss: 0.5950\n",
      "Iteration 4000, Loss: 0.5940\n",
      "Iteration 4100, Loss: 0.5929\n",
      "Iteration 4200, Loss: 0.5919\n",
      "Iteration 4300, Loss: 0.5910\n",
      "Iteration 4400, Loss: 0.5900\n",
      "Iteration 4500, Loss: 0.5891\n",
      "Iteration 4600, Loss: 0.5881\n",
      "Iteration 4700, Loss: 0.5873\n",
      "Iteration 4800, Loss: 0.5864\n",
      "Iteration 4900, Loss: 0.5855\n",
      "Iteration 5000, Loss: 0.5847\n",
      "Iteration 5100, Loss: 0.5839\n",
      "Iteration 5200, Loss: 0.5831\n",
      "Iteration 5300, Loss: 0.5823\n",
      "Iteration 5400, Loss: 0.5815\n",
      "Iteration 5500, Loss: 0.5808\n",
      "Iteration 5600, Loss: 0.5800\n",
      "Iteration 5700, Loss: 0.5793\n",
      "Iteration 5800, Loss: 0.5786\n",
      "Iteration 5900, Loss: 0.5779\n",
      "237 270\n",
      "Iteration 0, Loss: 1.0933\n",
      "Iteration 100, Loss: 0.8523\n",
      "Iteration 200, Loss: 0.7730\n",
      "Iteration 300, Loss: 0.7328\n",
      "Iteration 400, Loss: 0.7083\n",
      "Iteration 500, Loss: 0.6912\n",
      "Iteration 600, Loss: 0.6784\n",
      "Iteration 700, Loss: 0.6682\n",
      "Iteration 800, Loss: 0.6598\n",
      "Iteration 900, Loss: 0.6525\n",
      "Iteration 1000, Loss: 0.6462\n",
      "Iteration 1100, Loss: 0.6406\n",
      "Iteration 1200, Loss: 0.6357\n",
      "Iteration 1300, Loss: 0.6312\n",
      "Iteration 1400, Loss: 0.6272\n",
      "Iteration 1500, Loss: 0.6235\n",
      "Iteration 1600, Loss: 0.6200\n",
      "Iteration 1700, Loss: 0.6168\n",
      "Iteration 1800, Loss: 0.6139\n",
      "Iteration 1900, Loss: 0.6111\n",
      "Iteration 2000, Loss: 0.6085\n",
      "Iteration 2100, Loss: 0.6061\n",
      "Iteration 2200, Loss: 0.6038\n",
      "Iteration 2300, Loss: 0.6015\n",
      "Iteration 2400, Loss: 0.5994\n",
      "Iteration 2500, Loss: 0.5974\n",
      "Iteration 2600, Loss: 0.5955\n",
      "Iteration 2700, Loss: 0.5937\n",
      "Iteration 2800, Loss: 0.5919\n",
      "Iteration 2900, Loss: 0.5902\n",
      "Iteration 3000, Loss: 0.5885\n",
      "Iteration 3100, Loss: 0.5869\n",
      "Iteration 3200, Loss: 0.5854\n",
      "Iteration 3300, Loss: 0.5839\n",
      "Iteration 3400, Loss: 0.5825\n",
      "Iteration 3500, Loss: 0.5811\n",
      "Iteration 3600, Loss: 0.5797\n",
      "Iteration 3700, Loss: 0.5784\n",
      "Iteration 3800, Loss: 0.5771\n",
      "Iteration 3900, Loss: 0.5759\n",
      "Iteration 4000, Loss: 0.5747\n",
      "Iteration 4100, Loss: 0.5735\n",
      "Iteration 4200, Loss: 0.5724\n",
      "Iteration 4300, Loss: 0.5712\n",
      "Iteration 4400, Loss: 0.5701\n",
      "Iteration 4500, Loss: 0.5691\n",
      "Iteration 4600, Loss: 0.5680\n",
      "Iteration 4700, Loss: 0.5670\n",
      "Iteration 4800, Loss: 0.5660\n",
      "Iteration 4900, Loss: 0.5651\n",
      "Iteration 5000, Loss: 0.5641\n",
      "Iteration 5100, Loss: 0.5632\n",
      "Iteration 5200, Loss: 0.5623\n",
      "Iteration 5300, Loss: 0.5614\n",
      "Iteration 5400, Loss: 0.5605\n",
      "Iteration 5500, Loss: 0.5596\n",
      "Iteration 5600, Loss: 0.5588\n",
      "Iteration 5700, Loss: 0.5580\n",
      "Iteration 5800, Loss: 0.5572\n",
      "Iteration 5900, Loss: 0.5564\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8696\n",
      "Iteration 200, Loss: 0.7918\n",
      "Iteration 300, Loss: 0.7515\n",
      "Iteration 400, Loss: 0.7269\n",
      "Iteration 500, Loss: 0.7101\n",
      "Iteration 600, Loss: 0.6976\n",
      "Iteration 700, Loss: 0.6877\n",
      "Iteration 800, Loss: 0.6796\n",
      "Iteration 900, Loss: 0.6729\n",
      "Iteration 1000, Loss: 0.6671\n",
      "Iteration 1100, Loss: 0.6620\n",
      "Iteration 1200, Loss: 0.6575\n",
      "Iteration 1300, Loss: 0.6534\n",
      "Iteration 1400, Loss: 0.6497\n",
      "Iteration 1500, Loss: 0.6464\n",
      "Iteration 1600, Loss: 0.6433\n",
      "Iteration 1700, Loss: 0.6404\n",
      "Iteration 1800, Loss: 0.6378\n",
      "Iteration 1900, Loss: 0.6353\n",
      "Iteration 2000, Loss: 0.6329\n",
      "Iteration 2100, Loss: 0.6307\n",
      "Iteration 2200, Loss: 0.6286\n",
      "Iteration 2300, Loss: 0.6267\n",
      "Iteration 2400, Loss: 0.6248\n",
      "Iteration 2500, Loss: 0.6230\n",
      "Iteration 2600, Loss: 0.6213\n",
      "Iteration 2700, Loss: 0.6196\n",
      "Iteration 2800, Loss: 0.6181\n",
      "Iteration 2900, Loss: 0.6165\n",
      "Iteration 3000, Loss: 0.6151\n",
      "Iteration 3100, Loss: 0.6137\n",
      "Iteration 3200, Loss: 0.6123\n",
      "Iteration 3300, Loss: 0.6110\n",
      "Iteration 3400, Loss: 0.6097\n",
      "Iteration 3500, Loss: 0.6085\n",
      "Iteration 3600, Loss: 0.6073\n",
      "Iteration 3700, Loss: 0.6062\n",
      "Iteration 3800, Loss: 0.6050\n",
      "Iteration 3900, Loss: 0.6039\n",
      "Iteration 4000, Loss: 0.6029\n",
      "Iteration 4100, Loss: 0.6018\n",
      "Iteration 4200, Loss: 0.6008\n",
      "Iteration 4300, Loss: 0.5999\n",
      "Iteration 4400, Loss: 0.5989\n",
      "Iteration 4500, Loss: 0.5980\n",
      "Iteration 4600, Loss: 0.5971\n",
      "Iteration 4700, Loss: 0.5962\n",
      "Iteration 4800, Loss: 0.5953\n",
      "Iteration 4900, Loss: 0.5945\n",
      "Iteration 5000, Loss: 0.5936\n",
      "Iteration 5100, Loss: 0.5928\n",
      "Iteration 5200, Loss: 0.5920\n",
      "Iteration 5300, Loss: 0.5912\n",
      "Iteration 5400, Loss: 0.5904\n",
      "Iteration 5500, Loss: 0.5897\n",
      "Iteration 5600, Loss: 0.5889\n",
      "Iteration 5700, Loss: 0.5882\n",
      "Iteration 5800, Loss: 0.5875\n",
      "Iteration 5900, Loss: 0.5868\n",
      "238 270\n",
      "Iteration 0, Loss: 1.0935\n",
      "Iteration 100, Loss: 0.8540\n",
      "Iteration 200, Loss: 0.7752\n",
      "Iteration 300, Loss: 0.7354\n",
      "Iteration 400, Loss: 0.7112\n",
      "Iteration 500, Loss: 0.6948\n",
      "Iteration 600, Loss: 0.6826\n",
      "Iteration 700, Loss: 0.6727\n",
      "Iteration 800, Loss: 0.6646\n",
      "Iteration 900, Loss: 0.6577\n",
      "Iteration 1000, Loss: 0.6518\n",
      "Iteration 1100, Loss: 0.6466\n",
      "Iteration 1200, Loss: 0.6420\n",
      "Iteration 1300, Loss: 0.6379\n",
      "Iteration 1400, Loss: 0.6342\n",
      "Iteration 1500, Loss: 0.6308\n",
      "Iteration 1600, Loss: 0.6276\n",
      "Iteration 1700, Loss: 0.6247\n",
      "Iteration 1800, Loss: 0.6220\n",
      "Iteration 1900, Loss: 0.6195\n",
      "Iteration 2000, Loss: 0.6172\n",
      "Iteration 2100, Loss: 0.6149\n",
      "Iteration 2200, Loss: 0.6128\n",
      "Iteration 2300, Loss: 0.6109\n",
      "Iteration 2400, Loss: 0.6090\n",
      "Iteration 2500, Loss: 0.6072\n",
      "Iteration 2600, Loss: 0.6055\n",
      "Iteration 2700, Loss: 0.6038\n",
      "Iteration 2800, Loss: 0.6023\n",
      "Iteration 2900, Loss: 0.6008\n",
      "Iteration 3000, Loss: 0.5993\n",
      "Iteration 3100, Loss: 0.5979\n",
      "Iteration 3200, Loss: 0.5966\n",
      "Iteration 3300, Loss: 0.5952\n",
      "Iteration 3400, Loss: 0.5940\n",
      "Iteration 3500, Loss: 0.5928\n",
      "Iteration 3600, Loss: 0.5916\n",
      "Iteration 3700, Loss: 0.5904\n",
      "Iteration 3800, Loss: 0.5893\n",
      "Iteration 3900, Loss: 0.5883\n",
      "Iteration 4000, Loss: 0.5872\n",
      "Iteration 4100, Loss: 0.5862\n",
      "Iteration 4200, Loss: 0.5852\n",
      "Iteration 4300, Loss: 0.5842\n",
      "Iteration 4400, Loss: 0.5833\n",
      "Iteration 4500, Loss: 0.5823\n",
      "Iteration 4600, Loss: 0.5814\n",
      "Iteration 4700, Loss: 0.5805\n",
      "Iteration 4800, Loss: 0.5797\n",
      "Iteration 4900, Loss: 0.5788\n",
      "Iteration 5000, Loss: 0.5780\n",
      "Iteration 5100, Loss: 0.5772\n",
      "Iteration 5200, Loss: 0.5764\n",
      "Iteration 5300, Loss: 0.5756\n",
      "Iteration 5400, Loss: 0.5748\n",
      "Iteration 5500, Loss: 0.5741\n",
      "Iteration 5600, Loss: 0.5734\n",
      "Iteration 5700, Loss: 0.5726\n",
      "Iteration 5800, Loss: 0.5719\n",
      "Iteration 5900, Loss: 0.5712\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8653\n",
      "Iteration 200, Loss: 0.7868\n",
      "Iteration 300, Loss: 0.7466\n",
      "Iteration 400, Loss: 0.7220\n",
      "Iteration 500, Loss: 0.7048\n",
      "Iteration 600, Loss: 0.6918\n",
      "Iteration 700, Loss: 0.6815\n",
      "Iteration 800, Loss: 0.6730\n",
      "Iteration 900, Loss: 0.6659\n",
      "Iteration 1000, Loss: 0.6596\n",
      "Iteration 1100, Loss: 0.6541\n",
      "Iteration 1200, Loss: 0.6492\n",
      "Iteration 1300, Loss: 0.6449\n",
      "Iteration 1400, Loss: 0.6409\n",
      "Iteration 1500, Loss: 0.6372\n",
      "Iteration 1600, Loss: 0.6339\n",
      "Iteration 1700, Loss: 0.6307\n",
      "Iteration 1800, Loss: 0.6278\n",
      "Iteration 1900, Loss: 0.6251\n",
      "Iteration 2000, Loss: 0.6226\n",
      "Iteration 2100, Loss: 0.6202\n",
      "Iteration 2200, Loss: 0.6179\n",
      "Iteration 2300, Loss: 0.6158\n",
      "Iteration 2400, Loss: 0.6138\n",
      "Iteration 2500, Loss: 0.6118\n",
      "Iteration 2600, Loss: 0.6100\n",
      "Iteration 2700, Loss: 0.6082\n",
      "Iteration 2800, Loss: 0.6065\n",
      "Iteration 2900, Loss: 0.6048\n",
      "Iteration 3000, Loss: 0.6033\n",
      "Iteration 3100, Loss: 0.6017\n",
      "Iteration 3200, Loss: 0.6003\n",
      "Iteration 3300, Loss: 0.5989\n",
      "Iteration 3400, Loss: 0.5975\n",
      "Iteration 3500, Loss: 0.5962\n",
      "Iteration 3600, Loss: 0.5949\n",
      "Iteration 3700, Loss: 0.5936\n",
      "Iteration 3800, Loss: 0.5924\n",
      "Iteration 3900, Loss: 0.5912\n",
      "Iteration 4000, Loss: 0.5901\n",
      "Iteration 4100, Loss: 0.5889\n",
      "Iteration 4200, Loss: 0.5879\n",
      "Iteration 4300, Loss: 0.5868\n",
      "Iteration 4400, Loss: 0.5858\n",
      "Iteration 4500, Loss: 0.5848\n",
      "Iteration 4600, Loss: 0.5838\n",
      "Iteration 4700, Loss: 0.5828\n",
      "Iteration 4800, Loss: 0.5818\n",
      "Iteration 4900, Loss: 0.5809\n",
      "Iteration 5000, Loss: 0.5800\n",
      "Iteration 5100, Loss: 0.5792\n",
      "Iteration 5200, Loss: 0.5783\n",
      "Iteration 5300, Loss: 0.5775\n",
      "Iteration 5400, Loss: 0.5766\n",
      "Iteration 5500, Loss: 0.5758\n",
      "Iteration 5600, Loss: 0.5750\n",
      "Iteration 5700, Loss: 0.5743\n",
      "Iteration 5800, Loss: 0.5735\n",
      "Iteration 5900, Loss: 0.5728\n",
      "239 270\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8554\n",
      "Iteration 200, Loss: 0.7739\n",
      "Iteration 300, Loss: 0.7325\n",
      "Iteration 400, Loss: 0.7077\n",
      "Iteration 500, Loss: 0.6908\n",
      "Iteration 600, Loss: 0.6784\n",
      "Iteration 700, Loss: 0.6687\n",
      "Iteration 800, Loss: 0.6608\n",
      "Iteration 900, Loss: 0.6543\n",
      "Iteration 1000, Loss: 0.6486\n",
      "Iteration 1100, Loss: 0.6437\n",
      "Iteration 1200, Loss: 0.6393\n",
      "Iteration 1300, Loss: 0.6354\n",
      "Iteration 1400, Loss: 0.6318\n",
      "Iteration 1500, Loss: 0.6286\n",
      "Iteration 1600, Loss: 0.6256\n",
      "Iteration 1700, Loss: 0.6228\n",
      "Iteration 1800, Loss: 0.6202\n",
      "Iteration 1900, Loss: 0.6178\n",
      "Iteration 2000, Loss: 0.6156\n",
      "Iteration 2100, Loss: 0.6134\n",
      "Iteration 2200, Loss: 0.6114\n",
      "Iteration 2300, Loss: 0.6095\n",
      "Iteration 2400, Loss: 0.6077\n",
      "Iteration 2500, Loss: 0.6060\n",
      "Iteration 2600, Loss: 0.6043\n",
      "Iteration 2700, Loss: 0.6027\n",
      "Iteration 2800, Loss: 0.6012\n",
      "Iteration 2900, Loss: 0.5997\n",
      "Iteration 3000, Loss: 0.5982\n",
      "Iteration 3100, Loss: 0.5969\n",
      "Iteration 3200, Loss: 0.5955\n",
      "Iteration 3300, Loss: 0.5942\n",
      "Iteration 3400, Loss: 0.5930\n",
      "Iteration 3500, Loss: 0.5918\n",
      "Iteration 3600, Loss: 0.5906\n",
      "Iteration 3700, Loss: 0.5895\n",
      "Iteration 3800, Loss: 0.5883\n",
      "Iteration 3900, Loss: 0.5873\n",
      "Iteration 4000, Loss: 0.5862\n",
      "Iteration 4100, Loss: 0.5852\n",
      "Iteration 4200, Loss: 0.5842\n",
      "Iteration 4300, Loss: 0.5832\n",
      "Iteration 4400, Loss: 0.5823\n",
      "Iteration 4500, Loss: 0.5813\n",
      "Iteration 4600, Loss: 0.5804\n",
      "Iteration 4700, Loss: 0.5795\n",
      "Iteration 4800, Loss: 0.5786\n",
      "Iteration 4900, Loss: 0.5777\n",
      "Iteration 5000, Loss: 0.5769\n",
      "Iteration 5100, Loss: 0.5761\n",
      "Iteration 5200, Loss: 0.5753\n",
      "Iteration 5300, Loss: 0.5745\n",
      "Iteration 5400, Loss: 0.5737\n",
      "Iteration 5500, Loss: 0.5729\n",
      "Iteration 5600, Loss: 0.5722\n",
      "Iteration 5700, Loss: 0.5714\n",
      "Iteration 5800, Loss: 0.5707\n",
      "Iteration 5900, Loss: 0.5700\n",
      "Iteration 0, Loss: 1.0937\n",
      "Iteration 100, Loss: 0.8665\n",
      "Iteration 200, Loss: 0.7908\n",
      "Iteration 300, Loss: 0.7518\n",
      "Iteration 400, Loss: 0.7278\n",
      "Iteration 500, Loss: 0.7110\n",
      "Iteration 600, Loss: 0.6981\n",
      "Iteration 700, Loss: 0.6879\n",
      "Iteration 800, Loss: 0.6793\n",
      "Iteration 900, Loss: 0.6720\n",
      "Iteration 1000, Loss: 0.6657\n",
      "Iteration 1100, Loss: 0.6601\n",
      "Iteration 1200, Loss: 0.6551\n",
      "Iteration 1300, Loss: 0.6506\n",
      "Iteration 1400, Loss: 0.6465\n",
      "Iteration 1500, Loss: 0.6428\n",
      "Iteration 1600, Loss: 0.6394\n",
      "Iteration 1700, Loss: 0.6362\n",
      "Iteration 1800, Loss: 0.6333\n",
      "Iteration 1900, Loss: 0.6305\n",
      "Iteration 2000, Loss: 0.6279\n",
      "Iteration 2100, Loss: 0.6255\n",
      "Iteration 2200, Loss: 0.6232\n",
      "Iteration 2300, Loss: 0.6210\n",
      "Iteration 2400, Loss: 0.6189\n",
      "Iteration 2500, Loss: 0.6169\n",
      "Iteration 2600, Loss: 0.6151\n",
      "Iteration 2700, Loss: 0.6132\n",
      "Iteration 2800, Loss: 0.6115\n",
      "Iteration 2900, Loss: 0.6098\n",
      "Iteration 3000, Loss: 0.6082\n",
      "Iteration 3100, Loss: 0.6067\n",
      "Iteration 3200, Loss: 0.6052\n",
      "Iteration 3300, Loss: 0.6037\n",
      "Iteration 3400, Loss: 0.6023\n",
      "Iteration 3500, Loss: 0.6010\n",
      "Iteration 3600, Loss: 0.5997\n",
      "Iteration 3700, Loss: 0.5984\n",
      "Iteration 3800, Loss: 0.5972\n",
      "Iteration 3900, Loss: 0.5960\n",
      "Iteration 4000, Loss: 0.5948\n",
      "Iteration 4100, Loss: 0.5937\n",
      "Iteration 4200, Loss: 0.5926\n",
      "Iteration 4300, Loss: 0.5915\n",
      "Iteration 4400, Loss: 0.5904\n",
      "Iteration 4500, Loss: 0.5894\n",
      "Iteration 4600, Loss: 0.5884\n",
      "Iteration 4700, Loss: 0.5875\n",
      "Iteration 4800, Loss: 0.5865\n",
      "Iteration 4900, Loss: 0.5856\n",
      "Iteration 5000, Loss: 0.5847\n",
      "Iteration 5100, Loss: 0.5838\n",
      "Iteration 5200, Loss: 0.5829\n",
      "Iteration 5300, Loss: 0.5821\n",
      "Iteration 5400, Loss: 0.5813\n",
      "Iteration 5500, Loss: 0.5805\n",
      "Iteration 5600, Loss: 0.5797\n",
      "Iteration 5700, Loss: 0.5789\n",
      "Iteration 5800, Loss: 0.5781\n",
      "Iteration 5900, Loss: 0.5774\n",
      "240 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0183\n",
      "Iteration 200, Loss: 0.9629\n",
      "Iteration 300, Loss: 0.9214\n",
      "Iteration 400, Loss: 0.8889\n",
      "Iteration 500, Loss: 0.8625\n",
      "Iteration 600, Loss: 0.8407\n",
      "Iteration 700, Loss: 0.8223\n",
      "Iteration 800, Loss: 0.8066\n",
      "Iteration 900, Loss: 0.7931\n",
      "Iteration 1000, Loss: 0.7813\n",
      "Iteration 1100, Loss: 0.7709\n",
      "Iteration 1200, Loss: 0.7618\n",
      "Iteration 1300, Loss: 0.7536\n",
      "Iteration 1400, Loss: 0.7462\n",
      "Iteration 1500, Loss: 0.7396\n",
      "Iteration 1600, Loss: 0.7336\n",
      "Iteration 1700, Loss: 0.7281\n",
      "Iteration 1800, Loss: 0.7231\n",
      "Iteration 1900, Loss: 0.7185\n",
      "Iteration 2000, Loss: 0.7143\n",
      "Iteration 2100, Loss: 0.7103\n",
      "Iteration 2200, Loss: 0.7066\n",
      "Iteration 2300, Loss: 0.7032\n",
      "Iteration 2400, Loss: 0.7000\n",
      "Iteration 2500, Loss: 0.6969\n",
      "Iteration 2600, Loss: 0.6940\n",
      "Iteration 2700, Loss: 0.6913\n",
      "Iteration 2800, Loss: 0.6888\n",
      "Iteration 2900, Loss: 0.6863\n",
      "Iteration 3000, Loss: 0.6840\n",
      "Iteration 3100, Loss: 0.6818\n",
      "Iteration 3200, Loss: 0.6796\n",
      "Iteration 3300, Loss: 0.6776\n",
      "Iteration 3400, Loss: 0.6757\n",
      "Iteration 3500, Loss: 0.6738\n",
      "Iteration 3600, Loss: 0.6720\n",
      "Iteration 3700, Loss: 0.6703\n",
      "Iteration 3800, Loss: 0.6686\n",
      "Iteration 3900, Loss: 0.6670\n",
      "Iteration 4000, Loss: 0.6654\n",
      "Iteration 4100, Loss: 0.6639\n",
      "Iteration 4200, Loss: 0.6625\n",
      "Iteration 4300, Loss: 0.6611\n",
      "Iteration 4400, Loss: 0.6597\n",
      "Iteration 4500, Loss: 0.6584\n",
      "Iteration 4600, Loss: 0.6571\n",
      "Iteration 4700, Loss: 0.6558\n",
      "Iteration 4800, Loss: 0.6546\n",
      "Iteration 4900, Loss: 0.6534\n",
      "Iteration 5000, Loss: 0.6523\n",
      "Iteration 5100, Loss: 0.6512\n",
      "Iteration 5200, Loss: 0.6501\n",
      "Iteration 5300, Loss: 0.6490\n",
      "Iteration 5400, Loss: 0.6480\n",
      "Iteration 5500, Loss: 0.6469\n",
      "Iteration 5600, Loss: 0.6460\n",
      "Iteration 5700, Loss: 0.6450\n",
      "Iteration 5800, Loss: 0.6440\n",
      "Iteration 5900, Loss: 0.6431\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0217\n",
      "Iteration 200, Loss: 0.9686\n",
      "Iteration 300, Loss: 0.9288\n",
      "Iteration 400, Loss: 0.8975\n",
      "Iteration 500, Loss: 0.8723\n",
      "Iteration 600, Loss: 0.8515\n",
      "Iteration 700, Loss: 0.8340\n",
      "Iteration 800, Loss: 0.8191\n",
      "Iteration 900, Loss: 0.8062\n",
      "Iteration 1000, Loss: 0.7950\n",
      "Iteration 1100, Loss: 0.7852\n",
      "Iteration 1200, Loss: 0.7764\n",
      "Iteration 1300, Loss: 0.7687\n",
      "Iteration 1400, Loss: 0.7617\n",
      "Iteration 1500, Loss: 0.7553\n",
      "Iteration 1600, Loss: 0.7496\n",
      "Iteration 1700, Loss: 0.7444\n",
      "Iteration 1800, Loss: 0.7396\n",
      "Iteration 1900, Loss: 0.7351\n",
      "Iteration 2000, Loss: 0.7310\n",
      "Iteration 2100, Loss: 0.7272\n",
      "Iteration 2200, Loss: 0.7236\n",
      "Iteration 2300, Loss: 0.7203\n",
      "Iteration 2400, Loss: 0.7172\n",
      "Iteration 2500, Loss: 0.7142\n",
      "Iteration 2600, Loss: 0.7114\n",
      "Iteration 2700, Loss: 0.7088\n",
      "Iteration 2800, Loss: 0.7063\n",
      "Iteration 2900, Loss: 0.7039\n",
      "Iteration 3000, Loss: 0.7016\n",
      "Iteration 3100, Loss: 0.6995\n",
      "Iteration 3200, Loss: 0.6974\n",
      "Iteration 3300, Loss: 0.6954\n",
      "Iteration 3400, Loss: 0.6935\n",
      "Iteration 3500, Loss: 0.6917\n",
      "Iteration 3600, Loss: 0.6899\n",
      "Iteration 3700, Loss: 0.6882\n",
      "Iteration 3800, Loss: 0.6866\n",
      "Iteration 3900, Loss: 0.6850\n",
      "Iteration 4000, Loss: 0.6835\n",
      "Iteration 4100, Loss: 0.6820\n",
      "Iteration 4200, Loss: 0.6806\n",
      "Iteration 4300, Loss: 0.6792\n",
      "Iteration 4400, Loss: 0.6778\n",
      "Iteration 4500, Loss: 0.6765\n",
      "Iteration 4600, Loss: 0.6753\n",
      "Iteration 4700, Loss: 0.6740\n",
      "Iteration 4800, Loss: 0.6729\n",
      "Iteration 4900, Loss: 0.6717\n",
      "Iteration 5000, Loss: 0.6706\n",
      "Iteration 5100, Loss: 0.6695\n",
      "Iteration 5200, Loss: 0.6684\n",
      "Iteration 5300, Loss: 0.6673\n",
      "Iteration 5400, Loss: 0.6663\n",
      "Iteration 5500, Loss: 0.6653\n",
      "Iteration 5600, Loss: 0.6644\n",
      "Iteration 5700, Loss: 0.6634\n",
      "Iteration 5800, Loss: 0.6625\n",
      "Iteration 5900, Loss: 0.6616\n",
      "241 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0176\n",
      "Iteration 200, Loss: 0.9614\n",
      "Iteration 300, Loss: 0.9196\n",
      "Iteration 400, Loss: 0.8869\n",
      "Iteration 500, Loss: 0.8607\n",
      "Iteration 600, Loss: 0.8390\n",
      "Iteration 700, Loss: 0.8208\n",
      "Iteration 800, Loss: 0.8053\n",
      "Iteration 900, Loss: 0.7919\n",
      "Iteration 1000, Loss: 0.7803\n",
      "Iteration 1100, Loss: 0.7700\n",
      "Iteration 1200, Loss: 0.7608\n",
      "Iteration 1300, Loss: 0.7526\n",
      "Iteration 1400, Loss: 0.7452\n",
      "Iteration 1500, Loss: 0.7385\n",
      "Iteration 1600, Loss: 0.7324\n",
      "Iteration 1700, Loss: 0.7268\n",
      "Iteration 1800, Loss: 0.7216\n",
      "Iteration 1900, Loss: 0.7168\n",
      "Iteration 2000, Loss: 0.7124\n",
      "Iteration 2100, Loss: 0.7082\n",
      "Iteration 2200, Loss: 0.7043\n",
      "Iteration 2300, Loss: 0.7007\n",
      "Iteration 2400, Loss: 0.6973\n",
      "Iteration 2500, Loss: 0.6941\n",
      "Iteration 2600, Loss: 0.6910\n",
      "Iteration 2700, Loss: 0.6881\n",
      "Iteration 2800, Loss: 0.6854\n",
      "Iteration 2900, Loss: 0.6828\n",
      "Iteration 3000, Loss: 0.6803\n",
      "Iteration 3100, Loss: 0.6779\n",
      "Iteration 3200, Loss: 0.6757\n",
      "Iteration 3300, Loss: 0.6735\n",
      "Iteration 3400, Loss: 0.6714\n",
      "Iteration 3500, Loss: 0.6694\n",
      "Iteration 3600, Loss: 0.6675\n",
      "Iteration 3700, Loss: 0.6657\n",
      "Iteration 3800, Loss: 0.6639\n",
      "Iteration 3900, Loss: 0.6622\n",
      "Iteration 4000, Loss: 0.6605\n",
      "Iteration 4100, Loss: 0.6589\n",
      "Iteration 4200, Loss: 0.6574\n",
      "Iteration 4300, Loss: 0.6559\n",
      "Iteration 4400, Loss: 0.6544\n",
      "Iteration 4500, Loss: 0.6530\n",
      "Iteration 4600, Loss: 0.6516\n",
      "Iteration 4700, Loss: 0.6503\n",
      "Iteration 4800, Loss: 0.6490\n",
      "Iteration 4900, Loss: 0.6477\n",
      "Iteration 5000, Loss: 0.6465\n",
      "Iteration 5100, Loss: 0.6453\n",
      "Iteration 5200, Loss: 0.6442\n",
      "Iteration 5300, Loss: 0.6430\n",
      "Iteration 5400, Loss: 0.6419\n",
      "Iteration 5500, Loss: 0.6408\n",
      "Iteration 5600, Loss: 0.6398\n",
      "Iteration 5700, Loss: 0.6388\n",
      "Iteration 5800, Loss: 0.6378\n",
      "Iteration 5900, Loss: 0.6368\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0223\n",
      "Iteration 200, Loss: 0.9697\n",
      "Iteration 300, Loss: 0.9300\n",
      "Iteration 400, Loss: 0.8987\n",
      "Iteration 500, Loss: 0.8733\n",
      "Iteration 600, Loss: 0.8522\n",
      "Iteration 700, Loss: 0.8344\n",
      "Iteration 800, Loss: 0.8193\n",
      "Iteration 900, Loss: 0.8063\n",
      "Iteration 1000, Loss: 0.7950\n",
      "Iteration 1100, Loss: 0.7851\n",
      "Iteration 1200, Loss: 0.7763\n",
      "Iteration 1300, Loss: 0.7685\n",
      "Iteration 1400, Loss: 0.7615\n",
      "Iteration 1500, Loss: 0.7552\n",
      "Iteration 1600, Loss: 0.7495\n",
      "Iteration 1700, Loss: 0.7443\n",
      "Iteration 1800, Loss: 0.7396\n",
      "Iteration 1900, Loss: 0.7352\n",
      "Iteration 2000, Loss: 0.7313\n",
      "Iteration 2100, Loss: 0.7275\n",
      "Iteration 2200, Loss: 0.7241\n",
      "Iteration 2300, Loss: 0.7208\n",
      "Iteration 2400, Loss: 0.7178\n",
      "Iteration 2500, Loss: 0.7150\n",
      "Iteration 2600, Loss: 0.7123\n",
      "Iteration 2700, Loss: 0.7098\n",
      "Iteration 2800, Loss: 0.7074\n",
      "Iteration 2900, Loss: 0.7052\n",
      "Iteration 3000, Loss: 0.7030\n",
      "Iteration 3100, Loss: 0.7009\n",
      "Iteration 3200, Loss: 0.6990\n",
      "Iteration 3300, Loss: 0.6971\n",
      "Iteration 3400, Loss: 0.6953\n",
      "Iteration 3500, Loss: 0.6936\n",
      "Iteration 3600, Loss: 0.6919\n",
      "Iteration 3700, Loss: 0.6903\n",
      "Iteration 3800, Loss: 0.6888\n",
      "Iteration 3900, Loss: 0.6873\n",
      "Iteration 4000, Loss: 0.6858\n",
      "Iteration 4100, Loss: 0.6844\n",
      "Iteration 4200, Loss: 0.6831\n",
      "Iteration 4300, Loss: 0.6818\n",
      "Iteration 4400, Loss: 0.6805\n",
      "Iteration 4500, Loss: 0.6793\n",
      "Iteration 4600, Loss: 0.6781\n",
      "Iteration 4700, Loss: 0.6770\n",
      "Iteration 4800, Loss: 0.6758\n",
      "Iteration 4900, Loss: 0.6747\n",
      "Iteration 5000, Loss: 0.6737\n",
      "Iteration 5100, Loss: 0.6726\n",
      "Iteration 5200, Loss: 0.6716\n",
      "Iteration 5300, Loss: 0.6706\n",
      "Iteration 5400, Loss: 0.6697\n",
      "Iteration 5500, Loss: 0.6687\n",
      "Iteration 5600, Loss: 0.6678\n",
      "Iteration 5700, Loss: 0.6669\n",
      "Iteration 5800, Loss: 0.6660\n",
      "Iteration 5900, Loss: 0.6652\n",
      "242 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0207\n",
      "Iteration 200, Loss: 0.9655\n",
      "Iteration 300, Loss: 0.9236\n",
      "Iteration 400, Loss: 0.8904\n",
      "Iteration 500, Loss: 0.8634\n",
      "Iteration 600, Loss: 0.8410\n",
      "Iteration 700, Loss: 0.8220\n",
      "Iteration 800, Loss: 0.8058\n",
      "Iteration 900, Loss: 0.7919\n",
      "Iteration 1000, Loss: 0.7796\n",
      "Iteration 1100, Loss: 0.7689\n",
      "Iteration 1200, Loss: 0.7594\n",
      "Iteration 1300, Loss: 0.7509\n",
      "Iteration 1400, Loss: 0.7433\n",
      "Iteration 1500, Loss: 0.7365\n",
      "Iteration 1600, Loss: 0.7302\n",
      "Iteration 1700, Loss: 0.7246\n",
      "Iteration 1800, Loss: 0.7194\n",
      "Iteration 1900, Loss: 0.7146\n",
      "Iteration 2000, Loss: 0.7101\n",
      "Iteration 2100, Loss: 0.7060\n",
      "Iteration 2200, Loss: 0.7022\n",
      "Iteration 2300, Loss: 0.6986\n",
      "Iteration 2400, Loss: 0.6952\n",
      "Iteration 2500, Loss: 0.6921\n",
      "Iteration 2600, Loss: 0.6891\n",
      "Iteration 2700, Loss: 0.6863\n",
      "Iteration 2800, Loss: 0.6836\n",
      "Iteration 2900, Loss: 0.6810\n",
      "Iteration 3000, Loss: 0.6786\n",
      "Iteration 3100, Loss: 0.6763\n",
      "Iteration 3200, Loss: 0.6741\n",
      "Iteration 3300, Loss: 0.6720\n",
      "Iteration 3400, Loss: 0.6700\n",
      "Iteration 3500, Loss: 0.6680\n",
      "Iteration 3600, Loss: 0.6662\n",
      "Iteration 3700, Loss: 0.6643\n",
      "Iteration 3800, Loss: 0.6626\n",
      "Iteration 3900, Loss: 0.6609\n",
      "Iteration 4000, Loss: 0.6593\n",
      "Iteration 4100, Loss: 0.6578\n",
      "Iteration 4200, Loss: 0.6562\n",
      "Iteration 4300, Loss: 0.6548\n",
      "Iteration 4400, Loss: 0.6534\n",
      "Iteration 4500, Loss: 0.6520\n",
      "Iteration 4600, Loss: 0.6507\n",
      "Iteration 4700, Loss: 0.6493\n",
      "Iteration 4800, Loss: 0.6481\n",
      "Iteration 4900, Loss: 0.6469\n",
      "Iteration 5000, Loss: 0.6457\n",
      "Iteration 5100, Loss: 0.6445\n",
      "Iteration 5200, Loss: 0.6434\n",
      "Iteration 5300, Loss: 0.6422\n",
      "Iteration 5400, Loss: 0.6412\n",
      "Iteration 5500, Loss: 0.6401\n",
      "Iteration 5600, Loss: 0.6391\n",
      "Iteration 5700, Loss: 0.6381\n",
      "Iteration 5800, Loss: 0.6371\n",
      "Iteration 5900, Loss: 0.6361\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0196\n",
      "Iteration 200, Loss: 0.9663\n",
      "Iteration 300, Loss: 0.9268\n",
      "Iteration 400, Loss: 0.8961\n",
      "Iteration 500, Loss: 0.8714\n",
      "Iteration 600, Loss: 0.8511\n",
      "Iteration 700, Loss: 0.8340\n",
      "Iteration 800, Loss: 0.8196\n",
      "Iteration 900, Loss: 0.8071\n",
      "Iteration 1000, Loss: 0.7963\n",
      "Iteration 1100, Loss: 0.7867\n",
      "Iteration 1200, Loss: 0.7782\n",
      "Iteration 1300, Loss: 0.7706\n",
      "Iteration 1400, Loss: 0.7638\n",
      "Iteration 1500, Loss: 0.7577\n",
      "Iteration 1600, Loss: 0.7521\n",
      "Iteration 1700, Loss: 0.7470\n",
      "Iteration 1800, Loss: 0.7422\n",
      "Iteration 1900, Loss: 0.7379\n",
      "Iteration 2000, Loss: 0.7339\n",
      "Iteration 2100, Loss: 0.7302\n",
      "Iteration 2200, Loss: 0.7267\n",
      "Iteration 2300, Loss: 0.7234\n",
      "Iteration 2400, Loss: 0.7204\n",
      "Iteration 2500, Loss: 0.7175\n",
      "Iteration 2600, Loss: 0.7148\n",
      "Iteration 2700, Loss: 0.7122\n",
      "Iteration 2800, Loss: 0.7098\n",
      "Iteration 2900, Loss: 0.7075\n",
      "Iteration 3000, Loss: 0.7052\n",
      "Iteration 3100, Loss: 0.7031\n",
      "Iteration 3200, Loss: 0.7011\n",
      "Iteration 3300, Loss: 0.6992\n",
      "Iteration 3400, Loss: 0.6974\n",
      "Iteration 3500, Loss: 0.6956\n",
      "Iteration 3600, Loss: 0.6939\n",
      "Iteration 3700, Loss: 0.6923\n",
      "Iteration 3800, Loss: 0.6907\n",
      "Iteration 3900, Loss: 0.6892\n",
      "Iteration 4000, Loss: 0.6877\n",
      "Iteration 4100, Loss: 0.6863\n",
      "Iteration 4200, Loss: 0.6849\n",
      "Iteration 4300, Loss: 0.6835\n",
      "Iteration 4400, Loss: 0.6822\n",
      "Iteration 4500, Loss: 0.6810\n",
      "Iteration 4600, Loss: 0.6797\n",
      "Iteration 4700, Loss: 0.6786\n",
      "Iteration 4800, Loss: 0.6774\n",
      "Iteration 4900, Loss: 0.6763\n",
      "Iteration 5000, Loss: 0.6752\n",
      "Iteration 5100, Loss: 0.6741\n",
      "Iteration 5200, Loss: 0.6731\n",
      "Iteration 5300, Loss: 0.6721\n",
      "Iteration 5400, Loss: 0.6711\n",
      "Iteration 5500, Loss: 0.6701\n",
      "Iteration 5600, Loss: 0.6692\n",
      "Iteration 5700, Loss: 0.6682\n",
      "Iteration 5800, Loss: 0.6674\n",
      "Iteration 5900, Loss: 0.6665\n",
      "243 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0202\n",
      "Iteration 200, Loss: 0.9666\n",
      "Iteration 300, Loss: 0.9266\n",
      "Iteration 400, Loss: 0.8953\n",
      "Iteration 500, Loss: 0.8700\n",
      "Iteration 600, Loss: 0.8491\n",
      "Iteration 700, Loss: 0.8315\n",
      "Iteration 800, Loss: 0.8165\n",
      "Iteration 900, Loss: 0.8036\n",
      "Iteration 1000, Loss: 0.7923\n",
      "Iteration 1100, Loss: 0.7824\n",
      "Iteration 1200, Loss: 0.7736\n",
      "Iteration 1300, Loss: 0.7658\n",
      "Iteration 1400, Loss: 0.7588\n",
      "Iteration 1500, Loss: 0.7524\n",
      "Iteration 1600, Loss: 0.7466\n",
      "Iteration 1700, Loss: 0.7414\n",
      "Iteration 1800, Loss: 0.7365\n",
      "Iteration 1900, Loss: 0.7321\n",
      "Iteration 2000, Loss: 0.7280\n",
      "Iteration 2100, Loss: 0.7243\n",
      "Iteration 2200, Loss: 0.7207\n",
      "Iteration 2300, Loss: 0.7174\n",
      "Iteration 2400, Loss: 0.7144\n",
      "Iteration 2500, Loss: 0.7115\n",
      "Iteration 2600, Loss: 0.7087\n",
      "Iteration 2700, Loss: 0.7062\n",
      "Iteration 2800, Loss: 0.7037\n",
      "Iteration 2900, Loss: 0.7014\n",
      "Iteration 3000, Loss: 0.6993\n",
      "Iteration 3100, Loss: 0.6972\n",
      "Iteration 3200, Loss: 0.6952\n",
      "Iteration 3300, Loss: 0.6933\n",
      "Iteration 3400, Loss: 0.6915\n",
      "Iteration 3500, Loss: 0.6897\n",
      "Iteration 3600, Loss: 0.6881\n",
      "Iteration 3700, Loss: 0.6865\n",
      "Iteration 3800, Loss: 0.6849\n",
      "Iteration 3900, Loss: 0.6834\n",
      "Iteration 4000, Loss: 0.6820\n",
      "Iteration 4100, Loss: 0.6806\n",
      "Iteration 4200, Loss: 0.6793\n",
      "Iteration 4300, Loss: 0.6780\n",
      "Iteration 4400, Loss: 0.6767\n",
      "Iteration 4500, Loss: 0.6755\n",
      "Iteration 4600, Loss: 0.6743\n",
      "Iteration 4700, Loss: 0.6732\n",
      "Iteration 4800, Loss: 0.6721\n",
      "Iteration 4900, Loss: 0.6710\n",
      "Iteration 5000, Loss: 0.6699\n",
      "Iteration 5100, Loss: 0.6689\n",
      "Iteration 5200, Loss: 0.6679\n",
      "Iteration 5300, Loss: 0.6669\n",
      "Iteration 5400, Loss: 0.6660\n",
      "Iteration 5500, Loss: 0.6651\n",
      "Iteration 5600, Loss: 0.6642\n",
      "Iteration 5700, Loss: 0.6633\n",
      "Iteration 5800, Loss: 0.6624\n",
      "Iteration 5900, Loss: 0.6616\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0203\n",
      "Iteration 200, Loss: 0.9658\n",
      "Iteration 300, Loss: 0.9246\n",
      "Iteration 400, Loss: 0.8922\n",
      "Iteration 500, Loss: 0.8659\n",
      "Iteration 600, Loss: 0.8441\n",
      "Iteration 700, Loss: 0.8258\n",
      "Iteration 800, Loss: 0.8102\n",
      "Iteration 900, Loss: 0.7967\n",
      "Iteration 1000, Loss: 0.7849\n",
      "Iteration 1100, Loss: 0.7746\n",
      "Iteration 1200, Loss: 0.7654\n",
      "Iteration 1300, Loss: 0.7572\n",
      "Iteration 1400, Loss: 0.7499\n",
      "Iteration 1500, Loss: 0.7433\n",
      "Iteration 1600, Loss: 0.7372\n",
      "Iteration 1700, Loss: 0.7317\n",
      "Iteration 1800, Loss: 0.7266\n",
      "Iteration 1900, Loss: 0.7219\n",
      "Iteration 2000, Loss: 0.7175\n",
      "Iteration 2100, Loss: 0.7135\n",
      "Iteration 2200, Loss: 0.7097\n",
      "Iteration 2300, Loss: 0.7062\n",
      "Iteration 2400, Loss: 0.7029\n",
      "Iteration 2500, Loss: 0.6997\n",
      "Iteration 2600, Loss: 0.6967\n",
      "Iteration 2700, Loss: 0.6939\n",
      "Iteration 2800, Loss: 0.6912\n",
      "Iteration 2900, Loss: 0.6887\n",
      "Iteration 3000, Loss: 0.6862\n",
      "Iteration 3100, Loss: 0.6839\n",
      "Iteration 3200, Loss: 0.6816\n",
      "Iteration 3300, Loss: 0.6795\n",
      "Iteration 3400, Loss: 0.6774\n",
      "Iteration 3500, Loss: 0.6755\n",
      "Iteration 3600, Loss: 0.6736\n",
      "Iteration 3700, Loss: 0.6717\n",
      "Iteration 3800, Loss: 0.6699\n",
      "Iteration 3900, Loss: 0.6682\n",
      "Iteration 4000, Loss: 0.6665\n",
      "Iteration 4100, Loss: 0.6649\n",
      "Iteration 4200, Loss: 0.6634\n",
      "Iteration 4300, Loss: 0.6618\n",
      "Iteration 4400, Loss: 0.6604\n",
      "Iteration 4500, Loss: 0.6589\n",
      "Iteration 4600, Loss: 0.6576\n",
      "Iteration 4700, Loss: 0.6562\n",
      "Iteration 4800, Loss: 0.6549\n",
      "Iteration 4900, Loss: 0.6536\n",
      "Iteration 5000, Loss: 0.6524\n",
      "Iteration 5100, Loss: 0.6511\n",
      "Iteration 5200, Loss: 0.6499\n",
      "Iteration 5300, Loss: 0.6488\n",
      "Iteration 5400, Loss: 0.6476\n",
      "Iteration 5500, Loss: 0.6465\n",
      "Iteration 5600, Loss: 0.6455\n",
      "Iteration 5700, Loss: 0.6444\n",
      "Iteration 5800, Loss: 0.6434\n",
      "Iteration 5900, Loss: 0.6424\n",
      "244 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0200\n",
      "Iteration 200, Loss: 0.9663\n",
      "Iteration 300, Loss: 0.9257\n",
      "Iteration 400, Loss: 0.8939\n",
      "Iteration 500, Loss: 0.8682\n",
      "Iteration 600, Loss: 0.8469\n",
      "Iteration 700, Loss: 0.8290\n",
      "Iteration 800, Loss: 0.8136\n",
      "Iteration 900, Loss: 0.8003\n",
      "Iteration 1000, Loss: 0.7887\n",
      "Iteration 1100, Loss: 0.7784\n",
      "Iteration 1200, Loss: 0.7693\n",
      "Iteration 1300, Loss: 0.7611\n",
      "Iteration 1400, Loss: 0.7537\n",
      "Iteration 1500, Loss: 0.7470\n",
      "Iteration 1600, Loss: 0.7410\n",
      "Iteration 1700, Loss: 0.7354\n",
      "Iteration 1800, Loss: 0.7303\n",
      "Iteration 1900, Loss: 0.7256\n",
      "Iteration 2000, Loss: 0.7212\n",
      "Iteration 2100, Loss: 0.7172\n",
      "Iteration 2200, Loss: 0.7134\n",
      "Iteration 2300, Loss: 0.7098\n",
      "Iteration 2400, Loss: 0.7065\n",
      "Iteration 2500, Loss: 0.7034\n",
      "Iteration 2600, Loss: 0.7004\n",
      "Iteration 2700, Loss: 0.6976\n",
      "Iteration 2800, Loss: 0.6949\n",
      "Iteration 2900, Loss: 0.6924\n",
      "Iteration 3000, Loss: 0.6900\n",
      "Iteration 3100, Loss: 0.6877\n",
      "Iteration 3200, Loss: 0.6855\n",
      "Iteration 3300, Loss: 0.6834\n",
      "Iteration 3400, Loss: 0.6814\n",
      "Iteration 3500, Loss: 0.6795\n",
      "Iteration 3600, Loss: 0.6776\n",
      "Iteration 3700, Loss: 0.6758\n",
      "Iteration 3800, Loss: 0.6741\n",
      "Iteration 3900, Loss: 0.6725\n",
      "Iteration 4000, Loss: 0.6709\n",
      "Iteration 4100, Loss: 0.6693\n",
      "Iteration 4200, Loss: 0.6678\n",
      "Iteration 4300, Loss: 0.6664\n",
      "Iteration 4400, Loss: 0.6650\n",
      "Iteration 4500, Loss: 0.6636\n",
      "Iteration 4600, Loss: 0.6623\n",
      "Iteration 4700, Loss: 0.6610\n",
      "Iteration 4800, Loss: 0.6598\n",
      "Iteration 4900, Loss: 0.6586\n",
      "Iteration 5000, Loss: 0.6574\n",
      "Iteration 5100, Loss: 0.6563\n",
      "Iteration 5200, Loss: 0.6551\n",
      "Iteration 5300, Loss: 0.6541\n",
      "Iteration 5400, Loss: 0.6530\n",
      "Iteration 5500, Loss: 0.6520\n",
      "Iteration 5600, Loss: 0.6510\n",
      "Iteration 5700, Loss: 0.6500\n",
      "Iteration 5800, Loss: 0.6490\n",
      "Iteration 5900, Loss: 0.6481\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0206\n",
      "Iteration 200, Loss: 0.9665\n",
      "Iteration 300, Loss: 0.9257\n",
      "Iteration 400, Loss: 0.8938\n",
      "Iteration 500, Loss: 0.8679\n",
      "Iteration 600, Loss: 0.8467\n",
      "Iteration 700, Loss: 0.8289\n",
      "Iteration 800, Loss: 0.8138\n",
      "Iteration 900, Loss: 0.8007\n",
      "Iteration 1000, Loss: 0.7894\n",
      "Iteration 1100, Loss: 0.7795\n",
      "Iteration 1200, Loss: 0.7708\n",
      "Iteration 1300, Loss: 0.7630\n",
      "Iteration 1400, Loss: 0.7560\n",
      "Iteration 1500, Loss: 0.7497\n",
      "Iteration 1600, Loss: 0.7440\n",
      "Iteration 1700, Loss: 0.7388\n",
      "Iteration 1800, Loss: 0.7340\n",
      "Iteration 1900, Loss: 0.7296\n",
      "Iteration 2000, Loss: 0.7256\n",
      "Iteration 2100, Loss: 0.7218\n",
      "Iteration 2200, Loss: 0.7182\n",
      "Iteration 2300, Loss: 0.7149\n",
      "Iteration 2400, Loss: 0.7118\n",
      "Iteration 2500, Loss: 0.7089\n",
      "Iteration 2600, Loss: 0.7062\n",
      "Iteration 2700, Loss: 0.7035\n",
      "Iteration 2800, Loss: 0.7011\n",
      "Iteration 2900, Loss: 0.6987\n",
      "Iteration 3000, Loss: 0.6965\n",
      "Iteration 3100, Loss: 0.6943\n",
      "Iteration 3200, Loss: 0.6923\n",
      "Iteration 3300, Loss: 0.6903\n",
      "Iteration 3400, Loss: 0.6884\n",
      "Iteration 3500, Loss: 0.6866\n",
      "Iteration 3600, Loss: 0.6849\n",
      "Iteration 3700, Loss: 0.6832\n",
      "Iteration 3800, Loss: 0.6816\n",
      "Iteration 3900, Loss: 0.6800\n",
      "Iteration 4000, Loss: 0.6785\n",
      "Iteration 4100, Loss: 0.6770\n",
      "Iteration 4200, Loss: 0.6756\n",
      "Iteration 4300, Loss: 0.6742\n",
      "Iteration 4400, Loss: 0.6729\n",
      "Iteration 4500, Loss: 0.6716\n",
      "Iteration 4600, Loss: 0.6703\n",
      "Iteration 4700, Loss: 0.6691\n",
      "Iteration 4800, Loss: 0.6679\n",
      "Iteration 4900, Loss: 0.6667\n",
      "Iteration 5000, Loss: 0.6655\n",
      "Iteration 5100, Loss: 0.6644\n",
      "Iteration 5200, Loss: 0.6633\n",
      "Iteration 5300, Loss: 0.6623\n",
      "Iteration 5400, Loss: 0.6612\n",
      "Iteration 5500, Loss: 0.6602\n",
      "Iteration 5600, Loss: 0.6592\n",
      "Iteration 5700, Loss: 0.6583\n",
      "Iteration 5800, Loss: 0.6573\n",
      "Iteration 5900, Loss: 0.6564\n",
      "245 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0195\n",
      "Iteration 200, Loss: 0.9657\n",
      "Iteration 300, Loss: 0.9258\n",
      "Iteration 400, Loss: 0.8947\n",
      "Iteration 500, Loss: 0.8696\n",
      "Iteration 600, Loss: 0.8489\n",
      "Iteration 700, Loss: 0.8314\n",
      "Iteration 800, Loss: 0.8165\n",
      "Iteration 900, Loss: 0.8037\n",
      "Iteration 1000, Loss: 0.7925\n",
      "Iteration 1100, Loss: 0.7826\n",
      "Iteration 1200, Loss: 0.7739\n",
      "Iteration 1300, Loss: 0.7660\n",
      "Iteration 1400, Loss: 0.7590\n",
      "Iteration 1500, Loss: 0.7526\n",
      "Iteration 1600, Loss: 0.7467\n",
      "Iteration 1700, Loss: 0.7414\n",
      "Iteration 1800, Loss: 0.7364\n",
      "Iteration 1900, Loss: 0.7319\n",
      "Iteration 2000, Loss: 0.7277\n",
      "Iteration 2100, Loss: 0.7237\n",
      "Iteration 2200, Loss: 0.7201\n",
      "Iteration 2300, Loss: 0.7166\n",
      "Iteration 2400, Loss: 0.7134\n",
      "Iteration 2500, Loss: 0.7103\n",
      "Iteration 2600, Loss: 0.7074\n",
      "Iteration 2700, Loss: 0.7047\n",
      "Iteration 2800, Loss: 0.7021\n",
      "Iteration 2900, Loss: 0.6996\n",
      "Iteration 3000, Loss: 0.6972\n",
      "Iteration 3100, Loss: 0.6950\n",
      "Iteration 3200, Loss: 0.6928\n",
      "Iteration 3300, Loss: 0.6907\n",
      "Iteration 3400, Loss: 0.6887\n",
      "Iteration 3500, Loss: 0.6868\n",
      "Iteration 3600, Loss: 0.6849\n",
      "Iteration 3700, Loss: 0.6831\n",
      "Iteration 3800, Loss: 0.6814\n",
      "Iteration 3900, Loss: 0.6797\n",
      "Iteration 4000, Loss: 0.6781\n",
      "Iteration 4100, Loss: 0.6766\n",
      "Iteration 4200, Loss: 0.6750\n",
      "Iteration 4300, Loss: 0.6736\n",
      "Iteration 4400, Loss: 0.6722\n",
      "Iteration 4500, Loss: 0.6708\n",
      "Iteration 4600, Loss: 0.6694\n",
      "Iteration 4700, Loss: 0.6681\n",
      "Iteration 4800, Loss: 0.6668\n",
      "Iteration 4900, Loss: 0.6656\n",
      "Iteration 5000, Loss: 0.6644\n",
      "Iteration 5100, Loss: 0.6632\n",
      "Iteration 5200, Loss: 0.6621\n",
      "Iteration 5300, Loss: 0.6610\n",
      "Iteration 5400, Loss: 0.6599\n",
      "Iteration 5500, Loss: 0.6588\n",
      "Iteration 5600, Loss: 0.6578\n",
      "Iteration 5700, Loss: 0.6567\n",
      "Iteration 5800, Loss: 0.6558\n",
      "Iteration 5900, Loss: 0.6548\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0195\n",
      "Iteration 200, Loss: 0.9644\n",
      "Iteration 300, Loss: 0.9225\n",
      "Iteration 400, Loss: 0.8893\n",
      "Iteration 500, Loss: 0.8624\n",
      "Iteration 600, Loss: 0.8401\n",
      "Iteration 700, Loss: 0.8213\n",
      "Iteration 800, Loss: 0.8052\n",
      "Iteration 900, Loss: 0.7914\n",
      "Iteration 1000, Loss: 0.7793\n",
      "Iteration 1100, Loss: 0.7687\n",
      "Iteration 1200, Loss: 0.7594\n",
      "Iteration 1300, Loss: 0.7510\n",
      "Iteration 1400, Loss: 0.7436\n",
      "Iteration 1500, Loss: 0.7369\n",
      "Iteration 1600, Loss: 0.7308\n",
      "Iteration 1700, Loss: 0.7252\n",
      "Iteration 1800, Loss: 0.7201\n",
      "Iteration 1900, Loss: 0.7154\n",
      "Iteration 2000, Loss: 0.7111\n",
      "Iteration 2100, Loss: 0.7071\n",
      "Iteration 2200, Loss: 0.7034\n",
      "Iteration 2300, Loss: 0.6999\n",
      "Iteration 2400, Loss: 0.6966\n",
      "Iteration 2500, Loss: 0.6936\n",
      "Iteration 2600, Loss: 0.6907\n",
      "Iteration 2700, Loss: 0.6879\n",
      "Iteration 2800, Loss: 0.6854\n",
      "Iteration 2900, Loss: 0.6829\n",
      "Iteration 3000, Loss: 0.6806\n",
      "Iteration 3100, Loss: 0.6783\n",
      "Iteration 3200, Loss: 0.6762\n",
      "Iteration 3300, Loss: 0.6742\n",
      "Iteration 3400, Loss: 0.6722\n",
      "Iteration 3500, Loss: 0.6704\n",
      "Iteration 3600, Loss: 0.6686\n",
      "Iteration 3700, Loss: 0.6668\n",
      "Iteration 3800, Loss: 0.6652\n",
      "Iteration 3900, Loss: 0.6636\n",
      "Iteration 4000, Loss: 0.6620\n",
      "Iteration 4100, Loss: 0.6605\n",
      "Iteration 4200, Loss: 0.6591\n",
      "Iteration 4300, Loss: 0.6577\n",
      "Iteration 4400, Loss: 0.6563\n",
      "Iteration 4500, Loss: 0.6550\n",
      "Iteration 4600, Loss: 0.6537\n",
      "Iteration 4700, Loss: 0.6525\n",
      "Iteration 4800, Loss: 0.6513\n",
      "Iteration 4900, Loss: 0.6501\n",
      "Iteration 5000, Loss: 0.6490\n",
      "Iteration 5100, Loss: 0.6479\n",
      "Iteration 5200, Loss: 0.6468\n",
      "Iteration 5300, Loss: 0.6458\n",
      "Iteration 5400, Loss: 0.6447\n",
      "Iteration 5500, Loss: 0.6437\n",
      "Iteration 5600, Loss: 0.6428\n",
      "Iteration 5700, Loss: 0.6418\n",
      "Iteration 5800, Loss: 0.6409\n",
      "Iteration 5900, Loss: 0.6400\n",
      "246 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0220\n",
      "Iteration 200, Loss: 0.9700\n",
      "Iteration 300, Loss: 0.9309\n",
      "Iteration 400, Loss: 0.9002\n",
      "Iteration 500, Loss: 0.8754\n",
      "Iteration 600, Loss: 0.8548\n",
      "Iteration 700, Loss: 0.8376\n",
      "Iteration 800, Loss: 0.8228\n",
      "Iteration 900, Loss: 0.8101\n",
      "Iteration 1000, Loss: 0.7990\n",
      "Iteration 1100, Loss: 0.7892\n",
      "Iteration 1200, Loss: 0.7806\n",
      "Iteration 1300, Loss: 0.7729\n",
      "Iteration 1400, Loss: 0.7660\n",
      "Iteration 1500, Loss: 0.7597\n",
      "Iteration 1600, Loss: 0.7540\n",
      "Iteration 1700, Loss: 0.7488\n",
      "Iteration 1800, Loss: 0.7440\n",
      "Iteration 1900, Loss: 0.7396\n",
      "Iteration 2000, Loss: 0.7355\n",
      "Iteration 2100, Loss: 0.7317\n",
      "Iteration 2200, Loss: 0.7282\n",
      "Iteration 2300, Loss: 0.7249\n",
      "Iteration 2400, Loss: 0.7217\n",
      "Iteration 2500, Loss: 0.7188\n",
      "Iteration 2600, Loss: 0.7160\n",
      "Iteration 2700, Loss: 0.7134\n",
      "Iteration 2800, Loss: 0.7109\n",
      "Iteration 2900, Loss: 0.7085\n",
      "Iteration 3000, Loss: 0.7063\n",
      "Iteration 3100, Loss: 0.7041\n",
      "Iteration 3200, Loss: 0.7020\n",
      "Iteration 3300, Loss: 0.7001\n",
      "Iteration 3400, Loss: 0.6982\n",
      "Iteration 3500, Loss: 0.6963\n",
      "Iteration 3600, Loss: 0.6946\n",
      "Iteration 3700, Loss: 0.6929\n",
      "Iteration 3800, Loss: 0.6912\n",
      "Iteration 3900, Loss: 0.6897\n",
      "Iteration 4000, Loss: 0.6881\n",
      "Iteration 4100, Loss: 0.6867\n",
      "Iteration 4200, Loss: 0.6852\n",
      "Iteration 4300, Loss: 0.6838\n",
      "Iteration 4400, Loss: 0.6825\n",
      "Iteration 4500, Loss: 0.6812\n",
      "Iteration 4600, Loss: 0.6799\n",
      "Iteration 4700, Loss: 0.6787\n",
      "Iteration 4800, Loss: 0.6775\n",
      "Iteration 4900, Loss: 0.6763\n",
      "Iteration 5000, Loss: 0.6751\n",
      "Iteration 5100, Loss: 0.6740\n",
      "Iteration 5200, Loss: 0.6729\n",
      "Iteration 5300, Loss: 0.6719\n",
      "Iteration 5400, Loss: 0.6708\n",
      "Iteration 5500, Loss: 0.6698\n",
      "Iteration 5600, Loss: 0.6688\n",
      "Iteration 5700, Loss: 0.6679\n",
      "Iteration 5800, Loss: 0.6669\n",
      "Iteration 5900, Loss: 0.6660\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0172\n",
      "Iteration 200, Loss: 0.9603\n",
      "Iteration 300, Loss: 0.9176\n",
      "Iteration 400, Loss: 0.8841\n",
      "Iteration 500, Loss: 0.8570\n",
      "Iteration 600, Loss: 0.8346\n",
      "Iteration 700, Loss: 0.8159\n",
      "Iteration 800, Loss: 0.7998\n",
      "Iteration 900, Loss: 0.7859\n",
      "Iteration 1000, Loss: 0.7739\n",
      "Iteration 1100, Loss: 0.7633\n",
      "Iteration 1200, Loss: 0.7539\n",
      "Iteration 1300, Loss: 0.7455\n",
      "Iteration 1400, Loss: 0.7380\n",
      "Iteration 1500, Loss: 0.7312\n",
      "Iteration 1600, Loss: 0.7250\n",
      "Iteration 1700, Loss: 0.7194\n",
      "Iteration 1800, Loss: 0.7142\n",
      "Iteration 1900, Loss: 0.7094\n",
      "Iteration 2000, Loss: 0.7050\n",
      "Iteration 2100, Loss: 0.7009\n",
      "Iteration 2200, Loss: 0.6971\n",
      "Iteration 2300, Loss: 0.6935\n",
      "Iteration 2400, Loss: 0.6901\n",
      "Iteration 2500, Loss: 0.6870\n",
      "Iteration 2600, Loss: 0.6840\n",
      "Iteration 2700, Loss: 0.6811\n",
      "Iteration 2800, Loss: 0.6784\n",
      "Iteration 2900, Loss: 0.6759\n",
      "Iteration 3000, Loss: 0.6735\n",
      "Iteration 3100, Loss: 0.6711\n",
      "Iteration 3200, Loss: 0.6689\n",
      "Iteration 3300, Loss: 0.6668\n",
      "Iteration 3400, Loss: 0.6648\n",
      "Iteration 3500, Loss: 0.6628\n",
      "Iteration 3600, Loss: 0.6609\n",
      "Iteration 3700, Loss: 0.6591\n",
      "Iteration 3800, Loss: 0.6574\n",
      "Iteration 3900, Loss: 0.6557\n",
      "Iteration 4000, Loss: 0.6541\n",
      "Iteration 4100, Loss: 0.6525\n",
      "Iteration 4200, Loss: 0.6510\n",
      "Iteration 4300, Loss: 0.6495\n",
      "Iteration 4400, Loss: 0.6481\n",
      "Iteration 4500, Loss: 0.6467\n",
      "Iteration 4600, Loss: 0.6453\n",
      "Iteration 4700, Loss: 0.6440\n",
      "Iteration 4800, Loss: 0.6427\n",
      "Iteration 4900, Loss: 0.6415\n",
      "Iteration 5000, Loss: 0.6403\n",
      "Iteration 5100, Loss: 0.6391\n",
      "Iteration 5200, Loss: 0.6380\n",
      "Iteration 5300, Loss: 0.6369\n",
      "Iteration 5400, Loss: 0.6358\n",
      "Iteration 5500, Loss: 0.6347\n",
      "Iteration 5600, Loss: 0.6337\n",
      "Iteration 5700, Loss: 0.6327\n",
      "Iteration 5800, Loss: 0.6317\n",
      "Iteration 5900, Loss: 0.6307\n",
      "247 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0211\n",
      "Iteration 200, Loss: 0.9673\n",
      "Iteration 300, Loss: 0.9265\n",
      "Iteration 400, Loss: 0.8944\n",
      "Iteration 500, Loss: 0.8684\n",
      "Iteration 600, Loss: 0.8468\n",
      "Iteration 700, Loss: 0.8286\n",
      "Iteration 800, Loss: 0.8131\n",
      "Iteration 900, Loss: 0.7997\n",
      "Iteration 1000, Loss: 0.7880\n",
      "Iteration 1100, Loss: 0.7777\n",
      "Iteration 1200, Loss: 0.7685\n",
      "Iteration 1300, Loss: 0.7603\n",
      "Iteration 1400, Loss: 0.7530\n",
      "Iteration 1500, Loss: 0.7463\n",
      "Iteration 1600, Loss: 0.7402\n",
      "Iteration 1700, Loss: 0.7346\n",
      "Iteration 1800, Loss: 0.7295\n",
      "Iteration 1900, Loss: 0.7248\n",
      "Iteration 2000, Loss: 0.7204\n",
      "Iteration 2100, Loss: 0.7163\n",
      "Iteration 2200, Loss: 0.7125\n",
      "Iteration 2300, Loss: 0.7089\n",
      "Iteration 2400, Loss: 0.7056\n",
      "Iteration 2500, Loss: 0.7024\n",
      "Iteration 2600, Loss: 0.6994\n",
      "Iteration 2700, Loss: 0.6966\n",
      "Iteration 2800, Loss: 0.6939\n",
      "Iteration 2900, Loss: 0.6913\n",
      "Iteration 3000, Loss: 0.6889\n",
      "Iteration 3100, Loss: 0.6866\n",
      "Iteration 3200, Loss: 0.6843\n",
      "Iteration 3300, Loss: 0.6822\n",
      "Iteration 3400, Loss: 0.6801\n",
      "Iteration 3500, Loss: 0.6782\n",
      "Iteration 3600, Loss: 0.6763\n",
      "Iteration 3700, Loss: 0.6745\n",
      "Iteration 3800, Loss: 0.6727\n",
      "Iteration 3900, Loss: 0.6710\n",
      "Iteration 4000, Loss: 0.6694\n",
      "Iteration 4100, Loss: 0.6678\n",
      "Iteration 4200, Loss: 0.6663\n",
      "Iteration 4300, Loss: 0.6648\n",
      "Iteration 4400, Loss: 0.6633\n",
      "Iteration 4500, Loss: 0.6619\n",
      "Iteration 4600, Loss: 0.6606\n",
      "Iteration 4700, Loss: 0.6593\n",
      "Iteration 4800, Loss: 0.6580\n",
      "Iteration 4900, Loss: 0.6567\n",
      "Iteration 5000, Loss: 0.6555\n",
      "Iteration 5100, Loss: 0.6544\n",
      "Iteration 5200, Loss: 0.6532\n",
      "Iteration 5300, Loss: 0.6521\n",
      "Iteration 5400, Loss: 0.6510\n",
      "Iteration 5500, Loss: 0.6499\n",
      "Iteration 5600, Loss: 0.6489\n",
      "Iteration 5700, Loss: 0.6478\n",
      "Iteration 5800, Loss: 0.6469\n",
      "Iteration 5900, Loss: 0.6459\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0190\n",
      "Iteration 200, Loss: 0.9642\n",
      "Iteration 300, Loss: 0.9234\n",
      "Iteration 400, Loss: 0.8915\n",
      "Iteration 500, Loss: 0.8657\n",
      "Iteration 600, Loss: 0.8445\n",
      "Iteration 700, Loss: 0.8267\n",
      "Iteration 800, Loss: 0.8115\n",
      "Iteration 900, Loss: 0.7984\n",
      "Iteration 1000, Loss: 0.7870\n",
      "Iteration 1100, Loss: 0.7770\n",
      "Iteration 1200, Loss: 0.7681\n",
      "Iteration 1300, Loss: 0.7602\n",
      "Iteration 1400, Loss: 0.7532\n",
      "Iteration 1500, Loss: 0.7468\n",
      "Iteration 1600, Loss: 0.7410\n",
      "Iteration 1700, Loss: 0.7357\n",
      "Iteration 1800, Loss: 0.7309\n",
      "Iteration 1900, Loss: 0.7265\n",
      "Iteration 2000, Loss: 0.7224\n",
      "Iteration 2100, Loss: 0.7185\n",
      "Iteration 2200, Loss: 0.7150\n",
      "Iteration 2300, Loss: 0.7117\n",
      "Iteration 2400, Loss: 0.7086\n",
      "Iteration 2500, Loss: 0.7057\n",
      "Iteration 2600, Loss: 0.7029\n",
      "Iteration 2700, Loss: 0.7003\n",
      "Iteration 2800, Loss: 0.6978\n",
      "Iteration 2900, Loss: 0.6955\n",
      "Iteration 3000, Loss: 0.6932\n",
      "Iteration 3100, Loss: 0.6911\n",
      "Iteration 3200, Loss: 0.6890\n",
      "Iteration 3300, Loss: 0.6870\n",
      "Iteration 3400, Loss: 0.6852\n",
      "Iteration 3500, Loss: 0.6833\n",
      "Iteration 3600, Loss: 0.6816\n",
      "Iteration 3700, Loss: 0.6799\n",
      "Iteration 3800, Loss: 0.6783\n",
      "Iteration 3900, Loss: 0.6767\n",
      "Iteration 4000, Loss: 0.6752\n",
      "Iteration 4100, Loss: 0.6737\n",
      "Iteration 4200, Loss: 0.6723\n",
      "Iteration 4300, Loss: 0.6709\n",
      "Iteration 4400, Loss: 0.6696\n",
      "Iteration 4500, Loss: 0.6683\n",
      "Iteration 4600, Loss: 0.6670\n",
      "Iteration 4700, Loss: 0.6658\n",
      "Iteration 4800, Loss: 0.6646\n",
      "Iteration 4900, Loss: 0.6634\n",
      "Iteration 5000, Loss: 0.6623\n",
      "Iteration 5100, Loss: 0.6612\n",
      "Iteration 5200, Loss: 0.6601\n",
      "Iteration 5300, Loss: 0.6590\n",
      "Iteration 5400, Loss: 0.6580\n",
      "Iteration 5500, Loss: 0.6570\n",
      "Iteration 5600, Loss: 0.6560\n",
      "Iteration 5700, Loss: 0.6551\n",
      "Iteration 5800, Loss: 0.6541\n",
      "Iteration 5900, Loss: 0.6532\n",
      "248 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0209\n",
      "Iteration 200, Loss: 0.9678\n",
      "Iteration 300, Loss: 0.9278\n",
      "Iteration 400, Loss: 0.8964\n",
      "Iteration 500, Loss: 0.8708\n",
      "Iteration 600, Loss: 0.8497\n",
      "Iteration 700, Loss: 0.8319\n",
      "Iteration 800, Loss: 0.8166\n",
      "Iteration 900, Loss: 0.8035\n",
      "Iteration 1000, Loss: 0.7919\n",
      "Iteration 1100, Loss: 0.7817\n",
      "Iteration 1200, Loss: 0.7727\n",
      "Iteration 1300, Loss: 0.7647\n",
      "Iteration 1400, Loss: 0.7574\n",
      "Iteration 1500, Loss: 0.7509\n",
      "Iteration 1600, Loss: 0.7450\n",
      "Iteration 1700, Loss: 0.7395\n",
      "Iteration 1800, Loss: 0.7346\n",
      "Iteration 1900, Loss: 0.7300\n",
      "Iteration 2000, Loss: 0.7257\n",
      "Iteration 2100, Loss: 0.7218\n",
      "Iteration 2200, Loss: 0.7181\n",
      "Iteration 2300, Loss: 0.7147\n",
      "Iteration 2400, Loss: 0.7114\n",
      "Iteration 2500, Loss: 0.7084\n",
      "Iteration 2600, Loss: 0.7055\n",
      "Iteration 2700, Loss: 0.7028\n",
      "Iteration 2800, Loss: 0.7002\n",
      "Iteration 2900, Loss: 0.6977\n",
      "Iteration 3000, Loss: 0.6954\n",
      "Iteration 3100, Loss: 0.6932\n",
      "Iteration 3200, Loss: 0.6910\n",
      "Iteration 3300, Loss: 0.6890\n",
      "Iteration 3400, Loss: 0.6870\n",
      "Iteration 3500, Loss: 0.6852\n",
      "Iteration 3600, Loss: 0.6833\n",
      "Iteration 3700, Loss: 0.6816\n",
      "Iteration 3800, Loss: 0.6799\n",
      "Iteration 3900, Loss: 0.6783\n",
      "Iteration 4000, Loss: 0.6767\n",
      "Iteration 4100, Loss: 0.6752\n",
      "Iteration 4200, Loss: 0.6737\n",
      "Iteration 4300, Loss: 0.6723\n",
      "Iteration 4400, Loss: 0.6709\n",
      "Iteration 4500, Loss: 0.6695\n",
      "Iteration 4600, Loss: 0.6682\n",
      "Iteration 4700, Loss: 0.6669\n",
      "Iteration 4800, Loss: 0.6657\n",
      "Iteration 4900, Loss: 0.6645\n",
      "Iteration 5000, Loss: 0.6633\n",
      "Iteration 5100, Loss: 0.6622\n",
      "Iteration 5200, Loss: 0.6611\n",
      "Iteration 5300, Loss: 0.6600\n",
      "Iteration 5400, Loss: 0.6589\n",
      "Iteration 5500, Loss: 0.6579\n",
      "Iteration 5600, Loss: 0.6569\n",
      "Iteration 5700, Loss: 0.6559\n",
      "Iteration 5800, Loss: 0.6549\n",
      "Iteration 5900, Loss: 0.6539\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0182\n",
      "Iteration 200, Loss: 0.9626\n",
      "Iteration 300, Loss: 0.9208\n",
      "Iteration 400, Loss: 0.8883\n",
      "Iteration 500, Loss: 0.8620\n",
      "Iteration 600, Loss: 0.8404\n",
      "Iteration 700, Loss: 0.8223\n",
      "Iteration 800, Loss: 0.8069\n",
      "Iteration 900, Loss: 0.7936\n",
      "Iteration 1000, Loss: 0.7820\n",
      "Iteration 1100, Loss: 0.7718\n",
      "Iteration 1200, Loss: 0.7629\n",
      "Iteration 1300, Loss: 0.7549\n",
      "Iteration 1400, Loss: 0.7477\n",
      "Iteration 1500, Loss: 0.7412\n",
      "Iteration 1600, Loss: 0.7353\n",
      "Iteration 1700, Loss: 0.7299\n",
      "Iteration 1800, Loss: 0.7250\n",
      "Iteration 1900, Loss: 0.7204\n",
      "Iteration 2000, Loss: 0.7163\n",
      "Iteration 2100, Loss: 0.7123\n",
      "Iteration 2200, Loss: 0.7087\n",
      "Iteration 2300, Loss: 0.7053\n",
      "Iteration 2400, Loss: 0.7021\n",
      "Iteration 2500, Loss: 0.6991\n",
      "Iteration 2600, Loss: 0.6962\n",
      "Iteration 2700, Loss: 0.6935\n",
      "Iteration 2800, Loss: 0.6910\n",
      "Iteration 2900, Loss: 0.6885\n",
      "Iteration 3000, Loss: 0.6862\n",
      "Iteration 3100, Loss: 0.6840\n",
      "Iteration 3200, Loss: 0.6819\n",
      "Iteration 3300, Loss: 0.6799\n",
      "Iteration 3400, Loss: 0.6779\n",
      "Iteration 3500, Loss: 0.6760\n",
      "Iteration 3600, Loss: 0.6742\n",
      "Iteration 3700, Loss: 0.6725\n",
      "Iteration 3800, Loss: 0.6708\n",
      "Iteration 3900, Loss: 0.6692\n",
      "Iteration 4000, Loss: 0.6677\n",
      "Iteration 4100, Loss: 0.6662\n",
      "Iteration 4200, Loss: 0.6647\n",
      "Iteration 4300, Loss: 0.6633\n",
      "Iteration 4400, Loss: 0.6620\n",
      "Iteration 4500, Loss: 0.6606\n",
      "Iteration 4600, Loss: 0.6594\n",
      "Iteration 4700, Loss: 0.6581\n",
      "Iteration 4800, Loss: 0.6569\n",
      "Iteration 4900, Loss: 0.6557\n",
      "Iteration 5000, Loss: 0.6546\n",
      "Iteration 5100, Loss: 0.6534\n",
      "Iteration 5200, Loss: 0.6523\n",
      "Iteration 5300, Loss: 0.6513\n",
      "Iteration 5400, Loss: 0.6502\n",
      "Iteration 5500, Loss: 0.6492\n",
      "Iteration 5600, Loss: 0.6482\n",
      "Iteration 5700, Loss: 0.6473\n",
      "Iteration 5800, Loss: 0.6463\n",
      "Iteration 5900, Loss: 0.6454\n",
      "249 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0264\n",
      "Iteration 200, Loss: 0.9762\n",
      "Iteration 300, Loss: 0.9380\n",
      "Iteration 400, Loss: 0.9076\n",
      "Iteration 500, Loss: 0.8828\n",
      "Iteration 600, Loss: 0.8622\n",
      "Iteration 700, Loss: 0.8447\n",
      "Iteration 800, Loss: 0.8297\n",
      "Iteration 900, Loss: 0.8167\n",
      "Iteration 1000, Loss: 0.8053\n",
      "Iteration 1100, Loss: 0.7953\n",
      "Iteration 1200, Loss: 0.7864\n",
      "Iteration 1300, Loss: 0.7784\n",
      "Iteration 1400, Loss: 0.7712\n",
      "Iteration 1500, Loss: 0.7647\n",
      "Iteration 1600, Loss: 0.7588\n",
      "Iteration 1700, Loss: 0.7534\n",
      "Iteration 1800, Loss: 0.7485\n",
      "Iteration 1900, Loss: 0.7439\n",
      "Iteration 2000, Loss: 0.7397\n",
      "Iteration 2100, Loss: 0.7357\n",
      "Iteration 2200, Loss: 0.7321\n",
      "Iteration 2300, Loss: 0.7286\n",
      "Iteration 2400, Loss: 0.7254\n",
      "Iteration 2500, Loss: 0.7223\n",
      "Iteration 2600, Loss: 0.7194\n",
      "Iteration 2700, Loss: 0.7167\n",
      "Iteration 2800, Loss: 0.7141\n",
      "Iteration 2900, Loss: 0.7116\n",
      "Iteration 3000, Loss: 0.7093\n",
      "Iteration 3100, Loss: 0.7070\n",
      "Iteration 3200, Loss: 0.7049\n",
      "Iteration 3300, Loss: 0.7028\n",
      "Iteration 3400, Loss: 0.7008\n",
      "Iteration 3500, Loss: 0.6989\n",
      "Iteration 3600, Loss: 0.6970\n",
      "Iteration 3700, Loss: 0.6953\n",
      "Iteration 3800, Loss: 0.6936\n",
      "Iteration 3900, Loss: 0.6919\n",
      "Iteration 4000, Loss: 0.6903\n",
      "Iteration 4100, Loss: 0.6887\n",
      "Iteration 4200, Loss: 0.6872\n",
      "Iteration 4300, Loss: 0.6858\n",
      "Iteration 4400, Loss: 0.6844\n",
      "Iteration 4500, Loss: 0.6830\n",
      "Iteration 4600, Loss: 0.6816\n",
      "Iteration 4700, Loss: 0.6803\n",
      "Iteration 4800, Loss: 0.6791\n",
      "Iteration 4900, Loss: 0.6778\n",
      "Iteration 5000, Loss: 0.6766\n",
      "Iteration 5100, Loss: 0.6755\n",
      "Iteration 5200, Loss: 0.6743\n",
      "Iteration 5300, Loss: 0.6732\n",
      "Iteration 5400, Loss: 0.6721\n",
      "Iteration 5500, Loss: 0.6710\n",
      "Iteration 5600, Loss: 0.6700\n",
      "Iteration 5700, Loss: 0.6689\n",
      "Iteration 5800, Loss: 0.6679\n",
      "Iteration 5900, Loss: 0.6670\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0128\n",
      "Iteration 200, Loss: 0.9541\n",
      "Iteration 300, Loss: 0.9105\n",
      "Iteration 400, Loss: 0.8767\n",
      "Iteration 500, Loss: 0.8496\n",
      "Iteration 600, Loss: 0.8274\n",
      "Iteration 700, Loss: 0.8089\n",
      "Iteration 800, Loss: 0.7932\n",
      "Iteration 900, Loss: 0.7798\n",
      "Iteration 1000, Loss: 0.7681\n",
      "Iteration 1100, Loss: 0.7578\n",
      "Iteration 1200, Loss: 0.7487\n",
      "Iteration 1300, Loss: 0.7406\n",
      "Iteration 1400, Loss: 0.7334\n",
      "Iteration 1500, Loss: 0.7268\n",
      "Iteration 1600, Loss: 0.7209\n",
      "Iteration 1700, Loss: 0.7155\n",
      "Iteration 1800, Loss: 0.7105\n",
      "Iteration 1900, Loss: 0.7059\n",
      "Iteration 2000, Loss: 0.7017\n",
      "Iteration 2100, Loss: 0.6978\n",
      "Iteration 2200, Loss: 0.6941\n",
      "Iteration 2300, Loss: 0.6907\n",
      "Iteration 2400, Loss: 0.6875\n",
      "Iteration 2500, Loss: 0.6845\n",
      "Iteration 2600, Loss: 0.6816\n",
      "Iteration 2700, Loss: 0.6789\n",
      "Iteration 2800, Loss: 0.6764\n",
      "Iteration 2900, Loss: 0.6740\n",
      "Iteration 3000, Loss: 0.6717\n",
      "Iteration 3100, Loss: 0.6695\n",
      "Iteration 3200, Loss: 0.6674\n",
      "Iteration 3300, Loss: 0.6654\n",
      "Iteration 3400, Loss: 0.6635\n",
      "Iteration 3500, Loss: 0.6617\n",
      "Iteration 3600, Loss: 0.6599\n",
      "Iteration 3700, Loss: 0.6582\n",
      "Iteration 3800, Loss: 0.6566\n",
      "Iteration 3900, Loss: 0.6550\n",
      "Iteration 4000, Loss: 0.6535\n",
      "Iteration 4100, Loss: 0.6520\n",
      "Iteration 4200, Loss: 0.6506\n",
      "Iteration 4300, Loss: 0.6492\n",
      "Iteration 4400, Loss: 0.6479\n",
      "Iteration 4500, Loss: 0.6466\n",
      "Iteration 4600, Loss: 0.6454\n",
      "Iteration 4700, Loss: 0.6442\n",
      "Iteration 4800, Loss: 0.6430\n",
      "Iteration 4900, Loss: 0.6418\n",
      "Iteration 5000, Loss: 0.6407\n",
      "Iteration 5100, Loss: 0.6396\n",
      "Iteration 5200, Loss: 0.6386\n",
      "Iteration 5300, Loss: 0.6376\n",
      "Iteration 5400, Loss: 0.6366\n",
      "Iteration 5500, Loss: 0.6356\n",
      "Iteration 5600, Loss: 0.6346\n",
      "Iteration 5700, Loss: 0.6337\n",
      "Iteration 5800, Loss: 0.6328\n",
      "Iteration 5900, Loss: 0.6319\n",
      "250 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0194\n",
      "Iteration 200, Loss: 0.9637\n",
      "Iteration 300, Loss: 0.9212\n",
      "Iteration 400, Loss: 0.8878\n",
      "Iteration 500, Loss: 0.8606\n",
      "Iteration 600, Loss: 0.8381\n",
      "Iteration 700, Loss: 0.8192\n",
      "Iteration 800, Loss: 0.8030\n",
      "Iteration 900, Loss: 0.7890\n",
      "Iteration 1000, Loss: 0.7769\n",
      "Iteration 1100, Loss: 0.7663\n",
      "Iteration 1200, Loss: 0.7568\n",
      "Iteration 1300, Loss: 0.7484\n",
      "Iteration 1400, Loss: 0.7408\n",
      "Iteration 1500, Loss: 0.7340\n",
      "Iteration 1600, Loss: 0.7277\n",
      "Iteration 1700, Loss: 0.7221\n",
      "Iteration 1800, Loss: 0.7169\n",
      "Iteration 1900, Loss: 0.7121\n",
      "Iteration 2000, Loss: 0.7076\n",
      "Iteration 2100, Loss: 0.7035\n",
      "Iteration 2200, Loss: 0.6997\n",
      "Iteration 2300, Loss: 0.6961\n",
      "Iteration 2400, Loss: 0.6927\n",
      "Iteration 2500, Loss: 0.6896\n",
      "Iteration 2600, Loss: 0.6866\n",
      "Iteration 2700, Loss: 0.6837\n",
      "Iteration 2800, Loss: 0.6811\n",
      "Iteration 2900, Loss: 0.6785\n",
      "Iteration 3000, Loss: 0.6761\n",
      "Iteration 3100, Loss: 0.6738\n",
      "Iteration 3200, Loss: 0.6716\n",
      "Iteration 3300, Loss: 0.6695\n",
      "Iteration 3400, Loss: 0.6675\n",
      "Iteration 3500, Loss: 0.6655\n",
      "Iteration 3600, Loss: 0.6637\n",
      "Iteration 3700, Loss: 0.6619\n",
      "Iteration 3800, Loss: 0.6602\n",
      "Iteration 3900, Loss: 0.6585\n",
      "Iteration 4000, Loss: 0.6569\n",
      "Iteration 4100, Loss: 0.6554\n",
      "Iteration 4200, Loss: 0.6539\n",
      "Iteration 4300, Loss: 0.6524\n",
      "Iteration 4400, Loss: 0.6511\n",
      "Iteration 4500, Loss: 0.6497\n",
      "Iteration 4600, Loss: 0.6484\n",
      "Iteration 4700, Loss: 0.6471\n",
      "Iteration 4800, Loss: 0.6459\n",
      "Iteration 4900, Loss: 0.6447\n",
      "Iteration 5000, Loss: 0.6435\n",
      "Iteration 5100, Loss: 0.6424\n",
      "Iteration 5200, Loss: 0.6413\n",
      "Iteration 5300, Loss: 0.6402\n",
      "Iteration 5400, Loss: 0.6392\n",
      "Iteration 5500, Loss: 0.6381\n",
      "Iteration 5600, Loss: 0.6371\n",
      "Iteration 5700, Loss: 0.6362\n",
      "Iteration 5800, Loss: 0.6352\n",
      "Iteration 5900, Loss: 0.6343\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0199\n",
      "Iteration 200, Loss: 0.9669\n",
      "Iteration 300, Loss: 0.9276\n",
      "Iteration 400, Loss: 0.8970\n",
      "Iteration 500, Loss: 0.8724\n",
      "Iteration 600, Loss: 0.8521\n",
      "Iteration 700, Loss: 0.8351\n",
      "Iteration 800, Loss: 0.8207\n",
      "Iteration 900, Loss: 0.8083\n",
      "Iteration 1000, Loss: 0.7974\n",
      "Iteration 1100, Loss: 0.7879\n",
      "Iteration 1200, Loss: 0.7794\n",
      "Iteration 1300, Loss: 0.7718\n",
      "Iteration 1400, Loss: 0.7650\n",
      "Iteration 1500, Loss: 0.7589\n",
      "Iteration 1600, Loss: 0.7533\n",
      "Iteration 1700, Loss: 0.7482\n",
      "Iteration 1800, Loss: 0.7435\n",
      "Iteration 1900, Loss: 0.7391\n",
      "Iteration 2000, Loss: 0.7351\n",
      "Iteration 2100, Loss: 0.7313\n",
      "Iteration 2200, Loss: 0.7278\n",
      "Iteration 2300, Loss: 0.7246\n",
      "Iteration 2400, Loss: 0.7215\n",
      "Iteration 2500, Loss: 0.7186\n",
      "Iteration 2600, Loss: 0.7159\n",
      "Iteration 2700, Loss: 0.7133\n",
      "Iteration 2800, Loss: 0.7108\n",
      "Iteration 2900, Loss: 0.7085\n",
      "Iteration 3000, Loss: 0.7062\n",
      "Iteration 3100, Loss: 0.7041\n",
      "Iteration 3200, Loss: 0.7020\n",
      "Iteration 3300, Loss: 0.7001\n",
      "Iteration 3400, Loss: 0.6982\n",
      "Iteration 3500, Loss: 0.6964\n",
      "Iteration 3600, Loss: 0.6946\n",
      "Iteration 3700, Loss: 0.6929\n",
      "Iteration 3800, Loss: 0.6913\n",
      "Iteration 3900, Loss: 0.6897\n",
      "Iteration 4000, Loss: 0.6882\n",
      "Iteration 4100, Loss: 0.6867\n",
      "Iteration 4200, Loss: 0.6852\n",
      "Iteration 4300, Loss: 0.6838\n",
      "Iteration 4400, Loss: 0.6825\n",
      "Iteration 4500, Loss: 0.6812\n",
      "Iteration 4600, Loss: 0.6799\n",
      "Iteration 4700, Loss: 0.6787\n",
      "Iteration 4800, Loss: 0.6774\n",
      "Iteration 4900, Loss: 0.6763\n",
      "Iteration 5000, Loss: 0.6751\n",
      "Iteration 5100, Loss: 0.6740\n",
      "Iteration 5200, Loss: 0.6729\n",
      "Iteration 5300, Loss: 0.6718\n",
      "Iteration 5400, Loss: 0.6707\n",
      "Iteration 5500, Loss: 0.6697\n",
      "Iteration 5600, Loss: 0.6687\n",
      "Iteration 5700, Loss: 0.6677\n",
      "Iteration 5800, Loss: 0.6668\n",
      "Iteration 5900, Loss: 0.6658\n",
      "251 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0207\n",
      "Iteration 200, Loss: 0.9667\n",
      "Iteration 300, Loss: 0.9260\n",
      "Iteration 400, Loss: 0.8939\n",
      "Iteration 500, Loss: 0.8678\n",
      "Iteration 600, Loss: 0.8461\n",
      "Iteration 700, Loss: 0.8279\n",
      "Iteration 800, Loss: 0.8123\n",
      "Iteration 900, Loss: 0.7988\n",
      "Iteration 1000, Loss: 0.7871\n",
      "Iteration 1100, Loss: 0.7768\n",
      "Iteration 1200, Loss: 0.7677\n",
      "Iteration 1300, Loss: 0.7595\n",
      "Iteration 1400, Loss: 0.7522\n",
      "Iteration 1500, Loss: 0.7455\n",
      "Iteration 1600, Loss: 0.7395\n",
      "Iteration 1700, Loss: 0.7340\n",
      "Iteration 1800, Loss: 0.7290\n",
      "Iteration 1900, Loss: 0.7243\n",
      "Iteration 2000, Loss: 0.7200\n",
      "Iteration 2100, Loss: 0.7160\n",
      "Iteration 2200, Loss: 0.7123\n",
      "Iteration 2300, Loss: 0.7088\n",
      "Iteration 2400, Loss: 0.7055\n",
      "Iteration 2500, Loss: 0.7024\n",
      "Iteration 2600, Loss: 0.6995\n",
      "Iteration 2700, Loss: 0.6967\n",
      "Iteration 2800, Loss: 0.6941\n",
      "Iteration 2900, Loss: 0.6916\n",
      "Iteration 3000, Loss: 0.6892\n",
      "Iteration 3100, Loss: 0.6870\n",
      "Iteration 3200, Loss: 0.6848\n",
      "Iteration 3300, Loss: 0.6827\n",
      "Iteration 3400, Loss: 0.6807\n",
      "Iteration 3500, Loss: 0.6788\n",
      "Iteration 3600, Loss: 0.6770\n",
      "Iteration 3700, Loss: 0.6752\n",
      "Iteration 3800, Loss: 0.6735\n",
      "Iteration 3900, Loss: 0.6719\n",
      "Iteration 4000, Loss: 0.6703\n",
      "Iteration 4100, Loss: 0.6687\n",
      "Iteration 4200, Loss: 0.6672\n",
      "Iteration 4300, Loss: 0.6658\n",
      "Iteration 4400, Loss: 0.6643\n",
      "Iteration 4500, Loss: 0.6630\n",
      "Iteration 4600, Loss: 0.6616\n",
      "Iteration 4700, Loss: 0.6603\n",
      "Iteration 4800, Loss: 0.6591\n",
      "Iteration 4900, Loss: 0.6578\n",
      "Iteration 5000, Loss: 0.6566\n",
      "Iteration 5100, Loss: 0.6555\n",
      "Iteration 5200, Loss: 0.6543\n",
      "Iteration 5300, Loss: 0.6532\n",
      "Iteration 5400, Loss: 0.6521\n",
      "Iteration 5500, Loss: 0.6511\n",
      "Iteration 5600, Loss: 0.6500\n",
      "Iteration 5700, Loss: 0.6490\n",
      "Iteration 5800, Loss: 0.6480\n",
      "Iteration 5900, Loss: 0.6470\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0190\n",
      "Iteration 200, Loss: 0.9644\n",
      "Iteration 300, Loss: 0.9235\n",
      "Iteration 400, Loss: 0.8916\n",
      "Iteration 500, Loss: 0.8658\n",
      "Iteration 600, Loss: 0.8446\n",
      "Iteration 700, Loss: 0.8267\n",
      "Iteration 800, Loss: 0.8115\n",
      "Iteration 900, Loss: 0.7984\n",
      "Iteration 1000, Loss: 0.7869\n",
      "Iteration 1100, Loss: 0.7769\n",
      "Iteration 1200, Loss: 0.7680\n",
      "Iteration 1300, Loss: 0.7600\n",
      "Iteration 1400, Loss: 0.7529\n",
      "Iteration 1500, Loss: 0.7465\n",
      "Iteration 1600, Loss: 0.7406\n",
      "Iteration 1700, Loss: 0.7353\n",
      "Iteration 1800, Loss: 0.7304\n",
      "Iteration 1900, Loss: 0.7258\n",
      "Iteration 2000, Loss: 0.7217\n",
      "Iteration 2100, Loss: 0.7178\n",
      "Iteration 2200, Loss: 0.7142\n",
      "Iteration 2300, Loss: 0.7108\n",
      "Iteration 2400, Loss: 0.7077\n",
      "Iteration 2500, Loss: 0.7047\n",
      "Iteration 2600, Loss: 0.7019\n",
      "Iteration 2700, Loss: 0.6992\n",
      "Iteration 2800, Loss: 0.6967\n",
      "Iteration 2900, Loss: 0.6943\n",
      "Iteration 3000, Loss: 0.6921\n",
      "Iteration 3100, Loss: 0.6899\n",
      "Iteration 3200, Loss: 0.6878\n",
      "Iteration 3300, Loss: 0.6859\n",
      "Iteration 3400, Loss: 0.6840\n",
      "Iteration 3500, Loss: 0.6822\n",
      "Iteration 3600, Loss: 0.6804\n",
      "Iteration 3700, Loss: 0.6787\n",
      "Iteration 3800, Loss: 0.6771\n",
      "Iteration 3900, Loss: 0.6756\n",
      "Iteration 4000, Loss: 0.6740\n",
      "Iteration 4100, Loss: 0.6726\n",
      "Iteration 4200, Loss: 0.6712\n",
      "Iteration 4300, Loss: 0.6698\n",
      "Iteration 4400, Loss: 0.6685\n",
      "Iteration 4500, Loss: 0.6672\n",
      "Iteration 4600, Loss: 0.6660\n",
      "Iteration 4700, Loss: 0.6648\n",
      "Iteration 4800, Loss: 0.6636\n",
      "Iteration 4900, Loss: 0.6625\n",
      "Iteration 5000, Loss: 0.6614\n",
      "Iteration 5100, Loss: 0.6603\n",
      "Iteration 5200, Loss: 0.6593\n",
      "Iteration 5300, Loss: 0.6583\n",
      "Iteration 5400, Loss: 0.6573\n",
      "Iteration 5500, Loss: 0.6563\n",
      "Iteration 5600, Loss: 0.6553\n",
      "Iteration 5700, Loss: 0.6544\n",
      "Iteration 5800, Loss: 0.6535\n",
      "Iteration 5900, Loss: 0.6526\n",
      "252 270\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0169\n",
      "Iteration 200, Loss: 0.9599\n",
      "Iteration 300, Loss: 0.9169\n",
      "Iteration 400, Loss: 0.8830\n",
      "Iteration 500, Loss: 0.8556\n",
      "Iteration 600, Loss: 0.8330\n",
      "Iteration 700, Loss: 0.8140\n",
      "Iteration 800, Loss: 0.7978\n",
      "Iteration 900, Loss: 0.7839\n",
      "Iteration 1000, Loss: 0.7718\n",
      "Iteration 1100, Loss: 0.7612\n",
      "Iteration 1200, Loss: 0.7518\n",
      "Iteration 1300, Loss: 0.7435\n",
      "Iteration 1400, Loss: 0.7360\n",
      "Iteration 1500, Loss: 0.7293\n",
      "Iteration 1600, Loss: 0.7232\n",
      "Iteration 1700, Loss: 0.7176\n",
      "Iteration 1800, Loss: 0.7125\n",
      "Iteration 1900, Loss: 0.7078\n",
      "Iteration 2000, Loss: 0.7035\n",
      "Iteration 2100, Loss: 0.6995\n",
      "Iteration 2200, Loss: 0.6958\n",
      "Iteration 2300, Loss: 0.6923\n",
      "Iteration 2400, Loss: 0.6890\n",
      "Iteration 2500, Loss: 0.6860\n",
      "Iteration 2600, Loss: 0.6831\n",
      "Iteration 2700, Loss: 0.6804\n",
      "Iteration 2800, Loss: 0.6778\n",
      "Iteration 2900, Loss: 0.6754\n",
      "Iteration 3000, Loss: 0.6730\n",
      "Iteration 3100, Loss: 0.6708\n",
      "Iteration 3200, Loss: 0.6687\n",
      "Iteration 3300, Loss: 0.6666\n",
      "Iteration 3400, Loss: 0.6647\n",
      "Iteration 3500, Loss: 0.6628\n",
      "Iteration 3600, Loss: 0.6610\n",
      "Iteration 3700, Loss: 0.6593\n",
      "Iteration 3800, Loss: 0.6577\n",
      "Iteration 3900, Loss: 0.6561\n",
      "Iteration 4000, Loss: 0.6545\n",
      "Iteration 4100, Loss: 0.6530\n",
      "Iteration 4200, Loss: 0.6516\n",
      "Iteration 4300, Loss: 0.6502\n",
      "Iteration 4400, Loss: 0.6488\n",
      "Iteration 4500, Loss: 0.6475\n",
      "Iteration 4600, Loss: 0.6463\n",
      "Iteration 4700, Loss: 0.6450\n",
      "Iteration 4800, Loss: 0.6438\n",
      "Iteration 4900, Loss: 0.6426\n",
      "Iteration 5000, Loss: 0.6415\n",
      "Iteration 5100, Loss: 0.6404\n",
      "Iteration 5200, Loss: 0.6393\n",
      "Iteration 5300, Loss: 0.6383\n",
      "Iteration 5400, Loss: 0.6373\n",
      "Iteration 5500, Loss: 0.6363\n",
      "Iteration 5600, Loss: 0.6353\n",
      "Iteration 5700, Loss: 0.6343\n",
      "Iteration 5800, Loss: 0.6334\n",
      "Iteration 5900, Loss: 0.6325\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0222\n",
      "Iteration 200, Loss: 0.9702\n",
      "Iteration 300, Loss: 0.9315\n",
      "Iteration 400, Loss: 0.9011\n",
      "Iteration 500, Loss: 0.8765\n",
      "Iteration 600, Loss: 0.8562\n",
      "Iteration 700, Loss: 0.8390\n",
      "Iteration 800, Loss: 0.8244\n",
      "Iteration 900, Loss: 0.8117\n",
      "Iteration 1000, Loss: 0.8005\n",
      "Iteration 1100, Loss: 0.7908\n",
      "Iteration 1200, Loss: 0.7821\n",
      "Iteration 1300, Loss: 0.7743\n",
      "Iteration 1400, Loss: 0.7673\n",
      "Iteration 1500, Loss: 0.7609\n",
      "Iteration 1600, Loss: 0.7551\n",
      "Iteration 1700, Loss: 0.7498\n",
      "Iteration 1800, Loss: 0.7449\n",
      "Iteration 1900, Loss: 0.7404\n",
      "Iteration 2000, Loss: 0.7363\n",
      "Iteration 2100, Loss: 0.7324\n",
      "Iteration 2200, Loss: 0.7287\n",
      "Iteration 2300, Loss: 0.7253\n",
      "Iteration 2400, Loss: 0.7221\n",
      "Iteration 2500, Loss: 0.7191\n",
      "Iteration 2600, Loss: 0.7162\n",
      "Iteration 2700, Loss: 0.7135\n",
      "Iteration 2800, Loss: 0.7110\n",
      "Iteration 2900, Loss: 0.7085\n",
      "Iteration 3000, Loss: 0.7062\n",
      "Iteration 3100, Loss: 0.7040\n",
      "Iteration 3200, Loss: 0.7019\n",
      "Iteration 3300, Loss: 0.6998\n",
      "Iteration 3400, Loss: 0.6979\n",
      "Iteration 3500, Loss: 0.6960\n",
      "Iteration 3600, Loss: 0.6942\n",
      "Iteration 3700, Loss: 0.6924\n",
      "Iteration 3800, Loss: 0.6907\n",
      "Iteration 3900, Loss: 0.6891\n",
      "Iteration 4000, Loss: 0.6875\n",
      "Iteration 4100, Loss: 0.6860\n",
      "Iteration 4200, Loss: 0.6845\n",
      "Iteration 4300, Loss: 0.6831\n",
      "Iteration 4400, Loss: 0.6817\n",
      "Iteration 4500, Loss: 0.6803\n",
      "Iteration 4600, Loss: 0.6790\n",
      "Iteration 4700, Loss: 0.6778\n",
      "Iteration 4800, Loss: 0.6765\n",
      "Iteration 4900, Loss: 0.6753\n",
      "Iteration 5000, Loss: 0.6741\n",
      "Iteration 5100, Loss: 0.6730\n",
      "Iteration 5200, Loss: 0.6719\n",
      "Iteration 5300, Loss: 0.6708\n",
      "Iteration 5400, Loss: 0.6697\n",
      "Iteration 5500, Loss: 0.6686\n",
      "Iteration 5600, Loss: 0.6676\n",
      "Iteration 5700, Loss: 0.6666\n",
      "Iteration 5800, Loss: 0.6656\n",
      "Iteration 5900, Loss: 0.6647\n",
      "253 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0244\n",
      "Iteration 200, Loss: 0.9712\n",
      "Iteration 300, Loss: 0.9304\n",
      "Iteration 400, Loss: 0.8978\n",
      "Iteration 500, Loss: 0.8714\n",
      "Iteration 600, Loss: 0.8493\n",
      "Iteration 700, Loss: 0.8307\n",
      "Iteration 800, Loss: 0.8147\n",
      "Iteration 900, Loss: 0.8010\n",
      "Iteration 1000, Loss: 0.7889\n",
      "Iteration 1100, Loss: 0.7784\n",
      "Iteration 1200, Loss: 0.7690\n",
      "Iteration 1300, Loss: 0.7606\n",
      "Iteration 1400, Loss: 0.7532\n",
      "Iteration 1500, Loss: 0.7464\n",
      "Iteration 1600, Loss: 0.7402\n",
      "Iteration 1700, Loss: 0.7346\n",
      "Iteration 1800, Loss: 0.7294\n",
      "Iteration 1900, Loss: 0.7246\n",
      "Iteration 2000, Loss: 0.7202\n",
      "Iteration 2100, Loss: 0.7161\n",
      "Iteration 2200, Loss: 0.7123\n",
      "Iteration 2300, Loss: 0.7087\n",
      "Iteration 2400, Loss: 0.7053\n",
      "Iteration 2500, Loss: 0.7021\n",
      "Iteration 2600, Loss: 0.6991\n",
      "Iteration 2700, Loss: 0.6963\n",
      "Iteration 2800, Loss: 0.6936\n",
      "Iteration 2900, Loss: 0.6910\n",
      "Iteration 3000, Loss: 0.6886\n",
      "Iteration 3100, Loss: 0.6862\n",
      "Iteration 3200, Loss: 0.6840\n",
      "Iteration 3300, Loss: 0.6818\n",
      "Iteration 3400, Loss: 0.6798\n",
      "Iteration 3500, Loss: 0.6778\n",
      "Iteration 3600, Loss: 0.6759\n",
      "Iteration 3700, Loss: 0.6741\n",
      "Iteration 3800, Loss: 0.6723\n",
      "Iteration 3900, Loss: 0.6706\n",
      "Iteration 4000, Loss: 0.6690\n",
      "Iteration 4100, Loss: 0.6674\n",
      "Iteration 4200, Loss: 0.6658\n",
      "Iteration 4300, Loss: 0.6643\n",
      "Iteration 4400, Loss: 0.6629\n",
      "Iteration 4500, Loss: 0.6615\n",
      "Iteration 4600, Loss: 0.6601\n",
      "Iteration 4700, Loss: 0.6588\n",
      "Iteration 4800, Loss: 0.6575\n",
      "Iteration 4900, Loss: 0.6562\n",
      "Iteration 5000, Loss: 0.6550\n",
      "Iteration 5100, Loss: 0.6538\n",
      "Iteration 5200, Loss: 0.6527\n",
      "Iteration 5300, Loss: 0.6515\n",
      "Iteration 5400, Loss: 0.6504\n",
      "Iteration 5500, Loss: 0.6494\n",
      "Iteration 5600, Loss: 0.6483\n",
      "Iteration 5700, Loss: 0.6473\n",
      "Iteration 5800, Loss: 0.6463\n",
      "Iteration 5900, Loss: 0.6453\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0146\n",
      "Iteration 200, Loss: 0.9589\n",
      "Iteration 300, Loss: 0.9177\n",
      "Iteration 400, Loss: 0.8859\n",
      "Iteration 500, Loss: 0.8605\n",
      "Iteration 600, Loss: 0.8396\n",
      "Iteration 700, Loss: 0.8222\n",
      "Iteration 800, Loss: 0.8073\n",
      "Iteration 900, Loss: 0.7945\n",
      "Iteration 1000, Loss: 0.7834\n",
      "Iteration 1100, Loss: 0.7737\n",
      "Iteration 1200, Loss: 0.7651\n",
      "Iteration 1300, Loss: 0.7573\n",
      "Iteration 1400, Loss: 0.7504\n",
      "Iteration 1500, Loss: 0.7442\n",
      "Iteration 1600, Loss: 0.7385\n",
      "Iteration 1700, Loss: 0.7333\n",
      "Iteration 1800, Loss: 0.7285\n",
      "Iteration 1900, Loss: 0.7241\n",
      "Iteration 2000, Loss: 0.7201\n",
      "Iteration 2100, Loss: 0.7163\n",
      "Iteration 2200, Loss: 0.7127\n",
      "Iteration 2300, Loss: 0.7094\n",
      "Iteration 2400, Loss: 0.7063\n",
      "Iteration 2500, Loss: 0.7034\n",
      "Iteration 2600, Loss: 0.7007\n",
      "Iteration 2700, Loss: 0.6981\n",
      "Iteration 2800, Loss: 0.6957\n",
      "Iteration 2900, Loss: 0.6933\n",
      "Iteration 3000, Loss: 0.6911\n",
      "Iteration 3100, Loss: 0.6890\n",
      "Iteration 3200, Loss: 0.6869\n",
      "Iteration 3300, Loss: 0.6850\n",
      "Iteration 3400, Loss: 0.6831\n",
      "Iteration 3500, Loss: 0.6813\n",
      "Iteration 3600, Loss: 0.6796\n",
      "Iteration 3700, Loss: 0.6780\n",
      "Iteration 3800, Loss: 0.6764\n",
      "Iteration 3900, Loss: 0.6749\n",
      "Iteration 4000, Loss: 0.6734\n",
      "Iteration 4100, Loss: 0.6719\n",
      "Iteration 4200, Loss: 0.6705\n",
      "Iteration 4300, Loss: 0.6692\n",
      "Iteration 4400, Loss: 0.6679\n",
      "Iteration 4500, Loss: 0.6666\n",
      "Iteration 4600, Loss: 0.6654\n",
      "Iteration 4700, Loss: 0.6642\n",
      "Iteration 4800, Loss: 0.6630\n",
      "Iteration 4900, Loss: 0.6619\n",
      "Iteration 5000, Loss: 0.6608\n",
      "Iteration 5100, Loss: 0.6597\n",
      "Iteration 5200, Loss: 0.6587\n",
      "Iteration 5300, Loss: 0.6576\n",
      "Iteration 5400, Loss: 0.6566\n",
      "Iteration 5500, Loss: 0.6556\n",
      "Iteration 5600, Loss: 0.6547\n",
      "Iteration 5700, Loss: 0.6538\n",
      "Iteration 5800, Loss: 0.6528\n",
      "Iteration 5900, Loss: 0.6520\n",
      "254 270\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0217\n",
      "Iteration 200, Loss: 0.9677\n",
      "Iteration 300, Loss: 0.9267\n",
      "Iteration 400, Loss: 0.8941\n",
      "Iteration 500, Loss: 0.8677\n",
      "Iteration 600, Loss: 0.8458\n",
      "Iteration 700, Loss: 0.8273\n",
      "Iteration 800, Loss: 0.8115\n",
      "Iteration 900, Loss: 0.7978\n",
      "Iteration 1000, Loss: 0.7858\n",
      "Iteration 1100, Loss: 0.7753\n",
      "Iteration 1200, Loss: 0.7660\n",
      "Iteration 1300, Loss: 0.7577\n",
      "Iteration 1400, Loss: 0.7502\n",
      "Iteration 1500, Loss: 0.7435\n",
      "Iteration 1600, Loss: 0.7374\n",
      "Iteration 1700, Loss: 0.7318\n",
      "Iteration 1800, Loss: 0.7266\n",
      "Iteration 1900, Loss: 0.7219\n",
      "Iteration 2000, Loss: 0.7176\n",
      "Iteration 2100, Loss: 0.7135\n",
      "Iteration 2200, Loss: 0.7097\n",
      "Iteration 2300, Loss: 0.7062\n",
      "Iteration 2400, Loss: 0.7029\n",
      "Iteration 2500, Loss: 0.6998\n",
      "Iteration 2600, Loss: 0.6969\n",
      "Iteration 2700, Loss: 0.6941\n",
      "Iteration 2800, Loss: 0.6914\n",
      "Iteration 2900, Loss: 0.6889\n",
      "Iteration 3000, Loss: 0.6866\n",
      "Iteration 3100, Loss: 0.6843\n",
      "Iteration 3200, Loss: 0.6821\n",
      "Iteration 3300, Loss: 0.6800\n",
      "Iteration 3400, Loss: 0.6780\n",
      "Iteration 3500, Loss: 0.6761\n",
      "Iteration 3600, Loss: 0.6743\n",
      "Iteration 3700, Loss: 0.6725\n",
      "Iteration 3800, Loss: 0.6708\n",
      "Iteration 3900, Loss: 0.6691\n",
      "Iteration 4000, Loss: 0.6675\n",
      "Iteration 4100, Loss: 0.6660\n",
      "Iteration 4200, Loss: 0.6645\n",
      "Iteration 4300, Loss: 0.6630\n",
      "Iteration 4400, Loss: 0.6616\n",
      "Iteration 4500, Loss: 0.6602\n",
      "Iteration 4600, Loss: 0.6589\n",
      "Iteration 4700, Loss: 0.6576\n",
      "Iteration 4800, Loss: 0.6563\n",
      "Iteration 4900, Loss: 0.6551\n",
      "Iteration 5000, Loss: 0.6539\n",
      "Iteration 5100, Loss: 0.6527\n",
      "Iteration 5200, Loss: 0.6516\n",
      "Iteration 5300, Loss: 0.6505\n",
      "Iteration 5400, Loss: 0.6494\n",
      "Iteration 5500, Loss: 0.6483\n",
      "Iteration 5600, Loss: 0.6473\n",
      "Iteration 5700, Loss: 0.6463\n",
      "Iteration 5800, Loss: 0.6453\n",
      "Iteration 5900, Loss: 0.6443\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0172\n",
      "Iteration 200, Loss: 0.9621\n",
      "Iteration 300, Loss: 0.9214\n",
      "Iteration 400, Loss: 0.8897\n",
      "Iteration 500, Loss: 0.8643\n",
      "Iteration 600, Loss: 0.8434\n",
      "Iteration 700, Loss: 0.8258\n",
      "Iteration 800, Loss: 0.8108\n",
      "Iteration 900, Loss: 0.7980\n",
      "Iteration 1000, Loss: 0.7868\n",
      "Iteration 1100, Loss: 0.7769\n",
      "Iteration 1200, Loss: 0.7682\n",
      "Iteration 1300, Loss: 0.7604\n",
      "Iteration 1400, Loss: 0.7533\n",
      "Iteration 1500, Loss: 0.7470\n",
      "Iteration 1600, Loss: 0.7412\n",
      "Iteration 1700, Loss: 0.7359\n",
      "Iteration 1800, Loss: 0.7310\n",
      "Iteration 1900, Loss: 0.7265\n",
      "Iteration 2000, Loss: 0.7223\n",
      "Iteration 2100, Loss: 0.7184\n",
      "Iteration 2200, Loss: 0.7148\n",
      "Iteration 2300, Loss: 0.7114\n",
      "Iteration 2400, Loss: 0.7082\n",
      "Iteration 2500, Loss: 0.7052\n",
      "Iteration 2600, Loss: 0.7023\n",
      "Iteration 2700, Loss: 0.6996\n",
      "Iteration 2800, Loss: 0.6970\n",
      "Iteration 2900, Loss: 0.6946\n",
      "Iteration 3000, Loss: 0.6922\n",
      "Iteration 3100, Loss: 0.6900\n",
      "Iteration 3200, Loss: 0.6879\n",
      "Iteration 3300, Loss: 0.6858\n",
      "Iteration 3400, Loss: 0.6839\n",
      "Iteration 3500, Loss: 0.6820\n",
      "Iteration 3600, Loss: 0.6802\n",
      "Iteration 3700, Loss: 0.6785\n",
      "Iteration 3800, Loss: 0.6768\n",
      "Iteration 3900, Loss: 0.6752\n",
      "Iteration 4000, Loss: 0.6736\n",
      "Iteration 4100, Loss: 0.6721\n",
      "Iteration 4200, Loss: 0.6706\n",
      "Iteration 4300, Loss: 0.6692\n",
      "Iteration 4400, Loss: 0.6678\n",
      "Iteration 4500, Loss: 0.6665\n",
      "Iteration 4600, Loss: 0.6652\n",
      "Iteration 4700, Loss: 0.6639\n",
      "Iteration 4800, Loss: 0.6627\n",
      "Iteration 4900, Loss: 0.6615\n",
      "Iteration 5000, Loss: 0.6603\n",
      "Iteration 5100, Loss: 0.6592\n",
      "Iteration 5200, Loss: 0.6581\n",
      "Iteration 5300, Loss: 0.6570\n",
      "Iteration 5400, Loss: 0.6559\n",
      "Iteration 5500, Loss: 0.6549\n",
      "Iteration 5600, Loss: 0.6539\n",
      "Iteration 5700, Loss: 0.6529\n",
      "Iteration 5800, Loss: 0.6520\n",
      "Iteration 5900, Loss: 0.6510\n",
      "255 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0547\n",
      "Iteration 200, Loss: 1.0190\n",
      "Iteration 300, Loss: 0.9889\n",
      "Iteration 400, Loss: 0.9634\n",
      "Iteration 500, Loss: 0.9411\n",
      "Iteration 600, Loss: 0.9215\n",
      "Iteration 700, Loss: 0.9042\n",
      "Iteration 800, Loss: 0.8887\n",
      "Iteration 900, Loss: 0.8747\n",
      "Iteration 1000, Loss: 0.8620\n",
      "Iteration 1100, Loss: 0.8505\n",
      "Iteration 1200, Loss: 0.8400\n",
      "Iteration 1300, Loss: 0.8303\n",
      "Iteration 1400, Loss: 0.8215\n",
      "Iteration 1500, Loss: 0.8134\n",
      "Iteration 1600, Loss: 0.8058\n",
      "Iteration 1700, Loss: 0.7989\n",
      "Iteration 1800, Loss: 0.7923\n",
      "Iteration 1900, Loss: 0.7863\n",
      "Iteration 2000, Loss: 0.7806\n",
      "Iteration 2100, Loss: 0.7753\n",
      "Iteration 2200, Loss: 0.7703\n",
      "Iteration 2300, Loss: 0.7655\n",
      "Iteration 2400, Loss: 0.7611\n",
      "Iteration 2500, Loss: 0.7570\n",
      "Iteration 2600, Loss: 0.7530\n",
      "Iteration 2700, Loss: 0.7493\n",
      "Iteration 2800, Loss: 0.7457\n",
      "Iteration 2900, Loss: 0.7424\n",
      "Iteration 3000, Loss: 0.7392\n",
      "Iteration 3100, Loss: 0.7362\n",
      "Iteration 3200, Loss: 0.7333\n",
      "Iteration 3300, Loss: 0.7305\n",
      "Iteration 3400, Loss: 0.7278\n",
      "Iteration 3500, Loss: 0.7253\n",
      "Iteration 3600, Loss: 0.7229\n",
      "Iteration 3700, Loss: 0.7206\n",
      "Iteration 3800, Loss: 0.7183\n",
      "Iteration 3900, Loss: 0.7162\n",
      "Iteration 4000, Loss: 0.7141\n",
      "Iteration 4100, Loss: 0.7121\n",
      "Iteration 4200, Loss: 0.7102\n",
      "Iteration 4300, Loss: 0.7083\n",
      "Iteration 4400, Loss: 0.7065\n",
      "Iteration 4500, Loss: 0.7047\n",
      "Iteration 4600, Loss: 0.7030\n",
      "Iteration 4700, Loss: 0.7014\n",
      "Iteration 4800, Loss: 0.6998\n",
      "Iteration 4900, Loss: 0.6983\n",
      "Iteration 5000, Loss: 0.6968\n",
      "Iteration 5100, Loss: 0.6954\n",
      "Iteration 5200, Loss: 0.6939\n",
      "Iteration 5300, Loss: 0.6926\n",
      "Iteration 5400, Loss: 0.6912\n",
      "Iteration 5500, Loss: 0.6899\n",
      "Iteration 5600, Loss: 0.6887\n",
      "Iteration 5700, Loss: 0.6875\n",
      "Iteration 5800, Loss: 0.6863\n",
      "Iteration 5900, Loss: 0.6851\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0527\n",
      "Iteration 200, Loss: 1.0162\n",
      "Iteration 300, Loss: 0.9862\n",
      "Iteration 400, Loss: 0.9607\n",
      "Iteration 500, Loss: 0.9388\n",
      "Iteration 600, Loss: 0.9198\n",
      "Iteration 700, Loss: 0.9030\n",
      "Iteration 800, Loss: 0.8880\n",
      "Iteration 900, Loss: 0.8746\n",
      "Iteration 1000, Loss: 0.8625\n",
      "Iteration 1100, Loss: 0.8516\n",
      "Iteration 1200, Loss: 0.8417\n",
      "Iteration 1300, Loss: 0.8325\n",
      "Iteration 1400, Loss: 0.8242\n",
      "Iteration 1500, Loss: 0.8164\n",
      "Iteration 1600, Loss: 0.8093\n",
      "Iteration 1700, Loss: 0.8026\n",
      "Iteration 1800, Loss: 0.7965\n",
      "Iteration 1900, Loss: 0.7908\n",
      "Iteration 2000, Loss: 0.7854\n",
      "Iteration 2100, Loss: 0.7803\n",
      "Iteration 2200, Loss: 0.7756\n",
      "Iteration 2300, Loss: 0.7711\n",
      "Iteration 2400, Loss: 0.7669\n",
      "Iteration 2500, Loss: 0.7630\n",
      "Iteration 2600, Loss: 0.7592\n",
      "Iteration 2700, Loss: 0.7556\n",
      "Iteration 2800, Loss: 0.7522\n",
      "Iteration 2900, Loss: 0.7490\n",
      "Iteration 3000, Loss: 0.7460\n",
      "Iteration 3100, Loss: 0.7430\n",
      "Iteration 3200, Loss: 0.7403\n",
      "Iteration 3300, Loss: 0.7376\n",
      "Iteration 3400, Loss: 0.7351\n",
      "Iteration 3500, Loss: 0.7326\n",
      "Iteration 3600, Loss: 0.7303\n",
      "Iteration 3700, Loss: 0.7280\n",
      "Iteration 3800, Loss: 0.7259\n",
      "Iteration 3900, Loss: 0.7238\n",
      "Iteration 4000, Loss: 0.7218\n",
      "Iteration 4100, Loss: 0.7199\n",
      "Iteration 4200, Loss: 0.7180\n",
      "Iteration 4300, Loss: 0.7162\n",
      "Iteration 4400, Loss: 0.7144\n",
      "Iteration 4500, Loss: 0.7128\n",
      "Iteration 4600, Loss: 0.7111\n",
      "Iteration 4700, Loss: 0.7096\n",
      "Iteration 4800, Loss: 0.7080\n",
      "Iteration 4900, Loss: 0.7065\n",
      "Iteration 5000, Loss: 0.7051\n",
      "Iteration 5100, Loss: 0.7037\n",
      "Iteration 5200, Loss: 0.7023\n",
      "Iteration 5300, Loss: 0.7010\n",
      "Iteration 5400, Loss: 0.6997\n",
      "Iteration 5500, Loss: 0.6984\n",
      "Iteration 5600, Loss: 0.6972\n",
      "Iteration 5700, Loss: 0.6960\n",
      "Iteration 5800, Loss: 0.6948\n",
      "Iteration 5900, Loss: 0.6937\n",
      "256 270\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0574\n",
      "Iteration 200, Loss: 1.0238\n",
      "Iteration 300, Loss: 0.9954\n",
      "Iteration 400, Loss: 0.9708\n",
      "Iteration 500, Loss: 0.9494\n",
      "Iteration 600, Loss: 0.9305\n",
      "Iteration 700, Loss: 0.9137\n",
      "Iteration 800, Loss: 0.8985\n",
      "Iteration 900, Loss: 0.8849\n",
      "Iteration 1000, Loss: 0.8725\n",
      "Iteration 1100, Loss: 0.8612\n",
      "Iteration 1200, Loss: 0.8509\n",
      "Iteration 1300, Loss: 0.8414\n",
      "Iteration 1400, Loss: 0.8327\n",
      "Iteration 1500, Loss: 0.8247\n",
      "Iteration 1600, Loss: 0.8172\n",
      "Iteration 1700, Loss: 0.8103\n",
      "Iteration 1800, Loss: 0.8038\n",
      "Iteration 1900, Loss: 0.7978\n",
      "Iteration 2000, Loss: 0.7922\n",
      "Iteration 2100, Loss: 0.7869\n",
      "Iteration 2200, Loss: 0.7820\n",
      "Iteration 2300, Loss: 0.7773\n",
      "Iteration 2400, Loss: 0.7730\n",
      "Iteration 2500, Loss: 0.7688\n",
      "Iteration 2600, Loss: 0.7649\n",
      "Iteration 2700, Loss: 0.7612\n",
      "Iteration 2800, Loss: 0.7577\n",
      "Iteration 2900, Loss: 0.7543\n",
      "Iteration 3000, Loss: 0.7511\n",
      "Iteration 3100, Loss: 0.7481\n",
      "Iteration 3200, Loss: 0.7452\n",
      "Iteration 3300, Loss: 0.7424\n",
      "Iteration 3400, Loss: 0.7398\n",
      "Iteration 3500, Loss: 0.7372\n",
      "Iteration 3600, Loss: 0.7348\n",
      "Iteration 3700, Loss: 0.7324\n",
      "Iteration 3800, Loss: 0.7301\n",
      "Iteration 3900, Loss: 0.7280\n",
      "Iteration 4000, Loss: 0.7259\n",
      "Iteration 4100, Loss: 0.7238\n",
      "Iteration 4200, Loss: 0.7219\n",
      "Iteration 4300, Loss: 0.7200\n",
      "Iteration 4400, Loss: 0.7181\n",
      "Iteration 4500, Loss: 0.7163\n",
      "Iteration 4600, Loss: 0.7146\n",
      "Iteration 4700, Loss: 0.7129\n",
      "Iteration 4800, Loss: 0.7113\n",
      "Iteration 4900, Loss: 0.7097\n",
      "Iteration 5000, Loss: 0.7081\n",
      "Iteration 5100, Loss: 0.7066\n",
      "Iteration 5200, Loss: 0.7052\n",
      "Iteration 5300, Loss: 0.7038\n",
      "Iteration 5400, Loss: 0.7024\n",
      "Iteration 5500, Loss: 0.7010\n",
      "Iteration 5600, Loss: 0.6997\n",
      "Iteration 5700, Loss: 0.6984\n",
      "Iteration 5800, Loss: 0.6972\n",
      "Iteration 5900, Loss: 0.6959\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0498\n",
      "Iteration 200, Loss: 1.0112\n",
      "Iteration 300, Loss: 0.9797\n",
      "Iteration 400, Loss: 0.9532\n",
      "Iteration 500, Loss: 0.9305\n",
      "Iteration 600, Loss: 0.9108\n",
      "Iteration 700, Loss: 0.8936\n",
      "Iteration 800, Loss: 0.8784\n",
      "Iteration 900, Loss: 0.8648\n",
      "Iteration 1000, Loss: 0.8525\n",
      "Iteration 1100, Loss: 0.8414\n",
      "Iteration 1200, Loss: 0.8314\n",
      "Iteration 1300, Loss: 0.8222\n",
      "Iteration 1400, Loss: 0.8138\n",
      "Iteration 1500, Loss: 0.8060\n",
      "Iteration 1600, Loss: 0.7989\n",
      "Iteration 1700, Loss: 0.7923\n",
      "Iteration 1800, Loss: 0.7862\n",
      "Iteration 1900, Loss: 0.7804\n",
      "Iteration 2000, Loss: 0.7751\n",
      "Iteration 2100, Loss: 0.7701\n",
      "Iteration 2200, Loss: 0.7654\n",
      "Iteration 2300, Loss: 0.7610\n",
      "Iteration 2400, Loss: 0.7568\n",
      "Iteration 2500, Loss: 0.7529\n",
      "Iteration 2600, Loss: 0.7491\n",
      "Iteration 2700, Loss: 0.7456\n",
      "Iteration 2800, Loss: 0.7423\n",
      "Iteration 2900, Loss: 0.7391\n",
      "Iteration 3000, Loss: 0.7361\n",
      "Iteration 3100, Loss: 0.7332\n",
      "Iteration 3200, Loss: 0.7305\n",
      "Iteration 3300, Loss: 0.7279\n",
      "Iteration 3400, Loss: 0.7253\n",
      "Iteration 3500, Loss: 0.7229\n",
      "Iteration 3600, Loss: 0.7206\n",
      "Iteration 3700, Loss: 0.7184\n",
      "Iteration 3800, Loss: 0.7163\n",
      "Iteration 3900, Loss: 0.7143\n",
      "Iteration 4000, Loss: 0.7123\n",
      "Iteration 4100, Loss: 0.7104\n",
      "Iteration 4200, Loss: 0.7086\n",
      "Iteration 4300, Loss: 0.7068\n",
      "Iteration 4400, Loss: 0.7051\n",
      "Iteration 4500, Loss: 0.7035\n",
      "Iteration 4600, Loss: 0.7019\n",
      "Iteration 4700, Loss: 0.7004\n",
      "Iteration 4800, Loss: 0.6989\n",
      "Iteration 4900, Loss: 0.6974\n",
      "Iteration 5000, Loss: 0.6961\n",
      "Iteration 5100, Loss: 0.6947\n",
      "Iteration 5200, Loss: 0.6934\n",
      "Iteration 5300, Loss: 0.6921\n",
      "Iteration 5400, Loss: 0.6909\n",
      "Iteration 5500, Loss: 0.6897\n",
      "Iteration 5600, Loss: 0.6885\n",
      "Iteration 5700, Loss: 0.6873\n",
      "Iteration 5800, Loss: 0.6862\n",
      "Iteration 5900, Loss: 0.6851\n",
      "257 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0530\n",
      "Iteration 200, Loss: 1.0168\n",
      "Iteration 300, Loss: 0.9869\n",
      "Iteration 400, Loss: 0.9616\n",
      "Iteration 500, Loss: 0.9398\n",
      "Iteration 600, Loss: 0.9208\n",
      "Iteration 700, Loss: 0.9042\n",
      "Iteration 800, Loss: 0.8893\n",
      "Iteration 900, Loss: 0.8759\n",
      "Iteration 1000, Loss: 0.8640\n",
      "Iteration 1100, Loss: 0.8530\n",
      "Iteration 1200, Loss: 0.8431\n",
      "Iteration 1300, Loss: 0.8341\n",
      "Iteration 1400, Loss: 0.8258\n",
      "Iteration 1500, Loss: 0.8181\n",
      "Iteration 1600, Loss: 0.8110\n",
      "Iteration 1700, Loss: 0.8045\n",
      "Iteration 1800, Loss: 0.7984\n",
      "Iteration 1900, Loss: 0.7927\n",
      "Iteration 2000, Loss: 0.7874\n",
      "Iteration 2100, Loss: 0.7825\n",
      "Iteration 2200, Loss: 0.7779\n",
      "Iteration 2300, Loss: 0.7735\n",
      "Iteration 2400, Loss: 0.7693\n",
      "Iteration 2500, Loss: 0.7655\n",
      "Iteration 2600, Loss: 0.7618\n",
      "Iteration 2700, Loss: 0.7583\n",
      "Iteration 2800, Loss: 0.7550\n",
      "Iteration 2900, Loss: 0.7518\n",
      "Iteration 3000, Loss: 0.7488\n",
      "Iteration 3100, Loss: 0.7459\n",
      "Iteration 3200, Loss: 0.7432\n",
      "Iteration 3300, Loss: 0.7406\n",
      "Iteration 3400, Loss: 0.7381\n",
      "Iteration 3500, Loss: 0.7357\n",
      "Iteration 3600, Loss: 0.7334\n",
      "Iteration 3700, Loss: 0.7312\n",
      "Iteration 3800, Loss: 0.7291\n",
      "Iteration 3900, Loss: 0.7270\n",
      "Iteration 4000, Loss: 0.7251\n",
      "Iteration 4100, Loss: 0.7232\n",
      "Iteration 4200, Loss: 0.7213\n",
      "Iteration 4300, Loss: 0.7196\n",
      "Iteration 4400, Loss: 0.7179\n",
      "Iteration 4500, Loss: 0.7162\n",
      "Iteration 4600, Loss: 0.7146\n",
      "Iteration 4700, Loss: 0.7131\n",
      "Iteration 4800, Loss: 0.7116\n",
      "Iteration 4900, Loss: 0.7101\n",
      "Iteration 5000, Loss: 0.7087\n",
      "Iteration 5100, Loss: 0.7073\n",
      "Iteration 5200, Loss: 0.7059\n",
      "Iteration 5300, Loss: 0.7046\n",
      "Iteration 5400, Loss: 0.7034\n",
      "Iteration 5500, Loss: 0.7021\n",
      "Iteration 5600, Loss: 0.7009\n",
      "Iteration 5700, Loss: 0.6998\n",
      "Iteration 5800, Loss: 0.6986\n",
      "Iteration 5900, Loss: 0.6975\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0545\n",
      "Iteration 200, Loss: 1.0189\n",
      "Iteration 300, Loss: 0.9890\n",
      "Iteration 400, Loss: 0.9634\n",
      "Iteration 500, Loss: 0.9411\n",
      "Iteration 600, Loss: 0.9215\n",
      "Iteration 700, Loss: 0.9041\n",
      "Iteration 800, Loss: 0.8885\n",
      "Iteration 900, Loss: 0.8746\n",
      "Iteration 1000, Loss: 0.8619\n",
      "Iteration 1100, Loss: 0.8504\n",
      "Iteration 1200, Loss: 0.8398\n",
      "Iteration 1300, Loss: 0.8301\n",
      "Iteration 1400, Loss: 0.8213\n",
      "Iteration 1500, Loss: 0.8131\n",
      "Iteration 1600, Loss: 0.8055\n",
      "Iteration 1700, Loss: 0.7984\n",
      "Iteration 1800, Loss: 0.7919\n",
      "Iteration 1900, Loss: 0.7857\n",
      "Iteration 2000, Loss: 0.7799\n",
      "Iteration 2100, Loss: 0.7746\n",
      "Iteration 2200, Loss: 0.7695\n",
      "Iteration 2300, Loss: 0.7648\n",
      "Iteration 2400, Loss: 0.7603\n",
      "Iteration 2500, Loss: 0.7560\n",
      "Iteration 2600, Loss: 0.7520\n",
      "Iteration 2700, Loss: 0.7482\n",
      "Iteration 2800, Loss: 0.7446\n",
      "Iteration 2900, Loss: 0.7412\n",
      "Iteration 3000, Loss: 0.7379\n",
      "Iteration 3100, Loss: 0.7348\n",
      "Iteration 3200, Loss: 0.7318\n",
      "Iteration 3300, Loss: 0.7290\n",
      "Iteration 3400, Loss: 0.7263\n",
      "Iteration 3500, Loss: 0.7237\n",
      "Iteration 3600, Loss: 0.7212\n",
      "Iteration 3700, Loss: 0.7188\n",
      "Iteration 3800, Loss: 0.7165\n",
      "Iteration 3900, Loss: 0.7143\n",
      "Iteration 4000, Loss: 0.7122\n",
      "Iteration 4100, Loss: 0.7102\n",
      "Iteration 4200, Loss: 0.7082\n",
      "Iteration 4300, Loss: 0.7062\n",
      "Iteration 4400, Loss: 0.7044\n",
      "Iteration 4500, Loss: 0.7026\n",
      "Iteration 4600, Loss: 0.7009\n",
      "Iteration 4700, Loss: 0.6992\n",
      "Iteration 4800, Loss: 0.6976\n",
      "Iteration 4900, Loss: 0.6960\n",
      "Iteration 5000, Loss: 0.6945\n",
      "Iteration 5100, Loss: 0.6930\n",
      "Iteration 5200, Loss: 0.6915\n",
      "Iteration 5300, Loss: 0.6901\n",
      "Iteration 5400, Loss: 0.6887\n",
      "Iteration 5500, Loss: 0.6874\n",
      "Iteration 5600, Loss: 0.6861\n",
      "Iteration 5700, Loss: 0.6848\n",
      "Iteration 5800, Loss: 0.6836\n",
      "Iteration 5900, Loss: 0.6824\n",
      "258 270\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0547\n",
      "Iteration 200, Loss: 1.0196\n",
      "Iteration 300, Loss: 0.9904\n",
      "Iteration 400, Loss: 0.9659\n",
      "Iteration 500, Loss: 0.9447\n",
      "Iteration 600, Loss: 0.9262\n",
      "Iteration 700, Loss: 0.9099\n",
      "Iteration 800, Loss: 0.8952\n",
      "Iteration 900, Loss: 0.8821\n",
      "Iteration 1000, Loss: 0.8702\n",
      "Iteration 1100, Loss: 0.8594\n",
      "Iteration 1200, Loss: 0.8495\n",
      "Iteration 1300, Loss: 0.8405\n",
      "Iteration 1400, Loss: 0.8322\n",
      "Iteration 1500, Loss: 0.8245\n",
      "Iteration 1600, Loss: 0.8174\n",
      "Iteration 1700, Loss: 0.8108\n",
      "Iteration 1800, Loss: 0.8047\n",
      "Iteration 1900, Loss: 0.7989\n",
      "Iteration 2000, Loss: 0.7935\n",
      "Iteration 2100, Loss: 0.7885\n",
      "Iteration 2200, Loss: 0.7837\n",
      "Iteration 2300, Loss: 0.7793\n",
      "Iteration 2400, Loss: 0.7751\n",
      "Iteration 2500, Loss: 0.7711\n",
      "Iteration 2600, Loss: 0.7674\n",
      "Iteration 2700, Loss: 0.7638\n",
      "Iteration 2800, Loss: 0.7604\n",
      "Iteration 2900, Loss: 0.7571\n",
      "Iteration 3000, Loss: 0.7541\n",
      "Iteration 3100, Loss: 0.7512\n",
      "Iteration 3200, Loss: 0.7484\n",
      "Iteration 3300, Loss: 0.7458\n",
      "Iteration 3400, Loss: 0.7432\n",
      "Iteration 3500, Loss: 0.7408\n",
      "Iteration 3600, Loss: 0.7385\n",
      "Iteration 3700, Loss: 0.7362\n",
      "Iteration 3800, Loss: 0.7341\n",
      "Iteration 3900, Loss: 0.7320\n",
      "Iteration 4000, Loss: 0.7300\n",
      "Iteration 4100, Loss: 0.7281\n",
      "Iteration 4200, Loss: 0.7262\n",
      "Iteration 4300, Loss: 0.7244\n",
      "Iteration 4400, Loss: 0.7227\n",
      "Iteration 4500, Loss: 0.7210\n",
      "Iteration 4600, Loss: 0.7194\n",
      "Iteration 4700, Loss: 0.7179\n",
      "Iteration 4800, Loss: 0.7163\n",
      "Iteration 4900, Loss: 0.7149\n",
      "Iteration 5000, Loss: 0.7134\n",
      "Iteration 5100, Loss: 0.7120\n",
      "Iteration 5200, Loss: 0.7107\n",
      "Iteration 5300, Loss: 0.7094\n",
      "Iteration 5400, Loss: 0.7081\n",
      "Iteration 5500, Loss: 0.7068\n",
      "Iteration 5600, Loss: 0.7056\n",
      "Iteration 5700, Loss: 0.7044\n",
      "Iteration 5800, Loss: 0.7033\n",
      "Iteration 5900, Loss: 0.7022\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0530\n",
      "Iteration 200, Loss: 1.0161\n",
      "Iteration 300, Loss: 0.9852\n",
      "Iteration 400, Loss: 0.9589\n",
      "Iteration 500, Loss: 0.9361\n",
      "Iteration 600, Loss: 0.9162\n",
      "Iteration 700, Loss: 0.8986\n",
      "Iteration 800, Loss: 0.8829\n",
      "Iteration 900, Loss: 0.8688\n",
      "Iteration 1000, Loss: 0.8561\n",
      "Iteration 1100, Loss: 0.8447\n",
      "Iteration 1200, Loss: 0.8342\n",
      "Iteration 1300, Loss: 0.8246\n",
      "Iteration 1400, Loss: 0.8157\n",
      "Iteration 1500, Loss: 0.8076\n",
      "Iteration 1600, Loss: 0.8002\n",
      "Iteration 1700, Loss: 0.7932\n",
      "Iteration 1800, Loss: 0.7867\n",
      "Iteration 1900, Loss: 0.7807\n",
      "Iteration 2000, Loss: 0.7751\n",
      "Iteration 2100, Loss: 0.7699\n",
      "Iteration 2200, Loss: 0.7649\n",
      "Iteration 2300, Loss: 0.7603\n",
      "Iteration 2400, Loss: 0.7559\n",
      "Iteration 2500, Loss: 0.7518\n",
      "Iteration 2600, Loss: 0.7479\n",
      "Iteration 2700, Loss: 0.7442\n",
      "Iteration 2800, Loss: 0.7407\n",
      "Iteration 2900, Loss: 0.7373\n",
      "Iteration 3000, Loss: 0.7342\n",
      "Iteration 3100, Loss: 0.7312\n",
      "Iteration 3200, Loss: 0.7283\n",
      "Iteration 3300, Loss: 0.7255\n",
      "Iteration 3400, Loss: 0.7229\n",
      "Iteration 3500, Loss: 0.7204\n",
      "Iteration 3600, Loss: 0.7179\n",
      "Iteration 3700, Loss: 0.7156\n",
      "Iteration 3800, Loss: 0.7133\n",
      "Iteration 3900, Loss: 0.7112\n",
      "Iteration 4000, Loss: 0.7091\n",
      "Iteration 4100, Loss: 0.7071\n",
      "Iteration 4200, Loss: 0.7052\n",
      "Iteration 4300, Loss: 0.7033\n",
      "Iteration 4400, Loss: 0.7015\n",
      "Iteration 4500, Loss: 0.6997\n",
      "Iteration 4600, Loss: 0.6980\n",
      "Iteration 4700, Loss: 0.6964\n",
      "Iteration 4800, Loss: 0.6948\n",
      "Iteration 4900, Loss: 0.6932\n",
      "Iteration 5000, Loss: 0.6918\n",
      "Iteration 5100, Loss: 0.6903\n",
      "Iteration 5200, Loss: 0.6889\n",
      "Iteration 5300, Loss: 0.6875\n",
      "Iteration 5400, Loss: 0.6861\n",
      "Iteration 5500, Loss: 0.6848\n",
      "Iteration 5600, Loss: 0.6835\n",
      "Iteration 5700, Loss: 0.6823\n",
      "Iteration 5800, Loss: 0.6811\n",
      "Iteration 5900, Loss: 0.6799\n",
      "259 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0535\n",
      "Iteration 200, Loss: 1.0166\n",
      "Iteration 300, Loss: 0.9856\n",
      "Iteration 400, Loss: 0.9589\n",
      "Iteration 500, Loss: 0.9357\n",
      "Iteration 600, Loss: 0.9152\n",
      "Iteration 700, Loss: 0.8970\n",
      "Iteration 800, Loss: 0.8808\n",
      "Iteration 900, Loss: 0.8663\n",
      "Iteration 1000, Loss: 0.8531\n",
      "Iteration 1100, Loss: 0.8411\n",
      "Iteration 1200, Loss: 0.8302\n",
      "Iteration 1300, Loss: 0.8201\n",
      "Iteration 1400, Loss: 0.8108\n",
      "Iteration 1500, Loss: 0.8023\n",
      "Iteration 1600, Loss: 0.7944\n",
      "Iteration 1700, Loss: 0.7871\n",
      "Iteration 1800, Loss: 0.7803\n",
      "Iteration 1900, Loss: 0.7740\n",
      "Iteration 2000, Loss: 0.7680\n",
      "Iteration 2100, Loss: 0.7625\n",
      "Iteration 2200, Loss: 0.7572\n",
      "Iteration 2300, Loss: 0.7523\n",
      "Iteration 2400, Loss: 0.7477\n",
      "Iteration 2500, Loss: 0.7433\n",
      "Iteration 2600, Loss: 0.7392\n",
      "Iteration 2700, Loss: 0.7352\n",
      "Iteration 2800, Loss: 0.7315\n",
      "Iteration 2900, Loss: 0.7280\n",
      "Iteration 3000, Loss: 0.7246\n",
      "Iteration 3100, Loss: 0.7214\n",
      "Iteration 3200, Loss: 0.7183\n",
      "Iteration 3300, Loss: 0.7154\n",
      "Iteration 3400, Loss: 0.7126\n",
      "Iteration 3500, Loss: 0.7099\n",
      "Iteration 3600, Loss: 0.7074\n",
      "Iteration 3700, Loss: 0.7049\n",
      "Iteration 3800, Loss: 0.7025\n",
      "Iteration 3900, Loss: 0.7003\n",
      "Iteration 4000, Loss: 0.6981\n",
      "Iteration 4100, Loss: 0.6960\n",
      "Iteration 4200, Loss: 0.6939\n",
      "Iteration 4300, Loss: 0.6919\n",
      "Iteration 4400, Loss: 0.6900\n",
      "Iteration 4500, Loss: 0.6882\n",
      "Iteration 4600, Loss: 0.6864\n",
      "Iteration 4700, Loss: 0.6847\n",
      "Iteration 4800, Loss: 0.6830\n",
      "Iteration 4900, Loss: 0.6813\n",
      "Iteration 5000, Loss: 0.6797\n",
      "Iteration 5100, Loss: 0.6782\n",
      "Iteration 5200, Loss: 0.6767\n",
      "Iteration 5300, Loss: 0.6752\n",
      "Iteration 5400, Loss: 0.6738\n",
      "Iteration 5500, Loss: 0.6725\n",
      "Iteration 5600, Loss: 0.6711\n",
      "Iteration 5700, Loss: 0.6698\n",
      "Iteration 5800, Loss: 0.6685\n",
      "Iteration 5900, Loss: 0.6673\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0542\n",
      "Iteration 200, Loss: 1.0192\n",
      "Iteration 300, Loss: 0.9905\n",
      "Iteration 400, Loss: 0.9662\n",
      "Iteration 500, Loss: 0.9451\n",
      "Iteration 600, Loss: 0.9269\n",
      "Iteration 700, Loss: 0.9109\n",
      "Iteration 800, Loss: 0.8966\n",
      "Iteration 900, Loss: 0.8839\n",
      "Iteration 1000, Loss: 0.8723\n",
      "Iteration 1100, Loss: 0.8619\n",
      "Iteration 1200, Loss: 0.8525\n",
      "Iteration 1300, Loss: 0.8438\n",
      "Iteration 1400, Loss: 0.8359\n",
      "Iteration 1500, Loss: 0.8285\n",
      "Iteration 1600, Loss: 0.8217\n",
      "Iteration 1700, Loss: 0.8154\n",
      "Iteration 1800, Loss: 0.8096\n",
      "Iteration 1900, Loss: 0.8041\n",
      "Iteration 2000, Loss: 0.7990\n",
      "Iteration 2100, Loss: 0.7943\n",
      "Iteration 2200, Loss: 0.7898\n",
      "Iteration 2300, Loss: 0.7855\n",
      "Iteration 2400, Loss: 0.7815\n",
      "Iteration 2500, Loss: 0.7777\n",
      "Iteration 2600, Loss: 0.7742\n",
      "Iteration 2700, Loss: 0.7708\n",
      "Iteration 2800, Loss: 0.7676\n",
      "Iteration 2900, Loss: 0.7646\n",
      "Iteration 3000, Loss: 0.7617\n",
      "Iteration 3100, Loss: 0.7590\n",
      "Iteration 3200, Loss: 0.7563\n",
      "Iteration 3300, Loss: 0.7538\n",
      "Iteration 3400, Loss: 0.7514\n",
      "Iteration 3500, Loss: 0.7491\n",
      "Iteration 3600, Loss: 0.7469\n",
      "Iteration 3700, Loss: 0.7448\n",
      "Iteration 3800, Loss: 0.7428\n",
      "Iteration 3900, Loss: 0.7409\n",
      "Iteration 4000, Loss: 0.7390\n",
      "Iteration 4100, Loss: 0.7372\n",
      "Iteration 4200, Loss: 0.7354\n",
      "Iteration 4300, Loss: 0.7337\n",
      "Iteration 4400, Loss: 0.7321\n",
      "Iteration 4500, Loss: 0.7305\n",
      "Iteration 4600, Loss: 0.7290\n",
      "Iteration 4700, Loss: 0.7276\n",
      "Iteration 4800, Loss: 0.7261\n",
      "Iteration 4900, Loss: 0.7247\n",
      "Iteration 5000, Loss: 0.7234\n",
      "Iteration 5100, Loss: 0.7221\n",
      "Iteration 5200, Loss: 0.7208\n",
      "Iteration 5300, Loss: 0.7196\n",
      "Iteration 5400, Loss: 0.7184\n",
      "Iteration 5500, Loss: 0.7173\n",
      "Iteration 5600, Loss: 0.7161\n",
      "Iteration 5700, Loss: 0.7150\n",
      "Iteration 5800, Loss: 0.7139\n",
      "Iteration 5900, Loss: 0.7129\n",
      "260 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0561\n",
      "Iteration 200, Loss: 1.0217\n",
      "Iteration 300, Loss: 0.9928\n",
      "Iteration 400, Loss: 0.9683\n",
      "Iteration 500, Loss: 0.9470\n",
      "Iteration 600, Loss: 0.9282\n",
      "Iteration 700, Loss: 0.9115\n",
      "Iteration 800, Loss: 0.8966\n",
      "Iteration 900, Loss: 0.8832\n",
      "Iteration 1000, Loss: 0.8712\n",
      "Iteration 1100, Loss: 0.8603\n",
      "Iteration 1200, Loss: 0.8502\n",
      "Iteration 1300, Loss: 0.8410\n",
      "Iteration 1400, Loss: 0.8326\n",
      "Iteration 1500, Loss: 0.8247\n",
      "Iteration 1600, Loss: 0.8175\n",
      "Iteration 1700, Loss: 0.8109\n",
      "Iteration 1800, Loss: 0.8047\n",
      "Iteration 1900, Loss: 0.7988\n",
      "Iteration 2000, Loss: 0.7934\n",
      "Iteration 2100, Loss: 0.7883\n",
      "Iteration 2200, Loss: 0.7835\n",
      "Iteration 2300, Loss: 0.7790\n",
      "Iteration 2400, Loss: 0.7748\n",
      "Iteration 2500, Loss: 0.7708\n",
      "Iteration 2600, Loss: 0.7670\n",
      "Iteration 2700, Loss: 0.7634\n",
      "Iteration 2800, Loss: 0.7600\n",
      "Iteration 2900, Loss: 0.7568\n",
      "Iteration 3000, Loss: 0.7537\n",
      "Iteration 3100, Loss: 0.7508\n",
      "Iteration 3200, Loss: 0.7480\n",
      "Iteration 3300, Loss: 0.7454\n",
      "Iteration 3400, Loss: 0.7428\n",
      "Iteration 3500, Loss: 0.7404\n",
      "Iteration 3600, Loss: 0.7381\n",
      "Iteration 3700, Loss: 0.7358\n",
      "Iteration 3800, Loss: 0.7337\n",
      "Iteration 3900, Loss: 0.7316\n",
      "Iteration 4000, Loss: 0.7296\n",
      "Iteration 4100, Loss: 0.7277\n",
      "Iteration 4200, Loss: 0.7258\n",
      "Iteration 4300, Loss: 0.7240\n",
      "Iteration 4400, Loss: 0.7223\n",
      "Iteration 4500, Loss: 0.7206\n",
      "Iteration 4600, Loss: 0.7190\n",
      "Iteration 4700, Loss: 0.7174\n",
      "Iteration 4800, Loss: 0.7159\n",
      "Iteration 4900, Loss: 0.7144\n",
      "Iteration 5000, Loss: 0.7130\n",
      "Iteration 5100, Loss: 0.7115\n",
      "Iteration 5200, Loss: 0.7102\n",
      "Iteration 5300, Loss: 0.7088\n",
      "Iteration 5400, Loss: 0.7075\n",
      "Iteration 5500, Loss: 0.7063\n",
      "Iteration 5600, Loss: 0.7050\n",
      "Iteration 5700, Loss: 0.7038\n",
      "Iteration 5800, Loss: 0.7026\n",
      "Iteration 5900, Loss: 0.7015\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0512\n",
      "Iteration 200, Loss: 1.0133\n",
      "Iteration 300, Loss: 0.9820\n",
      "Iteration 400, Loss: 0.9556\n",
      "Iteration 500, Loss: 0.9327\n",
      "Iteration 600, Loss: 0.9129\n",
      "Iteration 700, Loss: 0.8953\n",
      "Iteration 800, Loss: 0.8797\n",
      "Iteration 900, Loss: 0.8657\n",
      "Iteration 1000, Loss: 0.8531\n",
      "Iteration 1100, Loss: 0.8416\n",
      "Iteration 1200, Loss: 0.8313\n",
      "Iteration 1300, Loss: 0.8218\n",
      "Iteration 1400, Loss: 0.8130\n",
      "Iteration 1500, Loss: 0.8050\n",
      "Iteration 1600, Loss: 0.7976\n",
      "Iteration 1700, Loss: 0.7907\n",
      "Iteration 1800, Loss: 0.7843\n",
      "Iteration 1900, Loss: 0.7783\n",
      "Iteration 2000, Loss: 0.7728\n",
      "Iteration 2100, Loss: 0.7676\n",
      "Iteration 2200, Loss: 0.7627\n",
      "Iteration 2300, Loss: 0.7580\n",
      "Iteration 2400, Loss: 0.7537\n",
      "Iteration 2500, Loss: 0.7496\n",
      "Iteration 2600, Loss: 0.7457\n",
      "Iteration 2700, Loss: 0.7420\n",
      "Iteration 2800, Loss: 0.7385\n",
      "Iteration 2900, Loss: 0.7352\n",
      "Iteration 3000, Loss: 0.7320\n",
      "Iteration 3100, Loss: 0.7290\n",
      "Iteration 3200, Loss: 0.7261\n",
      "Iteration 3300, Loss: 0.7233\n",
      "Iteration 3400, Loss: 0.7207\n",
      "Iteration 3500, Loss: 0.7182\n",
      "Iteration 3600, Loss: 0.7157\n",
      "Iteration 3700, Loss: 0.7134\n",
      "Iteration 3800, Loss: 0.7112\n",
      "Iteration 3900, Loss: 0.7090\n",
      "Iteration 4000, Loss: 0.7069\n",
      "Iteration 4100, Loss: 0.7049\n",
      "Iteration 4200, Loss: 0.7030\n",
      "Iteration 4300, Loss: 0.7012\n",
      "Iteration 4400, Loss: 0.6994\n",
      "Iteration 4500, Loss: 0.6976\n",
      "Iteration 4600, Loss: 0.6959\n",
      "Iteration 4700, Loss: 0.6943\n",
      "Iteration 4800, Loss: 0.6927\n",
      "Iteration 4900, Loss: 0.6912\n",
      "Iteration 5000, Loss: 0.6897\n",
      "Iteration 5100, Loss: 0.6882\n",
      "Iteration 5200, Loss: 0.6868\n",
      "Iteration 5300, Loss: 0.6855\n",
      "Iteration 5400, Loss: 0.6841\n",
      "Iteration 5500, Loss: 0.6828\n",
      "Iteration 5600, Loss: 0.6816\n",
      "Iteration 5700, Loss: 0.6803\n",
      "Iteration 5800, Loss: 0.6791\n",
      "Iteration 5900, Loss: 0.6780\n",
      "261 270\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0555\n",
      "Iteration 200, Loss: 1.0201\n",
      "Iteration 300, Loss: 0.9903\n",
      "Iteration 400, Loss: 0.9647\n",
      "Iteration 500, Loss: 0.9424\n",
      "Iteration 600, Loss: 0.9228\n",
      "Iteration 700, Loss: 0.9053\n",
      "Iteration 800, Loss: 0.8896\n",
      "Iteration 900, Loss: 0.8757\n",
      "Iteration 1000, Loss: 0.8630\n",
      "Iteration 1100, Loss: 0.8514\n",
      "Iteration 1200, Loss: 0.8407\n",
      "Iteration 1300, Loss: 0.8310\n",
      "Iteration 1400, Loss: 0.8221\n",
      "Iteration 1500, Loss: 0.8139\n",
      "Iteration 1600, Loss: 0.8062\n",
      "Iteration 1700, Loss: 0.7992\n",
      "Iteration 1800, Loss: 0.7926\n",
      "Iteration 1900, Loss: 0.7864\n",
      "Iteration 2000, Loss: 0.7807\n",
      "Iteration 2100, Loss: 0.7754\n",
      "Iteration 2200, Loss: 0.7703\n",
      "Iteration 2300, Loss: 0.7656\n",
      "Iteration 2400, Loss: 0.7612\n",
      "Iteration 2500, Loss: 0.7570\n",
      "Iteration 2600, Loss: 0.7531\n",
      "Iteration 2700, Loss: 0.7494\n",
      "Iteration 2800, Loss: 0.7458\n",
      "Iteration 2900, Loss: 0.7424\n",
      "Iteration 3000, Loss: 0.7392\n",
      "Iteration 3100, Loss: 0.7361\n",
      "Iteration 3200, Loss: 0.7332\n",
      "Iteration 3300, Loss: 0.7304\n",
      "Iteration 3400, Loss: 0.7277\n",
      "Iteration 3500, Loss: 0.7252\n",
      "Iteration 3600, Loss: 0.7227\n",
      "Iteration 3700, Loss: 0.7204\n",
      "Iteration 3800, Loss: 0.7181\n",
      "Iteration 3900, Loss: 0.7160\n",
      "Iteration 4000, Loss: 0.7138\n",
      "Iteration 4100, Loss: 0.7118\n",
      "Iteration 4200, Loss: 0.7099\n",
      "Iteration 4300, Loss: 0.7080\n",
      "Iteration 4400, Loss: 0.7062\n",
      "Iteration 4500, Loss: 0.7044\n",
      "Iteration 4600, Loss: 0.7027\n",
      "Iteration 4700, Loss: 0.7011\n",
      "Iteration 4800, Loss: 0.6995\n",
      "Iteration 4900, Loss: 0.6980\n",
      "Iteration 5000, Loss: 0.6965\n",
      "Iteration 5100, Loss: 0.6950\n",
      "Iteration 5200, Loss: 0.6936\n",
      "Iteration 5300, Loss: 0.6922\n",
      "Iteration 5400, Loss: 0.6909\n",
      "Iteration 5500, Loss: 0.6895\n",
      "Iteration 5600, Loss: 0.6883\n",
      "Iteration 5700, Loss: 0.6870\n",
      "Iteration 5800, Loss: 0.6858\n",
      "Iteration 5900, Loss: 0.6846\n",
      "Iteration 0, Loss: 1.0980\n",
      "Iteration 100, Loss: 1.0513\n",
      "Iteration 200, Loss: 1.0144\n",
      "Iteration 300, Loss: 0.9840\n",
      "Iteration 400, Loss: 0.9584\n",
      "Iteration 500, Loss: 0.9364\n",
      "Iteration 600, Loss: 0.9172\n",
      "Iteration 700, Loss: 0.9002\n",
      "Iteration 800, Loss: 0.8852\n",
      "Iteration 900, Loss: 0.8717\n",
      "Iteration 1000, Loss: 0.8597\n",
      "Iteration 1100, Loss: 0.8487\n",
      "Iteration 1200, Loss: 0.8388\n",
      "Iteration 1300, Loss: 0.8297\n",
      "Iteration 1400, Loss: 0.8214\n",
      "Iteration 1500, Loss: 0.8138\n",
      "Iteration 1600, Loss: 0.8067\n",
      "Iteration 1700, Loss: 0.8001\n",
      "Iteration 1800, Loss: 0.7940\n",
      "Iteration 1900, Loss: 0.7883\n",
      "Iteration 2000, Loss: 0.7829\n",
      "Iteration 2100, Loss: 0.7779\n",
      "Iteration 2200, Loss: 0.7732\n",
      "Iteration 2300, Loss: 0.7689\n",
      "Iteration 2400, Loss: 0.7647\n",
      "Iteration 2500, Loss: 0.7608\n",
      "Iteration 2600, Loss: 0.7571\n",
      "Iteration 2700, Loss: 0.7536\n",
      "Iteration 2800, Loss: 0.7502\n",
      "Iteration 2900, Loss: 0.7471\n",
      "Iteration 3000, Loss: 0.7441\n",
      "Iteration 3100, Loss: 0.7412\n",
      "Iteration 3200, Loss: 0.7385\n",
      "Iteration 3300, Loss: 0.7359\n",
      "Iteration 3400, Loss: 0.7334\n",
      "Iteration 3500, Loss: 0.7310\n",
      "Iteration 3600, Loss: 0.7287\n",
      "Iteration 3700, Loss: 0.7264\n",
      "Iteration 3800, Loss: 0.7243\n",
      "Iteration 3900, Loss: 0.7223\n",
      "Iteration 4000, Loss: 0.7203\n",
      "Iteration 4100, Loss: 0.7184\n",
      "Iteration 4200, Loss: 0.7165\n",
      "Iteration 4300, Loss: 0.7147\n",
      "Iteration 4400, Loss: 0.7130\n",
      "Iteration 4500, Loss: 0.7113\n",
      "Iteration 4600, Loss: 0.7097\n",
      "Iteration 4700, Loss: 0.7082\n",
      "Iteration 4800, Loss: 0.7067\n",
      "Iteration 4900, Loss: 0.7052\n",
      "Iteration 5000, Loss: 0.7037\n",
      "Iteration 5100, Loss: 0.7023\n",
      "Iteration 5200, Loss: 0.7010\n",
      "Iteration 5300, Loss: 0.6997\n",
      "Iteration 5400, Loss: 0.6984\n",
      "Iteration 5500, Loss: 0.6971\n",
      "Iteration 5600, Loss: 0.6959\n",
      "Iteration 5700, Loss: 0.6947\n",
      "Iteration 5800, Loss: 0.6936\n",
      "Iteration 5900, Loss: 0.6924\n",
      "262 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0533\n",
      "Iteration 200, Loss: 1.0172\n",
      "Iteration 300, Loss: 0.9872\n",
      "Iteration 400, Loss: 0.9617\n",
      "Iteration 500, Loss: 0.9396\n",
      "Iteration 600, Loss: 0.9204\n",
      "Iteration 700, Loss: 0.9034\n",
      "Iteration 800, Loss: 0.8883\n",
      "Iteration 900, Loss: 0.8748\n",
      "Iteration 1000, Loss: 0.8626\n",
      "Iteration 1100, Loss: 0.8515\n",
      "Iteration 1200, Loss: 0.8413\n",
      "Iteration 1300, Loss: 0.8321\n",
      "Iteration 1400, Loss: 0.8236\n",
      "Iteration 1500, Loss: 0.8157\n",
      "Iteration 1600, Loss: 0.8084\n",
      "Iteration 1700, Loss: 0.8017\n",
      "Iteration 1800, Loss: 0.7954\n",
      "Iteration 1900, Loss: 0.7896\n",
      "Iteration 2000, Loss: 0.7841\n",
      "Iteration 2100, Loss: 0.7790\n",
      "Iteration 2200, Loss: 0.7742\n",
      "Iteration 2300, Loss: 0.7696\n",
      "Iteration 2400, Loss: 0.7653\n",
      "Iteration 2500, Loss: 0.7613\n",
      "Iteration 2600, Loss: 0.7574\n",
      "Iteration 2700, Loss: 0.7537\n",
      "Iteration 2800, Loss: 0.7503\n",
      "Iteration 2900, Loss: 0.7470\n",
      "Iteration 3000, Loss: 0.7438\n",
      "Iteration 3100, Loss: 0.7408\n",
      "Iteration 3200, Loss: 0.7379\n",
      "Iteration 3300, Loss: 0.7352\n",
      "Iteration 3400, Loss: 0.7326\n",
      "Iteration 3500, Loss: 0.7300\n",
      "Iteration 3600, Loss: 0.7276\n",
      "Iteration 3700, Loss: 0.7253\n",
      "Iteration 3800, Loss: 0.7230\n",
      "Iteration 3900, Loss: 0.7208\n",
      "Iteration 4000, Loss: 0.7187\n",
      "Iteration 4100, Loss: 0.7167\n",
      "Iteration 4200, Loss: 0.7147\n",
      "Iteration 4300, Loss: 0.7128\n",
      "Iteration 4400, Loss: 0.7110\n",
      "Iteration 4500, Loss: 0.7092\n",
      "Iteration 4600, Loss: 0.7075\n",
      "Iteration 4700, Loss: 0.7058\n",
      "Iteration 4800, Loss: 0.7042\n",
      "Iteration 4900, Loss: 0.7026\n",
      "Iteration 5000, Loss: 0.7011\n",
      "Iteration 5100, Loss: 0.6996\n",
      "Iteration 5200, Loss: 0.6981\n",
      "Iteration 5300, Loss: 0.6967\n",
      "Iteration 5400, Loss: 0.6954\n",
      "Iteration 5500, Loss: 0.6940\n",
      "Iteration 5600, Loss: 0.6927\n",
      "Iteration 5700, Loss: 0.6914\n",
      "Iteration 5800, Loss: 0.6902\n",
      "Iteration 5900, Loss: 0.6889\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0536\n",
      "Iteration 200, Loss: 1.0175\n",
      "Iteration 300, Loss: 0.9873\n",
      "Iteration 400, Loss: 0.9615\n",
      "Iteration 500, Loss: 0.9392\n",
      "Iteration 600, Loss: 0.9195\n",
      "Iteration 700, Loss: 0.9021\n",
      "Iteration 800, Loss: 0.8866\n",
      "Iteration 900, Loss: 0.8726\n",
      "Iteration 1000, Loss: 0.8600\n",
      "Iteration 1100, Loss: 0.8486\n",
      "Iteration 1200, Loss: 0.8381\n",
      "Iteration 1300, Loss: 0.8285\n",
      "Iteration 1400, Loss: 0.8196\n",
      "Iteration 1500, Loss: 0.8115\n",
      "Iteration 1600, Loss: 0.8040\n",
      "Iteration 1700, Loss: 0.7971\n",
      "Iteration 1800, Loss: 0.7907\n",
      "Iteration 1900, Loss: 0.7846\n",
      "Iteration 2000, Loss: 0.7790\n",
      "Iteration 2100, Loss: 0.7737\n",
      "Iteration 2200, Loss: 0.7687\n",
      "Iteration 2300, Loss: 0.7641\n",
      "Iteration 2400, Loss: 0.7597\n",
      "Iteration 2500, Loss: 0.7556\n",
      "Iteration 2600, Loss: 0.7517\n",
      "Iteration 2700, Loss: 0.7480\n",
      "Iteration 2800, Loss: 0.7445\n",
      "Iteration 2900, Loss: 0.7412\n",
      "Iteration 3000, Loss: 0.7380\n",
      "Iteration 3100, Loss: 0.7351\n",
      "Iteration 3200, Loss: 0.7322\n",
      "Iteration 3300, Loss: 0.7295\n",
      "Iteration 3400, Loss: 0.7269\n",
      "Iteration 3500, Loss: 0.7244\n",
      "Iteration 3600, Loss: 0.7220\n",
      "Iteration 3700, Loss: 0.7197\n",
      "Iteration 3800, Loss: 0.7175\n",
      "Iteration 3900, Loss: 0.7154\n",
      "Iteration 4000, Loss: 0.7133\n",
      "Iteration 4100, Loss: 0.7114\n",
      "Iteration 4200, Loss: 0.7095\n",
      "Iteration 4300, Loss: 0.7077\n",
      "Iteration 4400, Loss: 0.7059\n",
      "Iteration 4500, Loss: 0.7042\n",
      "Iteration 4600, Loss: 0.7026\n",
      "Iteration 4700, Loss: 0.7010\n",
      "Iteration 4800, Loss: 0.6995\n",
      "Iteration 4900, Loss: 0.6980\n",
      "Iteration 5000, Loss: 0.6965\n",
      "Iteration 5100, Loss: 0.6951\n",
      "Iteration 5200, Loss: 0.6938\n",
      "Iteration 5300, Loss: 0.6924\n",
      "Iteration 5400, Loss: 0.6912\n",
      "Iteration 5500, Loss: 0.6899\n",
      "Iteration 5600, Loss: 0.6887\n",
      "Iteration 5700, Loss: 0.6875\n",
      "Iteration 5800, Loss: 0.6864\n",
      "Iteration 5900, Loss: 0.6852\n",
      "263 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0494\n",
      "Iteration 200, Loss: 1.0109\n",
      "Iteration 300, Loss: 0.9789\n",
      "Iteration 400, Loss: 0.9523\n",
      "Iteration 500, Loss: 0.9295\n",
      "Iteration 600, Loss: 0.9096\n",
      "Iteration 700, Loss: 0.8922\n",
      "Iteration 800, Loss: 0.8767\n",
      "Iteration 900, Loss: 0.8629\n",
      "Iteration 1000, Loss: 0.8505\n",
      "Iteration 1100, Loss: 0.8392\n",
      "Iteration 1200, Loss: 0.8290\n",
      "Iteration 1300, Loss: 0.8197\n",
      "Iteration 1400, Loss: 0.8111\n",
      "Iteration 1500, Loss: 0.8032\n",
      "Iteration 1600, Loss: 0.7958\n",
      "Iteration 1700, Loss: 0.7890\n",
      "Iteration 1800, Loss: 0.7828\n",
      "Iteration 1900, Loss: 0.7769\n",
      "Iteration 2000, Loss: 0.7714\n",
      "Iteration 2100, Loss: 0.7662\n",
      "Iteration 2200, Loss: 0.7614\n",
      "Iteration 2300, Loss: 0.7568\n",
      "Iteration 2400, Loss: 0.7525\n",
      "Iteration 2500, Loss: 0.7485\n",
      "Iteration 2600, Loss: 0.7446\n",
      "Iteration 2700, Loss: 0.7409\n",
      "Iteration 2800, Loss: 0.7375\n",
      "Iteration 2900, Loss: 0.7342\n",
      "Iteration 3000, Loss: 0.7311\n",
      "Iteration 3100, Loss: 0.7280\n",
      "Iteration 3200, Loss: 0.7252\n",
      "Iteration 3300, Loss: 0.7225\n",
      "Iteration 3400, Loss: 0.7198\n",
      "Iteration 3500, Loss: 0.7173\n",
      "Iteration 3600, Loss: 0.7149\n",
      "Iteration 3700, Loss: 0.7126\n",
      "Iteration 3800, Loss: 0.7104\n",
      "Iteration 3900, Loss: 0.7083\n",
      "Iteration 4000, Loss: 0.7062\n",
      "Iteration 4100, Loss: 0.7042\n",
      "Iteration 4200, Loss: 0.7023\n",
      "Iteration 4300, Loss: 0.7005\n",
      "Iteration 4400, Loss: 0.6987\n",
      "Iteration 4500, Loss: 0.6969\n",
      "Iteration 4600, Loss: 0.6953\n",
      "Iteration 4700, Loss: 0.6937\n",
      "Iteration 4800, Loss: 0.6921\n",
      "Iteration 4900, Loss: 0.6905\n",
      "Iteration 5000, Loss: 0.6891\n",
      "Iteration 5100, Loss: 0.6876\n",
      "Iteration 5200, Loss: 0.6862\n",
      "Iteration 5300, Loss: 0.6849\n",
      "Iteration 5400, Loss: 0.6835\n",
      "Iteration 5500, Loss: 0.6823\n",
      "Iteration 5600, Loss: 0.6810\n",
      "Iteration 5700, Loss: 0.6798\n",
      "Iteration 5800, Loss: 0.6786\n",
      "Iteration 5900, Loss: 0.6774\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0575\n",
      "Iteration 200, Loss: 1.0238\n",
      "Iteration 300, Loss: 0.9952\n",
      "Iteration 400, Loss: 0.9707\n",
      "Iteration 500, Loss: 0.9494\n",
      "Iteration 600, Loss: 0.9306\n",
      "Iteration 700, Loss: 0.9138\n",
      "Iteration 800, Loss: 0.8989\n",
      "Iteration 900, Loss: 0.8854\n",
      "Iteration 1000, Loss: 0.8732\n",
      "Iteration 1100, Loss: 0.8622\n",
      "Iteration 1200, Loss: 0.8520\n",
      "Iteration 1300, Loss: 0.8427\n",
      "Iteration 1400, Loss: 0.8341\n",
      "Iteration 1500, Loss: 0.8262\n",
      "Iteration 1600, Loss: 0.8189\n",
      "Iteration 1700, Loss: 0.8121\n",
      "Iteration 1800, Loss: 0.8058\n",
      "Iteration 1900, Loss: 0.7999\n",
      "Iteration 2000, Loss: 0.7944\n",
      "Iteration 2100, Loss: 0.7893\n",
      "Iteration 2200, Loss: 0.7844\n",
      "Iteration 2300, Loss: 0.7799\n",
      "Iteration 2400, Loss: 0.7756\n",
      "Iteration 2500, Loss: 0.7715\n",
      "Iteration 2600, Loss: 0.7677\n",
      "Iteration 2700, Loss: 0.7641\n",
      "Iteration 2800, Loss: 0.7607\n",
      "Iteration 2900, Loss: 0.7574\n",
      "Iteration 3000, Loss: 0.7543\n",
      "Iteration 3100, Loss: 0.7514\n",
      "Iteration 3200, Loss: 0.7486\n",
      "Iteration 3300, Loss: 0.7458\n",
      "Iteration 3400, Loss: 0.7432\n",
      "Iteration 3500, Loss: 0.7408\n",
      "Iteration 3600, Loss: 0.7384\n",
      "Iteration 3700, Loss: 0.7361\n",
      "Iteration 3800, Loss: 0.7339\n",
      "Iteration 3900, Loss: 0.7318\n",
      "Iteration 4000, Loss: 0.7297\n",
      "Iteration 4100, Loss: 0.7278\n",
      "Iteration 4200, Loss: 0.7259\n",
      "Iteration 4300, Loss: 0.7240\n",
      "Iteration 4400, Loss: 0.7223\n",
      "Iteration 4500, Loss: 0.7205\n",
      "Iteration 4600, Loss: 0.7189\n",
      "Iteration 4700, Loss: 0.7173\n",
      "Iteration 4800, Loss: 0.7157\n",
      "Iteration 4900, Loss: 0.7142\n",
      "Iteration 5000, Loss: 0.7127\n",
      "Iteration 5100, Loss: 0.7112\n",
      "Iteration 5200, Loss: 0.7098\n",
      "Iteration 5300, Loss: 0.7085\n",
      "Iteration 5400, Loss: 0.7071\n",
      "Iteration 5500, Loss: 0.7058\n",
      "Iteration 5600, Loss: 0.7046\n",
      "Iteration 5700, Loss: 0.7033\n",
      "Iteration 5800, Loss: 0.7021\n",
      "Iteration 5900, Loss: 0.7009\n",
      "264 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0501\n",
      "Iteration 200, Loss: 1.0115\n",
      "Iteration 300, Loss: 0.9795\n",
      "Iteration 400, Loss: 0.9524\n",
      "Iteration 500, Loss: 0.9291\n",
      "Iteration 600, Loss: 0.9089\n",
      "Iteration 700, Loss: 0.8911\n",
      "Iteration 800, Loss: 0.8753\n",
      "Iteration 900, Loss: 0.8611\n",
      "Iteration 1000, Loss: 0.8484\n",
      "Iteration 1100, Loss: 0.8369\n",
      "Iteration 1200, Loss: 0.8264\n",
      "Iteration 1300, Loss: 0.8168\n",
      "Iteration 1400, Loss: 0.8080\n",
      "Iteration 1500, Loss: 0.7999\n",
      "Iteration 1600, Loss: 0.7924\n",
      "Iteration 1700, Loss: 0.7854\n",
      "Iteration 1800, Loss: 0.7790\n",
      "Iteration 1900, Loss: 0.7729\n",
      "Iteration 2000, Loss: 0.7673\n",
      "Iteration 2100, Loss: 0.7620\n",
      "Iteration 2200, Loss: 0.7571\n",
      "Iteration 2300, Loss: 0.7524\n",
      "Iteration 2400, Loss: 0.7480\n",
      "Iteration 2500, Loss: 0.7439\n",
      "Iteration 2600, Loss: 0.7400\n",
      "Iteration 2700, Loss: 0.7363\n",
      "Iteration 2800, Loss: 0.7328\n",
      "Iteration 2900, Loss: 0.7294\n",
      "Iteration 3000, Loss: 0.7263\n",
      "Iteration 3100, Loss: 0.7232\n",
      "Iteration 3200, Loss: 0.7204\n",
      "Iteration 3300, Loss: 0.7176\n",
      "Iteration 3400, Loss: 0.7149\n",
      "Iteration 3500, Loss: 0.7124\n",
      "Iteration 3600, Loss: 0.7100\n",
      "Iteration 3700, Loss: 0.7076\n",
      "Iteration 3800, Loss: 0.7054\n",
      "Iteration 3900, Loss: 0.7032\n",
      "Iteration 4000, Loss: 0.7012\n",
      "Iteration 4100, Loss: 0.6992\n",
      "Iteration 4200, Loss: 0.6973\n",
      "Iteration 4300, Loss: 0.6954\n",
      "Iteration 4400, Loss: 0.6936\n",
      "Iteration 4500, Loss: 0.6919\n",
      "Iteration 4600, Loss: 0.6902\n",
      "Iteration 4700, Loss: 0.6886\n",
      "Iteration 4800, Loss: 0.6870\n",
      "Iteration 4900, Loss: 0.6855\n",
      "Iteration 5000, Loss: 0.6840\n",
      "Iteration 5100, Loss: 0.6826\n",
      "Iteration 5200, Loss: 0.6812\n",
      "Iteration 5300, Loss: 0.6798\n",
      "Iteration 5400, Loss: 0.6785\n",
      "Iteration 5500, Loss: 0.6772\n",
      "Iteration 5600, Loss: 0.6760\n",
      "Iteration 5700, Loss: 0.6748\n",
      "Iteration 5800, Loss: 0.6736\n",
      "Iteration 5900, Loss: 0.6724\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0570\n",
      "Iteration 200, Loss: 1.0234\n",
      "Iteration 300, Loss: 0.9953\n",
      "Iteration 400, Loss: 0.9712\n",
      "Iteration 500, Loss: 0.9501\n",
      "Iteration 600, Loss: 0.9317\n",
      "Iteration 700, Loss: 0.9154\n",
      "Iteration 800, Loss: 0.9008\n",
      "Iteration 900, Loss: 0.8876\n",
      "Iteration 1000, Loss: 0.8756\n",
      "Iteration 1100, Loss: 0.8648\n",
      "Iteration 1200, Loss: 0.8547\n",
      "Iteration 1300, Loss: 0.8456\n",
      "Iteration 1400, Loss: 0.8372\n",
      "Iteration 1500, Loss: 0.8294\n",
      "Iteration 1600, Loss: 0.8222\n",
      "Iteration 1700, Loss: 0.8155\n",
      "Iteration 1800, Loss: 0.8093\n",
      "Iteration 1900, Loss: 0.8035\n",
      "Iteration 2000, Loss: 0.7980\n",
      "Iteration 2100, Loss: 0.7929\n",
      "Iteration 2200, Loss: 0.7881\n",
      "Iteration 2300, Loss: 0.7836\n",
      "Iteration 2400, Loss: 0.7793\n",
      "Iteration 2500, Loss: 0.7754\n",
      "Iteration 2600, Loss: 0.7716\n",
      "Iteration 2700, Loss: 0.7680\n",
      "Iteration 2800, Loss: 0.7646\n",
      "Iteration 2900, Loss: 0.7613\n",
      "Iteration 3000, Loss: 0.7583\n",
      "Iteration 3100, Loss: 0.7553\n",
      "Iteration 3200, Loss: 0.7525\n",
      "Iteration 3300, Loss: 0.7498\n",
      "Iteration 3400, Loss: 0.7472\n",
      "Iteration 3500, Loss: 0.7448\n",
      "Iteration 3600, Loss: 0.7425\n",
      "Iteration 3700, Loss: 0.7402\n",
      "Iteration 3800, Loss: 0.7380\n",
      "Iteration 3900, Loss: 0.7359\n",
      "Iteration 4000, Loss: 0.7339\n",
      "Iteration 4100, Loss: 0.7319\n",
      "Iteration 4200, Loss: 0.7300\n",
      "Iteration 4300, Loss: 0.7282\n",
      "Iteration 4400, Loss: 0.7264\n",
      "Iteration 4500, Loss: 0.7247\n",
      "Iteration 4600, Loss: 0.7231\n",
      "Iteration 4700, Loss: 0.7215\n",
      "Iteration 4800, Loss: 0.7199\n",
      "Iteration 4900, Loss: 0.7184\n",
      "Iteration 5000, Loss: 0.7170\n",
      "Iteration 5100, Loss: 0.7156\n",
      "Iteration 5200, Loss: 0.7142\n",
      "Iteration 5300, Loss: 0.7128\n",
      "Iteration 5400, Loss: 0.7115\n",
      "Iteration 5500, Loss: 0.7102\n",
      "Iteration 5600, Loss: 0.7090\n",
      "Iteration 5700, Loss: 0.7077\n",
      "Iteration 5800, Loss: 0.7066\n",
      "Iteration 5900, Loss: 0.7054\n",
      "265 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0523\n",
      "Iteration 200, Loss: 1.0154\n",
      "Iteration 300, Loss: 0.9850\n",
      "Iteration 400, Loss: 0.9593\n",
      "Iteration 500, Loss: 0.9371\n",
      "Iteration 600, Loss: 0.9179\n",
      "Iteration 700, Loss: 0.9009\n",
      "Iteration 800, Loss: 0.8859\n",
      "Iteration 900, Loss: 0.8725\n",
      "Iteration 1000, Loss: 0.8604\n",
      "Iteration 1100, Loss: 0.8495\n",
      "Iteration 1200, Loss: 0.8396\n",
      "Iteration 1300, Loss: 0.8305\n",
      "Iteration 1400, Loss: 0.8221\n",
      "Iteration 1500, Loss: 0.8143\n",
      "Iteration 1600, Loss: 0.8072\n",
      "Iteration 1700, Loss: 0.8006\n",
      "Iteration 1800, Loss: 0.7945\n",
      "Iteration 1900, Loss: 0.7888\n",
      "Iteration 2000, Loss: 0.7835\n",
      "Iteration 2100, Loss: 0.7786\n",
      "Iteration 2200, Loss: 0.7739\n",
      "Iteration 2300, Loss: 0.7695\n",
      "Iteration 2400, Loss: 0.7654\n",
      "Iteration 2500, Loss: 0.7615\n",
      "Iteration 2600, Loss: 0.7578\n",
      "Iteration 2700, Loss: 0.7544\n",
      "Iteration 2800, Loss: 0.7511\n",
      "Iteration 2900, Loss: 0.7479\n",
      "Iteration 3000, Loss: 0.7449\n",
      "Iteration 3100, Loss: 0.7421\n",
      "Iteration 3200, Loss: 0.7395\n",
      "Iteration 3300, Loss: 0.7369\n",
      "Iteration 3400, Loss: 0.7344\n",
      "Iteration 3500, Loss: 0.7321\n",
      "Iteration 3600, Loss: 0.7298\n",
      "Iteration 3700, Loss: 0.7276\n",
      "Iteration 3800, Loss: 0.7256\n",
      "Iteration 3900, Loss: 0.7235\n",
      "Iteration 4000, Loss: 0.7216\n",
      "Iteration 4100, Loss: 0.7198\n",
      "Iteration 4200, Loss: 0.7180\n",
      "Iteration 4300, Loss: 0.7162\n",
      "Iteration 4400, Loss: 0.7145\n",
      "Iteration 4500, Loss: 0.7129\n",
      "Iteration 4600, Loss: 0.7113\n",
      "Iteration 4700, Loss: 0.7098\n",
      "Iteration 4800, Loss: 0.7083\n",
      "Iteration 4900, Loss: 0.7069\n",
      "Iteration 5000, Loss: 0.7055\n",
      "Iteration 5100, Loss: 0.7041\n",
      "Iteration 5200, Loss: 0.7028\n",
      "Iteration 5300, Loss: 0.7015\n",
      "Iteration 5400, Loss: 0.7003\n",
      "Iteration 5500, Loss: 0.6990\n",
      "Iteration 5600, Loss: 0.6978\n",
      "Iteration 5700, Loss: 0.6967\n",
      "Iteration 5800, Loss: 0.6955\n",
      "Iteration 5900, Loss: 0.6944\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0547\n",
      "Iteration 200, Loss: 1.0194\n",
      "Iteration 300, Loss: 0.9899\n",
      "Iteration 400, Loss: 0.9648\n",
      "Iteration 500, Loss: 0.9429\n",
      "Iteration 600, Loss: 0.9236\n",
      "Iteration 700, Loss: 0.9064\n",
      "Iteration 800, Loss: 0.8910\n",
      "Iteration 900, Loss: 0.8772\n",
      "Iteration 1000, Loss: 0.8646\n",
      "Iteration 1100, Loss: 0.8531\n",
      "Iteration 1200, Loss: 0.8426\n",
      "Iteration 1300, Loss: 0.8330\n",
      "Iteration 1400, Loss: 0.8242\n",
      "Iteration 1500, Loss: 0.8160\n",
      "Iteration 1600, Loss: 0.8084\n",
      "Iteration 1700, Loss: 0.8014\n",
      "Iteration 1800, Loss: 0.7949\n",
      "Iteration 1900, Loss: 0.7887\n",
      "Iteration 2000, Loss: 0.7830\n",
      "Iteration 2100, Loss: 0.7777\n",
      "Iteration 2200, Loss: 0.7726\n",
      "Iteration 2300, Loss: 0.7679\n",
      "Iteration 2400, Loss: 0.7634\n",
      "Iteration 2500, Loss: 0.7592\n",
      "Iteration 2600, Loss: 0.7551\n",
      "Iteration 2700, Loss: 0.7514\n",
      "Iteration 2800, Loss: 0.7477\n",
      "Iteration 2900, Loss: 0.7443\n",
      "Iteration 3000, Loss: 0.7410\n",
      "Iteration 3100, Loss: 0.7379\n",
      "Iteration 3200, Loss: 0.7349\n",
      "Iteration 3300, Loss: 0.7321\n",
      "Iteration 3400, Loss: 0.7294\n",
      "Iteration 3500, Loss: 0.7267\n",
      "Iteration 3600, Loss: 0.7243\n",
      "Iteration 3700, Loss: 0.7219\n",
      "Iteration 3800, Loss: 0.7195\n",
      "Iteration 3900, Loss: 0.7173\n",
      "Iteration 4000, Loss: 0.7152\n",
      "Iteration 4100, Loss: 0.7131\n",
      "Iteration 4200, Loss: 0.7111\n",
      "Iteration 4300, Loss: 0.7092\n",
      "Iteration 4400, Loss: 0.7073\n",
      "Iteration 4500, Loss: 0.7055\n",
      "Iteration 4600, Loss: 0.7038\n",
      "Iteration 4700, Loss: 0.7021\n",
      "Iteration 4800, Loss: 0.7004\n",
      "Iteration 4900, Loss: 0.6989\n",
      "Iteration 5000, Loss: 0.6973\n",
      "Iteration 5100, Loss: 0.6958\n",
      "Iteration 5200, Loss: 0.6943\n",
      "Iteration 5300, Loss: 0.6929\n",
      "Iteration 5400, Loss: 0.6915\n",
      "Iteration 5500, Loss: 0.6902\n",
      "Iteration 5600, Loss: 0.6889\n",
      "Iteration 5700, Loss: 0.6876\n",
      "Iteration 5800, Loss: 0.6864\n",
      "Iteration 5900, Loss: 0.6851\n",
      "266 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0549\n",
      "Iteration 200, Loss: 1.0199\n",
      "Iteration 300, Loss: 0.9905\n",
      "Iteration 400, Loss: 0.9652\n",
      "Iteration 500, Loss: 0.9433\n",
      "Iteration 600, Loss: 0.9239\n",
      "Iteration 700, Loss: 0.9068\n",
      "Iteration 800, Loss: 0.8915\n",
      "Iteration 900, Loss: 0.8778\n",
      "Iteration 1000, Loss: 0.8654\n",
      "Iteration 1100, Loss: 0.8541\n",
      "Iteration 1200, Loss: 0.8437\n",
      "Iteration 1300, Loss: 0.8343\n",
      "Iteration 1400, Loss: 0.8256\n",
      "Iteration 1500, Loss: 0.8176\n",
      "Iteration 1600, Loss: 0.8102\n",
      "Iteration 1700, Loss: 0.8033\n",
      "Iteration 1800, Loss: 0.7969\n",
      "Iteration 1900, Loss: 0.7909\n",
      "Iteration 2000, Loss: 0.7853\n",
      "Iteration 2100, Loss: 0.7801\n",
      "Iteration 2200, Loss: 0.7753\n",
      "Iteration 2300, Loss: 0.7707\n",
      "Iteration 2400, Loss: 0.7664\n",
      "Iteration 2500, Loss: 0.7623\n",
      "Iteration 2600, Loss: 0.7585\n",
      "Iteration 2700, Loss: 0.7548\n",
      "Iteration 2800, Loss: 0.7514\n",
      "Iteration 2900, Loss: 0.7481\n",
      "Iteration 3000, Loss: 0.7450\n",
      "Iteration 3100, Loss: 0.7421\n",
      "Iteration 3200, Loss: 0.7393\n",
      "Iteration 3300, Loss: 0.7366\n",
      "Iteration 3400, Loss: 0.7340\n",
      "Iteration 3500, Loss: 0.7315\n",
      "Iteration 3600, Loss: 0.7292\n",
      "Iteration 3700, Loss: 0.7269\n",
      "Iteration 3800, Loss: 0.7247\n",
      "Iteration 3900, Loss: 0.7227\n",
      "Iteration 4000, Loss: 0.7206\n",
      "Iteration 4100, Loss: 0.7187\n",
      "Iteration 4200, Loss: 0.7168\n",
      "Iteration 4300, Loss: 0.7150\n",
      "Iteration 4400, Loss: 0.7133\n",
      "Iteration 4500, Loss: 0.7116\n",
      "Iteration 4600, Loss: 0.7099\n",
      "Iteration 4700, Loss: 0.7083\n",
      "Iteration 4800, Loss: 0.7068\n",
      "Iteration 4900, Loss: 0.7053\n",
      "Iteration 5000, Loss: 0.7038\n",
      "Iteration 5100, Loss: 0.7024\n",
      "Iteration 5200, Loss: 0.7010\n",
      "Iteration 5300, Loss: 0.6997\n",
      "Iteration 5400, Loss: 0.6984\n",
      "Iteration 5500, Loss: 0.6971\n",
      "Iteration 5600, Loss: 0.6959\n",
      "Iteration 5700, Loss: 0.6947\n",
      "Iteration 5800, Loss: 0.6935\n",
      "Iteration 5900, Loss: 0.6924\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0524\n",
      "Iteration 200, Loss: 1.0153\n",
      "Iteration 300, Loss: 0.9847\n",
      "Iteration 400, Loss: 0.9589\n",
      "Iteration 500, Loss: 0.9368\n",
      "Iteration 600, Loss: 0.9175\n",
      "Iteration 700, Loss: 0.9004\n",
      "Iteration 800, Loss: 0.8853\n",
      "Iteration 900, Loss: 0.8718\n",
      "Iteration 1000, Loss: 0.8595\n",
      "Iteration 1100, Loss: 0.8484\n",
      "Iteration 1200, Loss: 0.8382\n",
      "Iteration 1300, Loss: 0.8289\n",
      "Iteration 1400, Loss: 0.8204\n",
      "Iteration 1500, Loss: 0.8125\n",
      "Iteration 1600, Loss: 0.8052\n",
      "Iteration 1700, Loss: 0.7985\n",
      "Iteration 1800, Loss: 0.7922\n",
      "Iteration 1900, Loss: 0.7864\n",
      "Iteration 2000, Loss: 0.7808\n",
      "Iteration 2100, Loss: 0.7757\n",
      "Iteration 2200, Loss: 0.7708\n",
      "Iteration 2300, Loss: 0.7663\n",
      "Iteration 2400, Loss: 0.7620\n",
      "Iteration 2500, Loss: 0.7579\n",
      "Iteration 2600, Loss: 0.7540\n",
      "Iteration 2700, Loss: 0.7504\n",
      "Iteration 2800, Loss: 0.7469\n",
      "Iteration 2900, Loss: 0.7436\n",
      "Iteration 3000, Loss: 0.7404\n",
      "Iteration 3100, Loss: 0.7374\n",
      "Iteration 3200, Loss: 0.7346\n",
      "Iteration 3300, Loss: 0.7318\n",
      "Iteration 3400, Loss: 0.7292\n",
      "Iteration 3500, Loss: 0.7267\n",
      "Iteration 3600, Loss: 0.7243\n",
      "Iteration 3700, Loss: 0.7220\n",
      "Iteration 3800, Loss: 0.7197\n",
      "Iteration 3900, Loss: 0.7176\n",
      "Iteration 4000, Loss: 0.7155\n",
      "Iteration 4100, Loss: 0.7135\n",
      "Iteration 4200, Loss: 0.7116\n",
      "Iteration 4300, Loss: 0.7098\n",
      "Iteration 4400, Loss: 0.7080\n",
      "Iteration 4500, Loss: 0.7062\n",
      "Iteration 4600, Loss: 0.7046\n",
      "Iteration 4700, Loss: 0.7029\n",
      "Iteration 4800, Loss: 0.7014\n",
      "Iteration 4900, Loss: 0.6998\n",
      "Iteration 5000, Loss: 0.6983\n",
      "Iteration 5100, Loss: 0.6969\n",
      "Iteration 5200, Loss: 0.6955\n",
      "Iteration 5300, Loss: 0.6941\n",
      "Iteration 5400, Loss: 0.6928\n",
      "Iteration 5500, Loss: 0.6915\n",
      "Iteration 5600, Loss: 0.6903\n",
      "Iteration 5700, Loss: 0.6890\n",
      "Iteration 5800, Loss: 0.6878\n",
      "Iteration 5900, Loss: 0.6867\n",
      "267 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0534\n",
      "Iteration 200, Loss: 1.0174\n",
      "Iteration 300, Loss: 0.9875\n",
      "Iteration 400, Loss: 0.9622\n",
      "Iteration 500, Loss: 0.9403\n",
      "Iteration 600, Loss: 0.9211\n",
      "Iteration 700, Loss: 0.9043\n",
      "Iteration 800, Loss: 0.8893\n",
      "Iteration 900, Loss: 0.8758\n",
      "Iteration 1000, Loss: 0.8637\n",
      "Iteration 1100, Loss: 0.8527\n",
      "Iteration 1200, Loss: 0.8426\n",
      "Iteration 1300, Loss: 0.8335\n",
      "Iteration 1400, Loss: 0.8251\n",
      "Iteration 1500, Loss: 0.8173\n",
      "Iteration 1600, Loss: 0.8101\n",
      "Iteration 1700, Loss: 0.8034\n",
      "Iteration 1800, Loss: 0.7972\n",
      "Iteration 1900, Loss: 0.7914\n",
      "Iteration 2000, Loss: 0.7860\n",
      "Iteration 2100, Loss: 0.7809\n",
      "Iteration 2200, Loss: 0.7762\n",
      "Iteration 2300, Loss: 0.7717\n",
      "Iteration 2400, Loss: 0.7675\n",
      "Iteration 2500, Loss: 0.7636\n",
      "Iteration 2600, Loss: 0.7598\n",
      "Iteration 2700, Loss: 0.7562\n",
      "Iteration 2800, Loss: 0.7529\n",
      "Iteration 2900, Loss: 0.7497\n",
      "Iteration 3000, Loss: 0.7466\n",
      "Iteration 3100, Loss: 0.7438\n",
      "Iteration 3200, Loss: 0.7410\n",
      "Iteration 3300, Loss: 0.7384\n",
      "Iteration 3400, Loss: 0.7358\n",
      "Iteration 3500, Loss: 0.7334\n",
      "Iteration 3600, Loss: 0.7311\n",
      "Iteration 3700, Loss: 0.7288\n",
      "Iteration 3800, Loss: 0.7267\n",
      "Iteration 3900, Loss: 0.7246\n",
      "Iteration 4000, Loss: 0.7227\n",
      "Iteration 4100, Loss: 0.7208\n",
      "Iteration 4200, Loss: 0.7189\n",
      "Iteration 4300, Loss: 0.7171\n",
      "Iteration 4400, Loss: 0.7154\n",
      "Iteration 4500, Loss: 0.7138\n",
      "Iteration 4600, Loss: 0.7122\n",
      "Iteration 4700, Loss: 0.7106\n",
      "Iteration 4800, Loss: 0.7091\n",
      "Iteration 4900, Loss: 0.7076\n",
      "Iteration 5000, Loss: 0.7062\n",
      "Iteration 5100, Loss: 0.7048\n",
      "Iteration 5200, Loss: 0.7035\n",
      "Iteration 5300, Loss: 0.7022\n",
      "Iteration 5400, Loss: 0.7010\n",
      "Iteration 5500, Loss: 0.6997\n",
      "Iteration 5600, Loss: 0.6985\n",
      "Iteration 5700, Loss: 0.6973\n",
      "Iteration 5800, Loss: 0.6962\n",
      "Iteration 5900, Loss: 0.6951\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0539\n",
      "Iteration 200, Loss: 1.0180\n",
      "Iteration 300, Loss: 0.9879\n",
      "Iteration 400, Loss: 0.9621\n",
      "Iteration 500, Loss: 0.9399\n",
      "Iteration 600, Loss: 0.9204\n",
      "Iteration 700, Loss: 0.9032\n",
      "Iteration 800, Loss: 0.8878\n",
      "Iteration 900, Loss: 0.8739\n",
      "Iteration 1000, Loss: 0.8614\n",
      "Iteration 1100, Loss: 0.8500\n",
      "Iteration 1200, Loss: 0.8396\n",
      "Iteration 1300, Loss: 0.8300\n",
      "Iteration 1400, Loss: 0.8212\n",
      "Iteration 1500, Loss: 0.8131\n",
      "Iteration 1600, Loss: 0.8056\n",
      "Iteration 1700, Loss: 0.7986\n",
      "Iteration 1800, Loss: 0.7921\n",
      "Iteration 1900, Loss: 0.7860\n",
      "Iteration 2000, Loss: 0.7803\n",
      "Iteration 2100, Loss: 0.7750\n",
      "Iteration 2200, Loss: 0.7701\n",
      "Iteration 2300, Loss: 0.7654\n",
      "Iteration 2400, Loss: 0.7610\n",
      "Iteration 2500, Loss: 0.7568\n",
      "Iteration 2600, Loss: 0.7528\n",
      "Iteration 2700, Loss: 0.7491\n",
      "Iteration 2800, Loss: 0.7455\n",
      "Iteration 2900, Loss: 0.7422\n",
      "Iteration 3000, Loss: 0.7389\n",
      "Iteration 3100, Loss: 0.7358\n",
      "Iteration 3200, Loss: 0.7329\n",
      "Iteration 3300, Loss: 0.7301\n",
      "Iteration 3400, Loss: 0.7274\n",
      "Iteration 3500, Loss: 0.7249\n",
      "Iteration 3600, Loss: 0.7224\n",
      "Iteration 3700, Loss: 0.7201\n",
      "Iteration 3800, Loss: 0.7178\n",
      "Iteration 3900, Loss: 0.7156\n",
      "Iteration 4000, Loss: 0.7135\n",
      "Iteration 4100, Loss: 0.7115\n",
      "Iteration 4200, Loss: 0.7095\n",
      "Iteration 4300, Loss: 0.7076\n",
      "Iteration 4400, Loss: 0.7058\n",
      "Iteration 4500, Loss: 0.7041\n",
      "Iteration 4600, Loss: 0.7024\n",
      "Iteration 4700, Loss: 0.7007\n",
      "Iteration 4800, Loss: 0.6991\n",
      "Iteration 4900, Loss: 0.6976\n",
      "Iteration 5000, Loss: 0.6961\n",
      "Iteration 5100, Loss: 0.6946\n",
      "Iteration 5200, Loss: 0.6932\n",
      "Iteration 5300, Loss: 0.6918\n",
      "Iteration 5400, Loss: 0.6904\n",
      "Iteration 5500, Loss: 0.6891\n",
      "Iteration 5600, Loss: 0.6878\n",
      "Iteration 5700, Loss: 0.6866\n",
      "Iteration 5800, Loss: 0.6853\n",
      "Iteration 5900, Loss: 0.6841\n",
      "268 270\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0545\n",
      "Iteration 200, Loss: 1.0189\n",
      "Iteration 300, Loss: 0.9893\n",
      "Iteration 400, Loss: 0.9640\n",
      "Iteration 500, Loss: 0.9421\n",
      "Iteration 600, Loss: 0.9230\n",
      "Iteration 700, Loss: 0.9060\n",
      "Iteration 800, Loss: 0.8909\n",
      "Iteration 900, Loss: 0.8774\n",
      "Iteration 1000, Loss: 0.8652\n",
      "Iteration 1100, Loss: 0.8540\n",
      "Iteration 1200, Loss: 0.8438\n",
      "Iteration 1300, Loss: 0.8345\n",
      "Iteration 1400, Loss: 0.8259\n",
      "Iteration 1500, Loss: 0.8181\n",
      "Iteration 1600, Loss: 0.8108\n",
      "Iteration 1700, Loss: 0.8040\n",
      "Iteration 1800, Loss: 0.7977\n",
      "Iteration 1900, Loss: 0.7918\n",
      "Iteration 2000, Loss: 0.7863\n",
      "Iteration 2100, Loss: 0.7811\n",
      "Iteration 2200, Loss: 0.7763\n",
      "Iteration 2300, Loss: 0.7717\n",
      "Iteration 2400, Loss: 0.7674\n",
      "Iteration 2500, Loss: 0.7633\n",
      "Iteration 2600, Loss: 0.7594\n",
      "Iteration 2700, Loss: 0.7557\n",
      "Iteration 2800, Loss: 0.7523\n",
      "Iteration 2900, Loss: 0.7489\n",
      "Iteration 3000, Loss: 0.7458\n",
      "Iteration 3100, Loss: 0.7428\n",
      "Iteration 3200, Loss: 0.7399\n",
      "Iteration 3300, Loss: 0.7371\n",
      "Iteration 3400, Loss: 0.7345\n",
      "Iteration 3500, Loss: 0.7319\n",
      "Iteration 3600, Loss: 0.7295\n",
      "Iteration 3700, Loss: 0.7272\n",
      "Iteration 3800, Loss: 0.7249\n",
      "Iteration 3900, Loss: 0.7227\n",
      "Iteration 4000, Loss: 0.7206\n",
      "Iteration 4100, Loss: 0.7186\n",
      "Iteration 4200, Loss: 0.7167\n",
      "Iteration 4300, Loss: 0.7148\n",
      "Iteration 4400, Loss: 0.7129\n",
      "Iteration 4500, Loss: 0.7112\n",
      "Iteration 4600, Loss: 0.7094\n",
      "Iteration 4700, Loss: 0.7078\n",
      "Iteration 4800, Loss: 0.7061\n",
      "Iteration 4900, Loss: 0.7046\n",
      "Iteration 5000, Loss: 0.7030\n",
      "Iteration 5100, Loss: 0.7016\n",
      "Iteration 5200, Loss: 0.7001\n",
      "Iteration 5300, Loss: 0.6987\n",
      "Iteration 5400, Loss: 0.6973\n",
      "Iteration 5500, Loss: 0.6960\n",
      "Iteration 5600, Loss: 0.6946\n",
      "Iteration 5700, Loss: 0.6934\n",
      "Iteration 5800, Loss: 0.6921\n",
      "Iteration 5900, Loss: 0.6909\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0526\n",
      "Iteration 200, Loss: 1.0162\n",
      "Iteration 300, Loss: 0.9857\n",
      "Iteration 400, Loss: 0.9600\n",
      "Iteration 500, Loss: 0.9376\n",
      "Iteration 600, Loss: 0.9182\n",
      "Iteration 700, Loss: 0.9009\n",
      "Iteration 800, Loss: 0.8855\n",
      "Iteration 900, Loss: 0.8718\n",
      "Iteration 1000, Loss: 0.8593\n",
      "Iteration 1100, Loss: 0.8480\n",
      "Iteration 1200, Loss: 0.8377\n",
      "Iteration 1300, Loss: 0.8284\n",
      "Iteration 1400, Loss: 0.8197\n",
      "Iteration 1500, Loss: 0.8117\n",
      "Iteration 1600, Loss: 0.8043\n",
      "Iteration 1700, Loss: 0.7974\n",
      "Iteration 1800, Loss: 0.7911\n",
      "Iteration 1900, Loss: 0.7852\n",
      "Iteration 2000, Loss: 0.7796\n",
      "Iteration 2100, Loss: 0.7744\n",
      "Iteration 2200, Loss: 0.7696\n",
      "Iteration 2300, Loss: 0.7650\n",
      "Iteration 2400, Loss: 0.7607\n",
      "Iteration 2500, Loss: 0.7566\n",
      "Iteration 2600, Loss: 0.7528\n",
      "Iteration 2700, Loss: 0.7491\n",
      "Iteration 2800, Loss: 0.7457\n",
      "Iteration 2900, Loss: 0.7424\n",
      "Iteration 3000, Loss: 0.7393\n",
      "Iteration 3100, Loss: 0.7363\n",
      "Iteration 3200, Loss: 0.7335\n",
      "Iteration 3300, Loss: 0.7308\n",
      "Iteration 3400, Loss: 0.7282\n",
      "Iteration 3500, Loss: 0.7257\n",
      "Iteration 3600, Loss: 0.7234\n",
      "Iteration 3700, Loss: 0.7211\n",
      "Iteration 3800, Loss: 0.7190\n",
      "Iteration 3900, Loss: 0.7169\n",
      "Iteration 4000, Loss: 0.7149\n",
      "Iteration 4100, Loss: 0.7129\n",
      "Iteration 4200, Loss: 0.7111\n",
      "Iteration 4300, Loss: 0.7093\n",
      "Iteration 4400, Loss: 0.7076\n",
      "Iteration 4500, Loss: 0.7059\n",
      "Iteration 4600, Loss: 0.7043\n",
      "Iteration 4700, Loss: 0.7027\n",
      "Iteration 4800, Loss: 0.7012\n",
      "Iteration 4900, Loss: 0.6997\n",
      "Iteration 5000, Loss: 0.6983\n",
      "Iteration 5100, Loss: 0.6969\n",
      "Iteration 5200, Loss: 0.6956\n",
      "Iteration 5300, Loss: 0.6942\n",
      "Iteration 5400, Loss: 0.6930\n",
      "Iteration 5500, Loss: 0.6917\n",
      "Iteration 5600, Loss: 0.6905\n",
      "Iteration 5700, Loss: 0.6893\n",
      "Iteration 5800, Loss: 0.6882\n",
      "Iteration 5900, Loss: 0.6871\n",
      "269 270\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0563\n",
      "Iteration 200, Loss: 1.0228\n",
      "Iteration 300, Loss: 0.9948\n",
      "Iteration 400, Loss: 0.9713\n",
      "Iteration 500, Loss: 0.9510\n",
      "Iteration 600, Loss: 0.9332\n",
      "Iteration 700, Loss: 0.9175\n",
      "Iteration 800, Loss: 0.9034\n",
      "Iteration 900, Loss: 0.8907\n",
      "Iteration 1000, Loss: 0.8792\n",
      "Iteration 1100, Loss: 0.8688\n",
      "Iteration 1200, Loss: 0.8593\n",
      "Iteration 1300, Loss: 0.8506\n",
      "Iteration 1400, Loss: 0.8427\n",
      "Iteration 1500, Loss: 0.8353\n",
      "Iteration 1600, Loss: 0.8285\n",
      "Iteration 1700, Loss: 0.8223\n",
      "Iteration 1800, Loss: 0.8164\n",
      "Iteration 1900, Loss: 0.8109\n",
      "Iteration 2000, Loss: 0.8059\n",
      "Iteration 2100, Loss: 0.8011\n",
      "Iteration 2200, Loss: 0.7967\n",
      "Iteration 2300, Loss: 0.7925\n",
      "Iteration 2400, Loss: 0.7886\n",
      "Iteration 2500, Loss: 0.7848\n",
      "Iteration 2600, Loss: 0.7813\n",
      "Iteration 2700, Loss: 0.7780\n",
      "Iteration 2800, Loss: 0.7748\n",
      "Iteration 2900, Loss: 0.7718\n",
      "Iteration 3000, Loss: 0.7690\n",
      "Iteration 3100, Loss: 0.7663\n",
      "Iteration 3200, Loss: 0.7637\n",
      "Iteration 3300, Loss: 0.7612\n",
      "Iteration 3400, Loss: 0.7588\n",
      "Iteration 3500, Loss: 0.7566\n",
      "Iteration 3600, Loss: 0.7544\n",
      "Iteration 3700, Loss: 0.7523\n",
      "Iteration 3800, Loss: 0.7504\n",
      "Iteration 3900, Loss: 0.7484\n",
      "Iteration 4000, Loss: 0.7466\n",
      "Iteration 4100, Loss: 0.7448\n",
      "Iteration 4200, Loss: 0.7431\n",
      "Iteration 4300, Loss: 0.7414\n",
      "Iteration 4400, Loss: 0.7398\n",
      "Iteration 4500, Loss: 0.7383\n",
      "Iteration 4600, Loss: 0.7368\n",
      "Iteration 4700, Loss: 0.7353\n",
      "Iteration 4800, Loss: 0.7339\n",
      "Iteration 4900, Loss: 0.7325\n",
      "Iteration 5000, Loss: 0.7312\n",
      "Iteration 5100, Loss: 0.7299\n",
      "Iteration 5200, Loss: 0.7287\n",
      "Iteration 5300, Loss: 0.7275\n",
      "Iteration 5400, Loss: 0.7263\n",
      "Iteration 5500, Loss: 0.7251\n",
      "Iteration 5600, Loss: 0.7240\n",
      "Iteration 5700, Loss: 0.7229\n",
      "Iteration 5800, Loss: 0.7218\n",
      "Iteration 5900, Loss: 0.7208\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0501\n",
      "Iteration 200, Loss: 1.0110\n",
      "Iteration 300, Loss: 0.9782\n",
      "Iteration 400, Loss: 0.9504\n",
      "Iteration 500, Loss: 0.9263\n",
      "Iteration 600, Loss: 0.9054\n",
      "Iteration 700, Loss: 0.8868\n",
      "Iteration 800, Loss: 0.8702\n",
      "Iteration 900, Loss: 0.8554\n",
      "Iteration 1000, Loss: 0.8420\n",
      "Iteration 1100, Loss: 0.8299\n",
      "Iteration 1200, Loss: 0.8188\n",
      "Iteration 1300, Loss: 0.8088\n",
      "Iteration 1400, Loss: 0.7995\n",
      "Iteration 1500, Loss: 0.7909\n",
      "Iteration 1600, Loss: 0.7830\n",
      "Iteration 1700, Loss: 0.7757\n",
      "Iteration 1800, Loss: 0.7689\n",
      "Iteration 1900, Loss: 0.7625\n",
      "Iteration 2000, Loss: 0.7565\n",
      "Iteration 2100, Loss: 0.7509\n",
      "Iteration 2200, Loss: 0.7457\n",
      "Iteration 2300, Loss: 0.7407\n",
      "Iteration 2400, Loss: 0.7360\n",
      "Iteration 2500, Loss: 0.7317\n",
      "Iteration 2600, Loss: 0.7275\n",
      "Iteration 2700, Loss: 0.7235\n",
      "Iteration 2800, Loss: 0.7198\n",
      "Iteration 2900, Loss: 0.7162\n",
      "Iteration 3000, Loss: 0.7128\n",
      "Iteration 3100, Loss: 0.7096\n",
      "Iteration 3200, Loss: 0.7065\n",
      "Iteration 3300, Loss: 0.7035\n",
      "Iteration 3400, Loss: 0.7007\n",
      "Iteration 3500, Loss: 0.6980\n",
      "Iteration 3600, Loss: 0.6954\n",
      "Iteration 3700, Loss: 0.6929\n",
      "Iteration 3800, Loss: 0.6905\n",
      "Iteration 3900, Loss: 0.6882\n",
      "Iteration 4000, Loss: 0.6860\n",
      "Iteration 4100, Loss: 0.6838\n",
      "Iteration 4200, Loss: 0.6817\n",
      "Iteration 4300, Loss: 0.6797\n",
      "Iteration 4400, Loss: 0.6778\n",
      "Iteration 4500, Loss: 0.6759\n",
      "Iteration 4600, Loss: 0.6740\n",
      "Iteration 4700, Loss: 0.6723\n",
      "Iteration 4800, Loss: 0.6706\n",
      "Iteration 4900, Loss: 0.6689\n",
      "Iteration 5000, Loss: 0.6673\n",
      "Iteration 5100, Loss: 0.6657\n",
      "Iteration 5200, Loss: 0.6641\n",
      "Iteration 5300, Loss: 0.6626\n",
      "Iteration 5400, Loss: 0.6612\n",
      "Iteration 5500, Loss: 0.6597\n",
      "Iteration 5600, Loss: 0.6584\n",
      "Iteration 5700, Loss: 0.6570\n",
      "Iteration 5800, Loss: 0.6557\n",
      "Iteration 5900, Loss: 0.6544\n",
      "{'n_iters': 4000, 'lr': 0.001, 'batch_size': 128, 'l': 0.001, 'alpha': 0.5} 0.7781007751937985\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T21:53:54.269391Z",
     "start_time": "2025-05-22T21:22:35.819544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import product\n",
    "# BASE LOGISTIC REGRESSION WITH L1 L2\n",
    "param_grid = {\n",
    "    \"n_iters\":[4000,5000,6000],\n",
    "    \"lr\":[0.001,0.0001,0.00001],\n",
    "    \"batch_size\":[64,128],\n",
    "    \"l\" :[0.001,0.0001,0.00001],\n",
    "}\n",
    "best = -1\n",
    "best_params = None\n",
    "keys = list(param_grid.keys())\n",
    "combinations = list(product(*param_grid.values()))\n",
    "for i,combo in enumerate(combinations):\n",
    "    print(i, len(combinations))\n",
    "    params = dict(zip(keys,combo))\n",
    "    model = BaseLogisticRegressionRegL2(**params)\n",
    "    mean = make_cross_validation_predict(model,\n",
    "                                           modified_features_preprocessor,\n",
    "                                           X_train, y_train,\n",
    "                                           k=2)[1]\n",
    "    if mean > best:\n",
    "        best = mean\n",
    "        best_params = params\n",
    "print(best_params,best)"
   ],
   "id": "5bf1e11f417b35a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36\n",
      "Iteration 0, Loss: 1.0185\n",
      "Iteration 100, Loss: 0.6062\n",
      "Iteration 200, Loss: 0.5740\n",
      "Iteration 300, Loss: 0.5562\n",
      "Iteration 400, Loss: 0.5443\n",
      "Iteration 500, Loss: 0.5355\n",
      "Iteration 600, Loss: 0.5286\n",
      "Iteration 700, Loss: 0.5229\n",
      "Iteration 800, Loss: 0.5182\n",
      "Iteration 900, Loss: 0.5141\n",
      "Iteration 1000, Loss: 0.5105\n",
      "Iteration 1100, Loss: 0.5073\n",
      "Iteration 1200, Loss: 0.5045\n",
      "Iteration 1300, Loss: 0.5019\n",
      "Iteration 1400, Loss: 0.4995\n",
      "Iteration 1500, Loss: 0.4973\n",
      "Iteration 1600, Loss: 0.4953\n",
      "Iteration 1700, Loss: 0.4934\n",
      "Iteration 1800, Loss: 0.4916\n",
      "Iteration 1900, Loss: 0.4899\n",
      "Iteration 2000, Loss: 0.4884\n",
      "Iteration 2100, Loss: 0.4869\n",
      "Iteration 2200, Loss: 0.4855\n",
      "Iteration 2300, Loss: 0.4842\n",
      "Iteration 2400, Loss: 0.4830\n",
      "Iteration 2500, Loss: 0.4818\n",
      "Iteration 2600, Loss: 0.4806\n",
      "Iteration 2700, Loss: 0.4796\n",
      "Iteration 2800, Loss: 0.4785\n",
      "Iteration 2900, Loss: 0.4775\n",
      "Iteration 3000, Loss: 0.4766\n",
      "Iteration 3100, Loss: 0.4757\n",
      "Iteration 3200, Loss: 0.4748\n",
      "Iteration 3300, Loss: 0.4739\n",
      "Iteration 3400, Loss: 0.4731\n",
      "Iteration 3500, Loss: 0.4723\n",
      "Iteration 3600, Loss: 0.4716\n",
      "Iteration 3700, Loss: 0.4709\n",
      "Iteration 3800, Loss: 0.4702\n",
      "Iteration 3900, Loss: 0.4695\n",
      "Iteration 0, Loss: 1.0214\n",
      "Iteration 100, Loss: 0.6380\n",
      "Iteration 200, Loss: 0.6082\n",
      "Iteration 300, Loss: 0.5911\n",
      "Iteration 400, Loss: 0.5789\n",
      "Iteration 500, Loss: 0.5696\n",
      "Iteration 600, Loss: 0.5621\n",
      "Iteration 700, Loss: 0.5558\n",
      "Iteration 800, Loss: 0.5505\n",
      "Iteration 900, Loss: 0.5458\n",
      "Iteration 1000, Loss: 0.5418\n",
      "Iteration 1100, Loss: 0.5381\n",
      "Iteration 1200, Loss: 0.5348\n",
      "Iteration 1300, Loss: 0.5317\n",
      "Iteration 1400, Loss: 0.5290\n",
      "Iteration 1500, Loss: 0.5264\n",
      "Iteration 1600, Loss: 0.5240\n",
      "Iteration 1700, Loss: 0.5218\n",
      "Iteration 1800, Loss: 0.5197\n",
      "Iteration 1900, Loss: 0.5178\n",
      "Iteration 2000, Loss: 0.5159\n",
      "Iteration 2100, Loss: 0.5142\n",
      "Iteration 2200, Loss: 0.5126\n",
      "Iteration 2300, Loss: 0.5110\n",
      "Iteration 2400, Loss: 0.5095\n",
      "Iteration 2500, Loss: 0.5081\n",
      "Iteration 2600, Loss: 0.5068\n",
      "Iteration 2700, Loss: 0.5055\n",
      "Iteration 2800, Loss: 0.5043\n",
      "Iteration 2900, Loss: 0.5031\n",
      "Iteration 3000, Loss: 0.5019\n",
      "Iteration 3100, Loss: 0.5008\n",
      "Iteration 3200, Loss: 0.4998\n",
      "Iteration 3300, Loss: 0.4988\n",
      "Iteration 3400, Loss: 0.4978\n",
      "Iteration 3500, Loss: 0.4969\n",
      "Iteration 3600, Loss: 0.4960\n",
      "Iteration 3700, Loss: 0.4951\n",
      "Iteration 3800, Loss: 0.4943\n",
      "Iteration 3900, Loss: 0.4934\n",
      "1 36\n",
      "Iteration 0, Loss: 1.0226\n",
      "Iteration 100, Loss: 0.6446\n",
      "Iteration 200, Loss: 0.6159\n",
      "Iteration 300, Loss: 0.5998\n",
      "Iteration 400, Loss: 0.5885\n",
      "Iteration 500, Loss: 0.5798\n",
      "Iteration 600, Loss: 0.5727\n",
      "Iteration 700, Loss: 0.5667\n",
      "Iteration 800, Loss: 0.5616\n",
      "Iteration 900, Loss: 0.5570\n",
      "Iteration 1000, Loss: 0.5529\n",
      "Iteration 1100, Loss: 0.5492\n",
      "Iteration 1200, Loss: 0.5458\n",
      "Iteration 1300, Loss: 0.5427\n",
      "Iteration 1400, Loss: 0.5398\n",
      "Iteration 1500, Loss: 0.5371\n",
      "Iteration 1600, Loss: 0.5346\n",
      "Iteration 1700, Loss: 0.5322\n",
      "Iteration 1800, Loss: 0.5300\n",
      "Iteration 1900, Loss: 0.5279\n",
      "Iteration 2000, Loss: 0.5259\n",
      "Iteration 2100, Loss: 0.5240\n",
      "Iteration 2200, Loss: 0.5222\n",
      "Iteration 2300, Loss: 0.5204\n",
      "Iteration 2400, Loss: 0.5188\n",
      "Iteration 2500, Loss: 0.5172\n",
      "Iteration 2600, Loss: 0.5157\n",
      "Iteration 2700, Loss: 0.5142\n",
      "Iteration 2800, Loss: 0.5128\n",
      "Iteration 2900, Loss: 0.5114\n",
      "Iteration 3000, Loss: 0.5101\n",
      "Iteration 3100, Loss: 0.5089\n",
      "Iteration 3200, Loss: 0.5076\n",
      "Iteration 3300, Loss: 0.5065\n",
      "Iteration 3400, Loss: 0.5053\n",
      "Iteration 3500, Loss: 0.5042\n",
      "Iteration 3600, Loss: 0.5031\n",
      "Iteration 3700, Loss: 0.5021\n",
      "Iteration 3800, Loss: 0.5011\n",
      "Iteration 3900, Loss: 0.5001\n",
      "Iteration 0, Loss: 1.0199\n",
      "Iteration 100, Loss: 0.6005\n",
      "Iteration 200, Loss: 0.5672\n",
      "Iteration 300, Loss: 0.5490\n",
      "Iteration 400, Loss: 0.5365\n",
      "Iteration 500, Loss: 0.5272\n",
      "Iteration 600, Loss: 0.5199\n",
      "Iteration 700, Loss: 0.5138\n",
      "Iteration 800, Loss: 0.5088\n",
      "Iteration 900, Loss: 0.5044\n",
      "Iteration 1000, Loss: 0.5006\n",
      "Iteration 1100, Loss: 0.4973\n",
      "Iteration 1200, Loss: 0.4943\n",
      "Iteration 1300, Loss: 0.4916\n",
      "Iteration 1400, Loss: 0.4891\n",
      "Iteration 1500, Loss: 0.4869\n",
      "Iteration 1600, Loss: 0.4848\n",
      "Iteration 1700, Loss: 0.4829\n",
      "Iteration 1800, Loss: 0.4811\n",
      "Iteration 1900, Loss: 0.4794\n",
      "Iteration 2000, Loss: 0.4778\n",
      "Iteration 2100, Loss: 0.4763\n",
      "Iteration 2200, Loss: 0.4749\n",
      "Iteration 2300, Loss: 0.4735\n",
      "Iteration 2400, Loss: 0.4722\n",
      "Iteration 2500, Loss: 0.4710\n",
      "Iteration 2600, Loss: 0.4698\n",
      "Iteration 2700, Loss: 0.4687\n",
      "Iteration 2800, Loss: 0.4676\n",
      "Iteration 2900, Loss: 0.4665\n",
      "Iteration 3000, Loss: 0.4655\n",
      "Iteration 3100, Loss: 0.4646\n",
      "Iteration 3200, Loss: 0.4636\n",
      "Iteration 3300, Loss: 0.4627\n",
      "Iteration 3400, Loss: 0.4618\n",
      "Iteration 3500, Loss: 0.4610\n",
      "Iteration 3600, Loss: 0.4601\n",
      "Iteration 3700, Loss: 0.4593\n",
      "Iteration 3800, Loss: 0.4585\n",
      "Iteration 3900, Loss: 0.4578\n",
      "2 36\n",
      "Iteration 0, Loss: 1.0543\n",
      "Iteration 100, Loss: 0.6431\n",
      "Iteration 200, Loss: 0.6072\n",
      "Iteration 300, Loss: 0.5890\n",
      "Iteration 400, Loss: 0.5767\n",
      "Iteration 500, Loss: 0.5675\n",
      "Iteration 600, Loss: 0.5601\n",
      "Iteration 700, Loss: 0.5540\n",
      "Iteration 800, Loss: 0.5489\n",
      "Iteration 900, Loss: 0.5443\n",
      "Iteration 1000, Loss: 0.5403\n",
      "Iteration 1100, Loss: 0.5367\n",
      "Iteration 1200, Loss: 0.5335\n",
      "Iteration 1300, Loss: 0.5306\n",
      "Iteration 1400, Loss: 0.5279\n",
      "Iteration 1500, Loss: 0.5253\n",
      "Iteration 1600, Loss: 0.5230\n",
      "Iteration 1700, Loss: 0.5209\n",
      "Iteration 1800, Loss: 0.5189\n",
      "Iteration 1900, Loss: 0.5170\n",
      "Iteration 2000, Loss: 0.5152\n",
      "Iteration 2100, Loss: 0.5135\n",
      "Iteration 2200, Loss: 0.5119\n",
      "Iteration 2300, Loss: 0.5103\n",
      "Iteration 2400, Loss: 0.5088\n",
      "Iteration 2500, Loss: 0.5074\n",
      "Iteration 2600, Loss: 0.5061\n",
      "Iteration 2700, Loss: 0.5048\n",
      "Iteration 2800, Loss: 0.5036\n",
      "Iteration 2900, Loss: 0.5024\n",
      "Iteration 3000, Loss: 0.5013\n",
      "Iteration 3100, Loss: 0.5002\n",
      "Iteration 3200, Loss: 0.4991\n",
      "Iteration 3300, Loss: 0.4981\n",
      "Iteration 3400, Loss: 0.4971\n",
      "Iteration 3500, Loss: 0.4961\n",
      "Iteration 3600, Loss: 0.4952\n",
      "Iteration 3700, Loss: 0.4943\n",
      "Iteration 3800, Loss: 0.4935\n",
      "Iteration 3900, Loss: 0.4926\n",
      "Iteration 0, Loss: 1.0547\n",
      "Iteration 100, Loss: 0.6695\n",
      "Iteration 200, Loss: 0.6351\n",
      "Iteration 300, Loss: 0.6167\n",
      "Iteration 400, Loss: 0.6039\n",
      "Iteration 500, Loss: 0.5943\n",
      "Iteration 600, Loss: 0.5866\n",
      "Iteration 700, Loss: 0.5802\n",
      "Iteration 800, Loss: 0.5747\n",
      "Iteration 900, Loss: 0.5700\n",
      "Iteration 1000, Loss: 0.5657\n",
      "Iteration 1100, Loss: 0.5620\n",
      "Iteration 1200, Loss: 0.5586\n",
      "Iteration 1300, Loss: 0.5555\n",
      "Iteration 1400, Loss: 0.5528\n",
      "Iteration 1500, Loss: 0.5502\n",
      "Iteration 1600, Loss: 0.5478\n",
      "Iteration 1700, Loss: 0.5455\n",
      "Iteration 1800, Loss: 0.5435\n",
      "Iteration 1900, Loss: 0.5415\n",
      "Iteration 2000, Loss: 0.5397\n",
      "Iteration 2100, Loss: 0.5380\n",
      "Iteration 2200, Loss: 0.5363\n",
      "Iteration 2300, Loss: 0.5348\n",
      "Iteration 2400, Loss: 0.5333\n",
      "Iteration 2500, Loss: 0.5318\n",
      "Iteration 2600, Loss: 0.5305\n",
      "Iteration 2700, Loss: 0.5293\n",
      "Iteration 2800, Loss: 0.5280\n",
      "Iteration 2900, Loss: 0.5268\n",
      "Iteration 3000, Loss: 0.5257\n",
      "Iteration 3100, Loss: 0.5246\n",
      "Iteration 3200, Loss: 0.5235\n",
      "Iteration 3300, Loss: 0.5225\n",
      "Iteration 3400, Loss: 0.5215\n",
      "Iteration 3500, Loss: 0.5206\n",
      "Iteration 3600, Loss: 0.5197\n",
      "Iteration 3700, Loss: 0.5188\n",
      "Iteration 3800, Loss: 0.5179\n",
      "Iteration 3900, Loss: 0.5171\n",
      "3 36\n",
      "Iteration 0, Loss: 1.0569\n",
      "Iteration 100, Loss: 0.6508\n",
      "Iteration 200, Loss: 0.6155\n",
      "Iteration 300, Loss: 0.5970\n",
      "Iteration 400, Loss: 0.5845\n",
      "Iteration 500, Loss: 0.5749\n",
      "Iteration 600, Loss: 0.5671\n",
      "Iteration 700, Loss: 0.5607\n",
      "Iteration 800, Loss: 0.5550\n",
      "Iteration 900, Loss: 0.5501\n",
      "Iteration 1000, Loss: 0.5458\n",
      "Iteration 1100, Loss: 0.5418\n",
      "Iteration 1200, Loss: 0.5382\n",
      "Iteration 1300, Loss: 0.5350\n",
      "Iteration 1400, Loss: 0.5320\n",
      "Iteration 1500, Loss: 0.5292\n",
      "Iteration 1600, Loss: 0.5266\n",
      "Iteration 1700, Loss: 0.5241\n",
      "Iteration 1800, Loss: 0.5218\n",
      "Iteration 1900, Loss: 0.5196\n",
      "Iteration 2000, Loss: 0.5175\n",
      "Iteration 2100, Loss: 0.5156\n",
      "Iteration 2200, Loss: 0.5137\n",
      "Iteration 2300, Loss: 0.5119\n",
      "Iteration 2400, Loss: 0.5102\n",
      "Iteration 2500, Loss: 0.5086\n",
      "Iteration 2600, Loss: 0.5071\n",
      "Iteration 2700, Loss: 0.5055\n",
      "Iteration 2800, Loss: 0.5040\n",
      "Iteration 2900, Loss: 0.5027\n",
      "Iteration 3000, Loss: 0.5013\n",
      "Iteration 3100, Loss: 0.5000\n",
      "Iteration 3200, Loss: 0.4987\n",
      "Iteration 3300, Loss: 0.4975\n",
      "Iteration 3400, Loss: 0.4963\n",
      "Iteration 3500, Loss: 0.4951\n",
      "Iteration 3600, Loss: 0.4940\n",
      "Iteration 3700, Loss: 0.4929\n",
      "Iteration 3800, Loss: 0.4918\n",
      "Iteration 3900, Loss: 0.4907\n",
      "Iteration 0, Loss: 1.0521\n",
      "Iteration 100, Loss: 0.6620\n",
      "Iteration 200, Loss: 0.6275\n",
      "Iteration 300, Loss: 0.6091\n",
      "Iteration 400, Loss: 0.5964\n",
      "Iteration 500, Loss: 0.5865\n",
      "Iteration 600, Loss: 0.5786\n",
      "Iteration 700, Loss: 0.5720\n",
      "Iteration 800, Loss: 0.5663\n",
      "Iteration 900, Loss: 0.5614\n",
      "Iteration 1000, Loss: 0.5570\n",
      "Iteration 1100, Loss: 0.5531\n",
      "Iteration 1200, Loss: 0.5496\n",
      "Iteration 1300, Loss: 0.5464\n",
      "Iteration 1400, Loss: 0.5435\n",
      "Iteration 1500, Loss: 0.5407\n",
      "Iteration 1600, Loss: 0.5381\n",
      "Iteration 1700, Loss: 0.5358\n",
      "Iteration 1800, Loss: 0.5336\n",
      "Iteration 1900, Loss: 0.5315\n",
      "Iteration 2000, Loss: 0.5296\n",
      "Iteration 2100, Loss: 0.5277\n",
      "Iteration 2200, Loss: 0.5259\n",
      "Iteration 2300, Loss: 0.5243\n",
      "Iteration 2400, Loss: 0.5226\n",
      "Iteration 2500, Loss: 0.5211\n",
      "Iteration 2600, Loss: 0.5197\n",
      "Iteration 2700, Loss: 0.5182\n",
      "Iteration 2800, Loss: 0.5169\n",
      "Iteration 2900, Loss: 0.5156\n",
      "Iteration 3000, Loss: 0.5144\n",
      "Iteration 3100, Loss: 0.5132\n",
      "Iteration 3200, Loss: 0.5120\n",
      "Iteration 3300, Loss: 0.5109\n",
      "Iteration 3400, Loss: 0.5098\n",
      "Iteration 3500, Loss: 0.5087\n",
      "Iteration 3600, Loss: 0.5077\n",
      "Iteration 3700, Loss: 0.5067\n",
      "Iteration 3800, Loss: 0.5057\n",
      "Iteration 3900, Loss: 0.5048\n",
      "4 36\n",
      "Iteration 0, Loss: 1.0888\n",
      "Iteration 100, Loss: 0.7782\n",
      "Iteration 200, Loss: 0.7117\n",
      "Iteration 300, Loss: 0.6807\n",
      "Iteration 400, Loss: 0.6616\n",
      "Iteration 500, Loss: 0.6481\n",
      "Iteration 600, Loss: 0.6378\n",
      "Iteration 700, Loss: 0.6294\n",
      "Iteration 800, Loss: 0.6226\n",
      "Iteration 900, Loss: 0.6167\n",
      "Iteration 1000, Loss: 0.6116\n",
      "Iteration 1100, Loss: 0.6071\n",
      "Iteration 1200, Loss: 0.6031\n",
      "Iteration 1300, Loss: 0.5995\n",
      "Iteration 1400, Loss: 0.5961\n",
      "Iteration 1500, Loss: 0.5931\n",
      "Iteration 1600, Loss: 0.5902\n",
      "Iteration 1700, Loss: 0.5876\n",
      "Iteration 1800, Loss: 0.5851\n",
      "Iteration 1900, Loss: 0.5828\n",
      "Iteration 2000, Loss: 0.5806\n",
      "Iteration 2100, Loss: 0.5785\n",
      "Iteration 2200, Loss: 0.5766\n",
      "Iteration 2300, Loss: 0.5747\n",
      "Iteration 2400, Loss: 0.5729\n",
      "Iteration 2500, Loss: 0.5712\n",
      "Iteration 2600, Loss: 0.5696\n",
      "Iteration 2700, Loss: 0.5680\n",
      "Iteration 2800, Loss: 0.5665\n",
      "Iteration 2900, Loss: 0.5651\n",
      "Iteration 3000, Loss: 0.5637\n",
      "Iteration 3100, Loss: 0.5624\n",
      "Iteration 3200, Loss: 0.5611\n",
      "Iteration 3300, Loss: 0.5598\n",
      "Iteration 3400, Loss: 0.5586\n",
      "Iteration 3500, Loss: 0.5575\n",
      "Iteration 3600, Loss: 0.5563\n",
      "Iteration 3700, Loss: 0.5552\n",
      "Iteration 3800, Loss: 0.5542\n",
      "Iteration 3900, Loss: 0.5532\n",
      "Iteration 0, Loss: 1.0897\n",
      "Iteration 100, Loss: 0.7942\n",
      "Iteration 200, Loss: 0.7307\n",
      "Iteration 300, Loss: 0.7016\n",
      "Iteration 400, Loss: 0.6836\n",
      "Iteration 500, Loss: 0.6706\n",
      "Iteration 600, Loss: 0.6606\n",
      "Iteration 700, Loss: 0.6526\n",
      "Iteration 800, Loss: 0.6458\n",
      "Iteration 900, Loss: 0.6401\n",
      "Iteration 1000, Loss: 0.6350\n",
      "Iteration 1100, Loss: 0.6306\n",
      "Iteration 1200, Loss: 0.6265\n",
      "Iteration 1300, Loss: 0.6229\n",
      "Iteration 1400, Loss: 0.6195\n",
      "Iteration 1500, Loss: 0.6164\n",
      "Iteration 1600, Loss: 0.6135\n",
      "Iteration 1700, Loss: 0.6108\n",
      "Iteration 1800, Loss: 0.6083\n",
      "Iteration 1900, Loss: 0.6059\n",
      "Iteration 2000, Loss: 0.6037\n",
      "Iteration 2100, Loss: 0.6015\n",
      "Iteration 2200, Loss: 0.5995\n",
      "Iteration 2300, Loss: 0.5976\n",
      "Iteration 2400, Loss: 0.5958\n",
      "Iteration 2500, Loss: 0.5940\n",
      "Iteration 2600, Loss: 0.5923\n",
      "Iteration 2700, Loss: 0.5907\n",
      "Iteration 2800, Loss: 0.5891\n",
      "Iteration 2900, Loss: 0.5876\n",
      "Iteration 3000, Loss: 0.5862\n",
      "Iteration 3100, Loss: 0.5848\n",
      "Iteration 3200, Loss: 0.5835\n",
      "Iteration 3300, Loss: 0.5822\n",
      "Iteration 3400, Loss: 0.5809\n",
      "Iteration 3500, Loss: 0.5797\n",
      "Iteration 3600, Loss: 0.5785\n",
      "Iteration 3700, Loss: 0.5774\n",
      "Iteration 3800, Loss: 0.5763\n",
      "Iteration 3900, Loss: 0.5752\n",
      "5 36\n",
      "Iteration 0, Loss: 1.0897\n",
      "Iteration 100, Loss: 0.7983\n",
      "Iteration 200, Loss: 0.7316\n",
      "Iteration 300, Loss: 0.6999\n",
      "Iteration 400, Loss: 0.6802\n",
      "Iteration 500, Loss: 0.6662\n",
      "Iteration 600, Loss: 0.6555\n",
      "Iteration 700, Loss: 0.6469\n",
      "Iteration 800, Loss: 0.6398\n",
      "Iteration 900, Loss: 0.6337\n",
      "Iteration 1000, Loss: 0.6284\n",
      "Iteration 1100, Loss: 0.6237\n",
      "Iteration 1200, Loss: 0.6195\n",
      "Iteration 1300, Loss: 0.6157\n",
      "Iteration 1400, Loss: 0.6122\n",
      "Iteration 1500, Loss: 0.6090\n",
      "Iteration 1600, Loss: 0.6060\n",
      "Iteration 1700, Loss: 0.6032\n",
      "Iteration 1800, Loss: 0.6006\n",
      "Iteration 1900, Loss: 0.5981\n",
      "Iteration 2000, Loss: 0.5958\n",
      "Iteration 2100, Loss: 0.5936\n",
      "Iteration 2200, Loss: 0.5915\n",
      "Iteration 2300, Loss: 0.5895\n",
      "Iteration 2400, Loss: 0.5876\n",
      "Iteration 2500, Loss: 0.5857\n",
      "Iteration 2600, Loss: 0.5840\n",
      "Iteration 2700, Loss: 0.5823\n",
      "Iteration 2800, Loss: 0.5807\n",
      "Iteration 2900, Loss: 0.5791\n",
      "Iteration 3000, Loss: 0.5776\n",
      "Iteration 3100, Loss: 0.5762\n",
      "Iteration 3200, Loss: 0.5747\n",
      "Iteration 3300, Loss: 0.5734\n",
      "Iteration 3400, Loss: 0.5721\n",
      "Iteration 3500, Loss: 0.5708\n",
      "Iteration 3600, Loss: 0.5696\n",
      "Iteration 3700, Loss: 0.5684\n",
      "Iteration 3800, Loss: 0.5672\n",
      "Iteration 3900, Loss: 0.5660\n",
      "Iteration 0, Loss: 1.0887\n",
      "Iteration 100, Loss: 0.7743\n",
      "Iteration 200, Loss: 0.7098\n",
      "Iteration 300, Loss: 0.6810\n",
      "Iteration 400, Loss: 0.6636\n",
      "Iteration 500, Loss: 0.6513\n",
      "Iteration 600, Loss: 0.6419\n",
      "Iteration 700, Loss: 0.6342\n",
      "Iteration 800, Loss: 0.6279\n",
      "Iteration 900, Loss: 0.6224\n",
      "Iteration 1000, Loss: 0.6176\n",
      "Iteration 1100, Loss: 0.6133\n",
      "Iteration 1200, Loss: 0.6095\n",
      "Iteration 1300, Loss: 0.6059\n",
      "Iteration 1400, Loss: 0.6027\n",
      "Iteration 1500, Loss: 0.5997\n",
      "Iteration 1600, Loss: 0.5969\n",
      "Iteration 1700, Loss: 0.5943\n",
      "Iteration 1800, Loss: 0.5918\n",
      "Iteration 1900, Loss: 0.5896\n",
      "Iteration 2000, Loss: 0.5874\n",
      "Iteration 2100, Loss: 0.5853\n",
      "Iteration 2200, Loss: 0.5833\n",
      "Iteration 2300, Loss: 0.5815\n",
      "Iteration 2400, Loss: 0.5797\n",
      "Iteration 2500, Loss: 0.5780\n",
      "Iteration 2600, Loss: 0.5764\n",
      "Iteration 2700, Loss: 0.5748\n",
      "Iteration 2800, Loss: 0.5733\n",
      "Iteration 2900, Loss: 0.5718\n",
      "Iteration 3000, Loss: 0.5704\n",
      "Iteration 3100, Loss: 0.5691\n",
      "Iteration 3200, Loss: 0.5678\n",
      "Iteration 3300, Loss: 0.5665\n",
      "Iteration 3400, Loss: 0.5653\n",
      "Iteration 3500, Loss: 0.5642\n",
      "Iteration 3600, Loss: 0.5630\n",
      "Iteration 3700, Loss: 0.5619\n",
      "Iteration 3800, Loss: 0.5609\n",
      "Iteration 3900, Loss: 0.5598\n",
      "6 36\n",
      "Iteration 0, Loss: 1.0930\n",
      "Iteration 100, Loss: 0.8530\n",
      "Iteration 200, Loss: 0.7747\n",
      "Iteration 300, Loss: 0.7342\n",
      "Iteration 400, Loss: 0.7092\n",
      "Iteration 500, Loss: 0.6920\n",
      "Iteration 600, Loss: 0.6791\n",
      "Iteration 700, Loss: 0.6689\n",
      "Iteration 800, Loss: 0.6605\n",
      "Iteration 900, Loss: 0.6535\n",
      "Iteration 1000, Loss: 0.6474\n",
      "Iteration 1100, Loss: 0.6421\n",
      "Iteration 1200, Loss: 0.6375\n",
      "Iteration 1300, Loss: 0.6332\n",
      "Iteration 1400, Loss: 0.6294\n",
      "Iteration 1500, Loss: 0.6260\n",
      "Iteration 1600, Loss: 0.6227\n",
      "Iteration 1700, Loss: 0.6198\n",
      "Iteration 1800, Loss: 0.6170\n",
      "Iteration 1900, Loss: 0.6145\n",
      "Iteration 2000, Loss: 0.6121\n",
      "Iteration 2100, Loss: 0.6098\n",
      "Iteration 2200, Loss: 0.6077\n",
      "Iteration 2300, Loss: 0.6056\n",
      "Iteration 2400, Loss: 0.6037\n",
      "Iteration 2500, Loss: 0.6019\n",
      "Iteration 2600, Loss: 0.6001\n",
      "Iteration 2700, Loss: 0.5985\n",
      "Iteration 2800, Loss: 0.5969\n",
      "Iteration 2900, Loss: 0.5953\n",
      "Iteration 3000, Loss: 0.5939\n",
      "Iteration 3100, Loss: 0.5924\n",
      "Iteration 3200, Loss: 0.5911\n",
      "Iteration 3300, Loss: 0.5897\n",
      "Iteration 3400, Loss: 0.5884\n",
      "Iteration 3500, Loss: 0.5872\n",
      "Iteration 3600, Loss: 0.5860\n",
      "Iteration 3700, Loss: 0.5848\n",
      "Iteration 3800, Loss: 0.5837\n",
      "Iteration 3900, Loss: 0.5826\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8685\n",
      "Iteration 200, Loss: 0.7899\n",
      "Iteration 300, Loss: 0.7502\n",
      "Iteration 400, Loss: 0.7261\n",
      "Iteration 500, Loss: 0.7096\n",
      "Iteration 600, Loss: 0.6973\n",
      "Iteration 700, Loss: 0.6874\n",
      "Iteration 800, Loss: 0.6793\n",
      "Iteration 900, Loss: 0.6724\n",
      "Iteration 1000, Loss: 0.6665\n",
      "Iteration 1100, Loss: 0.6612\n",
      "Iteration 1200, Loss: 0.6565\n",
      "Iteration 1300, Loss: 0.6523\n",
      "Iteration 1400, Loss: 0.6485\n",
      "Iteration 1500, Loss: 0.6450\n",
      "Iteration 1600, Loss: 0.6418\n",
      "Iteration 1700, Loss: 0.6389\n",
      "Iteration 1800, Loss: 0.6361\n",
      "Iteration 1900, Loss: 0.6336\n",
      "Iteration 2000, Loss: 0.6311\n",
      "Iteration 2100, Loss: 0.6289\n",
      "Iteration 2200, Loss: 0.6267\n",
      "Iteration 2300, Loss: 0.6246\n",
      "Iteration 2400, Loss: 0.6226\n",
      "Iteration 2500, Loss: 0.6208\n",
      "Iteration 2600, Loss: 0.6190\n",
      "Iteration 2700, Loss: 0.6173\n",
      "Iteration 2800, Loss: 0.6156\n",
      "Iteration 2900, Loss: 0.6140\n",
      "Iteration 3000, Loss: 0.6125\n",
      "Iteration 3100, Loss: 0.6110\n",
      "Iteration 3200, Loss: 0.6096\n",
      "Iteration 3300, Loss: 0.6082\n",
      "Iteration 3400, Loss: 0.6069\n",
      "Iteration 3500, Loss: 0.6056\n",
      "Iteration 3600, Loss: 0.6043\n",
      "Iteration 3700, Loss: 0.6031\n",
      "Iteration 3800, Loss: 0.6019\n",
      "Iteration 3900, Loss: 0.6007\n",
      "7 36\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8677\n",
      "Iteration 200, Loss: 0.7909\n",
      "Iteration 300, Loss: 0.7519\n",
      "Iteration 400, Loss: 0.7282\n",
      "Iteration 500, Loss: 0.7120\n",
      "Iteration 600, Loss: 0.6999\n",
      "Iteration 700, Loss: 0.6903\n",
      "Iteration 800, Loss: 0.6822\n",
      "Iteration 900, Loss: 0.6754\n",
      "Iteration 1000, Loss: 0.6695\n",
      "Iteration 1100, Loss: 0.6643\n",
      "Iteration 1200, Loss: 0.6596\n",
      "Iteration 1300, Loss: 0.6554\n",
      "Iteration 1400, Loss: 0.6516\n",
      "Iteration 1500, Loss: 0.6481\n",
      "Iteration 1600, Loss: 0.6448\n",
      "Iteration 1700, Loss: 0.6418\n",
      "Iteration 1800, Loss: 0.6390\n",
      "Iteration 1900, Loss: 0.6363\n",
      "Iteration 2000, Loss: 0.6339\n",
      "Iteration 2100, Loss: 0.6315\n",
      "Iteration 2200, Loss: 0.6293\n",
      "Iteration 2300, Loss: 0.6272\n",
      "Iteration 2400, Loss: 0.6252\n",
      "Iteration 2500, Loss: 0.6232\n",
      "Iteration 2600, Loss: 0.6214\n",
      "Iteration 2700, Loss: 0.6196\n",
      "Iteration 2800, Loss: 0.6179\n",
      "Iteration 2900, Loss: 0.6163\n",
      "Iteration 3000, Loss: 0.6147\n",
      "Iteration 3100, Loss: 0.6132\n",
      "Iteration 3200, Loss: 0.6117\n",
      "Iteration 3300, Loss: 0.6103\n",
      "Iteration 3400, Loss: 0.6089\n",
      "Iteration 3500, Loss: 0.6076\n",
      "Iteration 3600, Loss: 0.6062\n",
      "Iteration 3700, Loss: 0.6050\n",
      "Iteration 3800, Loss: 0.6037\n",
      "Iteration 3900, Loss: 0.6025\n",
      "Iteration 0, Loss: 1.0935\n",
      "Iteration 100, Loss: 0.8542\n",
      "Iteration 200, Loss: 0.7735\n",
      "Iteration 300, Loss: 0.7318\n",
      "Iteration 400, Loss: 0.7061\n",
      "Iteration 500, Loss: 0.6883\n",
      "Iteration 600, Loss: 0.6750\n",
      "Iteration 700, Loss: 0.6645\n",
      "Iteration 800, Loss: 0.6560\n",
      "Iteration 900, Loss: 0.6488\n",
      "Iteration 1000, Loss: 0.6426\n",
      "Iteration 1100, Loss: 0.6372\n",
      "Iteration 1200, Loss: 0.6325\n",
      "Iteration 1300, Loss: 0.6282\n",
      "Iteration 1400, Loss: 0.6244\n",
      "Iteration 1500, Loss: 0.6208\n",
      "Iteration 1600, Loss: 0.6176\n",
      "Iteration 1700, Loss: 0.6147\n",
      "Iteration 1800, Loss: 0.6120\n",
      "Iteration 1900, Loss: 0.6094\n",
      "Iteration 2000, Loss: 0.6071\n",
      "Iteration 2100, Loss: 0.6049\n",
      "Iteration 2200, Loss: 0.6027\n",
      "Iteration 2300, Loss: 0.6008\n",
      "Iteration 2400, Loss: 0.5989\n",
      "Iteration 2500, Loss: 0.5971\n",
      "Iteration 2600, Loss: 0.5954\n",
      "Iteration 2700, Loss: 0.5938\n",
      "Iteration 2800, Loss: 0.5922\n",
      "Iteration 2900, Loss: 0.5907\n",
      "Iteration 3000, Loss: 0.5893\n",
      "Iteration 3100, Loss: 0.5879\n",
      "Iteration 3200, Loss: 0.5865\n",
      "Iteration 3300, Loss: 0.5852\n",
      "Iteration 3400, Loss: 0.5840\n",
      "Iteration 3500, Loss: 0.5828\n",
      "Iteration 3600, Loss: 0.5816\n",
      "Iteration 3700, Loss: 0.5805\n",
      "Iteration 3800, Loss: 0.5794\n",
      "Iteration 3900, Loss: 0.5783\n",
      "8 36\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0134\n",
      "Iteration 200, Loss: 0.9554\n",
      "Iteration 300, Loss: 0.9124\n",
      "Iteration 400, Loss: 0.8790\n",
      "Iteration 500, Loss: 0.8521\n",
      "Iteration 600, Loss: 0.8301\n",
      "Iteration 700, Loss: 0.8116\n",
      "Iteration 800, Loss: 0.7959\n",
      "Iteration 900, Loss: 0.7824\n",
      "Iteration 1000, Loss: 0.7706\n",
      "Iteration 1100, Loss: 0.7603\n",
      "Iteration 1200, Loss: 0.7512\n",
      "Iteration 1300, Loss: 0.7430\n",
      "Iteration 1400, Loss: 0.7357\n",
      "Iteration 1500, Loss: 0.7291\n",
      "Iteration 1600, Loss: 0.7231\n",
      "Iteration 1700, Loss: 0.7176\n",
      "Iteration 1800, Loss: 0.7125\n",
      "Iteration 1900, Loss: 0.7079\n",
      "Iteration 2000, Loss: 0.7035\n",
      "Iteration 2100, Loss: 0.6995\n",
      "Iteration 2200, Loss: 0.6958\n",
      "Iteration 2300, Loss: 0.6922\n",
      "Iteration 2400, Loss: 0.6889\n",
      "Iteration 2500, Loss: 0.6858\n",
      "Iteration 2600, Loss: 0.6829\n",
      "Iteration 2700, Loss: 0.6801\n",
      "Iteration 2800, Loss: 0.6774\n",
      "Iteration 2900, Loss: 0.6749\n",
      "Iteration 3000, Loss: 0.6725\n",
      "Iteration 3100, Loss: 0.6702\n",
      "Iteration 3200, Loss: 0.6680\n",
      "Iteration 3300, Loss: 0.6659\n",
      "Iteration 3400, Loss: 0.6638\n",
      "Iteration 3500, Loss: 0.6619\n",
      "Iteration 3600, Loss: 0.6600\n",
      "Iteration 3700, Loss: 0.6582\n",
      "Iteration 3800, Loss: 0.6565\n",
      "Iteration 3900, Loss: 0.6548\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0259\n",
      "Iteration 200, Loss: 0.9750\n",
      "Iteration 300, Loss: 0.9363\n",
      "Iteration 400, Loss: 0.9057\n",
      "Iteration 500, Loss: 0.8807\n",
      "Iteration 600, Loss: 0.8600\n",
      "Iteration 700, Loss: 0.8425\n",
      "Iteration 800, Loss: 0.8275\n",
      "Iteration 900, Loss: 0.8145\n",
      "Iteration 1000, Loss: 0.8032\n",
      "Iteration 1100, Loss: 0.7932\n",
      "Iteration 1200, Loss: 0.7844\n",
      "Iteration 1300, Loss: 0.7765\n",
      "Iteration 1400, Loss: 0.7694\n",
      "Iteration 1500, Loss: 0.7629\n",
      "Iteration 1600, Loss: 0.7571\n",
      "Iteration 1700, Loss: 0.7518\n",
      "Iteration 1800, Loss: 0.7469\n",
      "Iteration 1900, Loss: 0.7424\n",
      "Iteration 2000, Loss: 0.7382\n",
      "Iteration 2100, Loss: 0.7344\n",
      "Iteration 2200, Loss: 0.7308\n",
      "Iteration 2300, Loss: 0.7274\n",
      "Iteration 2400, Loss: 0.7242\n",
      "Iteration 2500, Loss: 0.7213\n",
      "Iteration 2600, Loss: 0.7185\n",
      "Iteration 2700, Loss: 0.7158\n",
      "Iteration 2800, Loss: 0.7133\n",
      "Iteration 2900, Loss: 0.7109\n",
      "Iteration 3000, Loss: 0.7086\n",
      "Iteration 3100, Loss: 0.7065\n",
      "Iteration 3200, Loss: 0.7044\n",
      "Iteration 3300, Loss: 0.7024\n",
      "Iteration 3400, Loss: 0.7005\n",
      "Iteration 3500, Loss: 0.6986\n",
      "Iteration 3600, Loss: 0.6969\n",
      "Iteration 3700, Loss: 0.6952\n",
      "Iteration 3800, Loss: 0.6935\n",
      "Iteration 3900, Loss: 0.6919\n",
      "9 36\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0243\n",
      "Iteration 200, Loss: 0.9718\n",
      "Iteration 300, Loss: 0.9317\n",
      "Iteration 400, Loss: 0.8998\n",
      "Iteration 500, Loss: 0.8738\n",
      "Iteration 600, Loss: 0.8523\n",
      "Iteration 700, Loss: 0.8340\n",
      "Iteration 800, Loss: 0.8184\n",
      "Iteration 900, Loss: 0.8049\n",
      "Iteration 1000, Loss: 0.7932\n",
      "Iteration 1100, Loss: 0.7829\n",
      "Iteration 1200, Loss: 0.7737\n",
      "Iteration 1300, Loss: 0.7656\n",
      "Iteration 1400, Loss: 0.7583\n",
      "Iteration 1500, Loss: 0.7518\n",
      "Iteration 1600, Loss: 0.7458\n",
      "Iteration 1700, Loss: 0.7404\n",
      "Iteration 1800, Loss: 0.7354\n",
      "Iteration 1900, Loss: 0.7309\n",
      "Iteration 2000, Loss: 0.7266\n",
      "Iteration 2100, Loss: 0.7227\n",
      "Iteration 2200, Loss: 0.7190\n",
      "Iteration 2300, Loss: 0.7156\n",
      "Iteration 2400, Loss: 0.7124\n",
      "Iteration 2500, Loss: 0.7094\n",
      "Iteration 2600, Loss: 0.7065\n",
      "Iteration 2700, Loss: 0.7038\n",
      "Iteration 2800, Loss: 0.7012\n",
      "Iteration 2900, Loss: 0.6988\n",
      "Iteration 3000, Loss: 0.6965\n",
      "Iteration 3100, Loss: 0.6942\n",
      "Iteration 3200, Loss: 0.6921\n",
      "Iteration 3300, Loss: 0.6901\n",
      "Iteration 3400, Loss: 0.6881\n",
      "Iteration 3500, Loss: 0.6863\n",
      "Iteration 3600, Loss: 0.6845\n",
      "Iteration 3700, Loss: 0.6827\n",
      "Iteration 3800, Loss: 0.6811\n",
      "Iteration 3900, Loss: 0.6794\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0149\n",
      "Iteration 200, Loss: 0.9585\n",
      "Iteration 300, Loss: 0.9168\n",
      "Iteration 400, Loss: 0.8844\n",
      "Iteration 500, Loss: 0.8586\n",
      "Iteration 600, Loss: 0.8373\n",
      "Iteration 700, Loss: 0.8196\n",
      "Iteration 800, Loss: 0.8046\n",
      "Iteration 900, Loss: 0.7916\n",
      "Iteration 1000, Loss: 0.7804\n",
      "Iteration 1100, Loss: 0.7706\n",
      "Iteration 1200, Loss: 0.7618\n",
      "Iteration 1300, Loss: 0.7539\n",
      "Iteration 1400, Loss: 0.7469\n",
      "Iteration 1500, Loss: 0.7405\n",
      "Iteration 1600, Loss: 0.7347\n",
      "Iteration 1700, Loss: 0.7294\n",
      "Iteration 1800, Loss: 0.7245\n",
      "Iteration 1900, Loss: 0.7200\n",
      "Iteration 2000, Loss: 0.7158\n",
      "Iteration 2100, Loss: 0.7119\n",
      "Iteration 2200, Loss: 0.7083\n",
      "Iteration 2300, Loss: 0.7049\n",
      "Iteration 2400, Loss: 0.7017\n",
      "Iteration 2500, Loss: 0.6987\n",
      "Iteration 2600, Loss: 0.6958\n",
      "Iteration 2700, Loss: 0.6931\n",
      "Iteration 2800, Loss: 0.6905\n",
      "Iteration 2900, Loss: 0.6881\n",
      "Iteration 3000, Loss: 0.6857\n",
      "Iteration 3100, Loss: 0.6835\n",
      "Iteration 3200, Loss: 0.6814\n",
      "Iteration 3300, Loss: 0.6793\n",
      "Iteration 3400, Loss: 0.6774\n",
      "Iteration 3500, Loss: 0.6755\n",
      "Iteration 3600, Loss: 0.6737\n",
      "Iteration 3700, Loss: 0.6720\n",
      "Iteration 3800, Loss: 0.6703\n",
      "Iteration 3900, Loss: 0.6687\n",
      "10 36\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0537\n",
      "Iteration 200, Loss: 1.0179\n",
      "Iteration 300, Loss: 0.9880\n",
      "Iteration 400, Loss: 0.9625\n",
      "Iteration 500, Loss: 0.9405\n",
      "Iteration 600, Loss: 0.9213\n",
      "Iteration 700, Loss: 0.9043\n",
      "Iteration 800, Loss: 0.8892\n",
      "Iteration 900, Loss: 0.8756\n",
      "Iteration 1000, Loss: 0.8635\n",
      "Iteration 1100, Loss: 0.8524\n",
      "Iteration 1200, Loss: 0.8422\n",
      "Iteration 1300, Loss: 0.8329\n",
      "Iteration 1400, Loss: 0.8245\n",
      "Iteration 1500, Loss: 0.8167\n",
      "Iteration 1600, Loss: 0.8094\n",
      "Iteration 1700, Loss: 0.8027\n",
      "Iteration 1800, Loss: 0.7965\n",
      "Iteration 1900, Loss: 0.7907\n",
      "Iteration 2000, Loss: 0.7853\n",
      "Iteration 2100, Loss: 0.7802\n",
      "Iteration 2200, Loss: 0.7754\n",
      "Iteration 2300, Loss: 0.7710\n",
      "Iteration 2400, Loss: 0.7667\n",
      "Iteration 2500, Loss: 0.7628\n",
      "Iteration 2600, Loss: 0.7590\n",
      "Iteration 2700, Loss: 0.7555\n",
      "Iteration 2800, Loss: 0.7521\n",
      "Iteration 2900, Loss: 0.7489\n",
      "Iteration 3000, Loss: 0.7458\n",
      "Iteration 3100, Loss: 0.7429\n",
      "Iteration 3200, Loss: 0.7402\n",
      "Iteration 3300, Loss: 0.7375\n",
      "Iteration 3400, Loss: 0.7350\n",
      "Iteration 3500, Loss: 0.7325\n",
      "Iteration 3600, Loss: 0.7302\n",
      "Iteration 3700, Loss: 0.7280\n",
      "Iteration 3800, Loss: 0.7258\n",
      "Iteration 3900, Loss: 0.7238\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0534\n",
      "Iteration 200, Loss: 1.0173\n",
      "Iteration 300, Loss: 0.9872\n",
      "Iteration 400, Loss: 0.9615\n",
      "Iteration 500, Loss: 0.9393\n",
      "Iteration 600, Loss: 0.9198\n",
      "Iteration 700, Loss: 0.9026\n",
      "Iteration 800, Loss: 0.8874\n",
      "Iteration 900, Loss: 0.8735\n",
      "Iteration 1000, Loss: 0.8611\n",
      "Iteration 1100, Loss: 0.8498\n",
      "Iteration 1200, Loss: 0.8394\n",
      "Iteration 1300, Loss: 0.8299\n",
      "Iteration 1400, Loss: 0.8211\n",
      "Iteration 1500, Loss: 0.8131\n",
      "Iteration 1600, Loss: 0.8056\n",
      "Iteration 1700, Loss: 0.7986\n",
      "Iteration 1800, Loss: 0.7922\n",
      "Iteration 1900, Loss: 0.7861\n",
      "Iteration 2000, Loss: 0.7805\n",
      "Iteration 2100, Loss: 0.7753\n",
      "Iteration 2200, Loss: 0.7703\n",
      "Iteration 2300, Loss: 0.7657\n",
      "Iteration 2400, Loss: 0.7612\n",
      "Iteration 2500, Loss: 0.7570\n",
      "Iteration 2600, Loss: 0.7531\n",
      "Iteration 2700, Loss: 0.7494\n",
      "Iteration 2800, Loss: 0.7458\n",
      "Iteration 2900, Loss: 0.7424\n",
      "Iteration 3000, Loss: 0.7392\n",
      "Iteration 3100, Loss: 0.7362\n",
      "Iteration 3200, Loss: 0.7333\n",
      "Iteration 3300, Loss: 0.7304\n",
      "Iteration 3400, Loss: 0.7278\n",
      "Iteration 3500, Loss: 0.7252\n",
      "Iteration 3600, Loss: 0.7227\n",
      "Iteration 3700, Loss: 0.7204\n",
      "Iteration 3800, Loss: 0.7181\n",
      "Iteration 3900, Loss: 0.7159\n",
      "11 36\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0554\n",
      "Iteration 200, Loss: 1.0208\n",
      "Iteration 300, Loss: 0.9921\n",
      "Iteration 400, Loss: 0.9676\n",
      "Iteration 500, Loss: 0.9463\n",
      "Iteration 600, Loss: 0.9276\n",
      "Iteration 700, Loss: 0.9110\n",
      "Iteration 800, Loss: 0.8962\n",
      "Iteration 900, Loss: 0.8828\n",
      "Iteration 1000, Loss: 0.8708\n",
      "Iteration 1100, Loss: 0.8598\n",
      "Iteration 1200, Loss: 0.8498\n",
      "Iteration 1300, Loss: 0.8406\n",
      "Iteration 1400, Loss: 0.8321\n",
      "Iteration 1500, Loss: 0.8243\n",
      "Iteration 1600, Loss: 0.8170\n",
      "Iteration 1700, Loss: 0.8103\n",
      "Iteration 1800, Loss: 0.8039\n",
      "Iteration 1900, Loss: 0.7981\n",
      "Iteration 2000, Loss: 0.7926\n",
      "Iteration 2100, Loss: 0.7875\n",
      "Iteration 2200, Loss: 0.7826\n",
      "Iteration 2300, Loss: 0.7781\n",
      "Iteration 2400, Loss: 0.7738\n",
      "Iteration 2500, Loss: 0.7698\n",
      "Iteration 2600, Loss: 0.7659\n",
      "Iteration 2700, Loss: 0.7623\n",
      "Iteration 2800, Loss: 0.7589\n",
      "Iteration 2900, Loss: 0.7556\n",
      "Iteration 3000, Loss: 0.7525\n",
      "Iteration 3100, Loss: 0.7495\n",
      "Iteration 3200, Loss: 0.7467\n",
      "Iteration 3300, Loss: 0.7440\n",
      "Iteration 3400, Loss: 0.7414\n",
      "Iteration 3500, Loss: 0.7389\n",
      "Iteration 3600, Loss: 0.7365\n",
      "Iteration 3700, Loss: 0.7342\n",
      "Iteration 3800, Loss: 0.7320\n",
      "Iteration 3900, Loss: 0.7299\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0517\n",
      "Iteration 200, Loss: 1.0142\n",
      "Iteration 300, Loss: 0.9829\n",
      "Iteration 400, Loss: 0.9563\n",
      "Iteration 500, Loss: 0.9335\n",
      "Iteration 600, Loss: 0.9135\n",
      "Iteration 700, Loss: 0.8959\n",
      "Iteration 800, Loss: 0.8802\n",
      "Iteration 900, Loss: 0.8662\n",
      "Iteration 1000, Loss: 0.8536\n",
      "Iteration 1100, Loss: 0.8422\n",
      "Iteration 1200, Loss: 0.8318\n",
      "Iteration 1300, Loss: 0.8223\n",
      "Iteration 1400, Loss: 0.8136\n",
      "Iteration 1500, Loss: 0.8055\n",
      "Iteration 1600, Loss: 0.7981\n",
      "Iteration 1700, Loss: 0.7913\n",
      "Iteration 1800, Loss: 0.7849\n",
      "Iteration 1900, Loss: 0.7789\n",
      "Iteration 2000, Loss: 0.7734\n",
      "Iteration 2100, Loss: 0.7682\n",
      "Iteration 2200, Loss: 0.7633\n",
      "Iteration 2300, Loss: 0.7588\n",
      "Iteration 2400, Loss: 0.7544\n",
      "Iteration 2500, Loss: 0.7504\n",
      "Iteration 2600, Loss: 0.7466\n",
      "Iteration 2700, Loss: 0.7429\n",
      "Iteration 2800, Loss: 0.7394\n",
      "Iteration 2900, Loss: 0.7361\n",
      "Iteration 3000, Loss: 0.7330\n",
      "Iteration 3100, Loss: 0.7300\n",
      "Iteration 3200, Loss: 0.7271\n",
      "Iteration 3300, Loss: 0.7244\n",
      "Iteration 3400, Loss: 0.7218\n",
      "Iteration 3500, Loss: 0.7193\n",
      "Iteration 3600, Loss: 0.7169\n",
      "Iteration 3700, Loss: 0.7146\n",
      "Iteration 3800, Loss: 0.7124\n",
      "Iteration 3900, Loss: 0.7103\n",
      "12 36\n",
      "Iteration 0, Loss: 1.0237\n",
      "Iteration 100, Loss: 0.6384\n",
      "Iteration 200, Loss: 0.6061\n",
      "Iteration 300, Loss: 0.5876\n",
      "Iteration 400, Loss: 0.5746\n",
      "Iteration 500, Loss: 0.5648\n",
      "Iteration 600, Loss: 0.5568\n",
      "Iteration 700, Loss: 0.5503\n",
      "Iteration 800, Loss: 0.5446\n",
      "Iteration 900, Loss: 0.5397\n",
      "Iteration 1000, Loss: 0.5353\n",
      "Iteration 1100, Loss: 0.5314\n",
      "Iteration 1200, Loss: 0.5279\n",
      "Iteration 1300, Loss: 0.5247\n",
      "Iteration 1400, Loss: 0.5217\n",
      "Iteration 1500, Loss: 0.5190\n",
      "Iteration 1600, Loss: 0.5164\n",
      "Iteration 1700, Loss: 0.5141\n",
      "Iteration 1800, Loss: 0.5119\n",
      "Iteration 1900, Loss: 0.5098\n",
      "Iteration 2000, Loss: 0.5078\n",
      "Iteration 2100, Loss: 0.5059\n",
      "Iteration 2200, Loss: 0.5042\n",
      "Iteration 2300, Loss: 0.5025\n",
      "Iteration 2400, Loss: 0.5009\n",
      "Iteration 2500, Loss: 0.4994\n",
      "Iteration 2600, Loss: 0.4980\n",
      "Iteration 2700, Loss: 0.4966\n",
      "Iteration 2800, Loss: 0.4953\n",
      "Iteration 2900, Loss: 0.4940\n",
      "Iteration 3000, Loss: 0.4928\n",
      "Iteration 3100, Loss: 0.4916\n",
      "Iteration 3200, Loss: 0.4905\n",
      "Iteration 3300, Loss: 0.4894\n",
      "Iteration 3400, Loss: 0.4883\n",
      "Iteration 3500, Loss: 0.4873\n",
      "Iteration 3600, Loss: 0.4863\n",
      "Iteration 3700, Loss: 0.4854\n",
      "Iteration 3800, Loss: 0.4845\n",
      "Iteration 3900, Loss: 0.4836\n",
      "Iteration 4000, Loss: 0.4828\n",
      "Iteration 4100, Loss: 0.4819\n",
      "Iteration 4200, Loss: 0.4811\n",
      "Iteration 4300, Loss: 0.4803\n",
      "Iteration 4400, Loss: 0.4796\n",
      "Iteration 4500, Loss: 0.4788\n",
      "Iteration 4600, Loss: 0.4781\n",
      "Iteration 4700, Loss: 0.4774\n",
      "Iteration 4800, Loss: 0.4767\n",
      "Iteration 4900, Loss: 0.4761\n",
      "Iteration 0, Loss: 1.0150\n",
      "Iteration 100, Loss: 0.6036\n",
      "Iteration 200, Loss: 0.5739\n",
      "Iteration 300, Loss: 0.5573\n",
      "Iteration 400, Loss: 0.5458\n",
      "Iteration 500, Loss: 0.5372\n",
      "Iteration 600, Loss: 0.5303\n",
      "Iteration 700, Loss: 0.5245\n",
      "Iteration 800, Loss: 0.5196\n",
      "Iteration 900, Loss: 0.5153\n",
      "Iteration 1000, Loss: 0.5115\n",
      "Iteration 1100, Loss: 0.5082\n",
      "Iteration 1200, Loss: 0.5051\n",
      "Iteration 1300, Loss: 0.5023\n",
      "Iteration 1400, Loss: 0.4998\n",
      "Iteration 1500, Loss: 0.4975\n",
      "Iteration 1600, Loss: 0.4953\n",
      "Iteration 1700, Loss: 0.4933\n",
      "Iteration 1800, Loss: 0.4915\n",
      "Iteration 1900, Loss: 0.4898\n",
      "Iteration 2000, Loss: 0.4881\n",
      "Iteration 2100, Loss: 0.4866\n",
      "Iteration 2200, Loss: 0.4851\n",
      "Iteration 2300, Loss: 0.4838\n",
      "Iteration 2400, Loss: 0.4825\n",
      "Iteration 2500, Loss: 0.4812\n",
      "Iteration 2600, Loss: 0.4800\n",
      "Iteration 2700, Loss: 0.4789\n",
      "Iteration 2800, Loss: 0.4778\n",
      "Iteration 2900, Loss: 0.4768\n",
      "Iteration 3000, Loss: 0.4758\n",
      "Iteration 3100, Loss: 0.4749\n",
      "Iteration 3200, Loss: 0.4739\n",
      "Iteration 3300, Loss: 0.4730\n",
      "Iteration 3400, Loss: 0.4722\n",
      "Iteration 3500, Loss: 0.4714\n",
      "Iteration 3600, Loss: 0.4706\n",
      "Iteration 3700, Loss: 0.4698\n",
      "Iteration 3800, Loss: 0.4691\n",
      "Iteration 3900, Loss: 0.4683\n",
      "Iteration 4000, Loss: 0.4677\n",
      "Iteration 4100, Loss: 0.4670\n",
      "Iteration 4200, Loss: 0.4663\n",
      "Iteration 4300, Loss: 0.4657\n",
      "Iteration 4400, Loss: 0.4651\n",
      "Iteration 4500, Loss: 0.4645\n",
      "Iteration 4600, Loss: 0.4639\n",
      "Iteration 4700, Loss: 0.4633\n",
      "Iteration 4800, Loss: 0.4627\n",
      "Iteration 4900, Loss: 0.4622\n",
      "13 36\n",
      "Iteration 0, Loss: 1.0106\n",
      "Iteration 100, Loss: 0.6068\n",
      "Iteration 200, Loss: 0.5755\n",
      "Iteration 300, Loss: 0.5576\n",
      "Iteration 400, Loss: 0.5452\n",
      "Iteration 500, Loss: 0.5357\n",
      "Iteration 600, Loss: 0.5281\n",
      "Iteration 700, Loss: 0.5218\n",
      "Iteration 800, Loss: 0.5164\n",
      "Iteration 900, Loss: 0.5116\n",
      "Iteration 1000, Loss: 0.5075\n",
      "Iteration 1100, Loss: 0.5037\n",
      "Iteration 1200, Loss: 0.5003\n",
      "Iteration 1300, Loss: 0.4972\n",
      "Iteration 1400, Loss: 0.4944\n",
      "Iteration 1500, Loss: 0.4918\n",
      "Iteration 1600, Loss: 0.4893\n",
      "Iteration 1700, Loss: 0.4870\n",
      "Iteration 1800, Loss: 0.4849\n",
      "Iteration 1900, Loss: 0.4829\n",
      "Iteration 2000, Loss: 0.4810\n",
      "Iteration 2100, Loss: 0.4792\n",
      "Iteration 2200, Loss: 0.4775\n",
      "Iteration 2300, Loss: 0.4759\n",
      "Iteration 2400, Loss: 0.4744\n",
      "Iteration 2500, Loss: 0.4729\n",
      "Iteration 2600, Loss: 0.4715\n",
      "Iteration 2700, Loss: 0.4702\n",
      "Iteration 2800, Loss: 0.4689\n",
      "Iteration 2900, Loss: 0.4676\n",
      "Iteration 3000, Loss: 0.4664\n",
      "Iteration 3100, Loss: 0.4653\n",
      "Iteration 3200, Loss: 0.4642\n",
      "Iteration 3300, Loss: 0.4631\n",
      "Iteration 3400, Loss: 0.4621\n",
      "Iteration 3500, Loss: 0.4611\n",
      "Iteration 3600, Loss: 0.4602\n",
      "Iteration 3700, Loss: 0.4592\n",
      "Iteration 3800, Loss: 0.4583\n",
      "Iteration 3900, Loss: 0.4575\n",
      "Iteration 4000, Loss: 0.4566\n",
      "Iteration 4100, Loss: 0.4558\n",
      "Iteration 4200, Loss: 0.4550\n",
      "Iteration 4300, Loss: 0.4542\n",
      "Iteration 4400, Loss: 0.4534\n",
      "Iteration 4500, Loss: 0.4527\n",
      "Iteration 4600, Loss: 0.4520\n",
      "Iteration 4700, Loss: 0.4512\n",
      "Iteration 4800, Loss: 0.4506\n",
      "Iteration 4900, Loss: 0.4499\n",
      "Iteration 0, Loss: 1.0266\n",
      "Iteration 100, Loss: 0.6359\n",
      "Iteration 200, Loss: 0.6036\n",
      "Iteration 300, Loss: 0.5853\n",
      "Iteration 400, Loss: 0.5726\n",
      "Iteration 500, Loss: 0.5628\n",
      "Iteration 600, Loss: 0.5549\n",
      "Iteration 700, Loss: 0.5484\n",
      "Iteration 800, Loss: 0.5428\n",
      "Iteration 900, Loss: 0.5378\n",
      "Iteration 1000, Loss: 0.5335\n",
      "Iteration 1100, Loss: 0.5296\n",
      "Iteration 1200, Loss: 0.5261\n",
      "Iteration 1300, Loss: 0.5229\n",
      "Iteration 1400, Loss: 0.5200\n",
      "Iteration 1500, Loss: 0.5173\n",
      "Iteration 1600, Loss: 0.5148\n",
      "Iteration 1700, Loss: 0.5125\n",
      "Iteration 1800, Loss: 0.5103\n",
      "Iteration 1900, Loss: 0.5082\n",
      "Iteration 2000, Loss: 0.5063\n",
      "Iteration 2100, Loss: 0.5044\n",
      "Iteration 2200, Loss: 0.5027\n",
      "Iteration 2300, Loss: 0.5010\n",
      "Iteration 2400, Loss: 0.4994\n",
      "Iteration 2500, Loss: 0.4979\n",
      "Iteration 2600, Loss: 0.4965\n",
      "Iteration 2700, Loss: 0.4951\n",
      "Iteration 2800, Loss: 0.4938\n",
      "Iteration 2900, Loss: 0.4925\n",
      "Iteration 3000, Loss: 0.4913\n",
      "Iteration 3100, Loss: 0.4901\n",
      "Iteration 3200, Loss: 0.4889\n",
      "Iteration 3300, Loss: 0.4878\n",
      "Iteration 3400, Loss: 0.4868\n",
      "Iteration 3500, Loss: 0.4857\n",
      "Iteration 3600, Loss: 0.4847\n",
      "Iteration 3700, Loss: 0.4838\n",
      "Iteration 3800, Loss: 0.4828\n",
      "Iteration 3900, Loss: 0.4819\n",
      "Iteration 4000, Loss: 0.4810\n",
      "Iteration 4100, Loss: 0.4801\n",
      "Iteration 4200, Loss: 0.4793\n",
      "Iteration 4300, Loss: 0.4785\n",
      "Iteration 4400, Loss: 0.4777\n",
      "Iteration 4500, Loss: 0.4769\n",
      "Iteration 4600, Loss: 0.4762\n",
      "Iteration 4700, Loss: 0.4754\n",
      "Iteration 4800, Loss: 0.4747\n",
      "Iteration 4900, Loss: 0.4740\n",
      "14 36\n",
      "Iteration 0, Loss: 1.0517\n",
      "Iteration 100, Loss: 0.6514\n",
      "Iteration 200, Loss: 0.6179\n",
      "Iteration 300, Loss: 0.6003\n",
      "Iteration 400, Loss: 0.5882\n",
      "Iteration 500, Loss: 0.5791\n",
      "Iteration 600, Loss: 0.5714\n",
      "Iteration 700, Loss: 0.5651\n",
      "Iteration 800, Loss: 0.5598\n",
      "Iteration 900, Loss: 0.5552\n",
      "Iteration 1000, Loss: 0.5510\n",
      "Iteration 1100, Loss: 0.5472\n",
      "Iteration 1200, Loss: 0.5439\n",
      "Iteration 1300, Loss: 0.5408\n",
      "Iteration 1400, Loss: 0.5380\n",
      "Iteration 1500, Loss: 0.5354\n",
      "Iteration 1600, Loss: 0.5329\n",
      "Iteration 1700, Loss: 0.5306\n",
      "Iteration 1800, Loss: 0.5285\n",
      "Iteration 1900, Loss: 0.5264\n",
      "Iteration 2000, Loss: 0.5246\n",
      "Iteration 2100, Loss: 0.5227\n",
      "Iteration 2200, Loss: 0.5211\n",
      "Iteration 2300, Loss: 0.5194\n",
      "Iteration 2400, Loss: 0.5179\n",
      "Iteration 2500, Loss: 0.5164\n",
      "Iteration 2600, Loss: 0.5150\n",
      "Iteration 2700, Loss: 0.5136\n",
      "Iteration 2800, Loss: 0.5123\n",
      "Iteration 2900, Loss: 0.5111\n",
      "Iteration 3000, Loss: 0.5099\n",
      "Iteration 3100, Loss: 0.5087\n",
      "Iteration 3200, Loss: 0.5076\n",
      "Iteration 3300, Loss: 0.5066\n",
      "Iteration 3400, Loss: 0.5055\n",
      "Iteration 3500, Loss: 0.5045\n",
      "Iteration 3600, Loss: 0.5035\n",
      "Iteration 3700, Loss: 0.5025\n",
      "Iteration 3800, Loss: 0.5016\n",
      "Iteration 3900, Loss: 0.5007\n",
      "Iteration 4000, Loss: 0.4998\n",
      "Iteration 4100, Loss: 0.4990\n",
      "Iteration 4200, Loss: 0.4981\n",
      "Iteration 4300, Loss: 0.4974\n",
      "Iteration 4400, Loss: 0.4966\n",
      "Iteration 4500, Loss: 0.4958\n",
      "Iteration 4600, Loss: 0.4950\n",
      "Iteration 4700, Loss: 0.4944\n",
      "Iteration 4800, Loss: 0.4936\n",
      "Iteration 4900, Loss: 0.4929\n",
      "Iteration 0, Loss: 1.0548\n",
      "Iteration 100, Loss: 0.6611\n",
      "Iteration 200, Loss: 0.6239\n",
      "Iteration 300, Loss: 0.6046\n",
      "Iteration 400, Loss: 0.5916\n",
      "Iteration 500, Loss: 0.5818\n",
      "Iteration 600, Loss: 0.5741\n",
      "Iteration 700, Loss: 0.5675\n",
      "Iteration 800, Loss: 0.5620\n",
      "Iteration 900, Loss: 0.5572\n",
      "Iteration 1000, Loss: 0.5530\n",
      "Iteration 1100, Loss: 0.5493\n",
      "Iteration 1200, Loss: 0.5459\n",
      "Iteration 1300, Loss: 0.5428\n",
      "Iteration 1400, Loss: 0.5400\n",
      "Iteration 1500, Loss: 0.5374\n",
      "Iteration 1600, Loss: 0.5350\n",
      "Iteration 1700, Loss: 0.5328\n",
      "Iteration 1800, Loss: 0.5307\n",
      "Iteration 1900, Loss: 0.5287\n",
      "Iteration 2000, Loss: 0.5269\n",
      "Iteration 2100, Loss: 0.5251\n",
      "Iteration 2200, Loss: 0.5235\n",
      "Iteration 2300, Loss: 0.5219\n",
      "Iteration 2400, Loss: 0.5204\n",
      "Iteration 2500, Loss: 0.5190\n",
      "Iteration 2600, Loss: 0.5176\n",
      "Iteration 2700, Loss: 0.5163\n",
      "Iteration 2800, Loss: 0.5151\n",
      "Iteration 2900, Loss: 0.5138\n",
      "Iteration 3000, Loss: 0.5126\n",
      "Iteration 3100, Loss: 0.5115\n",
      "Iteration 3200, Loss: 0.5105\n",
      "Iteration 3300, Loss: 0.5094\n",
      "Iteration 3400, Loss: 0.5084\n",
      "Iteration 3500, Loss: 0.5074\n",
      "Iteration 3600, Loss: 0.5065\n",
      "Iteration 3700, Loss: 0.5056\n",
      "Iteration 3800, Loss: 0.5047\n",
      "Iteration 3900, Loss: 0.5039\n",
      "Iteration 4000, Loss: 0.5030\n",
      "Iteration 4100, Loss: 0.5022\n",
      "Iteration 4200, Loss: 0.5014\n",
      "Iteration 4300, Loss: 0.5006\n",
      "Iteration 4400, Loss: 0.4999\n",
      "Iteration 4500, Loss: 0.4992\n",
      "Iteration 4600, Loss: 0.4985\n",
      "Iteration 4700, Loss: 0.4978\n",
      "Iteration 4800, Loss: 0.4971\n",
      "Iteration 4900, Loss: 0.4964\n",
      "15 36\n",
      "Iteration 0, Loss: 1.0547\n",
      "Iteration 100, Loss: 0.6520\n",
      "Iteration 200, Loss: 0.6166\n",
      "Iteration 300, Loss: 0.5984\n",
      "Iteration 400, Loss: 0.5859\n",
      "Iteration 500, Loss: 0.5763\n",
      "Iteration 600, Loss: 0.5686\n",
      "Iteration 700, Loss: 0.5623\n",
      "Iteration 800, Loss: 0.5567\n",
      "Iteration 900, Loss: 0.5519\n",
      "Iteration 1000, Loss: 0.5477\n",
      "Iteration 1100, Loss: 0.5438\n",
      "Iteration 1200, Loss: 0.5403\n",
      "Iteration 1300, Loss: 0.5371\n",
      "Iteration 1400, Loss: 0.5342\n",
      "Iteration 1500, Loss: 0.5315\n",
      "Iteration 1600, Loss: 0.5291\n",
      "Iteration 1700, Loss: 0.5267\n",
      "Iteration 1800, Loss: 0.5244\n",
      "Iteration 1900, Loss: 0.5224\n",
      "Iteration 2000, Loss: 0.5204\n",
      "Iteration 2100, Loss: 0.5185\n",
      "Iteration 2200, Loss: 0.5168\n",
      "Iteration 2300, Loss: 0.5151\n",
      "Iteration 2400, Loss: 0.5135\n",
      "Iteration 2500, Loss: 0.5120\n",
      "Iteration 2600, Loss: 0.5105\n",
      "Iteration 2700, Loss: 0.5092\n",
      "Iteration 2800, Loss: 0.5078\n",
      "Iteration 2900, Loss: 0.5065\n",
      "Iteration 3000, Loss: 0.5052\n",
      "Iteration 3100, Loss: 0.5040\n",
      "Iteration 3200, Loss: 0.5028\n",
      "Iteration 3300, Loss: 0.5017\n",
      "Iteration 3400, Loss: 0.5006\n",
      "Iteration 3500, Loss: 0.4996\n",
      "Iteration 3600, Loss: 0.4985\n",
      "Iteration 3700, Loss: 0.4975\n",
      "Iteration 3800, Loss: 0.4965\n",
      "Iteration 3900, Loss: 0.4956\n",
      "Iteration 4000, Loss: 0.4947\n",
      "Iteration 4100, Loss: 0.4938\n",
      "Iteration 4200, Loss: 0.4929\n",
      "Iteration 4300, Loss: 0.4921\n",
      "Iteration 4400, Loss: 0.4913\n",
      "Iteration 4500, Loss: 0.4904\n",
      "Iteration 4600, Loss: 0.4896\n",
      "Iteration 4700, Loss: 0.4889\n",
      "Iteration 4800, Loss: 0.4881\n",
      "Iteration 4900, Loss: 0.4874\n",
      "Iteration 0, Loss: 1.0533\n",
      "Iteration 100, Loss: 0.6604\n",
      "Iteration 200, Loss: 0.6254\n",
      "Iteration 300, Loss: 0.6067\n",
      "Iteration 400, Loss: 0.5938\n",
      "Iteration 500, Loss: 0.5840\n",
      "Iteration 600, Loss: 0.5761\n",
      "Iteration 700, Loss: 0.5696\n",
      "Iteration 800, Loss: 0.5640\n",
      "Iteration 900, Loss: 0.5591\n",
      "Iteration 1000, Loss: 0.5547\n",
      "Iteration 1100, Loss: 0.5508\n",
      "Iteration 1200, Loss: 0.5473\n",
      "Iteration 1300, Loss: 0.5440\n",
      "Iteration 1400, Loss: 0.5411\n",
      "Iteration 1500, Loss: 0.5383\n",
      "Iteration 1600, Loss: 0.5357\n",
      "Iteration 1700, Loss: 0.5333\n",
      "Iteration 1800, Loss: 0.5311\n",
      "Iteration 1900, Loss: 0.5289\n",
      "Iteration 2000, Loss: 0.5269\n",
      "Iteration 2100, Loss: 0.5250\n",
      "Iteration 2200, Loss: 0.5232\n",
      "Iteration 2300, Loss: 0.5215\n",
      "Iteration 2400, Loss: 0.5198\n",
      "Iteration 2500, Loss: 0.5182\n",
      "Iteration 2600, Loss: 0.5167\n",
      "Iteration 2700, Loss: 0.5152\n",
      "Iteration 2800, Loss: 0.5138\n",
      "Iteration 2900, Loss: 0.5125\n",
      "Iteration 3000, Loss: 0.5112\n",
      "Iteration 3100, Loss: 0.5099\n",
      "Iteration 3200, Loss: 0.5087\n",
      "Iteration 3300, Loss: 0.5076\n",
      "Iteration 3400, Loss: 0.5064\n",
      "Iteration 3500, Loss: 0.5053\n",
      "Iteration 3600, Loss: 0.5042\n",
      "Iteration 3700, Loss: 0.5031\n",
      "Iteration 3800, Loss: 0.5021\n",
      "Iteration 3900, Loss: 0.5011\n",
      "Iteration 4000, Loss: 0.5001\n",
      "Iteration 4100, Loss: 0.4992\n",
      "Iteration 4200, Loss: 0.4982\n",
      "Iteration 4300, Loss: 0.4973\n",
      "Iteration 4400, Loss: 0.4964\n",
      "Iteration 4500, Loss: 0.4956\n",
      "Iteration 4600, Loss: 0.4948\n",
      "Iteration 4700, Loss: 0.4939\n",
      "Iteration 4800, Loss: 0.4931\n",
      "Iteration 4900, Loss: 0.4923\n",
      "16 36\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7901\n",
      "Iteration 200, Loss: 0.7261\n",
      "Iteration 300, Loss: 0.6965\n",
      "Iteration 400, Loss: 0.6781\n",
      "Iteration 500, Loss: 0.6650\n",
      "Iteration 600, Loss: 0.6549\n",
      "Iteration 700, Loss: 0.6467\n",
      "Iteration 800, Loss: 0.6399\n",
      "Iteration 900, Loss: 0.6339\n",
      "Iteration 1000, Loss: 0.6288\n",
      "Iteration 1100, Loss: 0.6241\n",
      "Iteration 1200, Loss: 0.6200\n",
      "Iteration 1300, Loss: 0.6162\n",
      "Iteration 1400, Loss: 0.6127\n",
      "Iteration 1500, Loss: 0.6095\n",
      "Iteration 1600, Loss: 0.6065\n",
      "Iteration 1700, Loss: 0.6038\n",
      "Iteration 1800, Loss: 0.6012\n",
      "Iteration 1900, Loss: 0.5987\n",
      "Iteration 2000, Loss: 0.5964\n",
      "Iteration 2100, Loss: 0.5942\n",
      "Iteration 2200, Loss: 0.5921\n",
      "Iteration 2300, Loss: 0.5901\n",
      "Iteration 2400, Loss: 0.5881\n",
      "Iteration 2500, Loss: 0.5863\n",
      "Iteration 2600, Loss: 0.5846\n",
      "Iteration 2700, Loss: 0.5829\n",
      "Iteration 2800, Loss: 0.5813\n",
      "Iteration 2900, Loss: 0.5797\n",
      "Iteration 3000, Loss: 0.5783\n",
      "Iteration 3100, Loss: 0.5768\n",
      "Iteration 3200, Loss: 0.5754\n",
      "Iteration 3300, Loss: 0.5741\n",
      "Iteration 3400, Loss: 0.5728\n",
      "Iteration 3500, Loss: 0.5715\n",
      "Iteration 3600, Loss: 0.5703\n",
      "Iteration 3700, Loss: 0.5691\n",
      "Iteration 3800, Loss: 0.5680\n",
      "Iteration 3900, Loss: 0.5669\n",
      "Iteration 4000, Loss: 0.5658\n",
      "Iteration 4100, Loss: 0.5647\n",
      "Iteration 4200, Loss: 0.5637\n",
      "Iteration 4300, Loss: 0.5627\n",
      "Iteration 4400, Loss: 0.5618\n",
      "Iteration 4500, Loss: 0.5608\n",
      "Iteration 4600, Loss: 0.5599\n",
      "Iteration 4700, Loss: 0.5590\n",
      "Iteration 4800, Loss: 0.5581\n",
      "Iteration 4900, Loss: 0.5573\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7814\n",
      "Iteration 200, Loss: 0.7143\n",
      "Iteration 300, Loss: 0.6835\n",
      "Iteration 400, Loss: 0.6644\n",
      "Iteration 500, Loss: 0.6508\n",
      "Iteration 600, Loss: 0.6404\n",
      "Iteration 700, Loss: 0.6321\n",
      "Iteration 800, Loss: 0.6251\n",
      "Iteration 900, Loss: 0.6192\n",
      "Iteration 1000, Loss: 0.6141\n",
      "Iteration 1100, Loss: 0.6096\n",
      "Iteration 1200, Loss: 0.6055\n",
      "Iteration 1300, Loss: 0.6018\n",
      "Iteration 1400, Loss: 0.5985\n",
      "Iteration 1500, Loss: 0.5954\n",
      "Iteration 1600, Loss: 0.5925\n",
      "Iteration 1700, Loss: 0.5898\n",
      "Iteration 1800, Loss: 0.5873\n",
      "Iteration 1900, Loss: 0.5849\n",
      "Iteration 2000, Loss: 0.5827\n",
      "Iteration 2100, Loss: 0.5805\n",
      "Iteration 2200, Loss: 0.5785\n",
      "Iteration 2300, Loss: 0.5766\n",
      "Iteration 2400, Loss: 0.5747\n",
      "Iteration 2500, Loss: 0.5730\n",
      "Iteration 2600, Loss: 0.5713\n",
      "Iteration 2700, Loss: 0.5696\n",
      "Iteration 2800, Loss: 0.5681\n",
      "Iteration 2900, Loss: 0.5666\n",
      "Iteration 3000, Loss: 0.5651\n",
      "Iteration 3100, Loss: 0.5637\n",
      "Iteration 3200, Loss: 0.5623\n",
      "Iteration 3300, Loss: 0.5610\n",
      "Iteration 3400, Loss: 0.5597\n",
      "Iteration 3500, Loss: 0.5585\n",
      "Iteration 3600, Loss: 0.5573\n",
      "Iteration 3700, Loss: 0.5561\n",
      "Iteration 3800, Loss: 0.5550\n",
      "Iteration 3900, Loss: 0.5539\n",
      "Iteration 4000, Loss: 0.5528\n",
      "Iteration 4100, Loss: 0.5518\n",
      "Iteration 4200, Loss: 0.5508\n",
      "Iteration 4300, Loss: 0.5498\n",
      "Iteration 4400, Loss: 0.5488\n",
      "Iteration 4500, Loss: 0.5479\n",
      "Iteration 4600, Loss: 0.5470\n",
      "Iteration 4700, Loss: 0.5461\n",
      "Iteration 4800, Loss: 0.5452\n",
      "Iteration 4900, Loss: 0.5443\n",
      "17 36\n",
      "Iteration 0, Loss: 1.0887\n",
      "Iteration 100, Loss: 0.7755\n",
      "Iteration 200, Loss: 0.7113\n",
      "Iteration 300, Loss: 0.6821\n",
      "Iteration 400, Loss: 0.6642\n",
      "Iteration 500, Loss: 0.6515\n",
      "Iteration 600, Loss: 0.6417\n",
      "Iteration 700, Loss: 0.6339\n",
      "Iteration 800, Loss: 0.6273\n",
      "Iteration 900, Loss: 0.6217\n",
      "Iteration 1000, Loss: 0.6168\n",
      "Iteration 1100, Loss: 0.6125\n",
      "Iteration 1200, Loss: 0.6087\n",
      "Iteration 1300, Loss: 0.6051\n",
      "Iteration 1400, Loss: 0.6019\n",
      "Iteration 1500, Loss: 0.5989\n",
      "Iteration 1600, Loss: 0.5962\n",
      "Iteration 1700, Loss: 0.5937\n",
      "Iteration 1800, Loss: 0.5913\n",
      "Iteration 1900, Loss: 0.5890\n",
      "Iteration 2000, Loss: 0.5869\n",
      "Iteration 2100, Loss: 0.5848\n",
      "Iteration 2200, Loss: 0.5829\n",
      "Iteration 2300, Loss: 0.5811\n",
      "Iteration 2400, Loss: 0.5794\n",
      "Iteration 2500, Loss: 0.5777\n",
      "Iteration 2600, Loss: 0.5761\n",
      "Iteration 2700, Loss: 0.5746\n",
      "Iteration 2800, Loss: 0.5731\n",
      "Iteration 2900, Loss: 0.5717\n",
      "Iteration 3000, Loss: 0.5703\n",
      "Iteration 3100, Loss: 0.5690\n",
      "Iteration 3200, Loss: 0.5677\n",
      "Iteration 3300, Loss: 0.5665\n",
      "Iteration 3400, Loss: 0.5653\n",
      "Iteration 3500, Loss: 0.5641\n",
      "Iteration 3600, Loss: 0.5630\n",
      "Iteration 3700, Loss: 0.5619\n",
      "Iteration 3800, Loss: 0.5609\n",
      "Iteration 3900, Loss: 0.5599\n",
      "Iteration 4000, Loss: 0.5589\n",
      "Iteration 4100, Loss: 0.5579\n",
      "Iteration 4200, Loss: 0.5569\n",
      "Iteration 4300, Loss: 0.5560\n",
      "Iteration 4400, Loss: 0.5551\n",
      "Iteration 4500, Loss: 0.5542\n",
      "Iteration 4600, Loss: 0.5534\n",
      "Iteration 4700, Loss: 0.5526\n",
      "Iteration 4800, Loss: 0.5517\n",
      "Iteration 4900, Loss: 0.5509\n",
      "Iteration 0, Loss: 1.0901\n",
      "Iteration 100, Loss: 0.7966\n",
      "Iteration 200, Loss: 0.7297\n",
      "Iteration 300, Loss: 0.6988\n",
      "Iteration 400, Loss: 0.6798\n",
      "Iteration 500, Loss: 0.6663\n",
      "Iteration 600, Loss: 0.6560\n",
      "Iteration 700, Loss: 0.6477\n",
      "Iteration 800, Loss: 0.6408\n",
      "Iteration 900, Loss: 0.6349\n",
      "Iteration 1000, Loss: 0.6297\n",
      "Iteration 1100, Loss: 0.6251\n",
      "Iteration 1200, Loss: 0.6210\n",
      "Iteration 1300, Loss: 0.6172\n",
      "Iteration 1400, Loss: 0.6137\n",
      "Iteration 1500, Loss: 0.6105\n",
      "Iteration 1600, Loss: 0.6075\n",
      "Iteration 1700, Loss: 0.6047\n",
      "Iteration 1800, Loss: 0.6021\n",
      "Iteration 1900, Loss: 0.5996\n",
      "Iteration 2000, Loss: 0.5973\n",
      "Iteration 2100, Loss: 0.5951\n",
      "Iteration 2200, Loss: 0.5929\n",
      "Iteration 2300, Loss: 0.5909\n",
      "Iteration 2400, Loss: 0.5890\n",
      "Iteration 2500, Loss: 0.5871\n",
      "Iteration 2600, Loss: 0.5854\n",
      "Iteration 2700, Loss: 0.5836\n",
      "Iteration 2800, Loss: 0.5820\n",
      "Iteration 2900, Loss: 0.5804\n",
      "Iteration 3000, Loss: 0.5789\n",
      "Iteration 3100, Loss: 0.5774\n",
      "Iteration 3200, Loss: 0.5760\n",
      "Iteration 3300, Loss: 0.5746\n",
      "Iteration 3400, Loss: 0.5733\n",
      "Iteration 3500, Loss: 0.5720\n",
      "Iteration 3600, Loss: 0.5707\n",
      "Iteration 3700, Loss: 0.5695\n",
      "Iteration 3800, Loss: 0.5683\n",
      "Iteration 3900, Loss: 0.5672\n",
      "Iteration 4000, Loss: 0.5660\n",
      "Iteration 4100, Loss: 0.5650\n",
      "Iteration 4200, Loss: 0.5639\n",
      "Iteration 4300, Loss: 0.5629\n",
      "Iteration 4400, Loss: 0.5618\n",
      "Iteration 4500, Loss: 0.5609\n",
      "Iteration 4600, Loss: 0.5599\n",
      "Iteration 4700, Loss: 0.5589\n",
      "Iteration 4800, Loss: 0.5580\n",
      "Iteration 4900, Loss: 0.5571\n",
      "18 36\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8518\n",
      "Iteration 200, Loss: 0.7700\n",
      "Iteration 300, Loss: 0.7291\n",
      "Iteration 400, Loss: 0.7051\n",
      "Iteration 500, Loss: 0.6887\n",
      "Iteration 600, Loss: 0.6765\n",
      "Iteration 700, Loss: 0.6671\n",
      "Iteration 800, Loss: 0.6594\n",
      "Iteration 900, Loss: 0.6529\n",
      "Iteration 1000, Loss: 0.6473\n",
      "Iteration 1100, Loss: 0.6424\n",
      "Iteration 1200, Loss: 0.6380\n",
      "Iteration 1300, Loss: 0.6341\n",
      "Iteration 1400, Loss: 0.6305\n",
      "Iteration 1500, Loss: 0.6273\n",
      "Iteration 1600, Loss: 0.6242\n",
      "Iteration 1700, Loss: 0.6215\n",
      "Iteration 1800, Loss: 0.6189\n",
      "Iteration 1900, Loss: 0.6164\n",
      "Iteration 2000, Loss: 0.6141\n",
      "Iteration 2100, Loss: 0.6119\n",
      "Iteration 2200, Loss: 0.6098\n",
      "Iteration 2300, Loss: 0.6079\n",
      "Iteration 2400, Loss: 0.6060\n",
      "Iteration 2500, Loss: 0.6042\n",
      "Iteration 2600, Loss: 0.6025\n",
      "Iteration 2700, Loss: 0.6008\n",
      "Iteration 2800, Loss: 0.5992\n",
      "Iteration 2900, Loss: 0.5977\n",
      "Iteration 3000, Loss: 0.5962\n",
      "Iteration 3100, Loss: 0.5948\n",
      "Iteration 3200, Loss: 0.5934\n",
      "Iteration 3300, Loss: 0.5920\n",
      "Iteration 3400, Loss: 0.5907\n",
      "Iteration 3500, Loss: 0.5895\n",
      "Iteration 3600, Loss: 0.5883\n",
      "Iteration 3700, Loss: 0.5871\n",
      "Iteration 3800, Loss: 0.5859\n",
      "Iteration 3900, Loss: 0.5848\n",
      "Iteration 4000, Loss: 0.5837\n",
      "Iteration 4100, Loss: 0.5827\n",
      "Iteration 4200, Loss: 0.5816\n",
      "Iteration 4300, Loss: 0.5806\n",
      "Iteration 4400, Loss: 0.5796\n",
      "Iteration 4500, Loss: 0.5786\n",
      "Iteration 4600, Loss: 0.5777\n",
      "Iteration 4700, Loss: 0.5768\n",
      "Iteration 4800, Loss: 0.5759\n",
      "Iteration 4900, Loss: 0.5750\n",
      "Iteration 0, Loss: 1.0938\n",
      "Iteration 100, Loss: 0.8683\n",
      "Iteration 200, Loss: 0.7922\n",
      "Iteration 300, Loss: 0.7526\n",
      "Iteration 400, Loss: 0.7279\n",
      "Iteration 500, Loss: 0.7106\n",
      "Iteration 600, Loss: 0.6975\n",
      "Iteration 700, Loss: 0.6870\n",
      "Iteration 800, Loss: 0.6782\n",
      "Iteration 900, Loss: 0.6708\n",
      "Iteration 1000, Loss: 0.6644\n",
      "Iteration 1100, Loss: 0.6587\n",
      "Iteration 1200, Loss: 0.6537\n",
      "Iteration 1300, Loss: 0.6492\n",
      "Iteration 1400, Loss: 0.6450\n",
      "Iteration 1500, Loss: 0.6413\n",
      "Iteration 1600, Loss: 0.6378\n",
      "Iteration 1700, Loss: 0.6347\n",
      "Iteration 1800, Loss: 0.6317\n",
      "Iteration 1900, Loss: 0.6289\n",
      "Iteration 2000, Loss: 0.6263\n",
      "Iteration 2100, Loss: 0.6239\n",
      "Iteration 2200, Loss: 0.6217\n",
      "Iteration 2300, Loss: 0.6195\n",
      "Iteration 2400, Loss: 0.6174\n",
      "Iteration 2500, Loss: 0.6155\n",
      "Iteration 2600, Loss: 0.6136\n",
      "Iteration 2700, Loss: 0.6119\n",
      "Iteration 2800, Loss: 0.6102\n",
      "Iteration 2900, Loss: 0.6085\n",
      "Iteration 3000, Loss: 0.6070\n",
      "Iteration 3100, Loss: 0.6054\n",
      "Iteration 3200, Loss: 0.6040\n",
      "Iteration 3300, Loss: 0.6026\n",
      "Iteration 3400, Loss: 0.6012\n",
      "Iteration 3500, Loss: 0.5999\n",
      "Iteration 3600, Loss: 0.5986\n",
      "Iteration 3700, Loss: 0.5974\n",
      "Iteration 3800, Loss: 0.5962\n",
      "Iteration 3900, Loss: 0.5950\n",
      "Iteration 4000, Loss: 0.5939\n",
      "Iteration 4100, Loss: 0.5928\n",
      "Iteration 4200, Loss: 0.5917\n",
      "Iteration 4300, Loss: 0.5907\n",
      "Iteration 4400, Loss: 0.5897\n",
      "Iteration 4500, Loss: 0.5887\n",
      "Iteration 4600, Loss: 0.5877\n",
      "Iteration 4700, Loss: 0.5867\n",
      "Iteration 4800, Loss: 0.5858\n",
      "Iteration 4900, Loss: 0.5849\n",
      "19 36\n",
      "Iteration 0, Loss: 1.0934\n",
      "Iteration 100, Loss: 0.8575\n",
      "Iteration 200, Loss: 0.7780\n",
      "Iteration 300, Loss: 0.7382\n",
      "Iteration 400, Loss: 0.7144\n",
      "Iteration 500, Loss: 0.6982\n",
      "Iteration 600, Loss: 0.6862\n",
      "Iteration 700, Loss: 0.6766\n",
      "Iteration 800, Loss: 0.6686\n",
      "Iteration 900, Loss: 0.6619\n",
      "Iteration 1000, Loss: 0.6561\n",
      "Iteration 1100, Loss: 0.6510\n",
      "Iteration 1200, Loss: 0.6464\n",
      "Iteration 1300, Loss: 0.6422\n",
      "Iteration 1400, Loss: 0.6384\n",
      "Iteration 1500, Loss: 0.6349\n",
      "Iteration 1600, Loss: 0.6317\n",
      "Iteration 1700, Loss: 0.6286\n",
      "Iteration 1800, Loss: 0.6258\n",
      "Iteration 1900, Loss: 0.6232\n",
      "Iteration 2000, Loss: 0.6207\n",
      "Iteration 2100, Loss: 0.6184\n",
      "Iteration 2200, Loss: 0.6161\n",
      "Iteration 2300, Loss: 0.6140\n",
      "Iteration 2400, Loss: 0.6120\n",
      "Iteration 2500, Loss: 0.6100\n",
      "Iteration 2600, Loss: 0.6082\n",
      "Iteration 2700, Loss: 0.6064\n",
      "Iteration 2800, Loss: 0.6047\n",
      "Iteration 2900, Loss: 0.6031\n",
      "Iteration 3000, Loss: 0.6015\n",
      "Iteration 3100, Loss: 0.6000\n",
      "Iteration 3200, Loss: 0.5986\n",
      "Iteration 3300, Loss: 0.5971\n",
      "Iteration 3400, Loss: 0.5958\n",
      "Iteration 3500, Loss: 0.5945\n",
      "Iteration 3600, Loss: 0.5932\n",
      "Iteration 3700, Loss: 0.5919\n",
      "Iteration 3800, Loss: 0.5907\n",
      "Iteration 3900, Loss: 0.5896\n",
      "Iteration 4000, Loss: 0.5884\n",
      "Iteration 4100, Loss: 0.5873\n",
      "Iteration 4200, Loss: 0.5862\n",
      "Iteration 4300, Loss: 0.5852\n",
      "Iteration 4400, Loss: 0.5842\n",
      "Iteration 4500, Loss: 0.5832\n",
      "Iteration 4600, Loss: 0.5822\n",
      "Iteration 4700, Loss: 0.5812\n",
      "Iteration 4800, Loss: 0.5803\n",
      "Iteration 4900, Loss: 0.5794\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8650\n",
      "Iteration 200, Loss: 0.7868\n",
      "Iteration 300, Loss: 0.7468\n",
      "Iteration 400, Loss: 0.7219\n",
      "Iteration 500, Loss: 0.7045\n",
      "Iteration 600, Loss: 0.6915\n",
      "Iteration 700, Loss: 0.6811\n",
      "Iteration 800, Loss: 0.6725\n",
      "Iteration 900, Loss: 0.6653\n",
      "Iteration 1000, Loss: 0.6590\n",
      "Iteration 1100, Loss: 0.6536\n",
      "Iteration 1200, Loss: 0.6487\n",
      "Iteration 1300, Loss: 0.6444\n",
      "Iteration 1400, Loss: 0.6404\n",
      "Iteration 1500, Loss: 0.6369\n",
      "Iteration 1600, Loss: 0.6336\n",
      "Iteration 1700, Loss: 0.6306\n",
      "Iteration 1800, Loss: 0.6278\n",
      "Iteration 1900, Loss: 0.6252\n",
      "Iteration 2000, Loss: 0.6228\n",
      "Iteration 2100, Loss: 0.6205\n",
      "Iteration 2200, Loss: 0.6184\n",
      "Iteration 2300, Loss: 0.6163\n",
      "Iteration 2400, Loss: 0.6144\n",
      "Iteration 2500, Loss: 0.6126\n",
      "Iteration 2600, Loss: 0.6108\n",
      "Iteration 2700, Loss: 0.6092\n",
      "Iteration 2800, Loss: 0.6075\n",
      "Iteration 2900, Loss: 0.6060\n",
      "Iteration 3000, Loss: 0.6045\n",
      "Iteration 3100, Loss: 0.6031\n",
      "Iteration 3200, Loss: 0.6017\n",
      "Iteration 3300, Loss: 0.6004\n",
      "Iteration 3400, Loss: 0.5991\n",
      "Iteration 3500, Loss: 0.5979\n",
      "Iteration 3600, Loss: 0.5966\n",
      "Iteration 3700, Loss: 0.5955\n",
      "Iteration 3800, Loss: 0.5943\n",
      "Iteration 3900, Loss: 0.5932\n",
      "Iteration 4000, Loss: 0.5921\n",
      "Iteration 4100, Loss: 0.5911\n",
      "Iteration 4200, Loss: 0.5901\n",
      "Iteration 4300, Loss: 0.5891\n",
      "Iteration 4400, Loss: 0.5881\n",
      "Iteration 4500, Loss: 0.5871\n",
      "Iteration 4600, Loss: 0.5862\n",
      "Iteration 4700, Loss: 0.5853\n",
      "Iteration 4800, Loss: 0.5844\n",
      "Iteration 4900, Loss: 0.5835\n",
      "20 36\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0210\n",
      "Iteration 200, Loss: 0.9671\n",
      "Iteration 300, Loss: 0.9262\n",
      "Iteration 400, Loss: 0.8942\n",
      "Iteration 500, Loss: 0.8683\n",
      "Iteration 600, Loss: 0.8469\n",
      "Iteration 700, Loss: 0.8290\n",
      "Iteration 800, Loss: 0.8137\n",
      "Iteration 900, Loss: 0.8005\n",
      "Iteration 1000, Loss: 0.7890\n",
      "Iteration 1100, Loss: 0.7790\n",
      "Iteration 1200, Loss: 0.7701\n",
      "Iteration 1300, Loss: 0.7621\n",
      "Iteration 1400, Loss: 0.7550\n",
      "Iteration 1500, Loss: 0.7486\n",
      "Iteration 1600, Loss: 0.7428\n",
      "Iteration 1700, Loss: 0.7375\n",
      "Iteration 1800, Loss: 0.7326\n",
      "Iteration 1900, Loss: 0.7282\n",
      "Iteration 2000, Loss: 0.7240\n",
      "Iteration 2100, Loss: 0.7202\n",
      "Iteration 2200, Loss: 0.7167\n",
      "Iteration 2300, Loss: 0.7133\n",
      "Iteration 2400, Loss: 0.7102\n",
      "Iteration 2500, Loss: 0.7073\n",
      "Iteration 2600, Loss: 0.7045\n",
      "Iteration 2700, Loss: 0.7018\n",
      "Iteration 2800, Loss: 0.6993\n",
      "Iteration 2900, Loss: 0.6970\n",
      "Iteration 3000, Loss: 0.6947\n",
      "Iteration 3100, Loss: 0.6926\n",
      "Iteration 3200, Loss: 0.6905\n",
      "Iteration 3300, Loss: 0.6885\n",
      "Iteration 3400, Loss: 0.6866\n",
      "Iteration 3500, Loss: 0.6848\n",
      "Iteration 3600, Loss: 0.6831\n",
      "Iteration 3700, Loss: 0.6814\n",
      "Iteration 3800, Loss: 0.6798\n",
      "Iteration 3900, Loss: 0.6782\n",
      "Iteration 4000, Loss: 0.6767\n",
      "Iteration 4100, Loss: 0.6752\n",
      "Iteration 4200, Loss: 0.6738\n",
      "Iteration 4300, Loss: 0.6725\n",
      "Iteration 4400, Loss: 0.6711\n",
      "Iteration 4500, Loss: 0.6698\n",
      "Iteration 4600, Loss: 0.6686\n",
      "Iteration 4700, Loss: 0.6674\n",
      "Iteration 4800, Loss: 0.6662\n",
      "Iteration 4900, Loss: 0.6650\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0184\n",
      "Iteration 200, Loss: 0.9639\n",
      "Iteration 300, Loss: 0.9230\n",
      "Iteration 400, Loss: 0.8911\n",
      "Iteration 500, Loss: 0.8652\n",
      "Iteration 600, Loss: 0.8439\n",
      "Iteration 700, Loss: 0.8259\n",
      "Iteration 800, Loss: 0.8106\n",
      "Iteration 900, Loss: 0.7974\n",
      "Iteration 1000, Loss: 0.7858\n",
      "Iteration 1100, Loss: 0.7757\n",
      "Iteration 1200, Loss: 0.7667\n",
      "Iteration 1300, Loss: 0.7586\n",
      "Iteration 1400, Loss: 0.7514\n",
      "Iteration 1500, Loss: 0.7448\n",
      "Iteration 1600, Loss: 0.7388\n",
      "Iteration 1700, Loss: 0.7333\n",
      "Iteration 1800, Loss: 0.7283\n",
      "Iteration 1900, Loss: 0.7237\n",
      "Iteration 2000, Loss: 0.7194\n",
      "Iteration 2100, Loss: 0.7154\n",
      "Iteration 2200, Loss: 0.7116\n",
      "Iteration 2300, Loss: 0.7081\n",
      "Iteration 2400, Loss: 0.7049\n",
      "Iteration 2500, Loss: 0.7018\n",
      "Iteration 2600, Loss: 0.6988\n",
      "Iteration 2700, Loss: 0.6961\n",
      "Iteration 2800, Loss: 0.6934\n",
      "Iteration 2900, Loss: 0.6909\n",
      "Iteration 3000, Loss: 0.6885\n",
      "Iteration 3100, Loss: 0.6862\n",
      "Iteration 3200, Loss: 0.6841\n",
      "Iteration 3300, Loss: 0.6820\n",
      "Iteration 3400, Loss: 0.6800\n",
      "Iteration 3500, Loss: 0.6781\n",
      "Iteration 3600, Loss: 0.6762\n",
      "Iteration 3700, Loss: 0.6744\n",
      "Iteration 3800, Loss: 0.6727\n",
      "Iteration 3900, Loss: 0.6711\n",
      "Iteration 4000, Loss: 0.6695\n",
      "Iteration 4100, Loss: 0.6679\n",
      "Iteration 4200, Loss: 0.6664\n",
      "Iteration 4300, Loss: 0.6650\n",
      "Iteration 4400, Loss: 0.6636\n",
      "Iteration 4500, Loss: 0.6622\n",
      "Iteration 4600, Loss: 0.6609\n",
      "Iteration 4700, Loss: 0.6596\n",
      "Iteration 4800, Loss: 0.6584\n",
      "Iteration 4900, Loss: 0.6571\n",
      "21 36\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0197\n",
      "Iteration 200, Loss: 0.9656\n",
      "Iteration 300, Loss: 0.9252\n",
      "Iteration 400, Loss: 0.8937\n",
      "Iteration 500, Loss: 0.8683\n",
      "Iteration 600, Loss: 0.8472\n",
      "Iteration 700, Loss: 0.8295\n",
      "Iteration 800, Loss: 0.8144\n",
      "Iteration 900, Loss: 0.8013\n",
      "Iteration 1000, Loss: 0.7899\n",
      "Iteration 1100, Loss: 0.7798\n",
      "Iteration 1200, Loss: 0.7708\n",
      "Iteration 1300, Loss: 0.7628\n",
      "Iteration 1400, Loss: 0.7556\n",
      "Iteration 1500, Loss: 0.7490\n",
      "Iteration 1600, Loss: 0.7429\n",
      "Iteration 1700, Loss: 0.7374\n",
      "Iteration 1800, Loss: 0.7323\n",
      "Iteration 1900, Loss: 0.7276\n",
      "Iteration 2000, Loss: 0.7232\n",
      "Iteration 2100, Loss: 0.7191\n",
      "Iteration 2200, Loss: 0.7153\n",
      "Iteration 2300, Loss: 0.7117\n",
      "Iteration 2400, Loss: 0.7083\n",
      "Iteration 2500, Loss: 0.7051\n",
      "Iteration 2600, Loss: 0.7021\n",
      "Iteration 2700, Loss: 0.6992\n",
      "Iteration 2800, Loss: 0.6965\n",
      "Iteration 2900, Loss: 0.6938\n",
      "Iteration 3000, Loss: 0.6913\n",
      "Iteration 3100, Loss: 0.6889\n",
      "Iteration 3200, Loss: 0.6866\n",
      "Iteration 3300, Loss: 0.6844\n",
      "Iteration 3400, Loss: 0.6823\n",
      "Iteration 3500, Loss: 0.6802\n",
      "Iteration 3600, Loss: 0.6782\n",
      "Iteration 3700, Loss: 0.6763\n",
      "Iteration 3800, Loss: 0.6745\n",
      "Iteration 3900, Loss: 0.6727\n",
      "Iteration 4000, Loss: 0.6710\n",
      "Iteration 4100, Loss: 0.6693\n",
      "Iteration 4200, Loss: 0.6677\n",
      "Iteration 4300, Loss: 0.6661\n",
      "Iteration 4400, Loss: 0.6646\n",
      "Iteration 4500, Loss: 0.6631\n",
      "Iteration 4600, Loss: 0.6616\n",
      "Iteration 4700, Loss: 0.6602\n",
      "Iteration 4800, Loss: 0.6588\n",
      "Iteration 4900, Loss: 0.6575\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0195\n",
      "Iteration 200, Loss: 0.9647\n",
      "Iteration 300, Loss: 0.9232\n",
      "Iteration 400, Loss: 0.8905\n",
      "Iteration 500, Loss: 0.8641\n",
      "Iteration 600, Loss: 0.8423\n",
      "Iteration 700, Loss: 0.8240\n",
      "Iteration 800, Loss: 0.8085\n",
      "Iteration 900, Loss: 0.7951\n",
      "Iteration 1000, Loss: 0.7835\n",
      "Iteration 1100, Loss: 0.7733\n",
      "Iteration 1200, Loss: 0.7643\n",
      "Iteration 1300, Loss: 0.7564\n",
      "Iteration 1400, Loss: 0.7492\n",
      "Iteration 1500, Loss: 0.7428\n",
      "Iteration 1600, Loss: 0.7370\n",
      "Iteration 1700, Loss: 0.7318\n",
      "Iteration 1800, Loss: 0.7269\n",
      "Iteration 1900, Loss: 0.7225\n",
      "Iteration 2000, Loss: 0.7184\n",
      "Iteration 2100, Loss: 0.7147\n",
      "Iteration 2200, Loss: 0.7111\n",
      "Iteration 2300, Loss: 0.7078\n",
      "Iteration 2400, Loss: 0.7048\n",
      "Iteration 2500, Loss: 0.7019\n",
      "Iteration 2600, Loss: 0.6992\n",
      "Iteration 2700, Loss: 0.6966\n",
      "Iteration 2800, Loss: 0.6942\n",
      "Iteration 2900, Loss: 0.6919\n",
      "Iteration 3000, Loss: 0.6897\n",
      "Iteration 3100, Loss: 0.6876\n",
      "Iteration 3200, Loss: 0.6856\n",
      "Iteration 3300, Loss: 0.6837\n",
      "Iteration 3400, Loss: 0.6819\n",
      "Iteration 3500, Loss: 0.6801\n",
      "Iteration 3600, Loss: 0.6784\n",
      "Iteration 3700, Loss: 0.6768\n",
      "Iteration 3800, Loss: 0.6753\n",
      "Iteration 3900, Loss: 0.6738\n",
      "Iteration 4000, Loss: 0.6723\n",
      "Iteration 4100, Loss: 0.6709\n",
      "Iteration 4200, Loss: 0.6696\n",
      "Iteration 4300, Loss: 0.6683\n",
      "Iteration 4400, Loss: 0.6670\n",
      "Iteration 4500, Loss: 0.6658\n",
      "Iteration 4600, Loss: 0.6646\n",
      "Iteration 4700, Loss: 0.6634\n",
      "Iteration 4800, Loss: 0.6623\n",
      "Iteration 4900, Loss: 0.6612\n",
      "22 36\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0517\n",
      "Iteration 200, Loss: 1.0144\n",
      "Iteration 300, Loss: 0.9835\n",
      "Iteration 400, Loss: 0.9574\n",
      "Iteration 500, Loss: 0.9350\n",
      "Iteration 600, Loss: 0.9154\n",
      "Iteration 700, Loss: 0.8982\n",
      "Iteration 800, Loss: 0.8829\n",
      "Iteration 900, Loss: 0.8691\n",
      "Iteration 1000, Loss: 0.8568\n",
      "Iteration 1100, Loss: 0.8457\n",
      "Iteration 1200, Loss: 0.8356\n",
      "Iteration 1300, Loss: 0.8264\n",
      "Iteration 1400, Loss: 0.8178\n",
      "Iteration 1500, Loss: 0.8100\n",
      "Iteration 1600, Loss: 0.8028\n",
      "Iteration 1700, Loss: 0.7961\n",
      "Iteration 1800, Loss: 0.7900\n",
      "Iteration 1900, Loss: 0.7842\n",
      "Iteration 2000, Loss: 0.7788\n",
      "Iteration 2100, Loss: 0.7737\n",
      "Iteration 2200, Loss: 0.7689\n",
      "Iteration 2300, Loss: 0.7645\n",
      "Iteration 2400, Loss: 0.7603\n",
      "Iteration 2500, Loss: 0.7564\n",
      "Iteration 2600, Loss: 0.7526\n",
      "Iteration 2700, Loss: 0.7491\n",
      "Iteration 2800, Loss: 0.7458\n",
      "Iteration 2900, Loss: 0.7426\n",
      "Iteration 3000, Loss: 0.7396\n",
      "Iteration 3100, Loss: 0.7366\n",
      "Iteration 3200, Loss: 0.7339\n",
      "Iteration 3300, Loss: 0.7313\n",
      "Iteration 3400, Loss: 0.7288\n",
      "Iteration 3500, Loss: 0.7264\n",
      "Iteration 3600, Loss: 0.7241\n",
      "Iteration 3700, Loss: 0.7219\n",
      "Iteration 3800, Loss: 0.7197\n",
      "Iteration 3900, Loss: 0.7177\n",
      "Iteration 4000, Loss: 0.7157\n",
      "Iteration 4100, Loss: 0.7139\n",
      "Iteration 4200, Loss: 0.7120\n",
      "Iteration 4300, Loss: 0.7103\n",
      "Iteration 4400, Loss: 0.7085\n",
      "Iteration 4500, Loss: 0.7069\n",
      "Iteration 4600, Loss: 0.7053\n",
      "Iteration 4700, Loss: 0.7037\n",
      "Iteration 4800, Loss: 0.7022\n",
      "Iteration 4900, Loss: 0.7008\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0555\n",
      "Iteration 200, Loss: 1.0205\n",
      "Iteration 300, Loss: 0.9913\n",
      "Iteration 400, Loss: 0.9663\n",
      "Iteration 500, Loss: 0.9447\n",
      "Iteration 600, Loss: 0.9256\n",
      "Iteration 700, Loss: 0.9086\n",
      "Iteration 800, Loss: 0.8933\n",
      "Iteration 900, Loss: 0.8797\n",
      "Iteration 1000, Loss: 0.8673\n",
      "Iteration 1100, Loss: 0.8559\n",
      "Iteration 1200, Loss: 0.8456\n",
      "Iteration 1300, Loss: 0.8361\n",
      "Iteration 1400, Loss: 0.8274\n",
      "Iteration 1500, Loss: 0.8192\n",
      "Iteration 1600, Loss: 0.8117\n",
      "Iteration 1700, Loss: 0.8047\n",
      "Iteration 1800, Loss: 0.7982\n",
      "Iteration 1900, Loss: 0.7921\n",
      "Iteration 2000, Loss: 0.7863\n",
      "Iteration 2100, Loss: 0.7810\n",
      "Iteration 2200, Loss: 0.7760\n",
      "Iteration 2300, Loss: 0.7713\n",
      "Iteration 2400, Loss: 0.7668\n",
      "Iteration 2500, Loss: 0.7626\n",
      "Iteration 2600, Loss: 0.7586\n",
      "Iteration 2700, Loss: 0.7548\n",
      "Iteration 2800, Loss: 0.7512\n",
      "Iteration 2900, Loss: 0.7477\n",
      "Iteration 3000, Loss: 0.7444\n",
      "Iteration 3100, Loss: 0.7413\n",
      "Iteration 3200, Loss: 0.7383\n",
      "Iteration 3300, Loss: 0.7355\n",
      "Iteration 3400, Loss: 0.7327\n",
      "Iteration 3500, Loss: 0.7301\n",
      "Iteration 3600, Loss: 0.7276\n",
      "Iteration 3700, Loss: 0.7252\n",
      "Iteration 3800, Loss: 0.7229\n",
      "Iteration 3900, Loss: 0.7206\n",
      "Iteration 4000, Loss: 0.7185\n",
      "Iteration 4100, Loss: 0.7164\n",
      "Iteration 4200, Loss: 0.7144\n",
      "Iteration 4300, Loss: 0.7124\n",
      "Iteration 4400, Loss: 0.7106\n",
      "Iteration 4500, Loss: 0.7087\n",
      "Iteration 4600, Loss: 0.7070\n",
      "Iteration 4700, Loss: 0.7053\n",
      "Iteration 4800, Loss: 0.7036\n",
      "Iteration 4900, Loss: 0.7020\n",
      "23 36\n",
      "Iteration 0, Loss: 1.0982\n",
      "Iteration 100, Loss: 1.0573\n",
      "Iteration 200, Loss: 1.0234\n",
      "Iteration 300, Loss: 0.9949\n",
      "Iteration 400, Loss: 0.9702\n",
      "Iteration 500, Loss: 0.9490\n",
      "Iteration 600, Loss: 0.9302\n",
      "Iteration 700, Loss: 0.9134\n",
      "Iteration 800, Loss: 0.8984\n",
      "Iteration 900, Loss: 0.8849\n",
      "Iteration 1000, Loss: 0.8727\n",
      "Iteration 1100, Loss: 0.8615\n",
      "Iteration 1200, Loss: 0.8513\n",
      "Iteration 1300, Loss: 0.8419\n",
      "Iteration 1400, Loss: 0.8333\n",
      "Iteration 1500, Loss: 0.8253\n",
      "Iteration 1600, Loss: 0.8179\n",
      "Iteration 1700, Loss: 0.8110\n",
      "Iteration 1800, Loss: 0.8046\n",
      "Iteration 1900, Loss: 0.7987\n",
      "Iteration 2000, Loss: 0.7931\n",
      "Iteration 2100, Loss: 0.7878\n",
      "Iteration 2200, Loss: 0.7830\n",
      "Iteration 2300, Loss: 0.7784\n",
      "Iteration 2400, Loss: 0.7740\n",
      "Iteration 2500, Loss: 0.7699\n",
      "Iteration 2600, Loss: 0.7661\n",
      "Iteration 2700, Loss: 0.7624\n",
      "Iteration 2800, Loss: 0.7590\n",
      "Iteration 2900, Loss: 0.7557\n",
      "Iteration 3000, Loss: 0.7526\n",
      "Iteration 3100, Loss: 0.7496\n",
      "Iteration 3200, Loss: 0.7468\n",
      "Iteration 3300, Loss: 0.7441\n",
      "Iteration 3400, Loss: 0.7415\n",
      "Iteration 3500, Loss: 0.7391\n",
      "Iteration 3600, Loss: 0.7367\n",
      "Iteration 3700, Loss: 0.7344\n",
      "Iteration 3800, Loss: 0.7323\n",
      "Iteration 3900, Loss: 0.7302\n",
      "Iteration 4000, Loss: 0.7282\n",
      "Iteration 4100, Loss: 0.7262\n",
      "Iteration 4200, Loss: 0.7244\n",
      "Iteration 4300, Loss: 0.7226\n",
      "Iteration 4400, Loss: 0.7208\n",
      "Iteration 4500, Loss: 0.7192\n",
      "Iteration 4600, Loss: 0.7175\n",
      "Iteration 4700, Loss: 0.7160\n",
      "Iteration 4800, Loss: 0.7145\n",
      "Iteration 4900, Loss: 0.7130\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0501\n",
      "Iteration 200, Loss: 1.0116\n",
      "Iteration 300, Loss: 0.9797\n",
      "Iteration 400, Loss: 0.9529\n",
      "Iteration 500, Loss: 0.9297\n",
      "Iteration 600, Loss: 0.9097\n",
      "Iteration 700, Loss: 0.8921\n",
      "Iteration 800, Loss: 0.8765\n",
      "Iteration 900, Loss: 0.8626\n",
      "Iteration 1000, Loss: 0.8500\n",
      "Iteration 1100, Loss: 0.8387\n",
      "Iteration 1200, Loss: 0.8284\n",
      "Iteration 1300, Loss: 0.8191\n",
      "Iteration 1400, Loss: 0.8105\n",
      "Iteration 1500, Loss: 0.8026\n",
      "Iteration 1600, Loss: 0.7952\n",
      "Iteration 1700, Loss: 0.7884\n",
      "Iteration 1800, Loss: 0.7821\n",
      "Iteration 1900, Loss: 0.7763\n",
      "Iteration 2000, Loss: 0.7708\n",
      "Iteration 2100, Loss: 0.7656\n",
      "Iteration 2200, Loss: 0.7608\n",
      "Iteration 2300, Loss: 0.7562\n",
      "Iteration 2400, Loss: 0.7519\n",
      "Iteration 2500, Loss: 0.7478\n",
      "Iteration 2600, Loss: 0.7439\n",
      "Iteration 2700, Loss: 0.7403\n",
      "Iteration 2800, Loss: 0.7368\n",
      "Iteration 2900, Loss: 0.7335\n",
      "Iteration 3000, Loss: 0.7304\n",
      "Iteration 3100, Loss: 0.7273\n",
      "Iteration 3200, Loss: 0.7244\n",
      "Iteration 3300, Loss: 0.7217\n",
      "Iteration 3400, Loss: 0.7191\n",
      "Iteration 3500, Loss: 0.7165\n",
      "Iteration 3600, Loss: 0.7141\n",
      "Iteration 3700, Loss: 0.7117\n",
      "Iteration 3800, Loss: 0.7095\n",
      "Iteration 3900, Loss: 0.7073\n",
      "Iteration 4000, Loss: 0.7052\n",
      "Iteration 4100, Loss: 0.7032\n",
      "Iteration 4200, Loss: 0.7013\n",
      "Iteration 4300, Loss: 0.6993\n",
      "Iteration 4400, Loss: 0.6975\n",
      "Iteration 4500, Loss: 0.6958\n",
      "Iteration 4600, Loss: 0.6940\n",
      "Iteration 4700, Loss: 0.6924\n",
      "Iteration 4800, Loss: 0.6908\n",
      "Iteration 4900, Loss: 0.6892\n",
      "24 36\n",
      "Iteration 0, Loss: 1.0223\n",
      "Iteration 100, Loss: 0.6397\n",
      "Iteration 200, Loss: 0.6084\n",
      "Iteration 300, Loss: 0.5901\n",
      "Iteration 400, Loss: 0.5773\n",
      "Iteration 500, Loss: 0.5673\n",
      "Iteration 600, Loss: 0.5594\n",
      "Iteration 700, Loss: 0.5528\n",
      "Iteration 800, Loss: 0.5471\n",
      "Iteration 900, Loss: 0.5423\n",
      "Iteration 1000, Loss: 0.5380\n",
      "Iteration 1100, Loss: 0.5342\n",
      "Iteration 1200, Loss: 0.5307\n",
      "Iteration 1300, Loss: 0.5276\n",
      "Iteration 1400, Loss: 0.5247\n",
      "Iteration 1500, Loss: 0.5221\n",
      "Iteration 1600, Loss: 0.5197\n",
      "Iteration 1700, Loss: 0.5175\n",
      "Iteration 1800, Loss: 0.5154\n",
      "Iteration 1900, Loss: 0.5134\n",
      "Iteration 2000, Loss: 0.5116\n",
      "Iteration 2100, Loss: 0.5099\n",
      "Iteration 2200, Loss: 0.5083\n",
      "Iteration 2300, Loss: 0.5068\n",
      "Iteration 2400, Loss: 0.5054\n",
      "Iteration 2500, Loss: 0.5040\n",
      "Iteration 2600, Loss: 0.5027\n",
      "Iteration 2700, Loss: 0.5015\n",
      "Iteration 2800, Loss: 0.5003\n",
      "Iteration 2900, Loss: 0.4992\n",
      "Iteration 3000, Loss: 0.4981\n",
      "Iteration 3100, Loss: 0.4971\n",
      "Iteration 3200, Loss: 0.4961\n",
      "Iteration 3300, Loss: 0.4952\n",
      "Iteration 3400, Loss: 0.4943\n",
      "Iteration 3500, Loss: 0.4934\n",
      "Iteration 3600, Loss: 0.4926\n",
      "Iteration 3700, Loss: 0.4917\n",
      "Iteration 3800, Loss: 0.4910\n",
      "Iteration 3900, Loss: 0.4902\n",
      "Iteration 4000, Loss: 0.4895\n",
      "Iteration 4100, Loss: 0.4888\n",
      "Iteration 4200, Loss: 0.4881\n",
      "Iteration 4300, Loss: 0.4875\n",
      "Iteration 4400, Loss: 0.4868\n",
      "Iteration 4500, Loss: 0.4862\n",
      "Iteration 4600, Loss: 0.4856\n",
      "Iteration 4700, Loss: 0.4851\n",
      "Iteration 4800, Loss: 0.4845\n",
      "Iteration 4900, Loss: 0.4839\n",
      "Iteration 5000, Loss: 0.4834\n",
      "Iteration 5100, Loss: 0.4829\n",
      "Iteration 5200, Loss: 0.4824\n",
      "Iteration 5300, Loss: 0.4819\n",
      "Iteration 5400, Loss: 0.4814\n",
      "Iteration 5500, Loss: 0.4810\n",
      "Iteration 5600, Loss: 0.4805\n",
      "Iteration 5700, Loss: 0.4801\n",
      "Iteration 5800, Loss: 0.4796\n",
      "Iteration 5900, Loss: 0.4792\n",
      "Iteration 0, Loss: 1.0185\n",
      "Iteration 100, Loss: 0.6037\n",
      "Iteration 200, Loss: 0.5717\n",
      "Iteration 300, Loss: 0.5548\n",
      "Iteration 400, Loss: 0.5434\n",
      "Iteration 500, Loss: 0.5349\n",
      "Iteration 600, Loss: 0.5281\n",
      "Iteration 700, Loss: 0.5225\n",
      "Iteration 800, Loss: 0.5177\n",
      "Iteration 900, Loss: 0.5135\n",
      "Iteration 1000, Loss: 0.5097\n",
      "Iteration 1100, Loss: 0.5063\n",
      "Iteration 1200, Loss: 0.5033\n",
      "Iteration 1300, Loss: 0.5004\n",
      "Iteration 1400, Loss: 0.4979\n",
      "Iteration 1500, Loss: 0.4954\n",
      "Iteration 1600, Loss: 0.4932\n",
      "Iteration 1700, Loss: 0.4911\n",
      "Iteration 1800, Loss: 0.4891\n",
      "Iteration 1900, Loss: 0.4872\n",
      "Iteration 2000, Loss: 0.4855\n",
      "Iteration 2100, Loss: 0.4838\n",
      "Iteration 2200, Loss: 0.4821\n",
      "Iteration 2300, Loss: 0.4806\n",
      "Iteration 2400, Loss: 0.4791\n",
      "Iteration 2500, Loss: 0.4777\n",
      "Iteration 2600, Loss: 0.4764\n",
      "Iteration 2700, Loss: 0.4751\n",
      "Iteration 2800, Loss: 0.4738\n",
      "Iteration 2900, Loss: 0.4726\n",
      "Iteration 3000, Loss: 0.4715\n",
      "Iteration 3100, Loss: 0.4704\n",
      "Iteration 3200, Loss: 0.4693\n",
      "Iteration 3300, Loss: 0.4682\n",
      "Iteration 3400, Loss: 0.4672\n",
      "Iteration 3500, Loss: 0.4662\n",
      "Iteration 3600, Loss: 0.4653\n",
      "Iteration 3700, Loss: 0.4644\n",
      "Iteration 3800, Loss: 0.4635\n",
      "Iteration 3900, Loss: 0.4626\n",
      "Iteration 4000, Loss: 0.4618\n",
      "Iteration 4100, Loss: 0.4609\n",
      "Iteration 4200, Loss: 0.4602\n",
      "Iteration 4300, Loss: 0.4594\n",
      "Iteration 4400, Loss: 0.4586\n",
      "Iteration 4500, Loss: 0.4579\n",
      "Iteration 4600, Loss: 0.4572\n",
      "Iteration 4700, Loss: 0.4565\n",
      "Iteration 4800, Loss: 0.4558\n",
      "Iteration 4900, Loss: 0.4551\n",
      "Iteration 5000, Loss: 0.4545\n",
      "Iteration 5100, Loss: 0.4539\n",
      "Iteration 5200, Loss: 0.4532\n",
      "Iteration 5300, Loss: 0.4526\n",
      "Iteration 5400, Loss: 0.4520\n",
      "Iteration 5500, Loss: 0.4515\n",
      "Iteration 5600, Loss: 0.4509\n",
      "Iteration 5700, Loss: 0.4504\n",
      "Iteration 5800, Loss: 0.4498\n",
      "Iteration 5900, Loss: 0.4493\n",
      "25 36\n",
      "Iteration 0, Loss: 1.0266\n",
      "Iteration 100, Loss: 0.6307\n",
      "Iteration 200, Loss: 0.5952\n",
      "Iteration 300, Loss: 0.5752\n",
      "Iteration 400, Loss: 0.5615\n",
      "Iteration 500, Loss: 0.5512\n",
      "Iteration 600, Loss: 0.5429\n",
      "Iteration 700, Loss: 0.5360\n",
      "Iteration 800, Loss: 0.5301\n",
      "Iteration 900, Loss: 0.5251\n",
      "Iteration 1000, Loss: 0.5205\n",
      "Iteration 1100, Loss: 0.5165\n",
      "Iteration 1200, Loss: 0.5129\n",
      "Iteration 1300, Loss: 0.5096\n",
      "Iteration 1400, Loss: 0.5065\n",
      "Iteration 1500, Loss: 0.5037\n",
      "Iteration 1600, Loss: 0.5011\n",
      "Iteration 1700, Loss: 0.4986\n",
      "Iteration 1800, Loss: 0.4964\n",
      "Iteration 1900, Loss: 0.4943\n",
      "Iteration 2000, Loss: 0.4922\n",
      "Iteration 2100, Loss: 0.4903\n",
      "Iteration 2200, Loss: 0.4885\n",
      "Iteration 2300, Loss: 0.4868\n",
      "Iteration 2400, Loss: 0.4851\n",
      "Iteration 2500, Loss: 0.4836\n",
      "Iteration 2600, Loss: 0.4820\n",
      "Iteration 2700, Loss: 0.4806\n",
      "Iteration 2800, Loss: 0.4792\n",
      "Iteration 2900, Loss: 0.4779\n",
      "Iteration 3000, Loss: 0.4766\n",
      "Iteration 3100, Loss: 0.4754\n",
      "Iteration 3200, Loss: 0.4742\n",
      "Iteration 3300, Loss: 0.4730\n",
      "Iteration 3400, Loss: 0.4719\n",
      "Iteration 3500, Loss: 0.4708\n",
      "Iteration 3600, Loss: 0.4698\n",
      "Iteration 3700, Loss: 0.4688\n",
      "Iteration 3800, Loss: 0.4678\n",
      "Iteration 3900, Loss: 0.4668\n",
      "Iteration 4000, Loss: 0.4659\n",
      "Iteration 4100, Loss: 0.4650\n",
      "Iteration 4200, Loss: 0.4641\n",
      "Iteration 4300, Loss: 0.4632\n",
      "Iteration 4400, Loss: 0.4624\n",
      "Iteration 4500, Loss: 0.4616\n",
      "Iteration 4600, Loss: 0.4608\n",
      "Iteration 4700, Loss: 0.4600\n",
      "Iteration 4800, Loss: 0.4592\n",
      "Iteration 4900, Loss: 0.4585\n",
      "Iteration 5000, Loss: 0.4577\n",
      "Iteration 5100, Loss: 0.4570\n",
      "Iteration 5200, Loss: 0.4563\n",
      "Iteration 5300, Loss: 0.4556\n",
      "Iteration 5400, Loss: 0.4549\n",
      "Iteration 5500, Loss: 0.4543\n",
      "Iteration 5600, Loss: 0.4536\n",
      "Iteration 5700, Loss: 0.4530\n",
      "Iteration 5800, Loss: 0.4523\n",
      "Iteration 5900, Loss: 0.4517\n",
      "Iteration 0, Loss: 1.0141\n",
      "Iteration 100, Loss: 0.6105\n",
      "Iteration 200, Loss: 0.5819\n",
      "Iteration 300, Loss: 0.5658\n",
      "Iteration 400, Loss: 0.5547\n",
      "Iteration 500, Loss: 0.5462\n",
      "Iteration 600, Loss: 0.5393\n",
      "Iteration 700, Loss: 0.5335\n",
      "Iteration 800, Loss: 0.5285\n",
      "Iteration 900, Loss: 0.5241\n",
      "Iteration 1000, Loss: 0.5202\n",
      "Iteration 1100, Loss: 0.5166\n",
      "Iteration 1200, Loss: 0.5134\n",
      "Iteration 1300, Loss: 0.5104\n",
      "Iteration 1400, Loss: 0.5077\n",
      "Iteration 1500, Loss: 0.5051\n",
      "Iteration 1600, Loss: 0.5027\n",
      "Iteration 1700, Loss: 0.5005\n",
      "Iteration 1800, Loss: 0.4983\n",
      "Iteration 1900, Loss: 0.4963\n",
      "Iteration 2000, Loss: 0.4944\n",
      "Iteration 2100, Loss: 0.4926\n",
      "Iteration 2200, Loss: 0.4909\n",
      "Iteration 2300, Loss: 0.4892\n",
      "Iteration 2400, Loss: 0.4876\n",
      "Iteration 2500, Loss: 0.4861\n",
      "Iteration 2600, Loss: 0.4847\n",
      "Iteration 2700, Loss: 0.4833\n",
      "Iteration 2800, Loss: 0.4819\n",
      "Iteration 2900, Loss: 0.4806\n",
      "Iteration 3000, Loss: 0.4794\n",
      "Iteration 3100, Loss: 0.4782\n",
      "Iteration 3200, Loss: 0.4770\n",
      "Iteration 3300, Loss: 0.4759\n",
      "Iteration 3400, Loss: 0.4748\n",
      "Iteration 3500, Loss: 0.4738\n",
      "Iteration 3600, Loss: 0.4728\n",
      "Iteration 3700, Loss: 0.4718\n",
      "Iteration 3800, Loss: 0.4708\n",
      "Iteration 3900, Loss: 0.4699\n",
      "Iteration 4000, Loss: 0.4690\n",
      "Iteration 4100, Loss: 0.4681\n",
      "Iteration 4200, Loss: 0.4673\n",
      "Iteration 4300, Loss: 0.4665\n",
      "Iteration 4400, Loss: 0.4657\n",
      "Iteration 4500, Loss: 0.4649\n",
      "Iteration 4600, Loss: 0.4641\n",
      "Iteration 4700, Loss: 0.4634\n",
      "Iteration 4800, Loss: 0.4627\n",
      "Iteration 4900, Loss: 0.4620\n",
      "Iteration 5000, Loss: 0.4613\n",
      "Iteration 5100, Loss: 0.4606\n",
      "Iteration 5200, Loss: 0.4599\n",
      "Iteration 5300, Loss: 0.4593\n",
      "Iteration 5400, Loss: 0.4587\n",
      "Iteration 5500, Loss: 0.4581\n",
      "Iteration 5600, Loss: 0.4575\n",
      "Iteration 5700, Loss: 0.4569\n",
      "Iteration 5800, Loss: 0.4563\n",
      "Iteration 5900, Loss: 0.4557\n",
      "26 36\n",
      "Iteration 0, Loss: 1.0545\n",
      "Iteration 100, Loss: 0.6553\n",
      "Iteration 200, Loss: 0.6159\n",
      "Iteration 300, Loss: 0.5955\n",
      "Iteration 400, Loss: 0.5819\n",
      "Iteration 500, Loss: 0.5717\n",
      "Iteration 600, Loss: 0.5636\n",
      "Iteration 700, Loss: 0.5569\n",
      "Iteration 800, Loss: 0.5512\n",
      "Iteration 900, Loss: 0.5463\n",
      "Iteration 1000, Loss: 0.5419\n",
      "Iteration 1100, Loss: 0.5381\n",
      "Iteration 1200, Loss: 0.5347\n",
      "Iteration 1300, Loss: 0.5316\n",
      "Iteration 1400, Loss: 0.5287\n",
      "Iteration 1500, Loss: 0.5261\n",
      "Iteration 1600, Loss: 0.5237\n",
      "Iteration 1700, Loss: 0.5214\n",
      "Iteration 1800, Loss: 0.5193\n",
      "Iteration 1900, Loss: 0.5173\n",
      "Iteration 2000, Loss: 0.5154\n",
      "Iteration 2100, Loss: 0.5136\n",
      "Iteration 2200, Loss: 0.5119\n",
      "Iteration 2300, Loss: 0.5103\n",
      "Iteration 2400, Loss: 0.5088\n",
      "Iteration 2500, Loss: 0.5073\n",
      "Iteration 2600, Loss: 0.5059\n",
      "Iteration 2700, Loss: 0.5046\n",
      "Iteration 2800, Loss: 0.5033\n",
      "Iteration 2900, Loss: 0.5020\n",
      "Iteration 3000, Loss: 0.5008\n",
      "Iteration 3100, Loss: 0.4997\n",
      "Iteration 3200, Loss: 0.4986\n",
      "Iteration 3300, Loss: 0.4975\n",
      "Iteration 3400, Loss: 0.4965\n",
      "Iteration 3500, Loss: 0.4955\n",
      "Iteration 3600, Loss: 0.4945\n",
      "Iteration 3700, Loss: 0.4936\n",
      "Iteration 3800, Loss: 0.4927\n",
      "Iteration 3900, Loss: 0.4918\n",
      "Iteration 4000, Loss: 0.4909\n",
      "Iteration 4100, Loss: 0.4901\n",
      "Iteration 4200, Loss: 0.4893\n",
      "Iteration 4300, Loss: 0.4885\n",
      "Iteration 4400, Loss: 0.4878\n",
      "Iteration 4500, Loss: 0.4870\n",
      "Iteration 4600, Loss: 0.4863\n",
      "Iteration 4700, Loss: 0.4855\n",
      "Iteration 4800, Loss: 0.4849\n",
      "Iteration 4900, Loss: 0.4842\n",
      "Iteration 5000, Loss: 0.4835\n",
      "Iteration 5100, Loss: 0.4829\n",
      "Iteration 5200, Loss: 0.4823\n",
      "Iteration 5300, Loss: 0.4816\n",
      "Iteration 5400, Loss: 0.4810\n",
      "Iteration 5500, Loss: 0.4804\n",
      "Iteration 5600, Loss: 0.4799\n",
      "Iteration 5700, Loss: 0.4793\n",
      "Iteration 5800, Loss: 0.4787\n",
      "Iteration 5900, Loss: 0.4782\n",
      "Iteration 0, Loss: 1.0518\n",
      "Iteration 100, Loss: 0.6538\n",
      "Iteration 200, Loss: 0.6226\n",
      "Iteration 300, Loss: 0.6053\n",
      "Iteration 400, Loss: 0.5929\n",
      "Iteration 500, Loss: 0.5834\n",
      "Iteration 600, Loss: 0.5756\n",
      "Iteration 700, Loss: 0.5690\n",
      "Iteration 800, Loss: 0.5633\n",
      "Iteration 900, Loss: 0.5583\n",
      "Iteration 1000, Loss: 0.5539\n",
      "Iteration 1100, Loss: 0.5499\n",
      "Iteration 1200, Loss: 0.5464\n",
      "Iteration 1300, Loss: 0.5431\n",
      "Iteration 1400, Loss: 0.5401\n",
      "Iteration 1500, Loss: 0.5373\n",
      "Iteration 1600, Loss: 0.5348\n",
      "Iteration 1700, Loss: 0.5324\n",
      "Iteration 1800, Loss: 0.5302\n",
      "Iteration 1900, Loss: 0.5281\n",
      "Iteration 2000, Loss: 0.5261\n",
      "Iteration 2100, Loss: 0.5242\n",
      "Iteration 2200, Loss: 0.5224\n",
      "Iteration 2300, Loss: 0.5208\n",
      "Iteration 2400, Loss: 0.5192\n",
      "Iteration 2500, Loss: 0.5177\n",
      "Iteration 2600, Loss: 0.5162\n",
      "Iteration 2700, Loss: 0.5148\n",
      "Iteration 2800, Loss: 0.5134\n",
      "Iteration 2900, Loss: 0.5121\n",
      "Iteration 3000, Loss: 0.5109\n",
      "Iteration 3100, Loss: 0.5097\n",
      "Iteration 3200, Loss: 0.5086\n",
      "Iteration 3300, Loss: 0.5075\n",
      "Iteration 3400, Loss: 0.5064\n",
      "Iteration 3500, Loss: 0.5054\n",
      "Iteration 3600, Loss: 0.5043\n",
      "Iteration 3700, Loss: 0.5034\n",
      "Iteration 3800, Loss: 0.5025\n",
      "Iteration 3900, Loss: 0.5015\n",
      "Iteration 4000, Loss: 0.5007\n",
      "Iteration 4100, Loss: 0.4998\n",
      "Iteration 4200, Loss: 0.4990\n",
      "Iteration 4300, Loss: 0.4981\n",
      "Iteration 4400, Loss: 0.4973\n",
      "Iteration 4500, Loss: 0.4966\n",
      "Iteration 4600, Loss: 0.4958\n",
      "Iteration 4700, Loss: 0.4952\n",
      "Iteration 4800, Loss: 0.4944\n",
      "Iteration 4900, Loss: 0.4937\n",
      "Iteration 5000, Loss: 0.4930\n",
      "Iteration 5100, Loss: 0.4923\n",
      "Iteration 5200, Loss: 0.4917\n",
      "Iteration 5300, Loss: 0.4910\n",
      "Iteration 5400, Loss: 0.4904\n",
      "Iteration 5500, Loss: 0.4898\n",
      "Iteration 5600, Loss: 0.4892\n",
      "Iteration 5700, Loss: 0.4886\n",
      "Iteration 5800, Loss: 0.4880\n",
      "Iteration 5900, Loss: 0.4875\n",
      "27 36\n",
      "Iteration 0, Loss: 1.0580\n",
      "Iteration 100, Loss: 0.6716\n",
      "Iteration 200, Loss: 0.6374\n",
      "Iteration 300, Loss: 0.6201\n",
      "Iteration 400, Loss: 0.6086\n",
      "Iteration 500, Loss: 0.5999\n",
      "Iteration 600, Loss: 0.5927\n",
      "Iteration 700, Loss: 0.5868\n",
      "Iteration 800, Loss: 0.5817\n",
      "Iteration 900, Loss: 0.5772\n",
      "Iteration 1000, Loss: 0.5733\n",
      "Iteration 1100, Loss: 0.5696\n",
      "Iteration 1200, Loss: 0.5664\n",
      "Iteration 1300, Loss: 0.5634\n",
      "Iteration 1400, Loss: 0.5607\n",
      "Iteration 1500, Loss: 0.5581\n",
      "Iteration 1600, Loss: 0.5557\n",
      "Iteration 1700, Loss: 0.5536\n",
      "Iteration 1800, Loss: 0.5515\n",
      "Iteration 1900, Loss: 0.5495\n",
      "Iteration 2000, Loss: 0.5476\n",
      "Iteration 2100, Loss: 0.5458\n",
      "Iteration 2200, Loss: 0.5442\n",
      "Iteration 2300, Loss: 0.5425\n",
      "Iteration 2400, Loss: 0.5410\n",
      "Iteration 2500, Loss: 0.5395\n",
      "Iteration 2600, Loss: 0.5381\n",
      "Iteration 2700, Loss: 0.5367\n",
      "Iteration 2800, Loss: 0.5354\n",
      "Iteration 2900, Loss: 0.5342\n",
      "Iteration 3000, Loss: 0.5329\n",
      "Iteration 3100, Loss: 0.5317\n",
      "Iteration 3200, Loss: 0.5306\n",
      "Iteration 3300, Loss: 0.5295\n",
      "Iteration 3400, Loss: 0.5284\n",
      "Iteration 3500, Loss: 0.5273\n",
      "Iteration 3600, Loss: 0.5264\n",
      "Iteration 3700, Loss: 0.5254\n",
      "Iteration 3800, Loss: 0.5244\n",
      "Iteration 3900, Loss: 0.5234\n",
      "Iteration 4000, Loss: 0.5225\n",
      "Iteration 4100, Loss: 0.5216\n",
      "Iteration 4200, Loss: 0.5208\n",
      "Iteration 4300, Loss: 0.5200\n",
      "Iteration 4400, Loss: 0.5191\n",
      "Iteration 4500, Loss: 0.5183\n",
      "Iteration 4600, Loss: 0.5175\n",
      "Iteration 4700, Loss: 0.5167\n",
      "Iteration 4800, Loss: 0.5159\n",
      "Iteration 4900, Loss: 0.5152\n",
      "Iteration 5000, Loss: 0.5145\n",
      "Iteration 5100, Loss: 0.5138\n",
      "Iteration 5200, Loss: 0.5130\n",
      "Iteration 5300, Loss: 0.5124\n",
      "Iteration 5400, Loss: 0.5117\n",
      "Iteration 5500, Loss: 0.5111\n",
      "Iteration 5600, Loss: 0.5104\n",
      "Iteration 5700, Loss: 0.5098\n",
      "Iteration 5800, Loss: 0.5091\n",
      "Iteration 5900, Loss: 0.5085\n",
      "Iteration 0, Loss: 1.0503\n",
      "Iteration 100, Loss: 0.6399\n",
      "Iteration 200, Loss: 0.6039\n",
      "Iteration 300, Loss: 0.5841\n",
      "Iteration 400, Loss: 0.5705\n",
      "Iteration 500, Loss: 0.5601\n",
      "Iteration 600, Loss: 0.5518\n",
      "Iteration 700, Loss: 0.5448\n",
      "Iteration 800, Loss: 0.5388\n",
      "Iteration 900, Loss: 0.5336\n",
      "Iteration 1000, Loss: 0.5289\n",
      "Iteration 1100, Loss: 0.5248\n",
      "Iteration 1200, Loss: 0.5210\n",
      "Iteration 1300, Loss: 0.5175\n",
      "Iteration 1400, Loss: 0.5144\n",
      "Iteration 1500, Loss: 0.5114\n",
      "Iteration 1600, Loss: 0.5087\n",
      "Iteration 1700, Loss: 0.5062\n",
      "Iteration 1800, Loss: 0.5038\n",
      "Iteration 1900, Loss: 0.5015\n",
      "Iteration 2000, Loss: 0.4994\n",
      "Iteration 2100, Loss: 0.4974\n",
      "Iteration 2200, Loss: 0.4955\n",
      "Iteration 2300, Loss: 0.4937\n",
      "Iteration 2400, Loss: 0.4919\n",
      "Iteration 2500, Loss: 0.4903\n",
      "Iteration 2600, Loss: 0.4887\n",
      "Iteration 2700, Loss: 0.4872\n",
      "Iteration 2800, Loss: 0.4858\n",
      "Iteration 2900, Loss: 0.4843\n",
      "Iteration 3000, Loss: 0.4830\n",
      "Iteration 3100, Loss: 0.4817\n",
      "Iteration 3200, Loss: 0.4805\n",
      "Iteration 3300, Loss: 0.4793\n",
      "Iteration 3400, Loss: 0.4781\n",
      "Iteration 3500, Loss: 0.4770\n",
      "Iteration 3600, Loss: 0.4759\n",
      "Iteration 3700, Loss: 0.4748\n",
      "Iteration 3800, Loss: 0.4738\n",
      "Iteration 3900, Loss: 0.4728\n",
      "Iteration 4000, Loss: 0.4718\n",
      "Iteration 4100, Loss: 0.4709\n",
      "Iteration 4200, Loss: 0.4699\n",
      "Iteration 4300, Loss: 0.4690\n",
      "Iteration 4400, Loss: 0.4682\n",
      "Iteration 4500, Loss: 0.4673\n",
      "Iteration 4600, Loss: 0.4665\n",
      "Iteration 4700, Loss: 0.4657\n",
      "Iteration 4800, Loss: 0.4649\n",
      "Iteration 4900, Loss: 0.4641\n",
      "Iteration 5000, Loss: 0.4633\n",
      "Iteration 5100, Loss: 0.4626\n",
      "Iteration 5200, Loss: 0.4619\n",
      "Iteration 5300, Loss: 0.4612\n",
      "Iteration 5400, Loss: 0.4605\n",
      "Iteration 5500, Loss: 0.4598\n",
      "Iteration 5600, Loss: 0.4591\n",
      "Iteration 5700, Loss: 0.4585\n",
      "Iteration 5800, Loss: 0.4578\n",
      "Iteration 5900, Loss: 0.4572\n",
      "28 36\n",
      "Iteration 0, Loss: 1.0889\n",
      "Iteration 100, Loss: 0.7788\n",
      "Iteration 200, Loss: 0.7120\n",
      "Iteration 300, Loss: 0.6812\n",
      "Iteration 400, Loss: 0.6622\n",
      "Iteration 500, Loss: 0.6489\n",
      "Iteration 600, Loss: 0.6387\n",
      "Iteration 700, Loss: 0.6305\n",
      "Iteration 800, Loss: 0.6237\n",
      "Iteration 900, Loss: 0.6178\n",
      "Iteration 1000, Loss: 0.6127\n",
      "Iteration 1100, Loss: 0.6082\n",
      "Iteration 1200, Loss: 0.6041\n",
      "Iteration 1300, Loss: 0.6004\n",
      "Iteration 1400, Loss: 0.5970\n",
      "Iteration 1500, Loss: 0.5939\n",
      "Iteration 1600, Loss: 0.5910\n",
      "Iteration 1700, Loss: 0.5882\n",
      "Iteration 1800, Loss: 0.5857\n",
      "Iteration 1900, Loss: 0.5832\n",
      "Iteration 2000, Loss: 0.5810\n",
      "Iteration 2100, Loss: 0.5788\n",
      "Iteration 2200, Loss: 0.5767\n",
      "Iteration 2300, Loss: 0.5747\n",
      "Iteration 2400, Loss: 0.5728\n",
      "Iteration 2500, Loss: 0.5710\n",
      "Iteration 2600, Loss: 0.5693\n",
      "Iteration 2700, Loss: 0.5677\n",
      "Iteration 2800, Loss: 0.5661\n",
      "Iteration 2900, Loss: 0.5645\n",
      "Iteration 3000, Loss: 0.5630\n",
      "Iteration 3100, Loss: 0.5616\n",
      "Iteration 3200, Loss: 0.5602\n",
      "Iteration 3300, Loss: 0.5588\n",
      "Iteration 3400, Loss: 0.5575\n",
      "Iteration 3500, Loss: 0.5563\n",
      "Iteration 3600, Loss: 0.5551\n",
      "Iteration 3700, Loss: 0.5539\n",
      "Iteration 3800, Loss: 0.5527\n",
      "Iteration 3900, Loss: 0.5516\n",
      "Iteration 4000, Loss: 0.5505\n",
      "Iteration 4100, Loss: 0.5494\n",
      "Iteration 4200, Loss: 0.5484\n",
      "Iteration 4300, Loss: 0.5474\n",
      "Iteration 4400, Loss: 0.5464\n",
      "Iteration 4500, Loss: 0.5454\n",
      "Iteration 4600, Loss: 0.5445\n",
      "Iteration 4700, Loss: 0.5436\n",
      "Iteration 4800, Loss: 0.5426\n",
      "Iteration 4900, Loss: 0.5418\n",
      "Iteration 5000, Loss: 0.5409\n",
      "Iteration 5100, Loss: 0.5401\n",
      "Iteration 5200, Loss: 0.5392\n",
      "Iteration 5300, Loss: 0.5384\n",
      "Iteration 5400, Loss: 0.5376\n",
      "Iteration 5500, Loss: 0.5369\n",
      "Iteration 5600, Loss: 0.5361\n",
      "Iteration 5700, Loss: 0.5353\n",
      "Iteration 5800, Loss: 0.5346\n",
      "Iteration 5900, Loss: 0.5339\n",
      "Iteration 0, Loss: 1.0896\n",
      "Iteration 100, Loss: 0.7937\n",
      "Iteration 200, Loss: 0.7297\n",
      "Iteration 300, Loss: 0.7008\n",
      "Iteration 400, Loss: 0.6829\n",
      "Iteration 500, Loss: 0.6701\n",
      "Iteration 600, Loss: 0.6603\n",
      "Iteration 700, Loss: 0.6523\n",
      "Iteration 800, Loss: 0.6456\n",
      "Iteration 900, Loss: 0.6399\n",
      "Iteration 1000, Loss: 0.6349\n",
      "Iteration 1100, Loss: 0.6305\n",
      "Iteration 1200, Loss: 0.6265\n",
      "Iteration 1300, Loss: 0.6229\n",
      "Iteration 1400, Loss: 0.6195\n",
      "Iteration 1500, Loss: 0.6164\n",
      "Iteration 1600, Loss: 0.6136\n",
      "Iteration 1700, Loss: 0.6109\n",
      "Iteration 1800, Loss: 0.6084\n",
      "Iteration 1900, Loss: 0.6060\n",
      "Iteration 2000, Loss: 0.6038\n",
      "Iteration 2100, Loss: 0.6017\n",
      "Iteration 2200, Loss: 0.5997\n",
      "Iteration 2300, Loss: 0.5978\n",
      "Iteration 2400, Loss: 0.5960\n",
      "Iteration 2500, Loss: 0.5943\n",
      "Iteration 2600, Loss: 0.5926\n",
      "Iteration 2700, Loss: 0.5910\n",
      "Iteration 2800, Loss: 0.5895\n",
      "Iteration 2900, Loss: 0.5880\n",
      "Iteration 3000, Loss: 0.5866\n",
      "Iteration 3100, Loss: 0.5852\n",
      "Iteration 3200, Loss: 0.5839\n",
      "Iteration 3300, Loss: 0.5827\n",
      "Iteration 3400, Loss: 0.5815\n",
      "Iteration 3500, Loss: 0.5803\n",
      "Iteration 3600, Loss: 0.5791\n",
      "Iteration 3700, Loss: 0.5780\n",
      "Iteration 3800, Loss: 0.5769\n",
      "Iteration 3900, Loss: 0.5759\n",
      "Iteration 4000, Loss: 0.5749\n",
      "Iteration 4100, Loss: 0.5739\n",
      "Iteration 4200, Loss: 0.5729\n",
      "Iteration 4300, Loss: 0.5720\n",
      "Iteration 4400, Loss: 0.5711\n",
      "Iteration 4500, Loss: 0.5702\n",
      "Iteration 4600, Loss: 0.5694\n",
      "Iteration 4700, Loss: 0.5685\n",
      "Iteration 4800, Loss: 0.5677\n",
      "Iteration 4900, Loss: 0.5669\n",
      "Iteration 5000, Loss: 0.5661\n",
      "Iteration 5100, Loss: 0.5653\n",
      "Iteration 5200, Loss: 0.5646\n",
      "Iteration 5300, Loss: 0.5638\n",
      "Iteration 5400, Loss: 0.5631\n",
      "Iteration 5500, Loss: 0.5624\n",
      "Iteration 5600, Loss: 0.5617\n",
      "Iteration 5700, Loss: 0.5610\n",
      "Iteration 5800, Loss: 0.5604\n",
      "Iteration 5900, Loss: 0.5597\n",
      "29 36\n",
      "Iteration 0, Loss: 1.0893\n",
      "Iteration 100, Loss: 0.7895\n",
      "Iteration 200, Loss: 0.7252\n",
      "Iteration 300, Loss: 0.6958\n",
      "Iteration 400, Loss: 0.6779\n",
      "Iteration 500, Loss: 0.6652\n",
      "Iteration 600, Loss: 0.6554\n",
      "Iteration 700, Loss: 0.6476\n",
      "Iteration 800, Loss: 0.6410\n",
      "Iteration 900, Loss: 0.6354\n",
      "Iteration 1000, Loss: 0.6304\n",
      "Iteration 1100, Loss: 0.6260\n",
      "Iteration 1200, Loss: 0.6220\n",
      "Iteration 1300, Loss: 0.6183\n",
      "Iteration 1400, Loss: 0.6150\n",
      "Iteration 1500, Loss: 0.6119\n",
      "Iteration 1600, Loss: 0.6090\n",
      "Iteration 1700, Loss: 0.6062\n",
      "Iteration 1800, Loss: 0.6037\n",
      "Iteration 1900, Loss: 0.6012\n",
      "Iteration 2000, Loss: 0.5989\n",
      "Iteration 2100, Loss: 0.5968\n",
      "Iteration 2200, Loss: 0.5947\n",
      "Iteration 2300, Loss: 0.5927\n",
      "Iteration 2400, Loss: 0.5908\n",
      "Iteration 2500, Loss: 0.5890\n",
      "Iteration 2600, Loss: 0.5872\n",
      "Iteration 2700, Loss: 0.5856\n",
      "Iteration 2800, Loss: 0.5840\n",
      "Iteration 2900, Loss: 0.5824\n",
      "Iteration 3000, Loss: 0.5809\n",
      "Iteration 3100, Loss: 0.5794\n",
      "Iteration 3200, Loss: 0.5780\n",
      "Iteration 3300, Loss: 0.5767\n",
      "Iteration 3400, Loss: 0.5754\n",
      "Iteration 3500, Loss: 0.5741\n",
      "Iteration 3600, Loss: 0.5729\n",
      "Iteration 3700, Loss: 0.5717\n",
      "Iteration 3800, Loss: 0.5705\n",
      "Iteration 3900, Loss: 0.5694\n",
      "Iteration 4000, Loss: 0.5683\n",
      "Iteration 4100, Loss: 0.5672\n",
      "Iteration 4200, Loss: 0.5662\n",
      "Iteration 4300, Loss: 0.5652\n",
      "Iteration 4400, Loss: 0.5642\n",
      "Iteration 4500, Loss: 0.5632\n",
      "Iteration 4600, Loss: 0.5623\n",
      "Iteration 4700, Loss: 0.5613\n",
      "Iteration 4800, Loss: 0.5604\n",
      "Iteration 4900, Loss: 0.5596\n",
      "Iteration 5000, Loss: 0.5587\n",
      "Iteration 5100, Loss: 0.5579\n",
      "Iteration 5200, Loss: 0.5570\n",
      "Iteration 5300, Loss: 0.5562\n",
      "Iteration 5400, Loss: 0.5555\n",
      "Iteration 5500, Loss: 0.5547\n",
      "Iteration 5600, Loss: 0.5539\n",
      "Iteration 5700, Loss: 0.5532\n",
      "Iteration 5800, Loss: 0.5525\n",
      "Iteration 5900, Loss: 0.5517\n",
      "Iteration 0, Loss: 1.0891\n",
      "Iteration 100, Loss: 0.7819\n",
      "Iteration 200, Loss: 0.7155\n",
      "Iteration 300, Loss: 0.6844\n",
      "Iteration 400, Loss: 0.6650\n",
      "Iteration 500, Loss: 0.6512\n",
      "Iteration 600, Loss: 0.6407\n",
      "Iteration 700, Loss: 0.6323\n",
      "Iteration 800, Loss: 0.6253\n",
      "Iteration 900, Loss: 0.6194\n",
      "Iteration 1000, Loss: 0.6142\n",
      "Iteration 1100, Loss: 0.6097\n",
      "Iteration 1200, Loss: 0.6057\n",
      "Iteration 1300, Loss: 0.6020\n",
      "Iteration 1400, Loss: 0.5987\n",
      "Iteration 1500, Loss: 0.5956\n",
      "Iteration 1600, Loss: 0.5927\n",
      "Iteration 1700, Loss: 0.5901\n",
      "Iteration 1800, Loss: 0.5876\n",
      "Iteration 1900, Loss: 0.5852\n",
      "Iteration 2000, Loss: 0.5830\n",
      "Iteration 2100, Loss: 0.5809\n",
      "Iteration 2200, Loss: 0.5789\n",
      "Iteration 2300, Loss: 0.5770\n",
      "Iteration 2400, Loss: 0.5752\n",
      "Iteration 2500, Loss: 0.5735\n",
      "Iteration 2600, Loss: 0.5718\n",
      "Iteration 2700, Loss: 0.5703\n",
      "Iteration 2800, Loss: 0.5687\n",
      "Iteration 2900, Loss: 0.5673\n",
      "Iteration 3000, Loss: 0.5659\n",
      "Iteration 3100, Loss: 0.5645\n",
      "Iteration 3200, Loss: 0.5632\n",
      "Iteration 3300, Loss: 0.5619\n",
      "Iteration 3400, Loss: 0.5607\n",
      "Iteration 3500, Loss: 0.5595\n",
      "Iteration 3600, Loss: 0.5583\n",
      "Iteration 3700, Loss: 0.5572\n",
      "Iteration 3800, Loss: 0.5561\n",
      "Iteration 3900, Loss: 0.5551\n",
      "Iteration 4000, Loss: 0.5540\n",
      "Iteration 4100, Loss: 0.5530\n",
      "Iteration 4200, Loss: 0.5521\n",
      "Iteration 4300, Loss: 0.5511\n",
      "Iteration 4400, Loss: 0.5502\n",
      "Iteration 4500, Loss: 0.5493\n",
      "Iteration 4600, Loss: 0.5484\n",
      "Iteration 4700, Loss: 0.5475\n",
      "Iteration 4800, Loss: 0.5467\n",
      "Iteration 4900, Loss: 0.5459\n",
      "Iteration 5000, Loss: 0.5451\n",
      "Iteration 5100, Loss: 0.5443\n",
      "Iteration 5200, Loss: 0.5435\n",
      "Iteration 5300, Loss: 0.5427\n",
      "Iteration 5400, Loss: 0.5420\n",
      "Iteration 5500, Loss: 0.5413\n",
      "Iteration 5600, Loss: 0.5406\n",
      "Iteration 5700, Loss: 0.5399\n",
      "Iteration 5800, Loss: 0.5392\n",
      "Iteration 5900, Loss: 0.5385\n",
      "30 36\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8621\n",
      "Iteration 200, Loss: 0.7846\n",
      "Iteration 300, Loss: 0.7459\n",
      "Iteration 400, Loss: 0.7224\n",
      "Iteration 500, Loss: 0.7064\n",
      "Iteration 600, Loss: 0.6945\n",
      "Iteration 700, Loss: 0.6852\n",
      "Iteration 800, Loss: 0.6776\n",
      "Iteration 900, Loss: 0.6711\n",
      "Iteration 1000, Loss: 0.6656\n",
      "Iteration 1100, Loss: 0.6608\n",
      "Iteration 1200, Loss: 0.6565\n",
      "Iteration 1300, Loss: 0.6526\n",
      "Iteration 1400, Loss: 0.6490\n",
      "Iteration 1500, Loss: 0.6457\n",
      "Iteration 1600, Loss: 0.6427\n",
      "Iteration 1700, Loss: 0.6399\n",
      "Iteration 1800, Loss: 0.6374\n",
      "Iteration 1900, Loss: 0.6349\n",
      "Iteration 2000, Loss: 0.6327\n",
      "Iteration 2100, Loss: 0.6305\n",
      "Iteration 2200, Loss: 0.6285\n",
      "Iteration 2300, Loss: 0.6266\n",
      "Iteration 2400, Loss: 0.6247\n",
      "Iteration 2500, Loss: 0.6230\n",
      "Iteration 2600, Loss: 0.6213\n",
      "Iteration 2700, Loss: 0.6197\n",
      "Iteration 2800, Loss: 0.6181\n",
      "Iteration 2900, Loss: 0.6167\n",
      "Iteration 3000, Loss: 0.6152\n",
      "Iteration 3100, Loss: 0.6139\n",
      "Iteration 3200, Loss: 0.6125\n",
      "Iteration 3300, Loss: 0.6112\n",
      "Iteration 3400, Loss: 0.6100\n",
      "Iteration 3500, Loss: 0.6088\n",
      "Iteration 3600, Loss: 0.6076\n",
      "Iteration 3700, Loss: 0.6065\n",
      "Iteration 3800, Loss: 0.6054\n",
      "Iteration 3900, Loss: 0.6043\n",
      "Iteration 4000, Loss: 0.6033\n",
      "Iteration 4100, Loss: 0.6022\n",
      "Iteration 4200, Loss: 0.6013\n",
      "Iteration 4300, Loss: 0.6003\n",
      "Iteration 4400, Loss: 0.5994\n",
      "Iteration 4500, Loss: 0.5985\n",
      "Iteration 4600, Loss: 0.5976\n",
      "Iteration 4700, Loss: 0.5967\n",
      "Iteration 4800, Loss: 0.5959\n",
      "Iteration 4900, Loss: 0.5950\n",
      "Iteration 5000, Loss: 0.5942\n",
      "Iteration 5100, Loss: 0.5934\n",
      "Iteration 5200, Loss: 0.5926\n",
      "Iteration 5300, Loss: 0.5919\n",
      "Iteration 5400, Loss: 0.5911\n",
      "Iteration 5500, Loss: 0.5904\n",
      "Iteration 5600, Loss: 0.5897\n",
      "Iteration 5700, Loss: 0.5890\n",
      "Iteration 5800, Loss: 0.5883\n",
      "Iteration 5900, Loss: 0.5876\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8607\n",
      "Iteration 200, Loss: 0.7809\n",
      "Iteration 300, Loss: 0.7395\n",
      "Iteration 400, Loss: 0.7139\n",
      "Iteration 500, Loss: 0.6960\n",
      "Iteration 600, Loss: 0.6825\n",
      "Iteration 700, Loss: 0.6718\n",
      "Iteration 800, Loss: 0.6630\n",
      "Iteration 900, Loss: 0.6555\n",
      "Iteration 1000, Loss: 0.6492\n",
      "Iteration 1100, Loss: 0.6436\n",
      "Iteration 1200, Loss: 0.6386\n",
      "Iteration 1300, Loss: 0.6342\n",
      "Iteration 1400, Loss: 0.6302\n",
      "Iteration 1500, Loss: 0.6265\n",
      "Iteration 1600, Loss: 0.6232\n",
      "Iteration 1700, Loss: 0.6201\n",
      "Iteration 1800, Loss: 0.6173\n",
      "Iteration 1900, Loss: 0.6146\n",
      "Iteration 2000, Loss: 0.6121\n",
      "Iteration 2100, Loss: 0.6097\n",
      "Iteration 2200, Loss: 0.6075\n",
      "Iteration 2300, Loss: 0.6054\n",
      "Iteration 2400, Loss: 0.6034\n",
      "Iteration 2500, Loss: 0.6015\n",
      "Iteration 2600, Loss: 0.5997\n",
      "Iteration 2700, Loss: 0.5980\n",
      "Iteration 2800, Loss: 0.5963\n",
      "Iteration 2900, Loss: 0.5947\n",
      "Iteration 3000, Loss: 0.5932\n",
      "Iteration 3100, Loss: 0.5917\n",
      "Iteration 3200, Loss: 0.5903\n",
      "Iteration 3300, Loss: 0.5889\n",
      "Iteration 3400, Loss: 0.5876\n",
      "Iteration 3500, Loss: 0.5863\n",
      "Iteration 3600, Loss: 0.5850\n",
      "Iteration 3700, Loss: 0.5838\n",
      "Iteration 3800, Loss: 0.5826\n",
      "Iteration 3900, Loss: 0.5815\n",
      "Iteration 4000, Loss: 0.5803\n",
      "Iteration 4100, Loss: 0.5793\n",
      "Iteration 4200, Loss: 0.5782\n",
      "Iteration 4300, Loss: 0.5772\n",
      "Iteration 4400, Loss: 0.5762\n",
      "Iteration 4500, Loss: 0.5752\n",
      "Iteration 4600, Loss: 0.5742\n",
      "Iteration 4700, Loss: 0.5733\n",
      "Iteration 4800, Loss: 0.5723\n",
      "Iteration 4900, Loss: 0.5714\n",
      "Iteration 5000, Loss: 0.5706\n",
      "Iteration 5100, Loss: 0.5697\n",
      "Iteration 5200, Loss: 0.5688\n",
      "Iteration 5300, Loss: 0.5680\n",
      "Iteration 5400, Loss: 0.5672\n",
      "Iteration 5500, Loss: 0.5664\n",
      "Iteration 5600, Loss: 0.5656\n",
      "Iteration 5700, Loss: 0.5648\n",
      "Iteration 5800, Loss: 0.5641\n",
      "Iteration 5900, Loss: 0.5634\n",
      "31 36\n",
      "Iteration 0, Loss: 1.0939\n",
      "Iteration 100, Loss: 0.8659\n",
      "Iteration 200, Loss: 0.7892\n",
      "Iteration 300, Loss: 0.7506\n",
      "Iteration 400, Loss: 0.7268\n",
      "Iteration 500, Loss: 0.7105\n",
      "Iteration 600, Loss: 0.6982\n",
      "Iteration 700, Loss: 0.6884\n",
      "Iteration 800, Loss: 0.6803\n",
      "Iteration 900, Loss: 0.6735\n",
      "Iteration 1000, Loss: 0.6677\n",
      "Iteration 1100, Loss: 0.6626\n",
      "Iteration 1200, Loss: 0.6580\n",
      "Iteration 1300, Loss: 0.6539\n",
      "Iteration 1400, Loss: 0.6502\n",
      "Iteration 1500, Loss: 0.6468\n",
      "Iteration 1600, Loss: 0.6437\n",
      "Iteration 1700, Loss: 0.6409\n",
      "Iteration 1800, Loss: 0.6382\n",
      "Iteration 1900, Loss: 0.6357\n",
      "Iteration 2000, Loss: 0.6334\n",
      "Iteration 2100, Loss: 0.6311\n",
      "Iteration 2200, Loss: 0.6291\n",
      "Iteration 2300, Loss: 0.6271\n",
      "Iteration 2400, Loss: 0.6252\n",
      "Iteration 2500, Loss: 0.6234\n",
      "Iteration 2600, Loss: 0.6217\n",
      "Iteration 2700, Loss: 0.6200\n",
      "Iteration 2800, Loss: 0.6184\n",
      "Iteration 2900, Loss: 0.6169\n",
      "Iteration 3000, Loss: 0.6155\n",
      "Iteration 3100, Loss: 0.6140\n",
      "Iteration 3200, Loss: 0.6127\n",
      "Iteration 3300, Loss: 0.6114\n",
      "Iteration 3400, Loss: 0.6101\n",
      "Iteration 3500, Loss: 0.6088\n",
      "Iteration 3600, Loss: 0.6076\n",
      "Iteration 3700, Loss: 0.6065\n",
      "Iteration 3800, Loss: 0.6053\n",
      "Iteration 3900, Loss: 0.6042\n",
      "Iteration 4000, Loss: 0.6032\n",
      "Iteration 4100, Loss: 0.6021\n",
      "Iteration 4200, Loss: 0.6011\n",
      "Iteration 4300, Loss: 0.6001\n",
      "Iteration 4400, Loss: 0.5991\n",
      "Iteration 4500, Loss: 0.5982\n",
      "Iteration 4600, Loss: 0.5973\n",
      "Iteration 4700, Loss: 0.5964\n",
      "Iteration 4800, Loss: 0.5955\n",
      "Iteration 4900, Loss: 0.5946\n",
      "Iteration 5000, Loss: 0.5938\n",
      "Iteration 5100, Loss: 0.5930\n",
      "Iteration 5200, Loss: 0.5922\n",
      "Iteration 5300, Loss: 0.5914\n",
      "Iteration 5400, Loss: 0.5906\n",
      "Iteration 5500, Loss: 0.5898\n",
      "Iteration 5600, Loss: 0.5891\n",
      "Iteration 5700, Loss: 0.5884\n",
      "Iteration 5800, Loss: 0.5877\n",
      "Iteration 5900, Loss: 0.5870\n",
      "Iteration 0, Loss: 1.0936\n",
      "Iteration 100, Loss: 0.8558\n",
      "Iteration 200, Loss: 0.7748\n",
      "Iteration 300, Loss: 0.7339\n",
      "Iteration 400, Loss: 0.7089\n",
      "Iteration 500, Loss: 0.6917\n",
      "Iteration 600, Loss: 0.6788\n",
      "Iteration 700, Loss: 0.6687\n",
      "Iteration 800, Loss: 0.6602\n",
      "Iteration 900, Loss: 0.6531\n",
      "Iteration 1000, Loss: 0.6469\n",
      "Iteration 1100, Loss: 0.6416\n",
      "Iteration 1200, Loss: 0.6367\n",
      "Iteration 1300, Loss: 0.6324\n",
      "Iteration 1400, Loss: 0.6285\n",
      "Iteration 1500, Loss: 0.6248\n",
      "Iteration 1600, Loss: 0.6215\n",
      "Iteration 1700, Loss: 0.6185\n",
      "Iteration 1800, Loss: 0.6156\n",
      "Iteration 1900, Loss: 0.6129\n",
      "Iteration 2000, Loss: 0.6104\n",
      "Iteration 2100, Loss: 0.6080\n",
      "Iteration 2200, Loss: 0.6058\n",
      "Iteration 2300, Loss: 0.6037\n",
      "Iteration 2400, Loss: 0.6017\n",
      "Iteration 2500, Loss: 0.5997\n",
      "Iteration 2600, Loss: 0.5979\n",
      "Iteration 2700, Loss: 0.5961\n",
      "Iteration 2800, Loss: 0.5944\n",
      "Iteration 2900, Loss: 0.5928\n",
      "Iteration 3000, Loss: 0.5913\n",
      "Iteration 3100, Loss: 0.5898\n",
      "Iteration 3200, Loss: 0.5883\n",
      "Iteration 3300, Loss: 0.5869\n",
      "Iteration 3400, Loss: 0.5855\n",
      "Iteration 3500, Loss: 0.5842\n",
      "Iteration 3600, Loss: 0.5829\n",
      "Iteration 3700, Loss: 0.5817\n",
      "Iteration 3800, Loss: 0.5805\n",
      "Iteration 3900, Loss: 0.5793\n",
      "Iteration 4000, Loss: 0.5782\n",
      "Iteration 4100, Loss: 0.5771\n",
      "Iteration 4200, Loss: 0.5760\n",
      "Iteration 4300, Loss: 0.5749\n",
      "Iteration 4400, Loss: 0.5739\n",
      "Iteration 4500, Loss: 0.5729\n",
      "Iteration 4600, Loss: 0.5719\n",
      "Iteration 4700, Loss: 0.5710\n",
      "Iteration 4800, Loss: 0.5700\n",
      "Iteration 4900, Loss: 0.5691\n",
      "Iteration 5000, Loss: 0.5682\n",
      "Iteration 5100, Loss: 0.5673\n",
      "Iteration 5200, Loss: 0.5665\n",
      "Iteration 5300, Loss: 0.5656\n",
      "Iteration 5400, Loss: 0.5648\n",
      "Iteration 5500, Loss: 0.5640\n",
      "Iteration 5600, Loss: 0.5632\n",
      "Iteration 5700, Loss: 0.5624\n",
      "Iteration 5800, Loss: 0.5617\n",
      "Iteration 5900, Loss: 0.5609\n",
      "32 36\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0218\n",
      "Iteration 200, Loss: 0.9688\n",
      "Iteration 300, Loss: 0.9290\n",
      "Iteration 400, Loss: 0.8978\n",
      "Iteration 500, Loss: 0.8725\n",
      "Iteration 600, Loss: 0.8516\n",
      "Iteration 700, Loss: 0.8341\n",
      "Iteration 800, Loss: 0.8190\n",
      "Iteration 900, Loss: 0.8060\n",
      "Iteration 1000, Loss: 0.7947\n",
      "Iteration 1100, Loss: 0.7847\n",
      "Iteration 1200, Loss: 0.7758\n",
      "Iteration 1300, Loss: 0.7680\n",
      "Iteration 1400, Loss: 0.7609\n",
      "Iteration 1500, Loss: 0.7545\n",
      "Iteration 1600, Loss: 0.7487\n",
      "Iteration 1700, Loss: 0.7434\n",
      "Iteration 1800, Loss: 0.7386\n",
      "Iteration 1900, Loss: 0.7341\n",
      "Iteration 2000, Loss: 0.7300\n",
      "Iteration 2100, Loss: 0.7261\n",
      "Iteration 2200, Loss: 0.7225\n",
      "Iteration 2300, Loss: 0.7192\n",
      "Iteration 2400, Loss: 0.7160\n",
      "Iteration 2500, Loss: 0.7131\n",
      "Iteration 2600, Loss: 0.7103\n",
      "Iteration 2700, Loss: 0.7077\n",
      "Iteration 2800, Loss: 0.7052\n",
      "Iteration 2900, Loss: 0.7028\n",
      "Iteration 3000, Loss: 0.7005\n",
      "Iteration 3100, Loss: 0.6984\n",
      "Iteration 3200, Loss: 0.6963\n",
      "Iteration 3300, Loss: 0.6943\n",
      "Iteration 3400, Loss: 0.6924\n",
      "Iteration 3500, Loss: 0.6906\n",
      "Iteration 3600, Loss: 0.6889\n",
      "Iteration 3700, Loss: 0.6872\n",
      "Iteration 3800, Loss: 0.6856\n",
      "Iteration 3900, Loss: 0.6840\n",
      "Iteration 4000, Loss: 0.6825\n",
      "Iteration 4100, Loss: 0.6811\n",
      "Iteration 4200, Loss: 0.6797\n",
      "Iteration 4300, Loss: 0.6783\n",
      "Iteration 4400, Loss: 0.6770\n",
      "Iteration 4500, Loss: 0.6757\n",
      "Iteration 4600, Loss: 0.6744\n",
      "Iteration 4700, Loss: 0.6732\n",
      "Iteration 4800, Loss: 0.6720\n",
      "Iteration 4900, Loss: 0.6709\n",
      "Iteration 5000, Loss: 0.6698\n",
      "Iteration 5100, Loss: 0.6687\n",
      "Iteration 5200, Loss: 0.6676\n",
      "Iteration 5300, Loss: 0.6666\n",
      "Iteration 5400, Loss: 0.6656\n",
      "Iteration 5500, Loss: 0.6646\n",
      "Iteration 5600, Loss: 0.6637\n",
      "Iteration 5700, Loss: 0.6627\n",
      "Iteration 5800, Loss: 0.6618\n",
      "Iteration 5900, Loss: 0.6609\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0177\n",
      "Iteration 200, Loss: 0.9621\n",
      "Iteration 300, Loss: 0.9204\n",
      "Iteration 400, Loss: 0.8877\n",
      "Iteration 500, Loss: 0.8612\n",
      "Iteration 600, Loss: 0.8393\n",
      "Iteration 700, Loss: 0.8210\n",
      "Iteration 800, Loss: 0.8053\n",
      "Iteration 900, Loss: 0.7918\n",
      "Iteration 1000, Loss: 0.7801\n",
      "Iteration 1100, Loss: 0.7698\n",
      "Iteration 1200, Loss: 0.7607\n",
      "Iteration 1300, Loss: 0.7526\n",
      "Iteration 1400, Loss: 0.7453\n",
      "Iteration 1500, Loss: 0.7387\n",
      "Iteration 1600, Loss: 0.7327\n",
      "Iteration 1700, Loss: 0.7272\n",
      "Iteration 1800, Loss: 0.7222\n",
      "Iteration 1900, Loss: 0.7176\n",
      "Iteration 2000, Loss: 0.7133\n",
      "Iteration 2100, Loss: 0.7093\n",
      "Iteration 2200, Loss: 0.7056\n",
      "Iteration 2300, Loss: 0.7021\n",
      "Iteration 2400, Loss: 0.6988\n",
      "Iteration 2500, Loss: 0.6958\n",
      "Iteration 2600, Loss: 0.6928\n",
      "Iteration 2700, Loss: 0.6901\n",
      "Iteration 2800, Loss: 0.6875\n",
      "Iteration 2900, Loss: 0.6850\n",
      "Iteration 3000, Loss: 0.6826\n",
      "Iteration 3100, Loss: 0.6803\n",
      "Iteration 3200, Loss: 0.6782\n",
      "Iteration 3300, Loss: 0.6761\n",
      "Iteration 3400, Loss: 0.6741\n",
      "Iteration 3500, Loss: 0.6722\n",
      "Iteration 3600, Loss: 0.6703\n",
      "Iteration 3700, Loss: 0.6685\n",
      "Iteration 3800, Loss: 0.6668\n",
      "Iteration 3900, Loss: 0.6652\n",
      "Iteration 4000, Loss: 0.6636\n",
      "Iteration 4100, Loss: 0.6620\n",
      "Iteration 4200, Loss: 0.6605\n",
      "Iteration 4300, Loss: 0.6591\n",
      "Iteration 4400, Loss: 0.6577\n",
      "Iteration 4500, Loss: 0.6563\n",
      "Iteration 4600, Loss: 0.6549\n",
      "Iteration 4700, Loss: 0.6536\n",
      "Iteration 4800, Loss: 0.6524\n",
      "Iteration 4900, Loss: 0.6512\n",
      "Iteration 5000, Loss: 0.6500\n",
      "Iteration 5100, Loss: 0.6488\n",
      "Iteration 5200, Loss: 0.6477\n",
      "Iteration 5300, Loss: 0.6466\n",
      "Iteration 5400, Loss: 0.6455\n",
      "Iteration 5500, Loss: 0.6444\n",
      "Iteration 5600, Loss: 0.6434\n",
      "Iteration 5700, Loss: 0.6424\n",
      "Iteration 5800, Loss: 0.6414\n",
      "Iteration 5900, Loss: 0.6404\n",
      "33 36\n",
      "Iteration 0, Loss: 1.0977\n",
      "Iteration 100, Loss: 1.0212\n",
      "Iteration 200, Loss: 0.9679\n",
      "Iteration 300, Loss: 0.9279\n",
      "Iteration 400, Loss: 0.8965\n",
      "Iteration 500, Loss: 0.8710\n",
      "Iteration 600, Loss: 0.8499\n",
      "Iteration 700, Loss: 0.8321\n",
      "Iteration 800, Loss: 0.8169\n",
      "Iteration 900, Loss: 0.8037\n",
      "Iteration 1000, Loss: 0.7922\n",
      "Iteration 1100, Loss: 0.7821\n",
      "Iteration 1200, Loss: 0.7732\n",
      "Iteration 1300, Loss: 0.7651\n",
      "Iteration 1400, Loss: 0.7579\n",
      "Iteration 1500, Loss: 0.7514\n",
      "Iteration 1600, Loss: 0.7455\n",
      "Iteration 1700, Loss: 0.7401\n",
      "Iteration 1800, Loss: 0.7352\n",
      "Iteration 1900, Loss: 0.7306\n",
      "Iteration 2000, Loss: 0.7264\n",
      "Iteration 2100, Loss: 0.7224\n",
      "Iteration 2200, Loss: 0.7188\n",
      "Iteration 2300, Loss: 0.7154\n",
      "Iteration 2400, Loss: 0.7121\n",
      "Iteration 2500, Loss: 0.7091\n",
      "Iteration 2600, Loss: 0.7063\n",
      "Iteration 2700, Loss: 0.7036\n",
      "Iteration 2800, Loss: 0.7010\n",
      "Iteration 2900, Loss: 0.6986\n",
      "Iteration 3000, Loss: 0.6962\n",
      "Iteration 3100, Loss: 0.6940\n",
      "Iteration 3200, Loss: 0.6919\n",
      "Iteration 3300, Loss: 0.6899\n",
      "Iteration 3400, Loss: 0.6879\n",
      "Iteration 3500, Loss: 0.6860\n",
      "Iteration 3600, Loss: 0.6842\n",
      "Iteration 3700, Loss: 0.6825\n",
      "Iteration 3800, Loss: 0.6808\n",
      "Iteration 3900, Loss: 0.6792\n",
      "Iteration 4000, Loss: 0.6777\n",
      "Iteration 4100, Loss: 0.6762\n",
      "Iteration 4200, Loss: 0.6747\n",
      "Iteration 4300, Loss: 0.6733\n",
      "Iteration 4400, Loss: 0.6719\n",
      "Iteration 4500, Loss: 0.6706\n",
      "Iteration 4600, Loss: 0.6693\n",
      "Iteration 4700, Loss: 0.6681\n",
      "Iteration 4800, Loss: 0.6668\n",
      "Iteration 4900, Loss: 0.6657\n",
      "Iteration 5000, Loss: 0.6645\n",
      "Iteration 5100, Loss: 0.6634\n",
      "Iteration 5200, Loss: 0.6623\n",
      "Iteration 5300, Loss: 0.6612\n",
      "Iteration 5400, Loss: 0.6602\n",
      "Iteration 5500, Loss: 0.6592\n",
      "Iteration 5600, Loss: 0.6582\n",
      "Iteration 5700, Loss: 0.6572\n",
      "Iteration 5800, Loss: 0.6563\n",
      "Iteration 5900, Loss: 0.6553\n",
      "Iteration 0, Loss: 1.0976\n",
      "Iteration 100, Loss: 1.0185\n",
      "Iteration 200, Loss: 0.9632\n",
      "Iteration 300, Loss: 0.9215\n",
      "Iteration 400, Loss: 0.8889\n",
      "Iteration 500, Loss: 0.8625\n",
      "Iteration 600, Loss: 0.8408\n",
      "Iteration 700, Loss: 0.8224\n",
      "Iteration 800, Loss: 0.8068\n",
      "Iteration 900, Loss: 0.7933\n",
      "Iteration 1000, Loss: 0.7816\n",
      "Iteration 1100, Loss: 0.7713\n",
      "Iteration 1200, Loss: 0.7622\n",
      "Iteration 1300, Loss: 0.7540\n",
      "Iteration 1400, Loss: 0.7467\n",
      "Iteration 1500, Loss: 0.7401\n",
      "Iteration 1600, Loss: 0.7340\n",
      "Iteration 1700, Loss: 0.7285\n",
      "Iteration 1800, Loss: 0.7235\n",
      "Iteration 1900, Loss: 0.7188\n",
      "Iteration 2000, Loss: 0.7145\n",
      "Iteration 2100, Loss: 0.7105\n",
      "Iteration 2200, Loss: 0.7068\n",
      "Iteration 2300, Loss: 0.7033\n",
      "Iteration 2400, Loss: 0.7001\n",
      "Iteration 2500, Loss: 0.6970\n",
      "Iteration 2600, Loss: 0.6941\n",
      "Iteration 2700, Loss: 0.6914\n",
      "Iteration 2800, Loss: 0.6888\n",
      "Iteration 2900, Loss: 0.6863\n",
      "Iteration 3000, Loss: 0.6839\n",
      "Iteration 3100, Loss: 0.6817\n",
      "Iteration 3200, Loss: 0.6796\n",
      "Iteration 3300, Loss: 0.6775\n",
      "Iteration 3400, Loss: 0.6756\n",
      "Iteration 3500, Loss: 0.6737\n",
      "Iteration 3600, Loss: 0.6719\n",
      "Iteration 3700, Loss: 0.6701\n",
      "Iteration 3800, Loss: 0.6684\n",
      "Iteration 3900, Loss: 0.6668\n",
      "Iteration 4000, Loss: 0.6653\n",
      "Iteration 4100, Loss: 0.6637\n",
      "Iteration 4200, Loss: 0.6623\n",
      "Iteration 4300, Loss: 0.6609\n",
      "Iteration 4400, Loss: 0.6595\n",
      "Iteration 4500, Loss: 0.6582\n",
      "Iteration 4600, Loss: 0.6569\n",
      "Iteration 4700, Loss: 0.6556\n",
      "Iteration 4800, Loss: 0.6544\n",
      "Iteration 4900, Loss: 0.6532\n",
      "Iteration 5000, Loss: 0.6521\n",
      "Iteration 5100, Loss: 0.6509\n",
      "Iteration 5200, Loss: 0.6498\n",
      "Iteration 5300, Loss: 0.6488\n",
      "Iteration 5400, Loss: 0.6477\n",
      "Iteration 5500, Loss: 0.6467\n",
      "Iteration 5600, Loss: 0.6457\n",
      "Iteration 5700, Loss: 0.6447\n",
      "Iteration 5800, Loss: 0.6438\n",
      "Iteration 5900, Loss: 0.6429\n",
      "34 36\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0525\n",
      "Iteration 200, Loss: 1.0157\n",
      "Iteration 300, Loss: 0.9853\n",
      "Iteration 400, Loss: 0.9595\n",
      "Iteration 500, Loss: 0.9374\n",
      "Iteration 600, Loss: 0.9180\n",
      "Iteration 700, Loss: 0.9010\n",
      "Iteration 800, Loss: 0.8859\n",
      "Iteration 900, Loss: 0.8724\n",
      "Iteration 1000, Loss: 0.8603\n",
      "Iteration 1100, Loss: 0.8493\n",
      "Iteration 1200, Loss: 0.8393\n",
      "Iteration 1300, Loss: 0.8302\n",
      "Iteration 1400, Loss: 0.8218\n",
      "Iteration 1500, Loss: 0.8140\n",
      "Iteration 1600, Loss: 0.8070\n",
      "Iteration 1700, Loss: 0.8004\n",
      "Iteration 1800, Loss: 0.7943\n",
      "Iteration 1900, Loss: 0.7886\n",
      "Iteration 2000, Loss: 0.7833\n",
      "Iteration 2100, Loss: 0.7783\n",
      "Iteration 2200, Loss: 0.7736\n",
      "Iteration 2300, Loss: 0.7693\n",
      "Iteration 2400, Loss: 0.7651\n",
      "Iteration 2500, Loss: 0.7612\n",
      "Iteration 2600, Loss: 0.7575\n",
      "Iteration 2700, Loss: 0.7540\n",
      "Iteration 2800, Loss: 0.7507\n",
      "Iteration 2900, Loss: 0.7476\n",
      "Iteration 3000, Loss: 0.7445\n",
      "Iteration 3100, Loss: 0.7417\n",
      "Iteration 3200, Loss: 0.7390\n",
      "Iteration 3300, Loss: 0.7364\n",
      "Iteration 3400, Loss: 0.7339\n",
      "Iteration 3500, Loss: 0.7315\n",
      "Iteration 3600, Loss: 0.7292\n",
      "Iteration 3700, Loss: 0.7270\n",
      "Iteration 3800, Loss: 0.7249\n",
      "Iteration 3900, Loss: 0.7229\n",
      "Iteration 4000, Loss: 0.7209\n",
      "Iteration 4100, Loss: 0.7190\n",
      "Iteration 4200, Loss: 0.7172\n",
      "Iteration 4300, Loss: 0.7154\n",
      "Iteration 4400, Loss: 0.7137\n",
      "Iteration 4500, Loss: 0.7121\n",
      "Iteration 4600, Loss: 0.7105\n",
      "Iteration 4700, Loss: 0.7089\n",
      "Iteration 4800, Loss: 0.7074\n",
      "Iteration 4900, Loss: 0.7060\n",
      "Iteration 5000, Loss: 0.7046\n",
      "Iteration 5100, Loss: 0.7032\n",
      "Iteration 5200, Loss: 0.7019\n",
      "Iteration 5300, Loss: 0.7006\n",
      "Iteration 5400, Loss: 0.6993\n",
      "Iteration 5500, Loss: 0.6981\n",
      "Iteration 5600, Loss: 0.6969\n",
      "Iteration 5700, Loss: 0.6957\n",
      "Iteration 5800, Loss: 0.6946\n",
      "Iteration 5900, Loss: 0.6935\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0549\n",
      "Iteration 200, Loss: 1.0196\n",
      "Iteration 300, Loss: 0.9897\n",
      "Iteration 400, Loss: 0.9643\n",
      "Iteration 500, Loss: 0.9424\n",
      "Iteration 600, Loss: 0.9231\n",
      "Iteration 700, Loss: 0.9059\n",
      "Iteration 800, Loss: 0.8905\n",
      "Iteration 900, Loss: 0.8766\n",
      "Iteration 1000, Loss: 0.8640\n",
      "Iteration 1100, Loss: 0.8526\n",
      "Iteration 1200, Loss: 0.8421\n",
      "Iteration 1300, Loss: 0.8325\n",
      "Iteration 1400, Loss: 0.8236\n",
      "Iteration 1500, Loss: 0.8155\n",
      "Iteration 1600, Loss: 0.8079\n",
      "Iteration 1700, Loss: 0.8008\n",
      "Iteration 1800, Loss: 0.7942\n",
      "Iteration 1900, Loss: 0.7881\n",
      "Iteration 2000, Loss: 0.7824\n",
      "Iteration 2100, Loss: 0.7770\n",
      "Iteration 2200, Loss: 0.7720\n",
      "Iteration 2300, Loss: 0.7672\n",
      "Iteration 2400, Loss: 0.7627\n",
      "Iteration 2500, Loss: 0.7585\n",
      "Iteration 2600, Loss: 0.7545\n",
      "Iteration 2700, Loss: 0.7508\n",
      "Iteration 2800, Loss: 0.7472\n",
      "Iteration 2900, Loss: 0.7437\n",
      "Iteration 3000, Loss: 0.7405\n",
      "Iteration 3100, Loss: 0.7374\n",
      "Iteration 3200, Loss: 0.7345\n",
      "Iteration 3300, Loss: 0.7316\n",
      "Iteration 3400, Loss: 0.7289\n",
      "Iteration 3500, Loss: 0.7264\n",
      "Iteration 3600, Loss: 0.7239\n",
      "Iteration 3700, Loss: 0.7215\n",
      "Iteration 3800, Loss: 0.7192\n",
      "Iteration 3900, Loss: 0.7170\n",
      "Iteration 4000, Loss: 0.7149\n",
      "Iteration 4100, Loss: 0.7129\n",
      "Iteration 4200, Loss: 0.7109\n",
      "Iteration 4300, Loss: 0.7090\n",
      "Iteration 4400, Loss: 0.7072\n",
      "Iteration 4500, Loss: 0.7054\n",
      "Iteration 4600, Loss: 0.7037\n",
      "Iteration 4700, Loss: 0.7020\n",
      "Iteration 4800, Loss: 0.7004\n",
      "Iteration 4900, Loss: 0.6988\n",
      "Iteration 5000, Loss: 0.6973\n",
      "Iteration 5100, Loss: 0.6958\n",
      "Iteration 5200, Loss: 0.6944\n",
      "Iteration 5300, Loss: 0.6930\n",
      "Iteration 5400, Loss: 0.6917\n",
      "Iteration 5500, Loss: 0.6903\n",
      "Iteration 5600, Loss: 0.6890\n",
      "Iteration 5700, Loss: 0.6878\n",
      "Iteration 5800, Loss: 0.6865\n",
      "Iteration 5900, Loss: 0.6853\n",
      "35 36\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0544\n",
      "Iteration 200, Loss: 1.0187\n",
      "Iteration 300, Loss: 0.9892\n",
      "Iteration 400, Loss: 0.9639\n",
      "Iteration 500, Loss: 0.9420\n",
      "Iteration 600, Loss: 0.9228\n",
      "Iteration 700, Loss: 0.9059\n",
      "Iteration 800, Loss: 0.8910\n",
      "Iteration 900, Loss: 0.8775\n",
      "Iteration 1000, Loss: 0.8652\n",
      "Iteration 1100, Loss: 0.8540\n",
      "Iteration 1200, Loss: 0.8439\n",
      "Iteration 1300, Loss: 0.8346\n",
      "Iteration 1400, Loss: 0.8261\n",
      "Iteration 1500, Loss: 0.8182\n",
      "Iteration 1600, Loss: 0.8110\n",
      "Iteration 1700, Loss: 0.8042\n",
      "Iteration 1800, Loss: 0.7980\n",
      "Iteration 1900, Loss: 0.7922\n",
      "Iteration 2000, Loss: 0.7867\n",
      "Iteration 2100, Loss: 0.7816\n",
      "Iteration 2200, Loss: 0.7768\n",
      "Iteration 2300, Loss: 0.7723\n",
      "Iteration 2400, Loss: 0.7680\n",
      "Iteration 2500, Loss: 0.7641\n",
      "Iteration 2600, Loss: 0.7603\n",
      "Iteration 2700, Loss: 0.7567\n",
      "Iteration 2800, Loss: 0.7533\n",
      "Iteration 2900, Loss: 0.7500\n",
      "Iteration 3000, Loss: 0.7470\n",
      "Iteration 3100, Loss: 0.7441\n",
      "Iteration 3200, Loss: 0.7413\n",
      "Iteration 3300, Loss: 0.7386\n",
      "Iteration 3400, Loss: 0.7360\n",
      "Iteration 3500, Loss: 0.7336\n",
      "Iteration 3600, Loss: 0.7312\n",
      "Iteration 3700, Loss: 0.7290\n",
      "Iteration 3800, Loss: 0.7268\n",
      "Iteration 3900, Loss: 0.7248\n",
      "Iteration 4000, Loss: 0.7228\n",
      "Iteration 4100, Loss: 0.7209\n",
      "Iteration 4200, Loss: 0.7190\n",
      "Iteration 4300, Loss: 0.7172\n",
      "Iteration 4400, Loss: 0.7155\n",
      "Iteration 4500, Loss: 0.7138\n",
      "Iteration 4600, Loss: 0.7122\n",
      "Iteration 4700, Loss: 0.7106\n",
      "Iteration 4800, Loss: 0.7091\n",
      "Iteration 4900, Loss: 0.7076\n",
      "Iteration 5000, Loss: 0.7062\n",
      "Iteration 5100, Loss: 0.7048\n",
      "Iteration 5200, Loss: 0.7034\n",
      "Iteration 5300, Loss: 0.7021\n",
      "Iteration 5400, Loss: 0.7008\n",
      "Iteration 5500, Loss: 0.6996\n",
      "Iteration 5600, Loss: 0.6984\n",
      "Iteration 5700, Loss: 0.6972\n",
      "Iteration 5800, Loss: 0.6961\n",
      "Iteration 5900, Loss: 0.6949\n",
      "Iteration 0, Loss: 1.0981\n",
      "Iteration 100, Loss: 1.0527\n",
      "Iteration 200, Loss: 1.0159\n",
      "Iteration 300, Loss: 0.9853\n",
      "Iteration 400, Loss: 0.9594\n",
      "Iteration 500, Loss: 0.9369\n",
      "Iteration 600, Loss: 0.9172\n",
      "Iteration 700, Loss: 0.9000\n",
      "Iteration 800, Loss: 0.8845\n",
      "Iteration 900, Loss: 0.8707\n",
      "Iteration 1000, Loss: 0.8582\n",
      "Iteration 1100, Loss: 0.8468\n",
      "Iteration 1200, Loss: 0.8364\n",
      "Iteration 1300, Loss: 0.8269\n",
      "Iteration 1400, Loss: 0.8182\n",
      "Iteration 1500, Loss: 0.8101\n",
      "Iteration 1600, Loss: 0.8026\n",
      "Iteration 1700, Loss: 0.7957\n",
      "Iteration 1800, Loss: 0.7892\n",
      "Iteration 1900, Loss: 0.7833\n",
      "Iteration 2000, Loss: 0.7776\n",
      "Iteration 2100, Loss: 0.7724\n",
      "Iteration 2200, Loss: 0.7674\n",
      "Iteration 2300, Loss: 0.7628\n",
      "Iteration 2400, Loss: 0.7584\n",
      "Iteration 2500, Loss: 0.7542\n",
      "Iteration 2600, Loss: 0.7503\n",
      "Iteration 2700, Loss: 0.7466\n",
      "Iteration 2800, Loss: 0.7430\n",
      "Iteration 2900, Loss: 0.7396\n",
      "Iteration 3000, Loss: 0.7364\n",
      "Iteration 3100, Loss: 0.7334\n",
      "Iteration 3200, Loss: 0.7304\n",
      "Iteration 3300, Loss: 0.7276\n",
      "Iteration 3400, Loss: 0.7250\n",
      "Iteration 3500, Loss: 0.7224\n",
      "Iteration 3600, Loss: 0.7199\n",
      "Iteration 3700, Loss: 0.7176\n",
      "Iteration 3800, Loss: 0.7153\n",
      "Iteration 3900, Loss: 0.7132\n",
      "Iteration 4000, Loss: 0.7111\n",
      "Iteration 4100, Loss: 0.7090\n",
      "Iteration 4200, Loss: 0.7071\n",
      "Iteration 4300, Loss: 0.7052\n",
      "Iteration 4400, Loss: 0.7033\n",
      "Iteration 4500, Loss: 0.7016\n",
      "Iteration 4600, Loss: 0.6999\n",
      "Iteration 4700, Loss: 0.6982\n",
      "Iteration 4800, Loss: 0.6966\n",
      "Iteration 4900, Loss: 0.6950\n",
      "Iteration 5000, Loss: 0.6935\n",
      "Iteration 5100, Loss: 0.6920\n",
      "Iteration 5200, Loss: 0.6906\n",
      "Iteration 5300, Loss: 0.6892\n",
      "Iteration 5400, Loss: 0.6878\n",
      "Iteration 5500, Loss: 0.6865\n",
      "Iteration 5600, Loss: 0.6852\n",
      "Iteration 5700, Loss: 0.6840\n",
      "Iteration 5800, Loss: 0.6827\n",
      "Iteration 5900, Loss: 0.6815\n",
      "{'n_iters': 4000, 'lr': 0.001, 'batch_size': 64, 'l': 1e-05} 0.775516795865633\n"
     ]
    }
   ],
   "execution_count": 270
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "WVI. Ensemble methods",
   "id": "ea57a47460bfe351"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T08:24:49.534216Z",
     "start_time": "2025-05-22T08:24:49.522899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining correlation matrix\n",
    "print(X_train.columns)"
   ],
   "id": "6e090856a1162d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Application order', 'Age at enrollment',\n",
      "       'Curricular units 1st sem (credited)',\n",
      "       'Curricular units 1st sem (enrolled)',\n",
      "       'Curricular units 1st sem (evaluations)',\n",
      "       'Curricular units 1st sem (approved)',\n",
      "       'Curricular units 1st sem (grade)',\n",
      "       'Curricular units 1st sem (without evaluations)',\n",
      "       'Curricular units 2nd sem (credited)',\n",
      "       'Curricular units 2nd sem (enrolled)',\n",
      "       'Curricular units 2nd sem (evaluations)',\n",
      "       'Curricular units 2nd sem (approved)',\n",
      "       'Curricular units 2nd sem (without evaluations)',\n",
      "       'Curricular units 2nd sem (grade)', 'Marital status',\n",
      "       'Application mode', 'Course', 'Daytime/evening attendance',\n",
      "       'Previous qualification', 'Mother's qualification',\n",
      "       'Father's qualification', 'Mother's occupation', 'Father's occupation',\n",
      "       'Displaced', 'Educational special needs', 'Debtor',\n",
      "       'Tuition fees up to date', 'Gender', 'Scholarship holder'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:23:45.615954Z",
     "start_time": "2025-05-23T09:20:52.330239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "X_train_proc1, X_test_proc1, y_train_proc1, y_test_proc1 = transform_in_pipeline(\n",
    "    modified_features_preprocessor, X_train, X_test, y_train, y_test)\n",
    "\n",
    "X_train_proc2, X_test_proc2, y_train_proc2, y_test_proc2 = transform_in_pipeline(\n",
    "    preprocessor_full_set, X_train, X_test, y_train, y_test)\n",
    "base = BaseLogisticRegression(n_iters=5000, lr=0.001, batch_size=64)\n",
    "base_L1 = BaseLogisticRegressionRegL1(n_iters=5000, lr=0.001, batch_size=64, l=0.001)\n",
    "base_L2 =BaseLogisticRegressionRegL2(n_iters=5000, lr=0.001, batch_size=64, l=0.001)\n",
    "base_L1L2 = BaseLogisticRegressionRegCombined(n_iters=5000, lr=0.001, batch_size=64, l=0.00101, alpha=0.70)\n",
    "models_list = [\n",
    "    (base, \"base\",1),\n",
    "    (base_L1, \"base+L1\",1),\n",
    "    (base_L2, \"base+L2\",2),\n",
    "    (base_L1L2, \"base+L1L2\",2)\n",
    "]\n",
    "\n",
    "trained_models = []\n",
    "for model, name,num in models_list:\n",
    "    if num==1:\n",
    "        model.fit(X_train_proc1,y_train_proc1)\n",
    "    if num==2:\n",
    "        model.fit(X_train_proc2,y_train_proc2)\n",
    "    trained_models.append((model, name,num))\n",
    "\n",
    "predictions = {}\n",
    "errors = {}\n",
    "\n",
    "for model, name,num in trained_models:\n",
    "    if num==1:\n",
    "        y_pred = model.predict_class(X_test_proc1)\n",
    "        predictions[name] = y_pred\n",
    "        y_true = np.argmax(y_test_proc1, axis=1) if y_test_proc1.ndim > 1 else y_test_proc1\n",
    "        errors[name] = (y_pred.flatten() != y_true).astype(int)\n",
    "    if num==2:\n",
    "        y_pred = model.predict_class(X_test_proc2)\n",
    "        predictions[name] = y_pred\n",
    "        y_true = np.argmax(y_test_proc2, axis=1) if y_test_proc2.ndim > 1 else y_test_proc2\n",
    "        errors[name] = (y_pred.flatten() != y_true).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "errors_df = pd.DataFrame(errors)\n",
    "\n",
    "error_correlation = errors_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(error_correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Macierz korelacji błędów między modelami')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "e171bffcfe15954a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.9677\n",
      "Iteration 100, Loss: 0.5998\n",
      "Iteration 200, Loss: 0.5735\n",
      "Iteration 300, Loss: 0.5593\n",
      "Iteration 400, Loss: 0.5498\n",
      "Iteration 500, Loss: 0.5428\n",
      "Iteration 600, Loss: 0.5372\n",
      "Iteration 700, Loss: 0.5326\n",
      "Iteration 800, Loss: 0.5287\n",
      "Iteration 900, Loss: 0.5253\n",
      "Iteration 1000, Loss: 0.5223\n",
      "Iteration 1100, Loss: 0.5196\n",
      "Iteration 1200, Loss: 0.5172\n",
      "Iteration 1300, Loss: 0.5150\n",
      "Iteration 1400, Loss: 0.5130\n",
      "Iteration 1500, Loss: 0.5111\n",
      "Iteration 1600, Loss: 0.5094\n",
      "Iteration 1700, Loss: 0.5078\n",
      "Iteration 1800, Loss: 0.5063\n",
      "Iteration 1900, Loss: 0.5048\n",
      "Iteration 2000, Loss: 0.5035\n",
      "Iteration 2100, Loss: 0.5023\n",
      "Iteration 2200, Loss: 0.5011\n",
      "Iteration 2300, Loss: 0.5000\n",
      "Iteration 2400, Loss: 0.4989\n",
      "Iteration 2500, Loss: 0.4979\n",
      "Iteration 2600, Loss: 0.4969\n",
      "Iteration 2700, Loss: 0.4960\n",
      "Iteration 2800, Loss: 0.4951\n",
      "Iteration 2900, Loss: 0.4943\n",
      "Iteration 3000, Loss: 0.4935\n",
      "Iteration 3100, Loss: 0.4927\n",
      "Iteration 3200, Loss: 0.4920\n",
      "Iteration 3300, Loss: 0.4912\n",
      "Iteration 3400, Loss: 0.4905\n",
      "Iteration 3500, Loss: 0.4899\n",
      "Iteration 3600, Loss: 0.4892\n",
      "Iteration 3700, Loss: 0.4886\n",
      "Iteration 3800, Loss: 0.4880\n",
      "Iteration 3900, Loss: 0.4874\n",
      "Iteration 4000, Loss: 0.4869\n",
      "Iteration 4100, Loss: 0.4863\n",
      "Iteration 4200, Loss: 0.4858\n",
      "Iteration 4300, Loss: 0.4853\n",
      "Iteration 4400, Loss: 0.4847\n",
      "Iteration 4500, Loss: 0.4843\n",
      "Iteration 4600, Loss: 0.4838\n",
      "Iteration 4700, Loss: 0.4833\n",
      "Iteration 4800, Loss: 0.4829\n",
      "Iteration 4900, Loss: 0.4824\n",
      "Iteration 0, Loss: 0.9686\n",
      "Iteration 100, Loss: 0.6036\n",
      "Iteration 200, Loss: 0.5793\n",
      "Iteration 300, Loss: 0.5667\n",
      "Iteration 400, Loss: 0.5586\n",
      "Iteration 500, Loss: 0.5527\n",
      "Iteration 600, Loss: 0.5482\n",
      "Iteration 700, Loss: 0.5446\n",
      "Iteration 800, Loss: 0.5415\n",
      "Iteration 900, Loss: 0.5389\n",
      "Iteration 1000, Loss: 0.5366\n",
      "Iteration 1100, Loss: 0.5346\n",
      "Iteration 1200, Loss: 0.5328\n",
      "Iteration 1300, Loss: 0.5311\n",
      "Iteration 1400, Loss: 0.5297\n",
      "Iteration 1500, Loss: 0.5283\n",
      "Iteration 1600, Loss: 0.5271\n",
      "Iteration 1700, Loss: 0.5259\n",
      "Iteration 1800, Loss: 0.5249\n",
      "Iteration 1900, Loss: 0.5239\n",
      "Iteration 2000, Loss: 0.5230\n",
      "Iteration 2100, Loss: 0.5222\n",
      "Iteration 2200, Loss: 0.5214\n",
      "Iteration 2300, Loss: 0.5207\n",
      "Iteration 2400, Loss: 0.5200\n",
      "Iteration 2500, Loss: 0.5193\n",
      "Iteration 2600, Loss: 0.5187\n",
      "Iteration 2700, Loss: 0.5181\n",
      "Iteration 2800, Loss: 0.5175\n",
      "Iteration 2900, Loss: 0.5170\n",
      "Iteration 3000, Loss: 0.5165\n",
      "Iteration 3100, Loss: 0.5160\n",
      "Iteration 3200, Loss: 0.5156\n",
      "Iteration 3300, Loss: 0.5151\n",
      "Iteration 3400, Loss: 0.5147\n",
      "Iteration 3500, Loss: 0.5143\n",
      "Iteration 3600, Loss: 0.5139\n",
      "Iteration 3700, Loss: 0.5135\n",
      "Iteration 3800, Loss: 0.5131\n",
      "Iteration 3900, Loss: 0.5128\n",
      "Iteration 4000, Loss: 0.5124\n",
      "Iteration 4100, Loss: 0.5121\n",
      "Iteration 4200, Loss: 0.5118\n",
      "Iteration 4300, Loss: 0.5115\n",
      "Iteration 4400, Loss: 0.5112\n",
      "Iteration 4500, Loss: 0.5109\n",
      "Iteration 4600, Loss: 0.5106\n",
      "Iteration 4700, Loss: 0.5104\n",
      "Iteration 4800, Loss: 0.5101\n",
      "Iteration 4900, Loss: 0.5099\n",
      "Iteration 0, Loss: 1.0447\n",
      "Iteration 100, Loss: 0.6517\n",
      "Iteration 200, Loss: 0.6090\n",
      "Iteration 300, Loss: 0.5887\n",
      "Iteration 400, Loss: 0.5760\n",
      "Iteration 500, Loss: 0.5671\n",
      "Iteration 600, Loss: 0.5602\n",
      "Iteration 700, Loss: 0.5547\n",
      "Iteration 800, Loss: 0.5502\n",
      "Iteration 900, Loss: 0.5464\n",
      "Iteration 1000, Loss: 0.5432\n",
      "Iteration 1100, Loss: 0.5403\n",
      "Iteration 1200, Loss: 0.5378\n",
      "Iteration 1300, Loss: 0.5356\n",
      "Iteration 1400, Loss: 0.5336\n",
      "Iteration 1500, Loss: 0.5318\n",
      "Iteration 1600, Loss: 0.5302\n",
      "Iteration 1700, Loss: 0.5287\n",
      "Iteration 1800, Loss: 0.5274\n",
      "Iteration 1900, Loss: 0.5262\n",
      "Iteration 2000, Loss: 0.5250\n",
      "Iteration 2100, Loss: 0.5240\n",
      "Iteration 2200, Loss: 0.5230\n",
      "Iteration 2300, Loss: 0.5221\n",
      "Iteration 2400, Loss: 0.5212\n",
      "Iteration 2500, Loss: 0.5204\n",
      "Iteration 2600, Loss: 0.5197\n",
      "Iteration 2700, Loss: 0.5190\n",
      "Iteration 2800, Loss: 0.5183\n",
      "Iteration 2900, Loss: 0.5177\n",
      "Iteration 3000, Loss: 0.5171\n",
      "Iteration 3100, Loss: 0.5166\n",
      "Iteration 3200, Loss: 0.5160\n",
      "Iteration 3300, Loss: 0.5155\n",
      "Iteration 3400, Loss: 0.5151\n",
      "Iteration 3500, Loss: 0.5146\n",
      "Iteration 3600, Loss: 0.5142\n",
      "Iteration 3700, Loss: 0.5138\n",
      "Iteration 3800, Loss: 0.5134\n",
      "Iteration 3900, Loss: 0.5130\n",
      "Iteration 4000, Loss: 0.5126\n",
      "Iteration 4100, Loss: 0.5123\n",
      "Iteration 4200, Loss: 0.5120\n",
      "Iteration 4300, Loss: 0.5116\n",
      "Iteration 4400, Loss: 0.5113\n",
      "Iteration 4500, Loss: 0.5111\n",
      "Iteration 4600, Loss: 0.5108\n",
      "Iteration 4700, Loss: 0.5105\n",
      "Iteration 4800, Loss: 0.5102\n",
      "Iteration 4900, Loss: 0.5100\n",
      "Iteration 0, Loss: 1.0453\n",
      "Iteration 100, Loss: 0.6535\n",
      "Iteration 200, Loss: 0.6118\n",
      "Iteration 300, Loss: 0.5921\n",
      "Iteration 400, Loss: 0.5800\n",
      "Iteration 500, Loss: 0.5715\n",
      "Iteration 600, Loss: 0.5651\n",
      "Iteration 700, Loss: 0.5600\n",
      "Iteration 800, Loss: 0.5558\n",
      "Iteration 900, Loss: 0.5523\n",
      "Iteration 1000, Loss: 0.5493\n",
      "Iteration 1100, Loss: 0.5467\n",
      "Iteration 1200, Loss: 0.5444\n",
      "Iteration 1300, Loss: 0.5424\n",
      "Iteration 1400, Loss: 0.5406\n",
      "Iteration 1500, Loss: 0.5390\n",
      "Iteration 1600, Loss: 0.5375\n",
      "Iteration 1700, Loss: 0.5362\n",
      "Iteration 1800, Loss: 0.5350\n",
      "Iteration 1900, Loss: 0.5339\n",
      "Iteration 2000, Loss: 0.5329\n",
      "Iteration 2100, Loss: 0.5319\n",
      "Iteration 2200, Loss: 0.5310\n",
      "Iteration 2300, Loss: 0.5302\n",
      "Iteration 2400, Loss: 0.5295\n",
      "Iteration 2500, Loss: 0.5288\n",
      "Iteration 2600, Loss: 0.5281\n",
      "Iteration 2700, Loss: 0.5275\n",
      "Iteration 2800, Loss: 0.5269\n",
      "Iteration 2900, Loss: 0.5264\n",
      "Iteration 3000, Loss: 0.5259\n",
      "Iteration 3100, Loss: 0.5254\n",
      "Iteration 3200, Loss: 0.5249\n",
      "Iteration 3300, Loss: 0.5245\n",
      "Iteration 3400, Loss: 0.5241\n",
      "Iteration 3500, Loss: 0.5237\n",
      "Iteration 3600, Loss: 0.5233\n",
      "Iteration 3700, Loss: 0.5229\n",
      "Iteration 3800, Loss: 0.5226\n",
      "Iteration 3900, Loss: 0.5223\n",
      "Iteration 4000, Loss: 0.5219\n",
      "Iteration 4100, Loss: 0.5216\n",
      "Iteration 4200, Loss: 0.5214\n",
      "Iteration 4300, Loss: 0.5211\n",
      "Iteration 4400, Loss: 0.5208\n",
      "Iteration 4500, Loss: 0.5206\n",
      "Iteration 4600, Loss: 0.5203\n",
      "Iteration 4700, Loss: 0.5201\n",
      "Iteration 4800, Loss: 0.5199\n",
      "Iteration 4900, Loss: 0.5197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 313
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see on this matrix that base model and base_L1L2 model has the lowest correlation score so they are better in different records (but still this is high correlation score)\n",
    "\n",
    "Ensemble methods:\n",
    "- HardVotingClassifier\n",
    "Here we have n classifiers and every of that returning their prediction for every record, HardVotingClassifier choose the most frequent label  and this is final prediction\n",
    "\n",
    "- StackingClassifier"
   ],
   "id": "ff9a19174ea18922"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T08:30:56.936284Z",
     "start_time": "2025-05-23T08:30:56.921042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = [\n",
    "    BaseLogisticRegression(n_iters=7000, lr=0.001, batch_size=64),\n",
    "    BaseLogisticRegressionRegCombined(n_iters=4000, lr=0.001, batch_size=128, l=0.0010, alpha=0.50)\n",
    "    ]\n",
    "def HardVotingClassifier(models,X_train,y_train,X_test,y_test):\n",
    "    models[0].fit(X_train,y_train)\n",
    "    models[1].fit(X_train,y_train)\n",
    "    y_pred_1 = models[0].predict_class(X_test)\n",
    "    y_pred_2 = models[1].predict_class(X_test)\n",
    "    pred = []\n",
    "    for row1,row2 in zip(y_pred_1,y_pred_2):\n",
    "        d ={0:0,1:0,2:0}\n",
    "        for elem in row1:\n",
    "            d[elem]+=1\n",
    "        for elem in row2:\n",
    "            d[elem]+=1\n",
    "        pred.append(max(d, key=d.get))\n",
    "\n",
    "    return pred"
   ],
   "id": "85af3fc03103620c",
   "outputs": [],
   "execution_count": 280
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T08:34:09.596782Z",
     "start_time": "2025-05-23T08:30:57.233044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred = HardVotingClassifier(models,X_train_proc1,y_train_proc1,X_test_proc1,y_test_proc1)\n",
    "p = np.array(pred)\n",
    "def score(predicted, y):\n",
    "    y_true = np.argmax(y, axis=1)\n",
    "    correct = (predicted.flatten() == y_true)\n",
    "    accuracy = np.mean(correct)\n",
    "    return accuracy\n",
    "score(p,y_test_proc1)"
   ],
   "id": "308b9159b7bc0e07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.9673\n",
      "Iteration 100, Loss: 0.5999\n",
      "Iteration 200, Loss: 0.5736\n",
      "Iteration 300, Loss: 0.5594\n",
      "Iteration 400, Loss: 0.5499\n",
      "Iteration 500, Loss: 0.5428\n",
      "Iteration 600, Loss: 0.5372\n",
      "Iteration 700, Loss: 0.5326\n",
      "Iteration 800, Loss: 0.5287\n",
      "Iteration 900, Loss: 0.5253\n",
      "Iteration 1000, Loss: 0.5223\n",
      "Iteration 1100, Loss: 0.5196\n",
      "Iteration 1200, Loss: 0.5172\n",
      "Iteration 1300, Loss: 0.5150\n",
      "Iteration 1400, Loss: 0.5130\n",
      "Iteration 1500, Loss: 0.5111\n",
      "Iteration 1600, Loss: 0.5094\n",
      "Iteration 1700, Loss: 0.5078\n",
      "Iteration 1800, Loss: 0.5063\n",
      "Iteration 1900, Loss: 0.5049\n",
      "Iteration 2000, Loss: 0.5035\n",
      "Iteration 2100, Loss: 0.5023\n",
      "Iteration 2200, Loss: 0.5011\n",
      "Iteration 2300, Loss: 0.5000\n",
      "Iteration 2400, Loss: 0.4989\n",
      "Iteration 2500, Loss: 0.4979\n",
      "Iteration 2600, Loss: 0.4969\n",
      "Iteration 2700, Loss: 0.4960\n",
      "Iteration 2800, Loss: 0.4951\n",
      "Iteration 2900, Loss: 0.4943\n",
      "Iteration 3000, Loss: 0.4935\n",
      "Iteration 3100, Loss: 0.4927\n",
      "Iteration 3200, Loss: 0.4920\n",
      "Iteration 3300, Loss: 0.4912\n",
      "Iteration 3400, Loss: 0.4905\n",
      "Iteration 3500, Loss: 0.4899\n",
      "Iteration 3600, Loss: 0.4892\n",
      "Iteration 3700, Loss: 0.4886\n",
      "Iteration 3800, Loss: 0.4880\n",
      "Iteration 3900, Loss: 0.4874\n",
      "Iteration 4000, Loss: 0.4869\n",
      "Iteration 4100, Loss: 0.4863\n",
      "Iteration 4200, Loss: 0.4858\n",
      "Iteration 4300, Loss: 0.4853\n",
      "Iteration 4400, Loss: 0.4848\n",
      "Iteration 4500, Loss: 0.4843\n",
      "Iteration 4600, Loss: 0.4838\n",
      "Iteration 4700, Loss: 0.4833\n",
      "Iteration 4800, Loss: 0.4829\n",
      "Iteration 4900, Loss: 0.4824\n",
      "Iteration 5000, Loss: 0.4820\n",
      "Iteration 5100, Loss: 0.4816\n",
      "Iteration 5200, Loss: 0.4812\n",
      "Iteration 5300, Loss: 0.4808\n",
      "Iteration 5400, Loss: 0.4804\n",
      "Iteration 5500, Loss: 0.4800\n",
      "Iteration 5600, Loss: 0.4796\n",
      "Iteration 5700, Loss: 0.4793\n",
      "Iteration 5800, Loss: 0.4789\n",
      "Iteration 5900, Loss: 0.4785\n",
      "Iteration 6000, Loss: 0.4782\n",
      "Iteration 6100, Loss: 0.4779\n",
      "Iteration 6200, Loss: 0.4775\n",
      "Iteration 6300, Loss: 0.4772\n",
      "Iteration 6400, Loss: 0.4769\n",
      "Iteration 6500, Loss: 0.4766\n",
      "Iteration 6600, Loss: 0.4763\n",
      "Iteration 6700, Loss: 0.4760\n",
      "Iteration 6800, Loss: 0.4757\n",
      "Iteration 6900, Loss: 0.4754\n",
      "Iteration 0, Loss: 1.0203\n",
      "Iteration 100, Loss: 0.6296\n",
      "Iteration 200, Loss: 0.6013\n",
      "Iteration 300, Loss: 0.5862\n",
      "Iteration 400, Loss: 0.5760\n",
      "Iteration 500, Loss: 0.5686\n",
      "Iteration 600, Loss: 0.5628\n",
      "Iteration 700, Loss: 0.5580\n",
      "Iteration 800, Loss: 0.5541\n",
      "Iteration 900, Loss: 0.5507\n",
      "Iteration 1000, Loss: 0.5478\n",
      "Iteration 1100, Loss: 0.5452\n",
      "Iteration 1200, Loss: 0.5429\n",
      "Iteration 1300, Loss: 0.5408\n",
      "Iteration 1400, Loss: 0.5388\n",
      "Iteration 1500, Loss: 0.5371\n",
      "Iteration 1600, Loss: 0.5355\n",
      "Iteration 1700, Loss: 0.5340\n",
      "Iteration 1800, Loss: 0.5326\n",
      "Iteration 1900, Loss: 0.5312\n",
      "Iteration 2000, Loss: 0.5300\n",
      "Iteration 2100, Loss: 0.5288\n",
      "Iteration 2200, Loss: 0.5277\n",
      "Iteration 2300, Loss: 0.5267\n",
      "Iteration 2400, Loss: 0.5257\n",
      "Iteration 2500, Loss: 0.5248\n",
      "Iteration 2600, Loss: 0.5239\n",
      "Iteration 2700, Loss: 0.5231\n",
      "Iteration 2800, Loss: 0.5223\n",
      "Iteration 2900, Loss: 0.5215\n",
      "Iteration 3000, Loss: 0.5208\n",
      "Iteration 3100, Loss: 0.5200\n",
      "Iteration 3200, Loss: 0.5194\n",
      "Iteration 3300, Loss: 0.5187\n",
      "Iteration 3400, Loss: 0.5181\n",
      "Iteration 3500, Loss: 0.5175\n",
      "Iteration 3600, Loss: 0.5169\n",
      "Iteration 3700, Loss: 0.5164\n",
      "Iteration 3800, Loss: 0.5158\n",
      "Iteration 3900, Loss: 0.5153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.7688253012048193)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 281
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:35:58.844564Z",
     "start_time": "2025-05-23T09:30:43.643584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def make_cross_validation_predict_voting(models, preproc, X_set, y_set, k=3):\n",
    "    scores = []\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    y_encoded = LabelEncoder().fit_transform(y_set)\n",
    "\n",
    "    for train_index, test_index in skf.split(X_set, y_encoded):\n",
    "        X_train_raw = X_set.iloc[train_index]\n",
    "        X_test_raw = X_set.iloc[test_index]\n",
    "\n",
    "        y_train_raw = y_set.iloc[train_index]\n",
    "        y_test_raw = y_set.iloc[test_index]\n",
    "\n",
    "        # Preprocessing\n",
    "        X_train = preproc.fit_transform(X_train_raw)\n",
    "        X_test = preproc.transform(X_test_raw)\n",
    "\n",
    "        # One-hot encode targets for training\n",
    "        y_train = pd.get_dummies(y_train_raw).astype(int).to_numpy()\n",
    "        y_test = pd.get_dummies(y_test_raw).astype(int).to_numpy()\n",
    "\n",
    "        # Fit both models\n",
    "        models[0].fit(X_train, y_train)\n",
    "        models[1].fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred_1 = models[0].predict_class(X_test)\n",
    "        y_pred_2 = models[1].predict_class(X_test)\n",
    "\n",
    "        # Hard voting\n",
    "        pred = []\n",
    "        for row1, row2 in zip(y_pred_1, y_pred_2):\n",
    "            d = {0: 0, 1: 0, 2: 0}\n",
    "            for elem in row1:\n",
    "                d[elem] += 1\n",
    "            for elem in row2:\n",
    "                d[elem] += 1\n",
    "            pred.append(max(d, key=d.get))\n",
    "\n",
    "        pred = np.array(pred)\n",
    "\n",
    "        # Use your custom scoring\n",
    "        fold_score = score(pred, y_test)\n",
    "        scores.append(fold_score)\n",
    "\n",
    "    return scores, np.mean(scores)\n",
    "\n",
    "def score(predicted, y):\n",
    "    y_true = np.argmax(y, axis=1)\n",
    "    correct = (predicted.flatten() == y_true)\n",
    "    return np.mean(correct)\n",
    "print(make_cross_validation_predict_voting(models, modified_features_preprocessor, X_train, y_train))"
   ],
   "id": "124bc7063feeccc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0024\n",
      "Iteration 100, Loss: 0.6156\n",
      "Iteration 200, Loss: 0.5868\n",
      "Iteration 300, Loss: 0.5709\n",
      "Iteration 400, Loss: 0.5599\n",
      "Iteration 500, Loss: 0.5516\n",
      "Iteration 600, Loss: 0.5451\n",
      "Iteration 700, Loss: 0.5396\n",
      "Iteration 800, Loss: 0.5349\n",
      "Iteration 900, Loss: 0.5309\n",
      "Iteration 1000, Loss: 0.5273\n",
      "Iteration 1100, Loss: 0.5241\n",
      "Iteration 1200, Loss: 0.5212\n",
      "Iteration 1300, Loss: 0.5185\n",
      "Iteration 1400, Loss: 0.5161\n",
      "Iteration 1500, Loss: 0.5138\n",
      "Iteration 1600, Loss: 0.5117\n",
      "Iteration 1700, Loss: 0.5098\n",
      "Iteration 1800, Loss: 0.5079\n",
      "Iteration 1900, Loss: 0.5062\n",
      "Iteration 2000, Loss: 0.5045\n",
      "Iteration 2100, Loss: 0.5029\n",
      "Iteration 2200, Loss: 0.5015\n",
      "Iteration 2300, Loss: 0.5000\n",
      "Iteration 2400, Loss: 0.4987\n",
      "Iteration 2500, Loss: 0.4974\n",
      "Iteration 2600, Loss: 0.4962\n",
      "Iteration 2700, Loss: 0.4950\n",
      "Iteration 2800, Loss: 0.4938\n",
      "Iteration 2900, Loss: 0.4927\n",
      "Iteration 3000, Loss: 0.4917\n",
      "Iteration 3100, Loss: 0.4907\n",
      "Iteration 3200, Loss: 0.4897\n",
      "Iteration 3300, Loss: 0.4887\n",
      "Iteration 3400, Loss: 0.4878\n",
      "Iteration 3500, Loss: 0.4869\n",
      "Iteration 3600, Loss: 0.4861\n",
      "Iteration 3700, Loss: 0.4852\n",
      "Iteration 3800, Loss: 0.4844\n",
      "Iteration 3900, Loss: 0.4836\n",
      "Iteration 4000, Loss: 0.4829\n",
      "Iteration 4100, Loss: 0.4821\n",
      "Iteration 4200, Loss: 0.4814\n",
      "Iteration 4300, Loss: 0.4807\n",
      "Iteration 4400, Loss: 0.4800\n",
      "Iteration 4500, Loss: 0.4794\n",
      "Iteration 4600, Loss: 0.4787\n",
      "Iteration 4700, Loss: 0.4781\n",
      "Iteration 4800, Loss: 0.4774\n",
      "Iteration 4900, Loss: 0.4768\n",
      "Iteration 5000, Loss: 0.4763\n",
      "Iteration 5100, Loss: 0.4757\n",
      "Iteration 5200, Loss: 0.4751\n",
      "Iteration 5300, Loss: 0.4746\n",
      "Iteration 5400, Loss: 0.4740\n",
      "Iteration 5500, Loss: 0.4735\n",
      "Iteration 5600, Loss: 0.4730\n",
      "Iteration 5700, Loss: 0.4725\n",
      "Iteration 5800, Loss: 0.4720\n",
      "Iteration 5900, Loss: 0.4715\n",
      "Iteration 6000, Loss: 0.4710\n",
      "Iteration 6100, Loss: 0.4706\n",
      "Iteration 6200, Loss: 0.4701\n",
      "Iteration 6300, Loss: 0.4697\n",
      "Iteration 6400, Loss: 0.4692\n",
      "Iteration 6500, Loss: 0.4688\n",
      "Iteration 6600, Loss: 0.4683\n",
      "Iteration 6700, Loss: 0.4679\n",
      "Iteration 6800, Loss: 0.4675\n",
      "Iteration 6900, Loss: 0.4671\n",
      "Iteration 0, Loss: 1.0437\n",
      "Iteration 100, Loss: 0.6486\n",
      "Iteration 200, Loss: 0.6163\n",
      "Iteration 300, Loss: 0.5997\n",
      "Iteration 400, Loss: 0.5884\n",
      "Iteration 500, Loss: 0.5800\n",
      "Iteration 600, Loss: 0.5733\n",
      "Iteration 700, Loss: 0.5678\n",
      "Iteration 800, Loss: 0.5632\n",
      "Iteration 900, Loss: 0.5591\n",
      "Iteration 1000, Loss: 0.5555\n",
      "Iteration 1100, Loss: 0.5524\n",
      "Iteration 1200, Loss: 0.5496\n",
      "Iteration 1300, Loss: 0.5471\n",
      "Iteration 1400, Loss: 0.5447\n",
      "Iteration 1500, Loss: 0.5425\n",
      "Iteration 1600, Loss: 0.5405\n",
      "Iteration 1700, Loss: 0.5387\n",
      "Iteration 1800, Loss: 0.5370\n",
      "Iteration 1900, Loss: 0.5354\n",
      "Iteration 2000, Loss: 0.5338\n",
      "Iteration 2100, Loss: 0.5324\n",
      "Iteration 2200, Loss: 0.5310\n",
      "Iteration 2300, Loss: 0.5297\n",
      "Iteration 2400, Loss: 0.5285\n",
      "Iteration 2500, Loss: 0.5273\n",
      "Iteration 2600, Loss: 0.5262\n",
      "Iteration 2700, Loss: 0.5251\n",
      "Iteration 2800, Loss: 0.5241\n",
      "Iteration 2900, Loss: 0.5231\n",
      "Iteration 3000, Loss: 0.5222\n",
      "Iteration 3100, Loss: 0.5213\n",
      "Iteration 3200, Loss: 0.5204\n",
      "Iteration 3300, Loss: 0.5196\n",
      "Iteration 3400, Loss: 0.5188\n",
      "Iteration 3500, Loss: 0.5180\n",
      "Iteration 3600, Loss: 0.5172\n",
      "Iteration 3700, Loss: 0.5165\n",
      "Iteration 3800, Loss: 0.5158\n",
      "Iteration 3900, Loss: 0.5151\n",
      "Iteration 0, Loss: 0.9998\n",
      "Iteration 100, Loss: 0.6159\n",
      "Iteration 200, Loss: 0.5884\n",
      "Iteration 300, Loss: 0.5733\n",
      "Iteration 400, Loss: 0.5631\n",
      "Iteration 500, Loss: 0.5555\n",
      "Iteration 600, Loss: 0.5495\n",
      "Iteration 700, Loss: 0.5444\n",
      "Iteration 800, Loss: 0.5402\n",
      "Iteration 900, Loss: 0.5365\n",
      "Iteration 1000, Loss: 0.5332\n",
      "Iteration 1100, Loss: 0.5302\n",
      "Iteration 1200, Loss: 0.5275\n",
      "Iteration 1300, Loss: 0.5251\n",
      "Iteration 1400, Loss: 0.5229\n",
      "Iteration 1500, Loss: 0.5208\n",
      "Iteration 1600, Loss: 0.5188\n",
      "Iteration 1700, Loss: 0.5170\n",
      "Iteration 1800, Loss: 0.5153\n",
      "Iteration 1900, Loss: 0.5137\n",
      "Iteration 2000, Loss: 0.5122\n",
      "Iteration 2100, Loss: 0.5107\n",
      "Iteration 2200, Loss: 0.5093\n",
      "Iteration 2300, Loss: 0.5081\n",
      "Iteration 2400, Loss: 0.5068\n",
      "Iteration 2500, Loss: 0.5056\n",
      "Iteration 2600, Loss: 0.5045\n",
      "Iteration 2700, Loss: 0.5034\n",
      "Iteration 2800, Loss: 0.5023\n",
      "Iteration 2900, Loss: 0.5013\n",
      "Iteration 3000, Loss: 0.5004\n",
      "Iteration 3100, Loss: 0.4994\n",
      "Iteration 3200, Loss: 0.4985\n",
      "Iteration 3300, Loss: 0.4977\n",
      "Iteration 3400, Loss: 0.4968\n",
      "Iteration 3500, Loss: 0.4960\n",
      "Iteration 3600, Loss: 0.4952\n",
      "Iteration 3700, Loss: 0.4944\n",
      "Iteration 3800, Loss: 0.4937\n",
      "Iteration 3900, Loss: 0.4930\n",
      "Iteration 4000, Loss: 0.4923\n",
      "Iteration 4100, Loss: 0.4916\n",
      "Iteration 4200, Loss: 0.4909\n",
      "Iteration 4300, Loss: 0.4903\n",
      "Iteration 4400, Loss: 0.4896\n",
      "Iteration 4500, Loss: 0.4890\n",
      "Iteration 4600, Loss: 0.4884\n",
      "Iteration 4700, Loss: 0.4878\n",
      "Iteration 4800, Loss: 0.4873\n",
      "Iteration 4900, Loss: 0.4867\n",
      "Iteration 5000, Loss: 0.4861\n",
      "Iteration 5100, Loss: 0.4856\n",
      "Iteration 5200, Loss: 0.4851\n",
      "Iteration 5300, Loss: 0.4846\n",
      "Iteration 5400, Loss: 0.4841\n",
      "Iteration 5500, Loss: 0.4836\n",
      "Iteration 5600, Loss: 0.4831\n",
      "Iteration 5700, Loss: 0.4826\n",
      "Iteration 5800, Loss: 0.4822\n",
      "Iteration 5900, Loss: 0.4817\n",
      "Iteration 6000, Loss: 0.4813\n",
      "Iteration 6100, Loss: 0.4808\n",
      "Iteration 6200, Loss: 0.4804\n",
      "Iteration 6300, Loss: 0.4799\n",
      "Iteration 6400, Loss: 0.4795\n",
      "Iteration 6500, Loss: 0.4791\n",
      "Iteration 6600, Loss: 0.4787\n",
      "Iteration 6700, Loss: 0.4783\n",
      "Iteration 6800, Loss: 0.4779\n",
      "Iteration 6900, Loss: 0.4775\n",
      "Iteration 0, Loss: 1.0410\n",
      "Iteration 100, Loss: 0.6468\n",
      "Iteration 200, Loss: 0.6166\n",
      "Iteration 300, Loss: 0.6007\n",
      "Iteration 400, Loss: 0.5899\n",
      "Iteration 500, Loss: 0.5819\n",
      "Iteration 600, Loss: 0.5756\n",
      "Iteration 700, Loss: 0.5703\n",
      "Iteration 800, Loss: 0.5660\n",
      "Iteration 900, Loss: 0.5623\n",
      "Iteration 1000, Loss: 0.5590\n",
      "Iteration 1100, Loss: 0.5562\n",
      "Iteration 1200, Loss: 0.5536\n",
      "Iteration 1300, Loss: 0.5513\n",
      "Iteration 1400, Loss: 0.5491\n",
      "Iteration 1500, Loss: 0.5472\n",
      "Iteration 1600, Loss: 0.5454\n",
      "Iteration 1700, Loss: 0.5437\n",
      "Iteration 1800, Loss: 0.5421\n",
      "Iteration 1900, Loss: 0.5406\n",
      "Iteration 2000, Loss: 0.5393\n",
      "Iteration 2100, Loss: 0.5380\n",
      "Iteration 2200, Loss: 0.5368\n",
      "Iteration 2300, Loss: 0.5356\n",
      "Iteration 2400, Loss: 0.5345\n",
      "Iteration 2500, Loss: 0.5334\n",
      "Iteration 2600, Loss: 0.5324\n",
      "Iteration 2700, Loss: 0.5315\n",
      "Iteration 2800, Loss: 0.5305\n",
      "Iteration 2900, Loss: 0.5296\n",
      "Iteration 3000, Loss: 0.5288\n",
      "Iteration 3100, Loss: 0.5279\n",
      "Iteration 3200, Loss: 0.5272\n",
      "Iteration 3300, Loss: 0.5264\n",
      "Iteration 3400, Loss: 0.5257\n",
      "Iteration 3500, Loss: 0.5249\n",
      "Iteration 3600, Loss: 0.5243\n",
      "Iteration 3700, Loss: 0.5236\n",
      "Iteration 3800, Loss: 0.5230\n",
      "Iteration 3900, Loss: 0.5223\n",
      "Iteration 0, Loss: 0.9988\n",
      "Iteration 100, Loss: 0.6075\n",
      "Iteration 200, Loss: 0.5770\n",
      "Iteration 300, Loss: 0.5601\n",
      "Iteration 400, Loss: 0.5486\n",
      "Iteration 500, Loss: 0.5401\n",
      "Iteration 600, Loss: 0.5332\n",
      "Iteration 700, Loss: 0.5276\n",
      "Iteration 800, Loss: 0.5227\n",
      "Iteration 900, Loss: 0.5185\n",
      "Iteration 1000, Loss: 0.5148\n",
      "Iteration 1100, Loss: 0.5114\n",
      "Iteration 1200, Loss: 0.5084\n",
      "Iteration 1300, Loss: 0.5056\n",
      "Iteration 1400, Loss: 0.5031\n",
      "Iteration 1500, Loss: 0.5007\n",
      "Iteration 1600, Loss: 0.4985\n",
      "Iteration 1700, Loss: 0.4965\n",
      "Iteration 1800, Loss: 0.4945\n",
      "Iteration 1900, Loss: 0.4927\n",
      "Iteration 2000, Loss: 0.4910\n",
      "Iteration 2100, Loss: 0.4894\n",
      "Iteration 2200, Loss: 0.4879\n",
      "Iteration 2300, Loss: 0.4865\n",
      "Iteration 2400, Loss: 0.4851\n",
      "Iteration 2500, Loss: 0.4838\n",
      "Iteration 2600, Loss: 0.4825\n",
      "Iteration 2700, Loss: 0.4813\n",
      "Iteration 2800, Loss: 0.4802\n",
      "Iteration 2900, Loss: 0.4791\n",
      "Iteration 3000, Loss: 0.4780\n",
      "Iteration 3100, Loss: 0.4770\n",
      "Iteration 3200, Loss: 0.4760\n",
      "Iteration 3300, Loss: 0.4750\n",
      "Iteration 3400, Loss: 0.4741\n",
      "Iteration 3500, Loss: 0.4732\n",
      "Iteration 3600, Loss: 0.4724\n",
      "Iteration 3700, Loss: 0.4715\n",
      "Iteration 3800, Loss: 0.4707\n",
      "Iteration 3900, Loss: 0.4699\n",
      "Iteration 4000, Loss: 0.4692\n",
      "Iteration 4100, Loss: 0.4685\n",
      "Iteration 4200, Loss: 0.4677\n",
      "Iteration 4300, Loss: 0.4670\n",
      "Iteration 4400, Loss: 0.4664\n",
      "Iteration 4500, Loss: 0.4657\n",
      "Iteration 4600, Loss: 0.4651\n",
      "Iteration 4700, Loss: 0.4644\n",
      "Iteration 4800, Loss: 0.4638\n",
      "Iteration 4900, Loss: 0.4632\n",
      "Iteration 5000, Loss: 0.4626\n",
      "Iteration 5100, Loss: 0.4621\n",
      "Iteration 5200, Loss: 0.4615\n",
      "Iteration 5300, Loss: 0.4610\n",
      "Iteration 5400, Loss: 0.4604\n",
      "Iteration 5500, Loss: 0.4599\n",
      "Iteration 5600, Loss: 0.4594\n",
      "Iteration 5700, Loss: 0.4589\n",
      "Iteration 5800, Loss: 0.4584\n",
      "Iteration 5900, Loss: 0.4579\n",
      "Iteration 6000, Loss: 0.4575\n",
      "Iteration 6100, Loss: 0.4570\n",
      "Iteration 6200, Loss: 0.4566\n",
      "Iteration 6300, Loss: 0.4561\n",
      "Iteration 6400, Loss: 0.4557\n",
      "Iteration 6500, Loss: 0.4553\n",
      "Iteration 6600, Loss: 0.4548\n",
      "Iteration 6700, Loss: 0.4544\n",
      "Iteration 6800, Loss: 0.4540\n",
      "Iteration 6900, Loss: 0.4536\n",
      "Iteration 0, Loss: 1.0413\n",
      "Iteration 100, Loss: 0.6413\n",
      "Iteration 200, Loss: 0.6082\n",
      "Iteration 300, Loss: 0.5905\n",
      "Iteration 400, Loss: 0.5785\n",
      "Iteration 500, Loss: 0.5696\n",
      "Iteration 600, Loss: 0.5625\n",
      "Iteration 700, Loss: 0.5567\n",
      "Iteration 800, Loss: 0.5518\n",
      "Iteration 900, Loss: 0.5476\n",
      "Iteration 1000, Loss: 0.5439\n",
      "Iteration 1100, Loss: 0.5406\n",
      "Iteration 1200, Loss: 0.5377\n",
      "Iteration 1300, Loss: 0.5350\n",
      "Iteration 1400, Loss: 0.5326\n",
      "Iteration 1500, Loss: 0.5303\n",
      "Iteration 1600, Loss: 0.5283\n",
      "Iteration 1700, Loss: 0.5264\n",
      "Iteration 1800, Loss: 0.5245\n",
      "Iteration 1900, Loss: 0.5229\n",
      "Iteration 2000, Loss: 0.5213\n",
      "Iteration 2100, Loss: 0.5198\n",
      "Iteration 2200, Loss: 0.5184\n",
      "Iteration 2300, Loss: 0.5170\n",
      "Iteration 2400, Loss: 0.5158\n",
      "Iteration 2500, Loss: 0.5145\n",
      "Iteration 2600, Loss: 0.5134\n",
      "Iteration 2700, Loss: 0.5122\n",
      "Iteration 2800, Loss: 0.5112\n",
      "Iteration 2900, Loss: 0.5102\n",
      "Iteration 3000, Loss: 0.5092\n",
      "Iteration 3100, Loss: 0.5083\n",
      "Iteration 3200, Loss: 0.5074\n",
      "Iteration 3300, Loss: 0.5065\n",
      "Iteration 3400, Loss: 0.5057\n",
      "Iteration 3500, Loss: 0.5049\n",
      "Iteration 3600, Loss: 0.5041\n",
      "Iteration 3700, Loss: 0.5033\n",
      "Iteration 3800, Loss: 0.5026\n",
      "Iteration 3900, Loss: 0.5019\n",
      "([np.float64(0.7877906976744186), np.float64(0.7887596899224806), np.float64(0.752906976744186)], np.float64(0.776485788113695))\n"
     ]
    }
   ],
   "execution_count": 322
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T08:34:09.778169Z",
     "start_time": "2025-05-23T08:34:09.768646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = [\n",
    "    BaseLogisticRegression(n_iters=7000, lr=0.001, batch_size=64),\n",
    "    BaseLogisticRegressionRegCombined(n_iters=4000, lr=0.001, batch_size=128, l=0.0010, alpha=0.50)\n",
    "    ]\n",
    "def StackingClassifier(models,X_train,y_train,X_test,y_test):\n",
    "    models[0].fit(X_train,y_train)\n",
    "    models[1].fit(X_train,y_train)\n",
    "    models[0].predict_class(X_test)\n",
    "    models[1].predict_class(X_test)\n",
    "    prob1 = models[0].probabilities\n",
    "    prob2 = models[1].probabilities\n",
    "    results = []\n",
    "    for row1,row2 in zip(prob1,prob2):\n",
    "        c = np.hstack((row1,row2))\n",
    "        results.append(c)\n",
    "    return np.array(results)"
   ],
   "id": "82265da8ebf2ae08",
   "outputs": [],
   "execution_count": 282
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T08:38:10.202476Z",
     "start_time": "2025-05-23T08:34:09.857025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred = StackingClassifier(models,X_train_proc1,y_train_proc1,X_test_proc1,y_test_proc1)\n",
    "pred"
   ],
   "id": "c37a1d8373fe5367",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.9686\n",
      "Iteration 100, Loss: 0.5999\n",
      "Iteration 200, Loss: 0.5735\n",
      "Iteration 300, Loss: 0.5593\n",
      "Iteration 400, Loss: 0.5498\n",
      "Iteration 500, Loss: 0.5428\n",
      "Iteration 600, Loss: 0.5372\n",
      "Iteration 700, Loss: 0.5326\n",
      "Iteration 800, Loss: 0.5287\n",
      "Iteration 900, Loss: 0.5253\n",
      "Iteration 1000, Loss: 0.5223\n",
      "Iteration 1100, Loss: 0.5196\n",
      "Iteration 1200, Loss: 0.5172\n",
      "Iteration 1300, Loss: 0.5150\n",
      "Iteration 1400, Loss: 0.5130\n",
      "Iteration 1500, Loss: 0.5111\n",
      "Iteration 1600, Loss: 0.5094\n",
      "Iteration 1700, Loss: 0.5078\n",
      "Iteration 1800, Loss: 0.5063\n",
      "Iteration 1900, Loss: 0.5049\n",
      "Iteration 2000, Loss: 0.5035\n",
      "Iteration 2100, Loss: 0.5023\n",
      "Iteration 2200, Loss: 0.5011\n",
      "Iteration 2300, Loss: 0.5000\n",
      "Iteration 2400, Loss: 0.4989\n",
      "Iteration 2500, Loss: 0.4979\n",
      "Iteration 2600, Loss: 0.4969\n",
      "Iteration 2700, Loss: 0.4960\n",
      "Iteration 2800, Loss: 0.4951\n",
      "Iteration 2900, Loss: 0.4943\n",
      "Iteration 3000, Loss: 0.4935\n",
      "Iteration 3100, Loss: 0.4927\n",
      "Iteration 3200, Loss: 0.4920\n",
      "Iteration 3300, Loss: 0.4913\n",
      "Iteration 3400, Loss: 0.4906\n",
      "Iteration 3500, Loss: 0.4899\n",
      "Iteration 3600, Loss: 0.4892\n",
      "Iteration 3700, Loss: 0.4886\n",
      "Iteration 3800, Loss: 0.4880\n",
      "Iteration 3900, Loss: 0.4874\n",
      "Iteration 4000, Loss: 0.4869\n",
      "Iteration 4100, Loss: 0.4863\n",
      "Iteration 4200, Loss: 0.4858\n",
      "Iteration 4300, Loss: 0.4853\n",
      "Iteration 4400, Loss: 0.4848\n",
      "Iteration 4500, Loss: 0.4843\n",
      "Iteration 4600, Loss: 0.4838\n",
      "Iteration 4700, Loss: 0.4833\n",
      "Iteration 4800, Loss: 0.4829\n",
      "Iteration 4900, Loss: 0.4824\n",
      "Iteration 5000, Loss: 0.4820\n",
      "Iteration 5100, Loss: 0.4816\n",
      "Iteration 5200, Loss: 0.4812\n",
      "Iteration 5300, Loss: 0.4808\n",
      "Iteration 5400, Loss: 0.4804\n",
      "Iteration 5500, Loss: 0.4800\n",
      "Iteration 5600, Loss: 0.4796\n",
      "Iteration 5700, Loss: 0.4793\n",
      "Iteration 5800, Loss: 0.4789\n",
      "Iteration 5900, Loss: 0.4786\n",
      "Iteration 6000, Loss: 0.4782\n",
      "Iteration 6100, Loss: 0.4779\n",
      "Iteration 6200, Loss: 0.4775\n",
      "Iteration 6300, Loss: 0.4772\n",
      "Iteration 6400, Loss: 0.4769\n",
      "Iteration 6500, Loss: 0.4766\n",
      "Iteration 6600, Loss: 0.4763\n",
      "Iteration 6700, Loss: 0.4760\n",
      "Iteration 6800, Loss: 0.4757\n",
      "Iteration 6900, Loss: 0.4754\n",
      "Iteration 0, Loss: 1.0212\n",
      "Iteration 100, Loss: 0.6296\n",
      "Iteration 200, Loss: 0.6013\n",
      "Iteration 300, Loss: 0.5862\n",
      "Iteration 400, Loss: 0.5761\n",
      "Iteration 500, Loss: 0.5686\n",
      "Iteration 600, Loss: 0.5628\n",
      "Iteration 700, Loss: 0.5581\n",
      "Iteration 800, Loss: 0.5541\n",
      "Iteration 900, Loss: 0.5507\n",
      "Iteration 1000, Loss: 0.5478\n",
      "Iteration 1100, Loss: 0.5452\n",
      "Iteration 1200, Loss: 0.5429\n",
      "Iteration 1300, Loss: 0.5407\n",
      "Iteration 1400, Loss: 0.5388\n",
      "Iteration 1500, Loss: 0.5371\n",
      "Iteration 1600, Loss: 0.5355\n",
      "Iteration 1700, Loss: 0.5340\n",
      "Iteration 1800, Loss: 0.5326\n",
      "Iteration 1900, Loss: 0.5313\n",
      "Iteration 2000, Loss: 0.5301\n",
      "Iteration 2100, Loss: 0.5289\n",
      "Iteration 2200, Loss: 0.5278\n",
      "Iteration 2300, Loss: 0.5268\n",
      "Iteration 2400, Loss: 0.5258\n",
      "Iteration 2500, Loss: 0.5249\n",
      "Iteration 2600, Loss: 0.5240\n",
      "Iteration 2700, Loss: 0.5231\n",
      "Iteration 2800, Loss: 0.5223\n",
      "Iteration 2900, Loss: 0.5215\n",
      "Iteration 3000, Loss: 0.5208\n",
      "Iteration 3100, Loss: 0.5201\n",
      "Iteration 3200, Loss: 0.5194\n",
      "Iteration 3300, Loss: 0.5188\n",
      "Iteration 3400, Loss: 0.5181\n",
      "Iteration 3500, Loss: 0.5175\n",
      "Iteration 3600, Loss: 0.5169\n",
      "Iteration 3700, Loss: 0.5164\n",
      "Iteration 3800, Loss: 0.5159\n",
      "Iteration 3900, Loss: 0.5153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.91125599, 0.08585787, 0.00288615, 0.78213382, 0.21313988,\n",
       "        0.0047263 ],\n",
       "       [0.01309751, 0.05657464, 0.93032785, 0.0361788 , 0.11411735,\n",
       "        0.84970385],\n",
       "       [0.09053526, 0.08566188, 0.82380286, 0.04992944, 0.0796161 ,\n",
       "        0.87045446],\n",
       "       ...,\n",
       "       [0.1270732 , 0.25415816, 0.61876864, 0.10354732, 0.22965815,\n",
       "        0.66679452],\n",
       "       [0.0812477 , 0.38812411, 0.53062819, 0.12478515, 0.37268835,\n",
       "        0.50252649],\n",
       "       [0.3543586 , 0.14787654, 0.49776485, 0.3530222 , 0.17206944,\n",
       "        0.47490837]], shape=(1328, 6))"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 283
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T08:38:18.000508Z",
     "start_time": "2025-05-23T08:38:10.824330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y =y_test_proc1.copy()\n",
    "model = BaseLogisticRegressionRegCombined(n_iters=4000, lr=0.001,batch_size=128,l=0.001, alpha=0.5)\n",
    "model.fit(pred, y)\n",
    "predicted = model.predict_class(pred)\n",
    "model.score(predicted, y_test_proc1)\n"
   ],
   "id": "16fb34e4ace981b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0960\n",
      "Iteration 100, Loss: 0.9140\n",
      "Iteration 200, Loss: 0.8216\n",
      "Iteration 300, Loss: 0.7666\n",
      "Iteration 400, Loss: 0.7302\n",
      "Iteration 500, Loss: 0.7044\n",
      "Iteration 600, Loss: 0.6852\n",
      "Iteration 700, Loss: 0.6704\n",
      "Iteration 800, Loss: 0.6585\n",
      "Iteration 900, Loss: 0.6489\n",
      "Iteration 1000, Loss: 0.6409\n",
      "Iteration 1100, Loss: 0.6342\n",
      "Iteration 1200, Loss: 0.6285\n",
      "Iteration 1300, Loss: 0.6236\n",
      "Iteration 1400, Loss: 0.6194\n",
      "Iteration 1500, Loss: 0.6157\n",
      "Iteration 1600, Loss: 0.6125\n",
      "Iteration 1700, Loss: 0.6096\n",
      "Iteration 1800, Loss: 0.6071\n",
      "Iteration 1900, Loss: 0.6049\n",
      "Iteration 2000, Loss: 0.6029\n",
      "Iteration 2100, Loss: 0.6011\n",
      "Iteration 2200, Loss: 0.5995\n",
      "Iteration 2300, Loss: 0.5981\n",
      "Iteration 2400, Loss: 0.5968\n",
      "Iteration 2500, Loss: 0.5956\n",
      "Iteration 2600, Loss: 0.5945\n",
      "Iteration 2700, Loss: 0.5935\n",
      "Iteration 2800, Loss: 0.5926\n",
      "Iteration 2900, Loss: 0.5918\n",
      "Iteration 3000, Loss: 0.5911\n",
      "Iteration 3100, Loss: 0.5904\n",
      "Iteration 3200, Loss: 0.5897\n",
      "Iteration 3300, Loss: 0.5891\n",
      "Iteration 3400, Loss: 0.5886\n",
      "Iteration 3500, Loss: 0.5881\n",
      "Iteration 3600, Loss: 0.5876\n",
      "Iteration 3700, Loss: 0.5872\n",
      "Iteration 3800, Loss: 0.5868\n",
      "Iteration 3900, Loss: 0.5864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.7643072289156626)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 284
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:52:25.359311Z",
     "start_time": "2025-05-23T09:49:23.420667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "def score(predicted, y):\n",
    "    y_true = np.argmax(y, axis=1)\n",
    "    correct = (predicted.flatten() == y_true)\n",
    "    return np.mean(correct)\n",
    "\n",
    "def make_cross_validation_predict_stacking(model_factories, meta_model_factory, preproc, X_set, y_set, k=5):\n",
    "    scores = []\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    y_encoded = LabelEncoder().fit_transform(y_set)\n",
    "\n",
    "    for train_index, test_index in skf.split(X_set, y_encoded):\n",
    "        X_train_raw = X_set.iloc[train_index]\n",
    "        X_test_raw = X_set.iloc[test_index]\n",
    "        y_train_raw = y_set.iloc[train_index]\n",
    "        y_test_raw = y_set.iloc[test_index]\n",
    "\n",
    "        # preprocess\n",
    "        X_train = preproc.fit_transform(X_train_raw)\n",
    "        X_test = preproc.transform(X_test_raw)\n",
    "\n",
    "        y_train = pd.get_dummies(y_train_raw).astype(int).to_numpy()\n",
    "        y_test = pd.get_dummies(y_test_raw).astype(int).to_numpy()\n",
    "\n",
    "        base_preds_train = []\n",
    "        base_preds_test = []\n",
    "\n",
    "        for factory in model_factories:\n",
    "            model = factory()\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            pred_train = model.predict_class(X_train)\n",
    "            pred_test = model.predict_class(X_test)\n",
    "\n",
    "            # flatten one-hot or probability to class\n",
    "            pred_train = np.array([np.bincount(row).argmax() for row in pred_train])\n",
    "            pred_test = np.array([np.bincount(row).argmax() for row in pred_test])\n",
    "\n",
    "            base_preds_train.append(pred_train.reshape(-1, 1))\n",
    "            base_preds_test.append(pred_test.reshape(-1, 1))\n",
    "\n",
    "        stacked_train = np.hstack(base_preds_train)\n",
    "        stacked_test = np.hstack(base_preds_test)\n",
    "\n",
    "\n",
    "        meta_model = meta_model_factory()\n",
    "        meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "        final_pred = meta_model.predict_class(stacked_test)\n",
    "        final_pred = np.array([np.bincount(row).argmax() for row in final_pred])\n",
    "\n",
    "        fold_score = score(final_pred, y_test)\n",
    "        scores.append(fold_score)\n",
    "\n",
    "    return scores, np.mean(scores)\n",
    "model_factories = [\n",
    "    lambda: BaseLogisticRegression(n_iters=5000, lr=0.001),\n",
    "    lambda: BaseLogisticRegressionRegCombined(n_iters=4000, lr=0.001, batch_size=128, l=0.0010, alpha=0.50)\n",
    "]\n",
    "\n",
    "meta_model_factory = lambda: BaseLogisticRegression(n_iters=3000, lr=0.001)\n",
    "\n",
    "scores, mean_score = make_cross_validation_predict_stacking(\n",
    "    model_factories,\n",
    "    meta_model_factory,\n",
    "    modified_features_preprocessor,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(\"Fold scores:\", scores)\n",
    "print(\"Mean score:\", mean_score)\n"
   ],
   "id": "d8270fde664c4172",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0020\n",
      "Iteration 100, Loss: 0.6156\n",
      "Iteration 200, Loss: 0.5868\n",
      "Iteration 300, Loss: 0.5708\n",
      "Iteration 400, Loss: 0.5599\n",
      "Iteration 500, Loss: 0.5516\n",
      "Iteration 600, Loss: 0.5451\n",
      "Iteration 700, Loss: 0.5396\n",
      "Iteration 800, Loss: 0.5350\n",
      "Iteration 900, Loss: 0.5309\n",
      "Iteration 1000, Loss: 0.5273\n",
      "Iteration 1100, Loss: 0.5241\n",
      "Iteration 1200, Loss: 0.5212\n",
      "Iteration 1300, Loss: 0.5186\n",
      "Iteration 1400, Loss: 0.5161\n",
      "Iteration 1500, Loss: 0.5139\n",
      "Iteration 1600, Loss: 0.5117\n",
      "Iteration 1700, Loss: 0.5098\n",
      "Iteration 1800, Loss: 0.5079\n",
      "Iteration 1900, Loss: 0.5062\n",
      "Iteration 2000, Loss: 0.5045\n",
      "Iteration 2100, Loss: 0.5030\n",
      "Iteration 2200, Loss: 0.5015\n",
      "Iteration 2300, Loss: 0.5000\n",
      "Iteration 2400, Loss: 0.4987\n",
      "Iteration 2500, Loss: 0.4974\n",
      "Iteration 2600, Loss: 0.4962\n",
      "Iteration 2700, Loss: 0.4950\n",
      "Iteration 2800, Loss: 0.4938\n",
      "Iteration 2900, Loss: 0.4927\n",
      "Iteration 3000, Loss: 0.4917\n",
      "Iteration 3100, Loss: 0.4907\n",
      "Iteration 3200, Loss: 0.4897\n",
      "Iteration 3300, Loss: 0.4887\n",
      "Iteration 3400, Loss: 0.4878\n",
      "Iteration 3500, Loss: 0.4869\n",
      "Iteration 3600, Loss: 0.4861\n",
      "Iteration 3700, Loss: 0.4852\n",
      "Iteration 3800, Loss: 0.4844\n",
      "Iteration 3900, Loss: 0.4836\n",
      "Iteration 4000, Loss: 0.4829\n",
      "Iteration 4100, Loss: 0.4821\n",
      "Iteration 4200, Loss: 0.4814\n",
      "Iteration 4300, Loss: 0.4807\n",
      "Iteration 4400, Loss: 0.4800\n",
      "Iteration 4500, Loss: 0.4794\n",
      "Iteration 4600, Loss: 0.4787\n",
      "Iteration 4700, Loss: 0.4781\n",
      "Iteration 4800, Loss: 0.4775\n",
      "Iteration 4900, Loss: 0.4768\n",
      "Iteration 0, Loss: 1.0419\n",
      "Iteration 100, Loss: 0.6488\n",
      "Iteration 200, Loss: 0.6162\n",
      "Iteration 300, Loss: 0.5996\n",
      "Iteration 400, Loss: 0.5884\n",
      "Iteration 500, Loss: 0.5799\n",
      "Iteration 600, Loss: 0.5733\n",
      "Iteration 700, Loss: 0.5677\n",
      "Iteration 800, Loss: 0.5631\n",
      "Iteration 900, Loss: 0.5590\n",
      "Iteration 1000, Loss: 0.5555\n",
      "Iteration 1100, Loss: 0.5523\n",
      "Iteration 1200, Loss: 0.5495\n",
      "Iteration 1300, Loss: 0.5469\n",
      "Iteration 1400, Loss: 0.5446\n",
      "Iteration 1500, Loss: 0.5424\n",
      "Iteration 1600, Loss: 0.5404\n",
      "Iteration 1700, Loss: 0.5386\n",
      "Iteration 1800, Loss: 0.5369\n",
      "Iteration 1900, Loss: 0.5353\n",
      "Iteration 2000, Loss: 0.5338\n",
      "Iteration 2100, Loss: 0.5323\n",
      "Iteration 2200, Loss: 0.5310\n",
      "Iteration 2300, Loss: 0.5297\n",
      "Iteration 2400, Loss: 0.5285\n",
      "Iteration 2500, Loss: 0.5273\n",
      "Iteration 2600, Loss: 0.5262\n",
      "Iteration 2700, Loss: 0.5251\n",
      "Iteration 2800, Loss: 0.5241\n",
      "Iteration 2900, Loss: 0.5231\n",
      "Iteration 3000, Loss: 0.5222\n",
      "Iteration 3100, Loss: 0.5213\n",
      "Iteration 3200, Loss: 0.5205\n",
      "Iteration 3300, Loss: 0.5196\n",
      "Iteration 3400, Loss: 0.5188\n",
      "Iteration 3500, Loss: 0.5180\n",
      "Iteration 3600, Loss: 0.5172\n",
      "Iteration 3700, Loss: 0.5165\n",
      "Iteration 3800, Loss: 0.5158\n",
      "Iteration 3900, Loss: 0.5151\n",
      "Iteration 0, Loss: 1.0693\n",
      "Iteration 100, Loss: 0.7753\n",
      "Iteration 200, Loss: 0.7244\n",
      "Iteration 300, Loss: 0.6965\n",
      "Iteration 400, Loss: 0.6801\n",
      "Iteration 500, Loss: 0.6699\n",
      "Iteration 600, Loss: 0.6632\n",
      "Iteration 700, Loss: 0.6584\n",
      "Iteration 800, Loss: 0.6549\n",
      "Iteration 900, Loss: 0.6522\n",
      "Iteration 1000, Loss: 0.6501\n",
      "Iteration 1100, Loss: 0.6484\n",
      "Iteration 1200, Loss: 0.6470\n",
      "Iteration 1300, Loss: 0.6459\n",
      "Iteration 1400, Loss: 0.6449\n",
      "Iteration 1500, Loss: 0.6440\n",
      "Iteration 1600, Loss: 0.6433\n",
      "Iteration 1700, Loss: 0.6427\n",
      "Iteration 1800, Loss: 0.6422\n",
      "Iteration 1900, Loss: 0.6417\n",
      "Iteration 2000, Loss: 0.6413\n",
      "Iteration 2100, Loss: 0.6409\n",
      "Iteration 2200, Loss: 0.6406\n",
      "Iteration 2300, Loss: 0.6404\n",
      "Iteration 2400, Loss: 0.6401\n",
      "Iteration 2500, Loss: 0.6399\n",
      "Iteration 2600, Loss: 0.6397\n",
      "Iteration 2700, Loss: 0.6395\n",
      "Iteration 2800, Loss: 0.6394\n",
      "Iteration 2900, Loss: 0.6392\n",
      "Iteration 0, Loss: 1.0012\n",
      "Iteration 100, Loss: 0.6158\n",
      "Iteration 200, Loss: 0.5884\n",
      "Iteration 300, Loss: 0.5733\n",
      "Iteration 400, Loss: 0.5631\n",
      "Iteration 500, Loss: 0.5555\n",
      "Iteration 600, Loss: 0.5495\n",
      "Iteration 700, Loss: 0.5445\n",
      "Iteration 800, Loss: 0.5402\n",
      "Iteration 900, Loss: 0.5365\n",
      "Iteration 1000, Loss: 0.5332\n",
      "Iteration 1100, Loss: 0.5302\n",
      "Iteration 1200, Loss: 0.5276\n",
      "Iteration 1300, Loss: 0.5251\n",
      "Iteration 1400, Loss: 0.5229\n",
      "Iteration 1500, Loss: 0.5208\n",
      "Iteration 1600, Loss: 0.5188\n",
      "Iteration 1700, Loss: 0.5170\n",
      "Iteration 1800, Loss: 0.5153\n",
      "Iteration 1900, Loss: 0.5137\n",
      "Iteration 2000, Loss: 0.5122\n",
      "Iteration 2100, Loss: 0.5107\n",
      "Iteration 2200, Loss: 0.5094\n",
      "Iteration 2300, Loss: 0.5081\n",
      "Iteration 2400, Loss: 0.5068\n",
      "Iteration 2500, Loss: 0.5056\n",
      "Iteration 2600, Loss: 0.5045\n",
      "Iteration 2700, Loss: 0.5034\n",
      "Iteration 2800, Loss: 0.5023\n",
      "Iteration 2900, Loss: 0.5013\n",
      "Iteration 3000, Loss: 0.5004\n",
      "Iteration 3100, Loss: 0.4994\n",
      "Iteration 3200, Loss: 0.4985\n",
      "Iteration 3300, Loss: 0.4976\n",
      "Iteration 3400, Loss: 0.4968\n",
      "Iteration 3500, Loss: 0.4960\n",
      "Iteration 3600, Loss: 0.4952\n",
      "Iteration 3700, Loss: 0.4944\n",
      "Iteration 3800, Loss: 0.4937\n",
      "Iteration 3900, Loss: 0.4929\n",
      "Iteration 4000, Loss: 0.4922\n",
      "Iteration 4100, Loss: 0.4916\n",
      "Iteration 4200, Loss: 0.4909\n",
      "Iteration 4300, Loss: 0.4902\n",
      "Iteration 4400, Loss: 0.4896\n",
      "Iteration 4500, Loss: 0.4890\n",
      "Iteration 4600, Loss: 0.4884\n",
      "Iteration 4700, Loss: 0.4878\n",
      "Iteration 4800, Loss: 0.4872\n",
      "Iteration 4900, Loss: 0.4867\n",
      "Iteration 0, Loss: 1.0447\n",
      "Iteration 100, Loss: 0.6468\n",
      "Iteration 200, Loss: 0.6165\n",
      "Iteration 300, Loss: 0.6007\n",
      "Iteration 400, Loss: 0.5899\n",
      "Iteration 500, Loss: 0.5819\n",
      "Iteration 600, Loss: 0.5756\n",
      "Iteration 700, Loss: 0.5704\n",
      "Iteration 800, Loss: 0.5660\n",
      "Iteration 900, Loss: 0.5623\n",
      "Iteration 1000, Loss: 0.5591\n",
      "Iteration 1100, Loss: 0.5562\n",
      "Iteration 1200, Loss: 0.5536\n",
      "Iteration 1300, Loss: 0.5512\n",
      "Iteration 1400, Loss: 0.5491\n",
      "Iteration 1500, Loss: 0.5472\n",
      "Iteration 1600, Loss: 0.5453\n",
      "Iteration 1700, Loss: 0.5436\n",
      "Iteration 1800, Loss: 0.5421\n",
      "Iteration 1900, Loss: 0.5406\n",
      "Iteration 2000, Loss: 0.5393\n",
      "Iteration 2100, Loss: 0.5380\n",
      "Iteration 2200, Loss: 0.5367\n",
      "Iteration 2300, Loss: 0.5356\n",
      "Iteration 2400, Loss: 0.5345\n",
      "Iteration 2500, Loss: 0.5334\n",
      "Iteration 2600, Loss: 0.5324\n",
      "Iteration 2700, Loss: 0.5314\n",
      "Iteration 2800, Loss: 0.5305\n",
      "Iteration 2900, Loss: 0.5296\n",
      "Iteration 3000, Loss: 0.5288\n",
      "Iteration 3100, Loss: 0.5280\n",
      "Iteration 3200, Loss: 0.5272\n",
      "Iteration 3300, Loss: 0.5264\n",
      "Iteration 3400, Loss: 0.5257\n",
      "Iteration 3500, Loss: 0.5250\n",
      "Iteration 3600, Loss: 0.5243\n",
      "Iteration 3700, Loss: 0.5236\n",
      "Iteration 3800, Loss: 0.5230\n",
      "Iteration 3900, Loss: 0.5224\n",
      "Iteration 0, Loss: 1.0690\n",
      "Iteration 100, Loss: 0.7705\n",
      "Iteration 200, Loss: 0.7183\n",
      "Iteration 300, Loss: 0.6894\n",
      "Iteration 400, Loss: 0.6724\n",
      "Iteration 500, Loss: 0.6617\n",
      "Iteration 600, Loss: 0.6545\n",
      "Iteration 700, Loss: 0.6495\n",
      "Iteration 800, Loss: 0.6457\n",
      "Iteration 900, Loss: 0.6428\n",
      "Iteration 1000, Loss: 0.6404\n",
      "Iteration 1100, Loss: 0.6386\n",
      "Iteration 1200, Loss: 0.6370\n",
      "Iteration 1300, Loss: 0.6357\n",
      "Iteration 1400, Loss: 0.6346\n",
      "Iteration 1500, Loss: 0.6336\n",
      "Iteration 1600, Loss: 0.6328\n",
      "Iteration 1700, Loss: 0.6320\n",
      "Iteration 1800, Loss: 0.6314\n",
      "Iteration 1900, Loss: 0.6309\n",
      "Iteration 2000, Loss: 0.6304\n",
      "Iteration 2100, Loss: 0.6299\n",
      "Iteration 2200, Loss: 0.6296\n",
      "Iteration 2300, Loss: 0.6292\n",
      "Iteration 2400, Loss: 0.6289\n",
      "Iteration 2500, Loss: 0.6286\n",
      "Iteration 2600, Loss: 0.6284\n",
      "Iteration 2700, Loss: 0.6282\n",
      "Iteration 2800, Loss: 0.6280\n",
      "Iteration 2900, Loss: 0.6278\n",
      "Iteration 0, Loss: 0.9996\n",
      "Iteration 100, Loss: 0.6075\n",
      "Iteration 200, Loss: 0.5770\n",
      "Iteration 300, Loss: 0.5601\n",
      "Iteration 400, Loss: 0.5486\n",
      "Iteration 500, Loss: 0.5400\n",
      "Iteration 600, Loss: 0.5332\n",
      "Iteration 700, Loss: 0.5275\n",
      "Iteration 800, Loss: 0.5227\n",
      "Iteration 900, Loss: 0.5185\n",
      "Iteration 1000, Loss: 0.5147\n",
      "Iteration 1100, Loss: 0.5114\n",
      "Iteration 1200, Loss: 0.5084\n",
      "Iteration 1300, Loss: 0.5056\n",
      "Iteration 1400, Loss: 0.5031\n",
      "Iteration 1500, Loss: 0.5007\n",
      "Iteration 1600, Loss: 0.4985\n",
      "Iteration 1700, Loss: 0.4965\n",
      "Iteration 1800, Loss: 0.4945\n",
      "Iteration 1900, Loss: 0.4927\n",
      "Iteration 2000, Loss: 0.4910\n",
      "Iteration 2100, Loss: 0.4894\n",
      "Iteration 2200, Loss: 0.4879\n",
      "Iteration 2300, Loss: 0.4865\n",
      "Iteration 2400, Loss: 0.4851\n",
      "Iteration 2500, Loss: 0.4838\n",
      "Iteration 2600, Loss: 0.4825\n",
      "Iteration 2700, Loss: 0.4813\n",
      "Iteration 2800, Loss: 0.4802\n",
      "Iteration 2900, Loss: 0.4790\n",
      "Iteration 3000, Loss: 0.4780\n",
      "Iteration 3100, Loss: 0.4770\n",
      "Iteration 3200, Loss: 0.4760\n",
      "Iteration 3300, Loss: 0.4750\n",
      "Iteration 3400, Loss: 0.4741\n",
      "Iteration 3500, Loss: 0.4732\n",
      "Iteration 3600, Loss: 0.4724\n",
      "Iteration 3700, Loss: 0.4715\n",
      "Iteration 3800, Loss: 0.4707\n",
      "Iteration 3900, Loss: 0.4699\n",
      "Iteration 4000, Loss: 0.4692\n",
      "Iteration 4100, Loss: 0.4684\n",
      "Iteration 4200, Loss: 0.4677\n",
      "Iteration 4300, Loss: 0.4670\n",
      "Iteration 4400, Loss: 0.4664\n",
      "Iteration 4500, Loss: 0.4657\n",
      "Iteration 4600, Loss: 0.4650\n",
      "Iteration 4700, Loss: 0.4644\n",
      "Iteration 4800, Loss: 0.4638\n",
      "Iteration 4900, Loss: 0.4632\n",
      "Iteration 0, Loss: 1.0407\n",
      "Iteration 100, Loss: 0.6411\n",
      "Iteration 200, Loss: 0.6082\n",
      "Iteration 300, Loss: 0.5906\n",
      "Iteration 400, Loss: 0.5786\n",
      "Iteration 500, Loss: 0.5697\n",
      "Iteration 600, Loss: 0.5625\n",
      "Iteration 700, Loss: 0.5567\n",
      "Iteration 800, Loss: 0.5518\n",
      "Iteration 900, Loss: 0.5476\n",
      "Iteration 1000, Loss: 0.5439\n",
      "Iteration 1100, Loss: 0.5407\n",
      "Iteration 1200, Loss: 0.5377\n",
      "Iteration 1300, Loss: 0.5350\n",
      "Iteration 1400, Loss: 0.5326\n",
      "Iteration 1500, Loss: 0.5304\n",
      "Iteration 1600, Loss: 0.5283\n",
      "Iteration 1700, Loss: 0.5264\n",
      "Iteration 1800, Loss: 0.5246\n",
      "Iteration 1900, Loss: 0.5229\n",
      "Iteration 2000, Loss: 0.5213\n",
      "Iteration 2100, Loss: 0.5198\n",
      "Iteration 2200, Loss: 0.5184\n",
      "Iteration 2300, Loss: 0.5170\n",
      "Iteration 2400, Loss: 0.5158\n",
      "Iteration 2500, Loss: 0.5146\n",
      "Iteration 2600, Loss: 0.5134\n",
      "Iteration 2700, Loss: 0.5123\n",
      "Iteration 2800, Loss: 0.5112\n",
      "Iteration 2900, Loss: 0.5102\n",
      "Iteration 3000, Loss: 0.5092\n",
      "Iteration 3100, Loss: 0.5083\n",
      "Iteration 3200, Loss: 0.5074\n",
      "Iteration 3300, Loss: 0.5065\n",
      "Iteration 3400, Loss: 0.5057\n",
      "Iteration 3500, Loss: 0.5048\n",
      "Iteration 3600, Loss: 0.5041\n",
      "Iteration 3700, Loss: 0.5033\n",
      "Iteration 3800, Loss: 0.5026\n",
      "Iteration 3900, Loss: 0.5019\n",
      "Iteration 0, Loss: 1.0686\n",
      "Iteration 100, Loss: 0.7609\n",
      "Iteration 200, Loss: 0.7045\n",
      "Iteration 300, Loss: 0.6735\n",
      "Iteration 400, Loss: 0.6554\n",
      "Iteration 500, Loss: 0.6439\n",
      "Iteration 600, Loss: 0.6362\n",
      "Iteration 700, Loss: 0.6308\n",
      "Iteration 800, Loss: 0.6268\n",
      "Iteration 900, Loss: 0.6237\n",
      "Iteration 1000, Loss: 0.6213\n",
      "Iteration 1100, Loss: 0.6193\n",
      "Iteration 1200, Loss: 0.6177\n",
      "Iteration 1300, Loss: 0.6163\n",
      "Iteration 1400, Loss: 0.6151\n",
      "Iteration 1500, Loss: 0.6141\n",
      "Iteration 1600, Loss: 0.6132\n",
      "Iteration 1700, Loss: 0.6124\n",
      "Iteration 1800, Loss: 0.6118\n",
      "Iteration 1900, Loss: 0.6112\n",
      "Iteration 2000, Loss: 0.6106\n",
      "Iteration 2100, Loss: 0.6102\n",
      "Iteration 2200, Loss: 0.6097\n",
      "Iteration 2300, Loss: 0.6094\n",
      "Iteration 2400, Loss: 0.6090\n",
      "Iteration 2500, Loss: 0.6087\n",
      "Iteration 2600, Loss: 0.6085\n",
      "Iteration 2700, Loss: 0.6082\n",
      "Iteration 2800, Loss: 0.6080\n",
      "Iteration 2900, Loss: 0.6078\n",
      "Fold scores: [np.float64(0.7558139534883721), np.float64(0.748062015503876), np.float64(0.7296511627906976)]\n",
      "Mean score: 0.7445090439276486\n"
     ]
    }
   ],
   "execution_count": 328
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "VII. Mixture of Experts\n",
    "Mixture of experts is a method where \"supervisor\" assigns weight (in meaning of importance of particular model from experts models embedded in this method). This \"supervisor\" here is called Gating Network - this is way to assign a value of trust to each model\n"
   ],
   "id": "a3fc031738d61702"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:21:43.857103Z",
     "start_time": "2025-05-22T13:21:43.769164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_proc1, X_test_proc1, y_train_proc1, y_test_proc1 = transform_in_pipeline(\n",
    "    modified_features_preprocessor, X_train, X_test, y_train, y_test)"
   ],
   "id": "f419c8610590cec7",
   "outputs": [],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:48:28.777180Z",
     "start_time": "2025-05-22T13:48:28.759378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "class MixtureOfExperts:\n",
    "    def __init__(self, model1, model2, num_classes):\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.gating_model = RandomForestClassifier()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model1.fit(X_train, y_train)\n",
    "        self.model2.fit(X_train, y_train)\n",
    "\n",
    "        self.model1.predict_class(X_train)\n",
    "        proba1 = self.model1.probabilities\n",
    "        self.model2.predict_class(X_train)\n",
    "        proba2 = self.model1.probabilities\n",
    "\n",
    "        if len(y_train.shape) > 1:\n",
    "            y_true = np.argmax(y_train, axis=1)\n",
    "        else:\n",
    "            y_true = y_train\n",
    "\n",
    "        X_gating_train = []\n",
    "        gating_labels = []\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            pred1 = np.argmax(proba1[i])\n",
    "            pred2 = np.argmax(proba2[i])\n",
    "\n",
    "            acc1 = int(pred1 == y_true[i])\n",
    "            acc2 = int(pred2 == y_true[i])\n",
    "\n",
    "            if acc1 > acc2:\n",
    "                label = 0\n",
    "            elif acc2 > acc1:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = i % 2  #random\n",
    "\n",
    "            features = np.concatenate([\n",
    "                X_train[i].flatten(),\n",
    "                proba1[i],\n",
    "                proba2[i],\n",
    "                np.abs(proba1[i] - proba2[i])\n",
    "            ])\n",
    "            X_gating_train.append(features)\n",
    "            gating_labels.append(label)\n",
    "\n",
    "        X_gating_train = np.array(X_gating_train)\n",
    "        gating_labels = np.array(gating_labels)\n",
    "\n",
    "        if len(gating_labels) == 0:\n",
    "            raise ValueError(\"Brak danych do treningu gating modelu.\")\n",
    "\n",
    "        self.gating_model.fit(X_gating_train, gating_labels)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        self.model1.predict_class(X_test)\n",
    "        proba1 = self.model1.probabilities\n",
    "        self.model2.predict_class(X_test)\n",
    "        proba2 = self.model1.probabilities\n",
    "\n",
    "        X_gating_test = []\n",
    "        for i in range(len(X_test)):\n",
    "            features = np.concatenate([\n",
    "                X_test[i].flatten(),\n",
    "                proba1[i],\n",
    "                proba2[i],\n",
    "                np.abs(proba1[i] - proba2[i])\n",
    "            ])\n",
    "            X_gating_test.append(features)\n",
    "        X_gating_test = np.array(X_gating_test)\n",
    "\n",
    "        gating_probs = self.gating_model.predict_proba(X_gating_test)\n",
    "\n",
    "        final_preds = []\n",
    "        for i in range(len(X_test)):\n",
    "            weighted_vote = gating_probs[i][0] * proba1[i] + gating_probs[i][1] * proba2[i]\n",
    "            final_preds.append(np.argmax(weighted_vote))\n",
    "        return np.array(final_preds)\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        preds = self.predict(X_test)\n",
    "        if len(y_test.shape) > 1:\n",
    "            y_true = np.argmax(y_test, axis=1)\n",
    "        else:\n",
    "            y_true = y_test\n",
    "        return np.mean(preds == y_true)\n"
   ],
   "id": "e8bee00d1c9e5980",
   "outputs": [],
   "execution_count": 228
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, we train our expert models independently. Then, for each record in the training data, we determine which expert made the correct prediction. Based on this, we build a gating matrix, which learns to identify which expert is more likely to be correct given a specific input sample X\n",
    "\n",
    "During prediction, for each test record, we retrieve the class probability distributions from both experts. We then weight these probabilities using the gating model's output (i.e., its trust in each expert). Finally, we combine the weighted predictions and select the class with the highest final score as the predicted class."
   ],
   "id": "a795976e172a5c20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T13:49:57.153048Z",
     "start_time": "2025-05-22T13:48:29.033019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m1 = BaseLogisticRegression(n_iters=7000, lr=0.001, batch_size=64)\n",
    "m2 = BaseLogisticRegressionRegCombined(n_iters=4000, lr=0.001, batch_size=128, l=0.00101, alpha=0.70)\n",
    "moe = MixtureOfExperts(m1, m2,3)\n",
    "moe.fit(X_train_proc1, y_train_proc1)\n",
    "acc = moe.score(X_test_proc1, y_test_proc1)\n",
    "print(f\"Mixture of Experts accuracy: {acc:.4f}\")\n"
   ],
   "id": "16e5e8af6eb97788",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixture of Experts accuracy: 0.7673\n"
     ]
    }
   ],
   "execution_count": 229
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5255b0b8fdc21b1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T12:59:52.376018Z",
     "start_time": "2025-05-22T12:59:52.373295Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6234fb128a99a49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T09:08:43.987059Z",
     "start_time": "2025-05-25T09:08:43.936578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "X = df.drop([\"Target\"],axis=1)\n",
    "df[\"Target\"] = LabelEncoder().fit_transform(df[\"Target\"])\n",
    "y = df[\"Target\"]"
   ],
   "id": "3e36a699d349a647",
   "outputs": [],
   "execution_count": 330
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T09:16:14.075895Z",
     "start_time": "2025-05-25T09:12:19.903746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features_modified),\n",
    "    ('cat', categorical_pipeline, categorical_features_modified)\n",
    "])\n",
    "\n",
    "\n",
    "random_forest_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Siatka hiperparametrów\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(random_forest_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Wyniki\n",
    "print( grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ],
   "id": "43ca15f9c52a667a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__max_depth': 20, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'model__n_estimators': 200}\n",
      "0.7742310550667658\n"
     ]
    }
   ],
   "execution_count": 336
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T09:11:46.483004Z",
     "start_time": "2025-05-25T09:11:46.455374Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "26dd7bde774e7efe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.774513940498166)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 334
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6914b1869f4148c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
