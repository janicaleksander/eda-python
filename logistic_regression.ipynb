{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-08T18:36:37.040521Z",
     "start_time": "2025-05-08T18:36:37.017012Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.compose import ColumnTransformer"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T18:36:37.154167Z",
     "start_time": "2025-05-08T18:36:37.116612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading data\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df.head()"
   ],
   "id": "6cbf8e1dd016ab90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Marital status  Application mode  Application order  Course  \\\n",
       "0               1                 8                  5       2   \n",
       "1               1                 6                  1      11   \n",
       "2               1                 1                  5       5   \n",
       "3               1                 8                  2      15   \n",
       "4               2                12                  1       3   \n",
       "\n",
       "   Daytime/evening attendance  Previous qualification  Nationality  \\\n",
       "0                           1                       1            1   \n",
       "1                           1                       1            1   \n",
       "2                           1                       1            1   \n",
       "3                           1                       1            1   \n",
       "4                           0                       1            1   \n",
       "\n",
       "   Mother's qualification  Father's qualification  Mother's occupation  ...  \\\n",
       "0                      13                      10                    6  ...   \n",
       "1                       1                       3                    4  ...   \n",
       "2                      22                      27                   10  ...   \n",
       "3                      23                      27                    6  ...   \n",
       "4                      22                      28                   10  ...   \n",
       "\n",
       "   Curricular units 2nd sem (credited)  Curricular units 2nd sem (enrolled)  \\\n",
       "0                                    0                                    0   \n",
       "1                                    0                                    6   \n",
       "2                                    0                                    6   \n",
       "3                                    0                                    6   \n",
       "4                                    0                                    6   \n",
       "\n",
       "   Curricular units 2nd sem (evaluations)  \\\n",
       "0                                       0   \n",
       "1                                       6   \n",
       "2                                       0   \n",
       "3                                      10   \n",
       "4                                       6   \n",
       "\n",
       "   Curricular units 2nd sem (approved)  Curricular units 2nd sem (grade)  \\\n",
       "0                                    0                          0.000000   \n",
       "1                                    6                         13.666667   \n",
       "2                                    0                          0.000000   \n",
       "3                                    5                         12.400000   \n",
       "4                                    6                         13.000000   \n",
       "\n",
       "   Curricular units 2nd sem (without evaluations)  Unemployment rate  \\\n",
       "0                                               0               10.8   \n",
       "1                                               0               13.9   \n",
       "2                                               0               10.8   \n",
       "3                                               0                9.4   \n",
       "4                                               0               13.9   \n",
       "\n",
       "   Inflation rate   GDP    Target  \n",
       "0             1.4  1.74   Dropout  \n",
       "1            -0.3  0.79  Graduate  \n",
       "2             1.4  1.74   Dropout  \n",
       "3            -0.8 -3.12  Graduate  \n",
       "4            -0.3  0.79  Graduate  \n",
       "\n",
       "[5 rows x 35 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Marital status</th>\n",
       "      <th>Application mode</th>\n",
       "      <th>Application order</th>\n",
       "      <th>Course</th>\n",
       "      <th>Daytime/evening attendance</th>\n",
       "      <th>Previous qualification</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Mother's qualification</th>\n",
       "      <th>Father's qualification</th>\n",
       "      <th>Mother's occupation</th>\n",
       "      <th>...</th>\n",
       "      <th>Curricular units 2nd sem (credited)</th>\n",
       "      <th>Curricular units 2nd sem (enrolled)</th>\n",
       "      <th>Curricular units 2nd sem (evaluations)</th>\n",
       "      <th>Curricular units 2nd sem (approved)</th>\n",
       "      <th>Curricular units 2nd sem (grade)</th>\n",
       "      <th>Curricular units 2nd sem (without evaluations)</th>\n",
       "      <th>Unemployment rate</th>\n",
       "      <th>Inflation rate</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T18:36:37.274345Z",
     "start_time": "2025-05-08T18:36:37.269766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numerical_features = [\n",
    "    \"Application order\",\"Age at enrollment\", \"Curricular units 1st sem (credited)\", \"Curricular units 1st sem (enrolled)\",\n",
    "    \"Curricular units 1st sem (evaluations)\",\"Curricular units 1st sem (approved)\",\"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 1st sem (without evaluations)\",\"Curricular units 2nd sem (credited)\",\"Curricular units 2nd sem (enrolled)\",\n",
    "    \"Curricular units 2nd sem (evaluations)\",\"Curricular units 2nd sem (approved)\",\n",
    "    \"Curricular units 2nd sem (without evaluations)\",\"Unemployment rate\",\"Inflation rate\",\"GDP\",\"Curricular units 2nd sem (grade)\"\n",
    "]\n",
    "categorical_features = [\n",
    "    \"Marital status\",\"Application mode\",\"Course\",\"Daytime/evening attendance\",\"Previous qualification\",\"Nationality\",\n",
    "    \"Mother's qualification\",\"Father's qualification\",\"Mother's occupation\",\"Father's occupation\",\"Displaced\",\n",
    "    \"Educational special needs\",\"Debtor\",\"Tuition fees up to date\",\"Gender\",\"Scholarship holder\",\"International\"\n",
    "]\n",
    "\n",
    "target = \"Target\"\n"
   ],
   "id": "39682b9b8172c5c0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I. Math introduction\n",
    "\n",
    "Because in our logistic regression we have 3 classes we have to implement one vs many solution, so our $W$ weight matrix has three rows where each row will have a weights where each row corresponds to one class and learns how to distinguish that class from all others.\n",
    "\n",
    "\n",
    "First we have to do linear combination of input variables with their weights\n",
    "$$ scores = X * W^T + b^T$$\n",
    "\n",
    "But scores is a matrix with values from $-\\inf$ to  $+\\inf$ so we have to changes this to range from 0 to 1 because in logistic regression we are predicting a probability. This is called softmax function\n",
    "\n",
    "$$ \\hat{p_k} = \\frac{\\exp{(scores_k)}}{\\sum_{j=1}^{K}\\exp{(scores)}}$$\n",
    "\n",
    "\n",
    "Because we are using gradient descent to get our model better, we will be minimalizing cost function. In our classification problem we are going to use Cross-Entropy cost function\n",
    "\n",
    "$$J(W,b)  = -\\frac{1}{m}\\sum_{k=1}^{K}(y_k*\\log(p_k))$$\n",
    "\n",
    "After one iteration we have to update our $W$ and $b$:\n",
    "\n",
    "\n",
    "$$ \\nabla_w{J(W,b)} = \\frac{1}{m}\\sum_{k=1}^{n}x(\\hat{p_k} - y_k)$$\n",
    "\n",
    "$$ \\nabla_b{J(W,b)} = \\frac{1}{m}\\sum_{k=1}^{n}(\\hat{p_k} - y_k)$$\n",
    "\n",
    "So after one iteration we can update $W$ and $b$:\n",
    "\n",
    "$$W_x = W - lr * dW$$\n",
    "\n",
    "$$b_x = b - lr * db$$\n",
    "\n",
    "\n",
    "We can repeat this process until current_iter < max_iteration"
   ],
   "id": "70e45c40d9f835ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After iterations,  we have to part our set to $n$ batches. Batches are a subsets of current training set, and after one iteration te set on which our model making calculations is changing:\n",
    "$$\n",
    "X_{\\text{new}} = \\text{Batches}[\\text{prev} + n_{\\text{batches}}]\n",
    "$$\n",
    "\n",
    "\n",
    "$$ prev = prev + n_{batches} $$"
   ],
   "id": "72014ecb6fa15e3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T18:36:37.360568Z",
     "start_time": "2025-05-08T18:36:37.351844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.001, n_iters=10000, batch_size=64):\n",
    "        self.lr = lr\n",
    "        self.batch_idx = 0\n",
    "        self.n_iters = n_iters\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors = []\n",
    "        self.n_classes = None\n",
    "\n",
    "    def error_function(self, y, predicted, m_samples):\n",
    "        return -np.sum(y * np.log(predicted + 1e-15)) / m_samples\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m_samples, n_features = X.shape\n",
    "        self.n_classes = y.shape[1]\n",
    "        self.weights = np.zeros((n_features, self.n_classes))\n",
    "        self.bias = np.zeros((1, self.n_classes))\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            indices = np.random.permutation(m_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for start_idx in range(0, m_samples, self.batch_size):\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                predict = self.predict(X_batch)\n",
    "                dw = (1 / X_batch.shape[0]) * np.dot(X_batch.T, (predict - y_batch))\n",
    "                db = (1 / X_batch.shape[0]) * np.sum(predict - y_batch, axis=0, keepdims=True)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "            full_predict = self.predict(X)\n",
    "            loss = self.error_function(y, full_predict, m_samples)\n",
    "            self.errors.append(loss)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "    def softmax(self,z):\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=1,keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.softmax(z)\n",
    "\n",
    "    def predict_class(self, X_test):\n",
    "        probabilities = self.predict(X_test)\n",
    "        return np.argmax(probabilities, axis=1).reshape(-1, 1)\n",
    "\n",
    "    def score(self, predicted, y,to_show):\n",
    "        y_true = np.argmax(y, axis=1)\n",
    "        correct = (predicted.flatten() == y_true)\n",
    "        accuracy = np.mean(correct)\n",
    "        print(\"Correct predictions:\", correct)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        if to_show:\n",
    "            plt.plot(np.arange(len(self.errors)), self.errors)\n",
    "            plt.show()\n"
   ],
   "id": "81b78e4ad325d3c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T18:37:03.524779Z",
     "start_time": "2025-05-08T18:36:37.555285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# HERE Y has to be in onehotencoder [0 0 ...  1]\n",
    "X = df.drop([\"Target\"], axis=1)\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# Dropout -  0\n",
    "# Graduate - 1\n",
    "# Enrolled - 2\n",
    "\n",
    "X_train,X_temp,y_train,y_temp = train_test_split(X,y,train_size=0.4,random_state=42)\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_temp,y_temp,random_state=42,train_size=0.5)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore',sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "y_train_processed = pd.get_dummies(y_train,columns=[\"Target\"]).astype(int).to_numpy()\n",
    "y_test_processed = pd.get_dummies(y_test,columns=[\"Target\"]).astype(int).to_numpy()\n",
    "y_val_processed = pd.get_dummies(y_val,columns=[\"Target\"]).astype(int).to_numpy()\n",
    "\n",
    "\n",
    "#main part\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_processed,y_train_processed)\n"
   ],
   "id": "256a6a376c48ad7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.0650\n",
      "Iteration 100, Loss: 0.6819\n",
      "Iteration 200, Loss: 0.6317\n",
      "Iteration 300, Loss: 0.6064\n",
      "Iteration 400, Loss: 0.5901\n",
      "Iteration 500, Loss: 0.5784\n",
      "Iteration 600, Loss: 0.5693\n",
      "Iteration 700, Loss: 0.5620\n",
      "Iteration 800, Loss: 0.5559\n",
      "Iteration 900, Loss: 0.5506\n",
      "Iteration 1000, Loss: 0.5460\n",
      "Iteration 1100, Loss: 0.5419\n",
      "Iteration 1200, Loss: 0.5383\n",
      "Iteration 1300, Loss: 0.5350\n",
      "Iteration 1400, Loss: 0.5320\n",
      "Iteration 1500, Loss: 0.5293\n",
      "Iteration 1600, Loss: 0.5268\n",
      "Iteration 1700, Loss: 0.5245\n",
      "Iteration 1800, Loss: 0.5223\n",
      "Iteration 1900, Loss: 0.5203\n",
      "Iteration 2000, Loss: 0.5184\n",
      "Iteration 2100, Loss: 0.5167\n",
      "Iteration 2200, Loss: 0.5150\n",
      "Iteration 2300, Loss: 0.5135\n",
      "Iteration 2400, Loss: 0.5120\n",
      "Iteration 2500, Loss: 0.5106\n",
      "Iteration 2600, Loss: 0.5093\n",
      "Iteration 2700, Loss: 0.5080\n",
      "Iteration 2800, Loss: 0.5068\n",
      "Iteration 2900, Loss: 0.5056\n",
      "Iteration 3000, Loss: 0.5046\n",
      "Iteration 3100, Loss: 0.5035\n",
      "Iteration 3200, Loss: 0.5025\n",
      "Iteration 3300, Loss: 0.5015\n",
      "Iteration 3400, Loss: 0.5006\n",
      "Iteration 3500, Loss: 0.4997\n",
      "Iteration 3600, Loss: 0.4989\n",
      "Iteration 3700, Loss: 0.4980\n",
      "Iteration 3800, Loss: 0.4972\n",
      "Iteration 3900, Loss: 0.4965\n",
      "Iteration 4000, Loss: 0.4957\n",
      "Iteration 4100, Loss: 0.4950\n",
      "Iteration 4200, Loss: 0.4943\n",
      "Iteration 4300, Loss: 0.4936\n",
      "Iteration 4400, Loss: 0.4930\n",
      "Iteration 4500, Loss: 0.4924\n",
      "Iteration 4600, Loss: 0.4917\n",
      "Iteration 4700, Loss: 0.4911\n",
      "Iteration 4800, Loss: 0.4906\n",
      "Iteration 4900, Loss: 0.4900\n",
      "Iteration 5000, Loss: 0.4895\n",
      "Iteration 5100, Loss: 0.4889\n",
      "Iteration 5200, Loss: 0.4884\n",
      "Iteration 5300, Loss: 0.4879\n",
      "Iteration 5400, Loss: 0.4874\n",
      "Iteration 5500, Loss: 0.4869\n",
      "Iteration 5600, Loss: 0.4864\n",
      "Iteration 5700, Loss: 0.4860\n",
      "Iteration 5800, Loss: 0.4855\n",
      "Iteration 5900, Loss: 0.4851\n",
      "Iteration 6000, Loss: 0.4846\n",
      "Iteration 6100, Loss: 0.4842\n",
      "Iteration 6200, Loss: 0.4838\n",
      "Iteration 6300, Loss: 0.4834\n",
      "Iteration 6400, Loss: 0.4830\n",
      "Iteration 6500, Loss: 0.4826\n",
      "Iteration 6600, Loss: 0.4822\n",
      "Iteration 6700, Loss: 0.4819\n",
      "Iteration 6800, Loss: 0.4815\n",
      "Iteration 6900, Loss: 0.4811\n",
      "Iteration 7000, Loss: 0.4808\n",
      "Iteration 7100, Loss: 0.4804\n",
      "Iteration 7200, Loss: 0.4801\n",
      "Iteration 7300, Loss: 0.4798\n",
      "Iteration 7400, Loss: 0.4794\n",
      "Iteration 7500, Loss: 0.4791\n",
      "Iteration 7600, Loss: 0.4788\n",
      "Iteration 7700, Loss: 0.4785\n",
      "Iteration 7800, Loss: 0.4782\n",
      "Iteration 7900, Loss: 0.4779\n",
      "Iteration 8000, Loss: 0.4776\n",
      "Iteration 8100, Loss: 0.4773\n",
      "Iteration 8200, Loss: 0.4770\n",
      "Iteration 8300, Loss: 0.4767\n",
      "Iteration 8400, Loss: 0.4764\n",
      "Iteration 8500, Loss: 0.4762\n",
      "Iteration 8600, Loss: 0.4759\n",
      "Iteration 8700, Loss: 0.4756\n",
      "Iteration 8800, Loss: 0.4754\n",
      "Iteration 8900, Loss: 0.4751\n",
      "Iteration 9000, Loss: 0.4748\n",
      "Iteration 9100, Loss: 0.4746\n",
      "Iteration 9200, Loss: 0.4743\n",
      "Iteration 9300, Loss: 0.4741\n",
      "Iteration 9400, Loss: 0.4739\n",
      "Iteration 9500, Loss: 0.4736\n",
      "Iteration 9600, Loss: 0.4734\n",
      "Iteration 9700, Loss: 0.4732\n",
      "Iteration 9800, Loss: 0.4729\n",
      "Iteration 9900, Loss: 0.4727\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T18:37:03.660770Z",
     "start_time": "2025-05-08T18:37:03.572713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TEST set\n",
    "predicted = clf.predict_class(X_test_processed)\n",
    "clf.score(predicted,y_test_processed,True)\n",
    "\n",
    "#VALIDATION set\n",
    "predicted = clf.predict_class(X_val_processed)\n",
    "clf.score(predicted,y_val_processed,False)\n",
    "\n",
    "#TRAIN set\n",
    "predicted = clf.predict_class(X_train_processed)\n",
    "clf.score(predicted,y_train_processed,False)"
   ],
   "id": "e941196b726cc1af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: [ True  True  True ... False  True  True]\n",
      "Accuracy: 0.7733433734939759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: [False False  True ...  True  True False]\n",
      "Accuracy: 0.7565938206480783\n",
      "Correct predictions: [ True  True  True ...  True  True  True]\n",
      "Accuracy: 0.819672131147541\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T18:37:03.887080Z",
     "start_time": "2025-05-08T18:37:03.759544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lm = linear_model.LogisticRegression(multi_class='multinomial',max_iter=10000)\n",
    "lm.fit(X_train_processed, y_train)\n",
    "s1 = lm.score(X_test_processed, y_test)\n",
    "s2 = lm.score(X_val_processed, y_val)\n",
    "s3 = lm.score(X_train_processed, y_train)\n",
    "\n",
    "s1, s2, s3"
   ],
   "id": "f1912cb5471707c4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aleksander\\anaconda3\\envs\\msid_lista_1\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7695783132530121, 0.7596081386586285, 0.8224985867721877)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
