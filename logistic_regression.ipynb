{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.compose import ColumnTransformer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loading data\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df.head()"
   ],
   "id": "6cbf8e1dd016ab90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numerical_features = [\n",
    "    \"Application order\",\"Age at enrollment\", \"Curricular units 1st sem (credited)\", \"Curricular units 1st sem (enrolled)\",\n",
    "    \"Curricular units 1st sem (evaluations)\",\"Curricular units 1st sem (approved)\",\"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 1st sem (without evaluations)\",\"Curricular units 2nd sem (credited)\",\"Curricular units 2nd sem (enrolled)\",\n",
    "    \"Curricular units 2nd sem (evaluations)\",\"Curricular units 2nd sem (approved)\",\n",
    "    \"Curricular units 2nd sem (without evaluations)\",\"Unemployment rate\",\"Inflation rate\",\"GDP\",\"Curricular units 2nd sem (grade)\"\n",
    "]\n",
    "categorical_features = [\n",
    "    \"Marital status\",\"Application mode\",\"Course\",\"Daytime/evening attendance\",\"Previous qualification\",\"Nationality\",\n",
    "    \"Mother's qualification\",\"Father's qualification\",\"Mother's occupation\",\"Father's occupation\",\"Displaced\",\n",
    "    \"Educational special needs\",\"Debtor\",\"Tuition fees up to date\",\"Gender\",\"Scholarship holder\",\"International\"\n",
    "]\n",
    "\n",
    "target = \"Target\"\n"
   ],
   "id": "39682b9b8172c5c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I. Math introduction\n",
    "\n",
    "Because in our logistic regression we have 3 classes we have to implement one vs many solution, so our $W$ weight matrix has three rows where each row will have a weights where each row corresponds to one class and learns how to distinguish that class from all others.\n",
    "\n",
    "\n",
    "First we have to do linear combination of input variables with their weights\n",
    "$$ scores = X * W^T + b^T$$\n",
    "\n",
    "But scores is a matrix with values from $-\\inf$ to  $+\\inf$ so we have to changes this to range from 0 to 1 because in logistic regression we are predicting a probability. This is called softmax function\n",
    "\n",
    "$$ \\hat{p_k} = \\frac{\\exp{(scores_k)}}{\\sum_{j=1}^{K}\\exp{(scores)}}$$\n",
    "\n",
    "\n",
    "Because we are using gradient descent to get our model better, we will be minimalizing cost function. In our classification problem we are going to use Cross-Entropy cost function\n",
    "\n",
    "$$J(W,b)  = -\\frac{1}{m}\\sum_{k=1}^{K}(y_k*\\log(p_k))$$\n",
    "\n",
    "After one iteration we have to update our $W$ and $b$:\n",
    "\n",
    "\n",
    "$$ \\nabla_w{J(W,b)} = \\frac{1}{m}\\sum_{k=1}^{n}x(\\hat{p_k} - y_k)$$\n",
    "\n",
    "$$ \\nabla_b{J(W,b)} = \\frac{1}{m}\\sum_{k=1}^{n}(\\hat{p_k} - y_k)$$\n",
    "\n",
    "So after one iteration we can update $W$ and $b$:\n",
    "\n",
    "$$W_x = W - lr * dW$$\n",
    "\n",
    "$$b_x = b - lr * db$$\n",
    "\n",
    "\n",
    "We can repeat this process until current_iter < max_iteration"
   ],
   "id": "70e45c40d9f835ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After iterations,  we have to part our set to $n$ batches. Batches are a subsets of current training set, and after one iteration te set on which our model making calculations is changing:\n",
    "$$\n",
    "X_{\\text{new}} = \\text{Batches}[\\text{prev} + n_{\\text{batches}}]\n",
    "$$\n",
    "\n",
    "\n",
    "$$ prev = prev + n_{batches} $$"
   ],
   "id": "72014ecb6fa15e3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.001, n_iters=10000, batch_size=64):\n",
    "        self.lr = lr\n",
    "        self.batch_idx = 0\n",
    "        self.n_iters = n_iters\n",
    "        self.batch_size = batch_size\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors = []\n",
    "        self.n_classes = None\n",
    "\n",
    "    def error_function(self, y, predicted, m_samples):\n",
    "        return -np.sum(y * np.log(predicted + 1e-15)) / m_samples\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m_samples, n_features = X.shape\n",
    "        self.n_classes = y.shape[1]\n",
    "        self.weights = np.zeros((n_features, self.n_classes))\n",
    "        self.bias = np.zeros((1, self.n_classes))\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            indices = np.random.permutation(m_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for start_idx in range(0, m_samples, self.batch_size):\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "\n",
    "                predict = self.predict(X_batch)\n",
    "                dw = (1 / X_batch.shape[0]) * np.dot(X_batch.T, (predict - y_batch))\n",
    "                db = (1 / X_batch.shape[0]) * np.sum(predict - y_batch, axis=0, keepdims=True)\n",
    "\n",
    "                self.weights -= self.lr * dw\n",
    "                self.bias -= self.lr * db\n",
    "\n",
    "            full_predict = self.predict(X)\n",
    "            loss = self.error_function(y, full_predict, m_samples)\n",
    "            self.errors.append(loss)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "    def softmax(self,z):\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=1,keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.softmax(z)\n",
    "\n",
    "    def predict_class(self, X_test):\n",
    "        probabilities = self.predict(X_test)\n",
    "        return np.argmax(probabilities, axis=1).reshape(-1, 1)\n",
    "\n",
    "    def score(self, predicted, y,to_show):\n",
    "        y_true = np.argmax(y, axis=1)\n",
    "        correct = (predicted.flatten() == y_true)\n",
    "        accuracy = np.mean(correct)\n",
    "        print(\"Correct predictions:\", correct)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        if to_show:\n",
    "            plt.plot(np.arange(len(self.errors)), self.errors)\n",
    "            plt.show()\n",
    "\n"
   ],
   "id": "81b78e4ad325d3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# HERE Y has to be in onehotencoder [0 0 ...  1]\n",
    "X = df.drop([\"Target\"], axis=1)\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# Dropout -  0\n",
    "# Graduate - 1\n",
    "# Enrolled - 2\n",
    "\n",
    "X_train,X_temp,y_train,y_temp = train_test_split(X,y,train_size=0.4,random_state=42)\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_temp,y_temp,random_state=42,train_size=0.5)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore',sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "y_train_processed = pd.get_dummies(y_train,columns=[\"Target\"]).astype(int).to_numpy()\n",
    "y_test_processed = pd.get_dummies(y_test,columns=[\"Target\"]).astype(int).to_numpy()\n",
    "y_val_processed = pd.get_dummies(y_val,columns=[\"Target\"]).astype(int).to_numpy()\n",
    "\n",
    "\n",
    "#main part\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_processed,y_train_processed)\n"
   ],
   "id": "256a6a376c48ad7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TEST set\n",
    "predicted = clf.predict_class(X_test_processed)\n",
    "clf.score(predicted,y_test_processed,True)\n",
    "\n",
    "#VALIDATION set\n",
    "predicted = clf.predict_class(X_val_processed)\n",
    "clf.score(predicted,y_val_processed,False)\n",
    "\n",
    "#TRAIN set\n",
    "predicted = clf.predict_class(X_train_processed)\n",
    "clf.score(predicted,y_train_processed,False)"
   ],
   "id": "e941196b726cc1af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lm = linear_model.LogisticRegression(multi_class='multinomial',max_iter=10000)\n",
    "lm.fit(X_train_processed, y_train)\n",
    "s1 = lm.score(X_test_processed, y_test)\n",
    "s2 = lm.score(X_val_processed, y_val)\n",
    "s3 = lm.score(X_train_processed, y_train)\n",
    "\n",
    "s1, s2, s3"
   ],
   "id": "f1912cb5471707c4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
